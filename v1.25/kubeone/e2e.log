I0224 11:01:28.557850      21 e2e.go:116] Starting e2e run "3953cf66-917a-4eb2-b776-6d0d4ab06829" on Ginkgo node 1
Feb 24 11:01:28.573: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677236488 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Feb 24 11:01:28.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:01:28.701: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0224 11:01:28.702797      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0224 11:01:28.702797      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Feb 24 11:01:28.722: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 24 11:01:28.773: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 24 11:01:28.773: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Feb 24 11:01:28.773: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 24 11:01:28.781: INFO: e2e test version: v1.25.6
Feb 24 11:01:28.782: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Feb 24 11:01:28.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:01:28.788: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.089 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 24 11:01:28.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:01:28.701: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0224 11:01:28.702797      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Feb 24 11:01:28.722: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 24 11:01:28.773: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb 24 11:01:28.773: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
    Feb 24 11:01:28.773: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
    Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
    Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Feb 24 11:01:28.781: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Feb 24 11:01:28.781: INFO: e2e test version: v1.25.6
    Feb 24 11:01:28.782: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 24 11:01:28.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:01:28.788: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:01:28.813
Feb 24 11:01:28.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:01:28.814
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:28.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:28.846
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-91feb97a-abec-4d42-8ada-22642ebf742e 02/24/23 11:01:28.86
STEP: Creating secret with name s-test-opt-upd-bd8c9f46-07b7-4e1f-ac4e-75173fc06a2c 02/24/23 11:01:28.878
STEP: Creating the pod 02/24/23 11:01:28.885
Feb 24 11:01:28.897: INFO: Waiting up to 5m0s for pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634" in namespace "secrets-2152" to be "running and ready"
Feb 24 11:01:28.904: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Pending", Reason="", readiness=false. Elapsed: 7.248792ms
Feb 24 11:01:28.904: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:30.911: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013672913s
Feb 24 11:01:30.911: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:33.004: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107163479s
Feb 24 11:01:33.004: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:01:34.909: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Running", Reason="", readiness=true. Elapsed: 6.011699534s
Feb 24 11:01:34.909: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Running (Ready = true)
Feb 24 11:01:34.909: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-91feb97a-abec-4d42-8ada-22642ebf742e 02/24/23 11:01:34.946
STEP: Updating secret s-test-opt-upd-bd8c9f46-07b7-4e1f-ac4e-75173fc06a2c 02/24/23 11:01:34.956
STEP: Creating secret with name s-test-opt-create-d8247214-7edf-4949-bf63-ceb825aa9e56 02/24/23 11:01:34.962
STEP: waiting to observe update in volume 02/24/23 11:01:34.968
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:01:37.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2152" for this suite. 02/24/23 11:01:37.029
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":1,"skipped":11,"failed":0}
------------------------------
• [SLOW TEST] [8.227 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:01:28.813
    Feb 24 11:01:28.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:01:28.814
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:28.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:28.846
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-91feb97a-abec-4d42-8ada-22642ebf742e 02/24/23 11:01:28.86
    STEP: Creating secret with name s-test-opt-upd-bd8c9f46-07b7-4e1f-ac4e-75173fc06a2c 02/24/23 11:01:28.878
    STEP: Creating the pod 02/24/23 11:01:28.885
    Feb 24 11:01:28.897: INFO: Waiting up to 5m0s for pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634" in namespace "secrets-2152" to be "running and ready"
    Feb 24 11:01:28.904: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Pending", Reason="", readiness=false. Elapsed: 7.248792ms
    Feb 24 11:01:28.904: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:30.911: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013672913s
    Feb 24 11:01:30.911: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:33.004: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107163479s
    Feb 24 11:01:33.004: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:01:34.909: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634": Phase="Running", Reason="", readiness=true. Elapsed: 6.011699534s
    Feb 24 11:01:34.909: INFO: The phase of Pod pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634 is Running (Ready = true)
    Feb 24 11:01:34.909: INFO: Pod "pod-secrets-a109ed71-d4b5-4b40-8ecd-c9c1d6ed2634" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-91feb97a-abec-4d42-8ada-22642ebf742e 02/24/23 11:01:34.946
    STEP: Updating secret s-test-opt-upd-bd8c9f46-07b7-4e1f-ac4e-75173fc06a2c 02/24/23 11:01:34.956
    STEP: Creating secret with name s-test-opt-create-d8247214-7edf-4949-bf63-ceb825aa9e56 02/24/23 11:01:34.962
    STEP: waiting to observe update in volume 02/24/23 11:01:34.968
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:01:37.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2152" for this suite. 02/24/23 11:01:37.029
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:01:37.04
Feb 24 11:01:37.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:01:37.041
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:37.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:37.08
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-9636 02/24/23 11:01:37.084
STEP: creating service affinity-clusterip in namespace services-9636 02/24/23 11:01:37.084
STEP: creating replication controller affinity-clusterip in namespace services-9636 02/24/23 11:01:37.11
I0224 11:01:37.132963      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9636, replica count: 3
I0224 11:01:40.185132      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 11:01:43.185835      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:01:43.195: INFO: Creating new exec pod
Feb 24 11:01:43.204: INFO: Waiting up to 5m0s for pod "execpod-affinityhfdfz" in namespace "services-9636" to be "running"
Feb 24 11:01:43.210: INFO: Pod "execpod-affinityhfdfz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281044ms
Feb 24 11:01:45.217: INFO: Pod "execpod-affinityhfdfz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012696752s
Feb 24 11:01:45.217: INFO: Pod "execpod-affinityhfdfz" satisfied condition "running"
Feb 24 11:01:46.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-9636 exec execpod-affinityhfdfz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 24 11:01:46.675: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 24 11:01:46.675: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:01:46.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-9636 exec execpod-affinityhfdfz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.15.107 80'
Feb 24 11:01:46.946: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.15.107 80\nConnection to 10.96.15.107 80 port [tcp/http] succeeded!\n"
Feb 24 11:01:46.946: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:01:46.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-9636 exec execpod-affinityhfdfz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.15.107:80/ ; done'
Feb 24 11:01:47.292: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n"
Feb 24 11:01:47.292: INFO: stdout: "\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv"
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
Feb 24 11:01:47.292: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9636, will wait for the garbage collector to delete the pods 02/24/23 11:01:47.311
Feb 24 11:01:47.380: INFO: Deleting ReplicationController affinity-clusterip took: 8.618206ms
Feb 24 11:01:47.480: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.217914ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:01:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9636" for this suite. 02/24/23 11:01:51.239
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":2,"skipped":11,"failed":0}
------------------------------
• [SLOW TEST] [14.208 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:01:37.04
    Feb 24 11:01:37.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:01:37.041
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:37.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:37.08
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-9636 02/24/23 11:01:37.084
    STEP: creating service affinity-clusterip in namespace services-9636 02/24/23 11:01:37.084
    STEP: creating replication controller affinity-clusterip in namespace services-9636 02/24/23 11:01:37.11
    I0224 11:01:37.132963      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9636, replica count: 3
    I0224 11:01:40.185132      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0224 11:01:43.185835      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:01:43.195: INFO: Creating new exec pod
    Feb 24 11:01:43.204: INFO: Waiting up to 5m0s for pod "execpod-affinityhfdfz" in namespace "services-9636" to be "running"
    Feb 24 11:01:43.210: INFO: Pod "execpod-affinityhfdfz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281044ms
    Feb 24 11:01:45.217: INFO: Pod "execpod-affinityhfdfz": Phase="Running", Reason="", readiness=true. Elapsed: 2.012696752s
    Feb 24 11:01:45.217: INFO: Pod "execpod-affinityhfdfz" satisfied condition "running"
    Feb 24 11:01:46.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-9636 exec execpod-affinityhfdfz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Feb 24 11:01:46.675: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 24 11:01:46.675: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:01:46.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-9636 exec execpod-affinityhfdfz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.15.107 80'
    Feb 24 11:01:46.946: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.15.107 80\nConnection to 10.96.15.107 80 port [tcp/http] succeeded!\n"
    Feb 24 11:01:46.946: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:01:46.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-9636 exec execpod-affinityhfdfz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.15.107:80/ ; done'
    Feb 24 11:01:47.292: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.15.107:80/\n"
    Feb 24 11:01:47.292: INFO: stdout: "\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv\naffinity-clusterip-d79zv"
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Received response from host: affinity-clusterip-d79zv
    Feb 24 11:01:47.292: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-9636, will wait for the garbage collector to delete the pods 02/24/23 11:01:47.311
    Feb 24 11:01:47.380: INFO: Deleting ReplicationController affinity-clusterip took: 8.618206ms
    Feb 24 11:01:47.480: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.217914ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:01:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9636" for this suite. 02/24/23 11:01:51.239
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:01:51.249
Feb 24 11:01:51.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 11:01:51.25
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:51.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:51.301
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 24 11:01:51.310: INFO: Creating deployment "test-recreate-deployment"
Feb 24 11:01:51.323: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 24 11:01:51.345: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 24 11:01:53.354: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 24 11:01:53.360: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 24 11:01:53.374: INFO: Updating deployment test-recreate-deployment
Feb 24 11:01:53.376: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:01:53.545: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1542  7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e 4456 2 2023-02-24 11:01:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aaf5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:01:53 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-24 11:01:53 +0000 UTC,LastTransitionTime:2023-02-24 11:01:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 24 11:01:53.550: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-1542  27d718f0-ad17-4c2b-810e-7a2c116620bb 4453 1 2023-02-24 11:01:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e 0xc002aafaa0 0xc002aafaa1}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aafb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:01:53.550: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 24 11:01:53.550: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-1542  1a4e4ced-405d-405b-a33b-f1a31fc2db5b 4443 2 2023-02-24 11:01:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e 0xc002aaf997 0xc002aaf998}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aafa48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:01:53.555: INFO: Pod "test-recreate-deployment-9d58999df-gwkg6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-gwkg6 test-recreate-deployment-9d58999df- deployment-1542  28d81919-2d4a-4fad-bcf4-fe3bfdb7a25f 4450 0 2023-02-24 11:01:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 27d718f0-ad17-4c2b-810e-7a2c116620bb 0xc0029a9e40 0xc0029a9e41}] [] [{kube-controller-manager Update v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27d718f0-ad17-4c2b-810e-7a2c116620bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:01:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 11:01:53.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1542" for this suite. 02/24/23 11:01:53.569
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":3,"skipped":17,"failed":0}
------------------------------
• [2.330 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:01:51.249
    Feb 24 11:01:51.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 11:01:51.25
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:51.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:51.301
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 24 11:01:51.310: INFO: Creating deployment "test-recreate-deployment"
    Feb 24 11:01:51.323: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 24 11:01:51.345: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 24 11:01:53.354: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 24 11:01:53.360: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 24 11:01:53.374: INFO: Updating deployment test-recreate-deployment
    Feb 24 11:01:53.376: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:01:53.545: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1542  7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e 4456 2 2023-02-24 11:01:51 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aaf5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:01:53 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-24 11:01:53 +0000 UTC,LastTransitionTime:2023-02-24 11:01:51 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 24 11:01:53.550: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-1542  27d718f0-ad17-4c2b-810e-7a2c116620bb 4453 1 2023-02-24 11:01:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e 0xc002aafaa0 0xc002aafaa1}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aafb38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:01:53.550: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 24 11:01:53.550: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-1542  1a4e4ced-405d-405b-a33b-f1a31fc2db5b 4443 2 2023-02-24 11:01:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e 0xc002aaf997 0xc002aaf998}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a682cbc-bff6-48b8-8c08-3d8b3ecfbe4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aafa48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:01:53.555: INFO: Pod "test-recreate-deployment-9d58999df-gwkg6" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-gwkg6 test-recreate-deployment-9d58999df- deployment-1542  28d81919-2d4a-4fad-bcf4-fe3bfdb7a25f 4450 0 2023-02-24 11:01:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 27d718f0-ad17-4c2b-810e-7a2c116620bb 0xc0029a9e40 0xc0029a9e41}] [] [{kube-controller-manager Update v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27d718f0-ad17-4c2b-810e-7a2c116620bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:01:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xgf6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xgf6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:01:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:01:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 11:01:53.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1542" for this suite. 02/24/23 11:01:53.569
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:01:53.58
Feb 24 11:01:53.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:01:53.581
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:53.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:53.611
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Feb 24 11:01:53.620: INFO: Got root ca configmap in namespace "svcaccounts-493"
Feb 24 11:01:53.627: INFO: Deleted root ca configmap in namespace "svcaccounts-493"
STEP: waiting for a new root ca configmap created 02/24/23 11:01:54.127
Feb 24 11:01:54.133: INFO: Recreated root ca configmap in namespace "svcaccounts-493"
Feb 24 11:01:54.139: INFO: Updated root ca configmap in namespace "svcaccounts-493"
STEP: waiting for the root ca configmap reconciled 02/24/23 11:01:54.64
Feb 24 11:01:54.645: INFO: Reconciled root ca configmap in namespace "svcaccounts-493"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 24 11:01:54.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-493" for this suite. 02/24/23 11:01:54.652
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":4,"skipped":18,"failed":0}
------------------------------
• [1.080 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:01:53.58
    Feb 24 11:01:53.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:01:53.581
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:53.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:53.611
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Feb 24 11:01:53.620: INFO: Got root ca configmap in namespace "svcaccounts-493"
    Feb 24 11:01:53.627: INFO: Deleted root ca configmap in namespace "svcaccounts-493"
    STEP: waiting for a new root ca configmap created 02/24/23 11:01:54.127
    Feb 24 11:01:54.133: INFO: Recreated root ca configmap in namespace "svcaccounts-493"
    Feb 24 11:01:54.139: INFO: Updated root ca configmap in namespace "svcaccounts-493"
    STEP: waiting for the root ca configmap reconciled 02/24/23 11:01:54.64
    Feb 24 11:01:54.645: INFO: Reconciled root ca configmap in namespace "svcaccounts-493"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 24 11:01:54.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-493" for this suite. 02/24/23 11:01:54.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:01:54.669
Feb 24 11:01:54.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename job 02/24/23 11:01:54.67
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:54.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:54.699
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 02/24/23 11:01:54.703
STEP: Ensuring active pods == parallelism 02/24/23 11:01:54.711
STEP: Orphaning one of the Job's Pods 02/24/23 11:01:58.718
Feb 24 11:01:59.254: INFO: Successfully updated pod "adopt-release-m4q4q"
STEP: Checking that the Job readopts the Pod 02/24/23 11:01:59.255
Feb 24 11:01:59.255: INFO: Waiting up to 15m0s for pod "adopt-release-m4q4q" in namespace "job-9482" to be "adopted"
Feb 24 11:01:59.260: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 5.087304ms
Feb 24 11:02:01.267: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.011687212s
Feb 24 11:02:01.267: INFO: Pod "adopt-release-m4q4q" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/24/23 11:02:01.267
Feb 24 11:02:01.784: INFO: Successfully updated pod "adopt-release-m4q4q"
STEP: Checking that the Job releases the Pod 02/24/23 11:02:01.784
Feb 24 11:02:01.784: INFO: Waiting up to 15m0s for pod "adopt-release-m4q4q" in namespace "job-9482" to be "released"
Feb 24 11:02:01.792: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 7.279909ms
Feb 24 11:02:03.801: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.0162754s
Feb 24 11:02:03.801: INFO: Pod "adopt-release-m4q4q" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 24 11:02:03.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9482" for this suite. 02/24/23 11:02:03.81
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":5,"skipped":88,"failed":0}
------------------------------
• [SLOW TEST] [9.151 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:01:54.669
    Feb 24 11:01:54.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename job 02/24/23 11:01:54.67
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:01:54.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:01:54.699
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 02/24/23 11:01:54.703
    STEP: Ensuring active pods == parallelism 02/24/23 11:01:54.711
    STEP: Orphaning one of the Job's Pods 02/24/23 11:01:58.718
    Feb 24 11:01:59.254: INFO: Successfully updated pod "adopt-release-m4q4q"
    STEP: Checking that the Job readopts the Pod 02/24/23 11:01:59.255
    Feb 24 11:01:59.255: INFO: Waiting up to 15m0s for pod "adopt-release-m4q4q" in namespace "job-9482" to be "adopted"
    Feb 24 11:01:59.260: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 5.087304ms
    Feb 24 11:02:01.267: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.011687212s
    Feb 24 11:02:01.267: INFO: Pod "adopt-release-m4q4q" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/24/23 11:02:01.267
    Feb 24 11:02:01.784: INFO: Successfully updated pod "adopt-release-m4q4q"
    STEP: Checking that the Job releases the Pod 02/24/23 11:02:01.784
    Feb 24 11:02:01.784: INFO: Waiting up to 15m0s for pod "adopt-release-m4q4q" in namespace "job-9482" to be "released"
    Feb 24 11:02:01.792: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 7.279909ms
    Feb 24 11:02:03.801: INFO: Pod "adopt-release-m4q4q": Phase="Running", Reason="", readiness=true. Elapsed: 2.0162754s
    Feb 24 11:02:03.801: INFO: Pod "adopt-release-m4q4q" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 24 11:02:03.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9482" for this suite. 02/24/23 11:02:03.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:03.824
Feb 24 11:02:03.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:02:03.826
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:03.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:03.886
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 02/24/23 11:02:03.891
Feb 24 11:02:03.902: INFO: Waiting up to 5m0s for pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43" in namespace "var-expansion-3574" to be "Succeeded or Failed"
Feb 24 11:02:03.910: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Pending", Reason="", readiness=false. Elapsed: 7.622265ms
Feb 24 11:02:05.916: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013242842s
Feb 24 11:02:07.917: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01426126s
Feb 24 11:02:09.915: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012492063s
STEP: Saw pod success 02/24/23 11:02:09.915
Feb 24 11:02:09.915: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43" satisfied condition "Succeeded or Failed"
Feb 24 11:02:09.920: INFO: Trying to get logs from node ip-172-31-217-191.eu-west-3.compute.internal pod var-expansion-afb4e420-455c-431a-8b99-1e51df956e43 container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:02:09.938
Feb 24 11:02:09.957: INFO: Waiting for pod var-expansion-afb4e420-455c-431a-8b99-1e51df956e43 to disappear
Feb 24 11:02:09.962: INFO: Pod var-expansion-afb4e420-455c-431a-8b99-1e51df956e43 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 11:02:09.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3574" for this suite. 02/24/23 11:02:09.97
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":6,"skipped":102,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:03.824
    Feb 24 11:02:03.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:02:03.826
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:03.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:03.886
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 02/24/23 11:02:03.891
    Feb 24 11:02:03.902: INFO: Waiting up to 5m0s for pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43" in namespace "var-expansion-3574" to be "Succeeded or Failed"
    Feb 24 11:02:03.910: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Pending", Reason="", readiness=false. Elapsed: 7.622265ms
    Feb 24 11:02:05.916: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013242842s
    Feb 24 11:02:07.917: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01426126s
    Feb 24 11:02:09.915: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012492063s
    STEP: Saw pod success 02/24/23 11:02:09.915
    Feb 24 11:02:09.915: INFO: Pod "var-expansion-afb4e420-455c-431a-8b99-1e51df956e43" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:09.920: INFO: Trying to get logs from node ip-172-31-217-191.eu-west-3.compute.internal pod var-expansion-afb4e420-455c-431a-8b99-1e51df956e43 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:02:09.938
    Feb 24 11:02:09.957: INFO: Waiting for pod var-expansion-afb4e420-455c-431a-8b99-1e51df956e43 to disappear
    Feb 24 11:02:09.962: INFO: Pod var-expansion-afb4e420-455c-431a-8b99-1e51df956e43 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 11:02:09.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3574" for this suite. 02/24/23 11:02:09.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:09.983
Feb 24 11:02:09.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename proxy 02/24/23 11:02:09.984
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:10.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:10.021
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 24 11:02:10.027: INFO: Creating pod...
Feb 24 11:02:10.038: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5495" to be "running"
Feb 24 11:02:10.046: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779776ms
Feb 24 11:02:12.053: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014834824s
Feb 24 11:02:12.053: INFO: Pod "agnhost" satisfied condition "running"
Feb 24 11:02:12.054: INFO: Creating service...
Feb 24 11:02:12.073: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=DELETE
Feb 24 11:02:12.085: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 11:02:12.085: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=OPTIONS
Feb 24 11:02:12.120: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 11:02:12.121: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=PATCH
Feb 24 11:02:12.134: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 11:02:12.134: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=POST
Feb 24 11:02:12.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 11:02:12.142: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=PUT
Feb 24 11:02:12.155: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 24 11:02:12.156: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 24 11:02:12.164: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 11:02:12.165: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 24 11:02:12.174: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 11:02:12.175: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 24 11:02:12.187: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 11:02:12.187: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=POST
Feb 24 11:02:12.201: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 11:02:12.201: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=PUT
Feb 24 11:02:12.216: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 24 11:02:12.216: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=GET
Feb 24 11:02:12.224: INFO: http.Client request:GET StatusCode:301
Feb 24 11:02:12.224: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=GET
Feb 24 11:02:12.235: INFO: http.Client request:GET StatusCode:301
Feb 24 11:02:12.237: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=HEAD
Feb 24 11:02:12.246: INFO: http.Client request:HEAD StatusCode:301
Feb 24 11:02:12.247: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 24 11:02:12.257: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 24 11:02:12.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5495" for this suite. 02/24/23 11:02:12.27
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":7,"skipped":132,"failed":0}
------------------------------
• [2.299 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:09.983
    Feb 24 11:02:09.983: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename proxy 02/24/23 11:02:09.984
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:10.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:10.021
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 24 11:02:10.027: INFO: Creating pod...
    Feb 24 11:02:10.038: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5495" to be "running"
    Feb 24 11:02:10.046: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779776ms
    Feb 24 11:02:12.053: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014834824s
    Feb 24 11:02:12.053: INFO: Pod "agnhost" satisfied condition "running"
    Feb 24 11:02:12.054: INFO: Creating service...
    Feb 24 11:02:12.073: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=DELETE
    Feb 24 11:02:12.085: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 11:02:12.085: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=OPTIONS
    Feb 24 11:02:12.120: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 11:02:12.121: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=PATCH
    Feb 24 11:02:12.134: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 11:02:12.134: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=POST
    Feb 24 11:02:12.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 11:02:12.142: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=PUT
    Feb 24 11:02:12.155: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 24 11:02:12.156: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 24 11:02:12.164: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 11:02:12.165: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 24 11:02:12.174: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 11:02:12.175: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 24 11:02:12.187: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 11:02:12.187: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=POST
    Feb 24 11:02:12.201: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 11:02:12.201: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 24 11:02:12.216: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 24 11:02:12.216: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=GET
    Feb 24 11:02:12.224: INFO: http.Client request:GET StatusCode:301
    Feb 24 11:02:12.224: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=GET
    Feb 24 11:02:12.235: INFO: http.Client request:GET StatusCode:301
    Feb 24 11:02:12.237: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/pods/agnhost/proxy?method=HEAD
    Feb 24 11:02:12.246: INFO: http.Client request:HEAD StatusCode:301
    Feb 24 11:02:12.247: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5495/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 24 11:02:12.257: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 24 11:02:12.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5495" for this suite. 02/24/23 11:02:12.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:12.304
Feb 24 11:02:12.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:02:12.306
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:12.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:12.342
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-6e398f1a-1e62-45dd-b2f1-e4853b059c72 02/24/23 11:02:12.348
STEP: Creating a pod to test consume configMaps 02/24/23 11:02:12.36
Feb 24 11:02:12.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03" in namespace "configmap-709" to be "Succeeded or Failed"
Feb 24 11:02:12.377: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.673132ms
Feb 24 11:02:14.382: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01025022s
Feb 24 11:02:16.384: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011982623s
STEP: Saw pod success 02/24/23 11:02:16.384
Feb 24 11:02:16.385: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03" satisfied condition "Succeeded or Failed"
Feb 24 11:02:16.390: INFO: Trying to get logs from node ip-172-31-217-191.eu-west-3.compute.internal pod pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:02:16.403
Feb 24 11:02:16.421: INFO: Waiting for pod pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03 to disappear
Feb 24 11:02:16.431: INFO: Pod pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:02:16.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-709" for this suite. 02/24/23 11:02:16.447
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":8,"skipped":176,"failed":0}
------------------------------
• [4.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:12.304
    Feb 24 11:02:12.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:02:12.306
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:12.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:12.342
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-6e398f1a-1e62-45dd-b2f1-e4853b059c72 02/24/23 11:02:12.348
    STEP: Creating a pod to test consume configMaps 02/24/23 11:02:12.36
    Feb 24 11:02:12.372: INFO: Waiting up to 5m0s for pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03" in namespace "configmap-709" to be "Succeeded or Failed"
    Feb 24 11:02:12.377: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.673132ms
    Feb 24 11:02:14.382: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01025022s
    Feb 24 11:02:16.384: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011982623s
    STEP: Saw pod success 02/24/23 11:02:16.384
    Feb 24 11:02:16.385: INFO: Pod "pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:16.390: INFO: Trying to get logs from node ip-172-31-217-191.eu-west-3.compute.internal pod pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:02:16.403
    Feb 24 11:02:16.421: INFO: Waiting for pod pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03 to disappear
    Feb 24 11:02:16.431: INFO: Pod pod-configmaps-86c7e9a0-6026-4f69-8c48-b1f606e10c03 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:02:16.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-709" for this suite. 02/24/23 11:02:16.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:16.473
Feb 24 11:02:16.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:02:16.475
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:16.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:16.529
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-1804 02/24/23 11:02:16.535
STEP: creating service affinity-nodeport in namespace services-1804 02/24/23 11:02:16.535
STEP: creating replication controller affinity-nodeport in namespace services-1804 02/24/23 11:02:16.569
I0224 11:02:16.597224      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1804, replica count: 3
I0224 11:02:19.652896      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:02:19.669: INFO: Creating new exec pod
Feb 24 11:02:19.678: INFO: Waiting up to 5m0s for pod "execpod-affinityf2r44" in namespace "services-1804" to be "running"
Feb 24 11:02:19.686: INFO: Pod "execpod-affinityf2r44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.439065ms
Feb 24 11:02:21.693: INFO: Pod "execpod-affinityf2r44": Phase="Running", Reason="", readiness=true. Elapsed: 2.0146681s
Feb 24 11:02:21.693: INFO: Pod "execpod-affinityf2r44" satisfied condition "running"
Feb 24 11:02:22.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 24 11:02:22.894: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 24 11:02:22.894: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:02:22.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.165.53 80'
Feb 24 11:02:23.058: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.165.53 80\nConnection to 10.102.165.53 80 port [tcp/http] succeeded!\n"
Feb 24 11:02:23.058: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:02:23.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31553'
Feb 24 11:02:23.241: INFO: stderr: "+ nc -v -t -w 2 172.31.216.47 31553\n+ echo hostName\nConnection to 172.31.216.47 31553 port [tcp/*] succeeded!\n"
Feb 24 11:02:23.241: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:02:23.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 31553'
Feb 24 11:02:23.434: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 31553\nConnection to 172.31.217.191 31553 port [tcp/*] succeeded!\n"
Feb 24 11:02:23.434: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:02:23.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:31553/ ; done'
Feb 24 11:02:23.721: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n"
Feb 24 11:02:23.721: INFO: stdout: "\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx"
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
Feb 24 11:02:23.721: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1804, will wait for the garbage collector to delete the pods 02/24/23 11:02:23.741
Feb 24 11:02:23.808: INFO: Deleting ReplicationController affinity-nodeport took: 11.986986ms
Feb 24 11:02:23.909: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.954705ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:02:26.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1804" for this suite. 02/24/23 11:02:26.452
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":9,"skipped":182,"failed":0}
------------------------------
• [SLOW TEST] [9.994 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:16.473
    Feb 24 11:02:16.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:02:16.475
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:16.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:16.529
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-1804 02/24/23 11:02:16.535
    STEP: creating service affinity-nodeport in namespace services-1804 02/24/23 11:02:16.535
    STEP: creating replication controller affinity-nodeport in namespace services-1804 02/24/23 11:02:16.569
    I0224 11:02:16.597224      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1804, replica count: 3
    I0224 11:02:19.652896      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:02:19.669: INFO: Creating new exec pod
    Feb 24 11:02:19.678: INFO: Waiting up to 5m0s for pod "execpod-affinityf2r44" in namespace "services-1804" to be "running"
    Feb 24 11:02:19.686: INFO: Pod "execpod-affinityf2r44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.439065ms
    Feb 24 11:02:21.693: INFO: Pod "execpod-affinityf2r44": Phase="Running", Reason="", readiness=true. Elapsed: 2.0146681s
    Feb 24 11:02:21.693: INFO: Pod "execpod-affinityf2r44" satisfied condition "running"
    Feb 24 11:02:22.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Feb 24 11:02:22.894: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 24 11:02:22.894: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:02:22.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.165.53 80'
    Feb 24 11:02:23.058: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.165.53 80\nConnection to 10.102.165.53 80 port [tcp/http] succeeded!\n"
    Feb 24 11:02:23.058: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:02:23.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31553'
    Feb 24 11:02:23.241: INFO: stderr: "+ nc -v -t -w 2 172.31.216.47 31553\n+ echo hostName\nConnection to 172.31.216.47 31553 port [tcp/*] succeeded!\n"
    Feb 24 11:02:23.241: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:02:23.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 31553'
    Feb 24 11:02:23.434: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 31553\nConnection to 172.31.217.191 31553 port [tcp/*] succeeded!\n"
    Feb 24 11:02:23.434: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:02:23.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1804 exec execpod-affinityf2r44 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:31553/ ; done'
    Feb 24 11:02:23.721: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31553/\n"
    Feb 24 11:02:23.721: INFO: stdout: "\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx\naffinity-nodeport-x5rdx"
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Received response from host: affinity-nodeport-x5rdx
    Feb 24 11:02:23.721: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1804, will wait for the garbage collector to delete the pods 02/24/23 11:02:23.741
    Feb 24 11:02:23.808: INFO: Deleting ReplicationController affinity-nodeport took: 11.986986ms
    Feb 24 11:02:23.909: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.954705ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:02:26.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1804" for this suite. 02/24/23 11:02:26.452
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:26.468
Feb 24 11:02:26.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:02:26.469
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:26.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:26.536
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:02:26.54
Feb 24 11:02:26.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75" in namespace "projected-3881" to be "Succeeded or Failed"
Feb 24 11:02:26.559: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.553231ms
Feb 24 11:02:28.566: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012510012s
Feb 24 11:02:30.566: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013023416s
STEP: Saw pod success 02/24/23 11:02:30.566
Feb 24 11:02:30.567: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75" satisfied condition "Succeeded or Failed"
Feb 24 11:02:30.572: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75 container client-container: <nil>
STEP: delete the pod 02/24/23 11:02:30.581
Feb 24 11:02:30.599: INFO: Waiting for pod downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75 to disappear
Feb 24 11:02:30.604: INFO: Pod downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 11:02:30.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3881" for this suite. 02/24/23 11:02:30.612
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":10,"skipped":183,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:26.468
    Feb 24 11:02:26.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:02:26.469
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:26.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:26.536
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:02:26.54
    Feb 24 11:02:26.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75" in namespace "projected-3881" to be "Succeeded or Failed"
    Feb 24 11:02:26.559: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 5.553231ms
    Feb 24 11:02:28.566: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012510012s
    Feb 24 11:02:30.566: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013023416s
    STEP: Saw pod success 02/24/23 11:02:30.566
    Feb 24 11:02:30.567: INFO: Pod "downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:30.572: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:02:30.581
    Feb 24 11:02:30.599: INFO: Waiting for pod downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75 to disappear
    Feb 24 11:02:30.604: INFO: Pod downwardapi-volume-ce27e719-9b07-4438-8970-ad5458f90a75 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 11:02:30.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3881" for this suite. 02/24/23 11:02:30.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:30.631
Feb 24 11:02:30.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:02:30.633
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:30.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:30.672
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 24 11:02:30.692: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5233 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 24 11:02:30.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5233" for this suite. 02/24/23 11:02:30.722
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":11,"skipped":203,"failed":0}
------------------------------
• [0.101 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:30.631
    Feb 24 11:02:30.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 11:02:30.633
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:30.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:30.672
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 24 11:02:30.692: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5233 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 24 11:02:30.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5233" for this suite. 02/24/23 11:02:30.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:30.741
Feb 24 11:02:30.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:02:30.744
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:30.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:30.774
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Feb 24 11:02:30.778: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/24/23 11:02:31.794
STEP: Checking rc "condition-test" has the desired failure condition set 02/24/23 11:02:31.801
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/24/23 11:02:32.809
Feb 24 11:02:32.823: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/24/23 11:02:32.823
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 24 11:02:33.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9344" for this suite. 02/24/23 11:02:33.842
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":12,"skipped":217,"failed":0}
------------------------------
• [3.109 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:30.741
    Feb 24 11:02:30.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:02:30.744
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:30.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:30.774
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Feb 24 11:02:30.778: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/24/23 11:02:31.794
    STEP: Checking rc "condition-test" has the desired failure condition set 02/24/23 11:02:31.801
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/24/23 11:02:32.809
    Feb 24 11:02:32.823: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/24/23 11:02:32.823
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 24 11:02:33.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9344" for this suite. 02/24/23 11:02:33.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:33.851
Feb 24 11:02:33.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename job 02/24/23 11:02:33.852
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:33.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:33.878
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 02/24/23 11:02:33.89
STEP: Patching the Job 02/24/23 11:02:33.898
STEP: Watching for Job to be patched 02/24/23 11:02:33.917
Feb 24 11:02:33.920: INFO: Event ADDED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 24 11:02:33.920: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 24 11:02:33.920: INFO: Event MODIFIED found for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/24/23 11:02:33.92
STEP: Watching for Job to be updated 02/24/23 11:02:33.936
Feb 24 11:02:33.938: INFO: Event MODIFIED found for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:33.938: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/24/23 11:02:33.938
Feb 24 11:02:33.980: INFO: Job: e2e-fsgpz as labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz]
STEP: Waiting for job to complete 02/24/23 11:02:33.98
STEP: Delete a job collection with a labelselector 02/24/23 11:02:43.985
STEP: Watching for Job to be deleted 02/24/23 11:02:43.996
Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:44.000: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:44.000: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:44.000: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 24 11:02:44.000: INFO: Event DELETED found for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/24/23 11:02:44
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 24 11:02:44.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5455" for this suite. 02/24/23 11:02:44.017
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":13,"skipped":230,"failed":0}
------------------------------
• [SLOW TEST] [10.181 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:33.851
    Feb 24 11:02:33.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename job 02/24/23 11:02:33.852
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:33.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:33.878
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 02/24/23 11:02:33.89
    STEP: Patching the Job 02/24/23 11:02:33.898
    STEP: Watching for Job to be patched 02/24/23 11:02:33.917
    Feb 24 11:02:33.920: INFO: Event ADDED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 24 11:02:33.920: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 24 11:02:33.920: INFO: Event MODIFIED found for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/24/23 11:02:33.92
    STEP: Watching for Job to be updated 02/24/23 11:02:33.936
    Feb 24 11:02:33.938: INFO: Event MODIFIED found for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:33.938: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/24/23 11:02:33.938
    Feb 24 11:02:33.980: INFO: Job: e2e-fsgpz as labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz]
    STEP: Waiting for job to complete 02/24/23 11:02:33.98
    STEP: Delete a job collection with a labelselector 02/24/23 11:02:43.985
    STEP: Watching for Job to be deleted 02/24/23 11:02:43.996
    Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:43.999: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:44.000: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:44.000: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:44.000: INFO: Event MODIFIED observed for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 24 11:02:44.000: INFO: Event DELETED found for Job e2e-fsgpz in namespace job-5455 with labels: map[e2e-fsgpz:patched e2e-job-label:e2e-fsgpz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/24/23 11:02:44
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 24 11:02:44.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5455" for this suite. 02/24/23 11:02:44.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:44.033
Feb 24 11:02:44.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename containers 02/24/23 11:02:44.034
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:44.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:44.09
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 02/24/23 11:02:44.094
Feb 24 11:02:44.107: INFO: Waiting up to 5m0s for pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c" in namespace "containers-3756" to be "Succeeded or Failed"
Feb 24 11:02:44.115: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.243945ms
Feb 24 11:02:46.120: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012893968s
Feb 24 11:02:48.127: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019336624s
STEP: Saw pod success 02/24/23 11:02:48.127
Feb 24 11:02:48.127: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c" satisfied condition "Succeeded or Failed"
Feb 24 11:02:48.131: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:02:48.141
Feb 24 11:02:48.159: INFO: Waiting for pod client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c to disappear
Feb 24 11:02:48.164: INFO: Pod client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 24 11:02:48.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3756" for this suite. 02/24/23 11:02:48.172
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":14,"skipped":244,"failed":0}
------------------------------
• [4.147 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:44.033
    Feb 24 11:02:44.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename containers 02/24/23 11:02:44.034
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:44.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:44.09
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 02/24/23 11:02:44.094
    Feb 24 11:02:44.107: INFO: Waiting up to 5m0s for pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c" in namespace "containers-3756" to be "Succeeded or Failed"
    Feb 24 11:02:44.115: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.243945ms
    Feb 24 11:02:46.120: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012893968s
    Feb 24 11:02:48.127: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019336624s
    STEP: Saw pod success 02/24/23 11:02:48.127
    Feb 24 11:02:48.127: INFO: Pod "client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:48.131: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:02:48.141
    Feb 24 11:02:48.159: INFO: Waiting for pod client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c to disappear
    Feb 24 11:02:48.164: INFO: Pod client-containers-b709761b-6ca9-4ef8-b898-14d35cbaf22c no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 24 11:02:48.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3756" for this suite. 02/24/23 11:02:48.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:48.182
Feb 24 11:02:48.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:02:48.183
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:48.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:48.213
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-130b4199-af85-4066-bc23-339c868467b6 02/24/23 11:02:48.218
STEP: Creating a pod to test consume secrets 02/24/23 11:02:48.224
Feb 24 11:02:48.234: INFO: Waiting up to 5m0s for pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089" in namespace "secrets-7211" to be "Succeeded or Failed"
Feb 24 11:02:48.244: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089": Phase="Pending", Reason="", readiness=false. Elapsed: 9.431867ms
Feb 24 11:02:50.249: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014615378s
Feb 24 11:02:52.251: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016584053s
STEP: Saw pod success 02/24/23 11:02:52.251
Feb 24 11:02:52.251: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089" satisfied condition "Succeeded or Failed"
Feb 24 11:02:52.255: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:02:52.264
Feb 24 11:02:52.277: INFO: Waiting for pod pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089 to disappear
Feb 24 11:02:52.281: INFO: Pod pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:02:52.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7211" for this suite. 02/24/23 11:02:52.288
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":15,"skipped":261,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:48.182
    Feb 24 11:02:48.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:02:48.183
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:48.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:48.213
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-130b4199-af85-4066-bc23-339c868467b6 02/24/23 11:02:48.218
    STEP: Creating a pod to test consume secrets 02/24/23 11:02:48.224
    Feb 24 11:02:48.234: INFO: Waiting up to 5m0s for pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089" in namespace "secrets-7211" to be "Succeeded or Failed"
    Feb 24 11:02:48.244: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089": Phase="Pending", Reason="", readiness=false. Elapsed: 9.431867ms
    Feb 24 11:02:50.249: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014615378s
    Feb 24 11:02:52.251: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016584053s
    STEP: Saw pod success 02/24/23 11:02:52.251
    Feb 24 11:02:52.251: INFO: Pod "pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:52.255: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:02:52.264
    Feb 24 11:02:52.277: INFO: Waiting for pod pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089 to disappear
    Feb 24 11:02:52.281: INFO: Pod pod-secrets-49f3d039-b2bf-44b7-b9a0-e281aa4ef089 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:02:52.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7211" for this suite. 02/24/23 11:02:52.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:52.302
Feb 24 11:02:52.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:02:52.303
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:52.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:52.338
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-6017/configmap-test-e029cb56-1ece-4b78-aeae-96eaf2babc0b 02/24/23 11:02:52.342
STEP: Creating a pod to test consume configMaps 02/24/23 11:02:52.348
Feb 24 11:02:52.357: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44" in namespace "configmap-6017" to be "Succeeded or Failed"
Feb 24 11:02:52.365: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.648322ms
Feb 24 11:02:54.371: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44": Phase="Running", Reason="", readiness=false. Elapsed: 2.01477412s
Feb 24 11:02:56.371: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01465532s
STEP: Saw pod success 02/24/23 11:02:56.371
Feb 24 11:02:56.372: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44" satisfied condition "Succeeded or Failed"
Feb 24 11:02:56.376: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44 container env-test: <nil>
STEP: delete the pod 02/24/23 11:02:56.387
Feb 24 11:02:56.403: INFO: Waiting for pod pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44 to disappear
Feb 24 11:02:56.408: INFO: Pod pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:02:56.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6017" for this suite. 02/24/23 11:02:56.415
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":16,"skipped":281,"failed":0}
------------------------------
• [4.125 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:52.302
    Feb 24 11:02:52.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:02:52.303
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:52.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:52.338
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-6017/configmap-test-e029cb56-1ece-4b78-aeae-96eaf2babc0b 02/24/23 11:02:52.342
    STEP: Creating a pod to test consume configMaps 02/24/23 11:02:52.348
    Feb 24 11:02:52.357: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44" in namespace "configmap-6017" to be "Succeeded or Failed"
    Feb 24 11:02:52.365: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.648322ms
    Feb 24 11:02:54.371: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44": Phase="Running", Reason="", readiness=false. Elapsed: 2.01477412s
    Feb 24 11:02:56.371: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01465532s
    STEP: Saw pod success 02/24/23 11:02:56.371
    Feb 24 11:02:56.372: INFO: Pod "pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44" satisfied condition "Succeeded or Failed"
    Feb 24 11:02:56.376: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44 container env-test: <nil>
    STEP: delete the pod 02/24/23 11:02:56.387
    Feb 24 11:02:56.403: INFO: Waiting for pod pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44 to disappear
    Feb 24 11:02:56.408: INFO: Pod pod-configmaps-3ce1fa32-5ab4-41dd-9da0-4bb9c2432e44 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:02:56.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6017" for this suite. 02/24/23 11:02:56.415
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:56.43
Feb 24 11:02:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename cronjob 02/24/23 11:02:56.432
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:56.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:56.471
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/24/23 11:02:56.479
STEP: creating 02/24/23 11:02:56.479
STEP: getting 02/24/23 11:02:56.487
STEP: listing 02/24/23 11:02:56.496
STEP: watching 02/24/23 11:02:56.501
Feb 24 11:02:56.501: INFO: starting watch
STEP: cluster-wide listing 02/24/23 11:02:56.503
STEP: cluster-wide watching 02/24/23 11:02:56.511
Feb 24 11:02:56.511: INFO: starting watch
STEP: patching 02/24/23 11:02:56.513
STEP: updating 02/24/23 11:02:56.523
Feb 24 11:02:56.534: INFO: waiting for watch events with expected annotations
Feb 24 11:02:56.534: INFO: saw patched and updated annotations
STEP: patching /status 02/24/23 11:02:56.535
STEP: updating /status 02/24/23 11:02:56.541
STEP: get /status 02/24/23 11:02:56.56
STEP: deleting 02/24/23 11:02:56.568
STEP: deleting a collection 02/24/23 11:02:56.595
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 24 11:02:56.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7637" for this suite. 02/24/23 11:02:56.632
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":17,"skipped":285,"failed":0}
------------------------------
• [0.219 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:56.43
    Feb 24 11:02:56.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename cronjob 02/24/23 11:02:56.432
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:56.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:56.471
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/24/23 11:02:56.479
    STEP: creating 02/24/23 11:02:56.479
    STEP: getting 02/24/23 11:02:56.487
    STEP: listing 02/24/23 11:02:56.496
    STEP: watching 02/24/23 11:02:56.501
    Feb 24 11:02:56.501: INFO: starting watch
    STEP: cluster-wide listing 02/24/23 11:02:56.503
    STEP: cluster-wide watching 02/24/23 11:02:56.511
    Feb 24 11:02:56.511: INFO: starting watch
    STEP: patching 02/24/23 11:02:56.513
    STEP: updating 02/24/23 11:02:56.523
    Feb 24 11:02:56.534: INFO: waiting for watch events with expected annotations
    Feb 24 11:02:56.534: INFO: saw patched and updated annotations
    STEP: patching /status 02/24/23 11:02:56.535
    STEP: updating /status 02/24/23 11:02:56.541
    STEP: get /status 02/24/23 11:02:56.56
    STEP: deleting 02/24/23 11:02:56.568
    STEP: deleting a collection 02/24/23 11:02:56.595
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 24 11:02:56.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7637" for this suite. 02/24/23 11:02:56.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:02:56.656
Feb 24 11:02:56.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:02:56.658
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:56.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:56.686
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 24 11:02:56.708: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:03:56.832: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:03:56.839
Feb 24 11:03:56.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:03:56.84
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:56.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:56.886
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Feb 24 11:03:56.907: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 24 11:03:56.911: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Feb 24 11:03:56.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6168" for this suite. 02/24/23 11:03:56.938
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:03:56.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4875" for this suite. 02/24/23 11:03:56.967
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":18,"skipped":305,"failed":0}
------------------------------
• [SLOW TEST] [60.373 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:02:56.656
    Feb 24 11:02:56.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:02:56.658
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:02:56.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:02:56.686
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 24 11:02:56.708: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:03:56.832: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:03:56.839
    Feb 24 11:03:56.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:03:56.84
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:56.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:56.886
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Feb 24 11:03:56.907: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 24 11:03:56.911: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Feb 24 11:03:56.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-6168" for this suite. 02/24/23 11:03:56.938
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:03:56.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4875" for this suite. 02/24/23 11:03:56.967
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:03:57.03
Feb 24 11:03:57.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:03:57.031
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:57.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:57.071
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 02/24/23 11:03:57.076
STEP: submitting the pod to kubernetes 02/24/23 11:03:57.076
Feb 24 11:03:57.085: INFO: Waiting up to 5m0s for pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" in namespace "pods-4155" to be "running and ready"
Feb 24 11:03:57.092: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162045ms
Feb 24 11:03:57.092: INFO: The phase of Pod pod-update-9f823b6b-9710-4738-be83-aae074cd1d06 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:03:59.097: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06": Phase="Running", Reason="", readiness=true. Elapsed: 2.011468261s
Feb 24 11:03:59.097: INFO: The phase of Pod pod-update-9f823b6b-9710-4738-be83-aae074cd1d06 is Running (Ready = true)
Feb 24 11:03:59.097: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/24/23 11:03:59.102
STEP: updating the pod 02/24/23 11:03:59.108
Feb 24 11:03:59.624: INFO: Successfully updated pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06"
Feb 24 11:03:59.624: INFO: Waiting up to 5m0s for pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" in namespace "pods-4155" to be "running"
Feb 24 11:03:59.629: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06": Phase="Running", Reason="", readiness=true. Elapsed: 4.554601ms
Feb 24 11:03:59.629: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/24/23 11:03:59.629
Feb 24 11:03:59.634: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 11:03:59.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4155" for this suite. 02/24/23 11:03:59.642
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":19,"skipped":318,"failed":0}
------------------------------
• [2.626 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:03:57.03
    Feb 24 11:03:57.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:03:57.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:57.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:57.071
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 02/24/23 11:03:57.076
    STEP: submitting the pod to kubernetes 02/24/23 11:03:57.076
    Feb 24 11:03:57.085: INFO: Waiting up to 5m0s for pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" in namespace "pods-4155" to be "running and ready"
    Feb 24 11:03:57.092: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162045ms
    Feb 24 11:03:57.092: INFO: The phase of Pod pod-update-9f823b6b-9710-4738-be83-aae074cd1d06 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:03:59.097: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06": Phase="Running", Reason="", readiness=true. Elapsed: 2.011468261s
    Feb 24 11:03:59.097: INFO: The phase of Pod pod-update-9f823b6b-9710-4738-be83-aae074cd1d06 is Running (Ready = true)
    Feb 24 11:03:59.097: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/24/23 11:03:59.102
    STEP: updating the pod 02/24/23 11:03:59.108
    Feb 24 11:03:59.624: INFO: Successfully updated pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06"
    Feb 24 11:03:59.624: INFO: Waiting up to 5m0s for pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" in namespace "pods-4155" to be "running"
    Feb 24 11:03:59.629: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06": Phase="Running", Reason="", readiness=true. Elapsed: 4.554601ms
    Feb 24 11:03:59.629: INFO: Pod "pod-update-9f823b6b-9710-4738-be83-aae074cd1d06" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/24/23 11:03:59.629
    Feb 24 11:03:59.634: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 11:03:59.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4155" for this suite. 02/24/23 11:03:59.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:03:59.657
Feb 24 11:03:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-pred 02/24/23 11:03:59.658
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:59.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:59.689
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 24 11:03:59.692: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 11:03:59.705: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 11:03:59.710: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
Feb 24 11:03:59.722: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.722: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:03:59.722: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:03:59.722: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
Feb 24 11:03:59.722: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:03:59.722: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:03:59.722: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:03:59.722: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.722: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:03:59.722: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.722: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:03:59.722: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 11:03:59.722: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:03:59.722: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:03:59.722: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
Feb 24 11:03:59.732: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.732: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:03:59.732: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:03:59.732: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
Feb 24 11:03:59.732: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:03:59.732: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:03:59.732: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:03:59.732: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.732: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:03:59.732: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.732: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:03:59.732: INFO: pod-update-9f823b6b-9710-4738-be83-aae074cd1d06 from pods-4155 started at 2023-02-24 11:03:57 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.732: INFO: 	Container pause ready: true, restart count 0
Feb 24 11:03:59.732: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:03:59.732: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:03:59.732: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
Feb 24 11:03:59.744: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.744: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:03:59.744: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:03:59.744: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
Feb 24 11:03:59.744: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:03:59.744: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:03:59.744: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:03:59.744: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.744: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:03:59.744: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 11:03:59.744: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:03:59.744: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.744: INFO: 	Container e2e ready: true, restart count 0
Feb 24 11:03:59.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:03:59.744: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:03:59.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:03:59.744: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/24/23 11:03:59.744
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1746bdd6660f2587], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 02/24/23 11:03:59.783
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:04:00.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6890" for this suite. 02/24/23 11:04:00.788
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":20,"skipped":328,"failed":0}
------------------------------
• [1.143 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:03:59.657
    Feb 24 11:03:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-pred 02/24/23 11:03:59.658
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:03:59.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:03:59.689
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 24 11:03:59.692: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 11:03:59.705: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 11:03:59.710: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
    Feb 24 11:03:59.722: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.722: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
    Feb 24 11:03:59.722: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.722: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.722: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.722: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:03:59.722: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
    Feb 24 11:03:59.732: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.732: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
    Feb 24 11:03:59.732: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.732: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.732: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: pod-update-9f823b6b-9710-4738-be83-aae074cd1d06 from pods-4155 started at 2023-02-24 11:03:57 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.732: INFO: 	Container pause ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:03:59.732: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
    Feb 24 11:03:59.744: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.744: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
    Feb 24 11:03:59.744: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.744: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 11:03:59.744: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.744: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:03:59.744: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:03:59.744: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/24/23 11:03:59.744
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1746bdd6660f2587], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 02/24/23 11:03:59.783
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:04:00.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6890" for this suite. 02/24/23 11:04:00.788
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:00.802
Feb 24 11:04:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:04:00.803
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:00.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:00.837
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-d921a747-de92-427c-8944-753c92b6d4ba 02/24/23 11:04:00.841
STEP: Creating a pod to test consume secrets 02/24/23 11:04:00.846
Feb 24 11:04:00.856: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a" in namespace "projected-4387" to be "Succeeded or Failed"
Feb 24 11:04:00.862: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89466ms
Feb 24 11:04:02.868: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011261588s
Feb 24 11:04:04.867: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010726328s
STEP: Saw pod success 02/24/23 11:04:04.867
Feb 24 11:04:04.867: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a" satisfied condition "Succeeded or Failed"
Feb 24 11:04:04.872: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:04:04.888
Feb 24 11:04:04.905: INFO: Waiting for pod pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a to disappear
Feb 24 11:04:04.911: INFO: Pod pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 11:04:04.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4387" for this suite. 02/24/23 11:04:04.92
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":21,"skipped":345,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:00.802
    Feb 24 11:04:00.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:04:00.803
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:00.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:00.837
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-d921a747-de92-427c-8944-753c92b6d4ba 02/24/23 11:04:00.841
    STEP: Creating a pod to test consume secrets 02/24/23 11:04:00.846
    Feb 24 11:04:00.856: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a" in namespace "projected-4387" to be "Succeeded or Failed"
    Feb 24 11:04:00.862: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89466ms
    Feb 24 11:04:02.868: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011261588s
    Feb 24 11:04:04.867: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010726328s
    STEP: Saw pod success 02/24/23 11:04:04.867
    Feb 24 11:04:04.867: INFO: Pod "pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a" satisfied condition "Succeeded or Failed"
    Feb 24 11:04:04.872: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:04:04.888
    Feb 24 11:04:04.905: INFO: Waiting for pod pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a to disappear
    Feb 24 11:04:04.911: INFO: Pod pod-projected-secrets-203b3750-f07a-4215-aada-6588d888cc5a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 11:04:04.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4387" for this suite. 02/24/23 11:04:04.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:04.933
Feb 24 11:04:04.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:04:04.934
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:04.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:04.989
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 02/24/23 11:04:04.993
Feb 24 11:04:05.008: INFO: Waiting up to 5m0s for pod "pod-kxwrg" in namespace "pods-8830" to be "running"
Feb 24 11:04:05.020: INFO: Pod "pod-kxwrg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0343ms
Feb 24 11:04:07.026: INFO: Pod "pod-kxwrg": Phase="Running", Reason="", readiness=true. Elapsed: 2.017178375s
Feb 24 11:04:07.026: INFO: Pod "pod-kxwrg" satisfied condition "running"
STEP: patching /status 02/24/23 11:04:07.026
Feb 24 11:04:07.039: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 11:04:07.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8830" for this suite. 02/24/23 11:04:07.05
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":22,"skipped":369,"failed":0}
------------------------------
• [2.127 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:04.933
    Feb 24 11:04:04.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:04:04.934
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:04.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:04.989
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 02/24/23 11:04:04.993
    Feb 24 11:04:05.008: INFO: Waiting up to 5m0s for pod "pod-kxwrg" in namespace "pods-8830" to be "running"
    Feb 24 11:04:05.020: INFO: Pod "pod-kxwrg": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0343ms
    Feb 24 11:04:07.026: INFO: Pod "pod-kxwrg": Phase="Running", Reason="", readiness=true. Elapsed: 2.017178375s
    Feb 24 11:04:07.026: INFO: Pod "pod-kxwrg" satisfied condition "running"
    STEP: patching /status 02/24/23 11:04:07.026
    Feb 24 11:04:07.039: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 11:04:07.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8830" for this suite. 02/24/23 11:04:07.05
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:07.062
Feb 24 11:04:07.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:04:07.063
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:07.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:07.091
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 02/24/23 11:04:07.094
STEP: When the matched label of one of its pods change 02/24/23 11:04:07.1
Feb 24 11:04:07.105: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 24 11:04:12.109: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/24/23 11:04:12.122
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 24 11:04:13.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1909" for this suite. 02/24/23 11:04:13.139
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":23,"skipped":371,"failed":0}
------------------------------
• [SLOW TEST] [6.086 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:07.062
    Feb 24 11:04:07.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:04:07.063
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:07.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:07.091
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 02/24/23 11:04:07.094
    STEP: When the matched label of one of its pods change 02/24/23 11:04:07.1
    Feb 24 11:04:07.105: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb 24 11:04:12.109: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/24/23 11:04:12.122
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 24 11:04:13.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1909" for this suite. 02/24/23 11:04:13.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:13.149
Feb 24 11:04:13.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 11:04:13.15
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:13.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:13.178
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 24 11:04:13.260: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"be20840f-0498-4c8d-ac6c-143e60727ca7", Controller:(*bool)(0xc0033efbee), BlockOwnerDeletion:(*bool)(0xc0033efbef)}}
Feb 24 11:04:13.302: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c894c029-9e04-4774-8ce2-6c1689f627aa", Controller:(*bool)(0xc0033efe46), BlockOwnerDeletion:(*bool)(0xc0033efe47)}}
Feb 24 11:04:13.320: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ec88bbc1-fdc1-49a7-bcdd-e456517352ed", Controller:(*bool)(0xc0035cc0d6), BlockOwnerDeletion:(*bool)(0xc0035cc0d7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 11:04:18.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2566" for this suite. 02/24/23 11:04:18.356
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":24,"skipped":381,"failed":0}
------------------------------
• [SLOW TEST] [5.218 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:13.149
    Feb 24 11:04:13.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 11:04:13.15
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:13.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:13.178
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 24 11:04:13.260: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"be20840f-0498-4c8d-ac6c-143e60727ca7", Controller:(*bool)(0xc0033efbee), BlockOwnerDeletion:(*bool)(0xc0033efbef)}}
    Feb 24 11:04:13.302: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c894c029-9e04-4774-8ce2-6c1689f627aa", Controller:(*bool)(0xc0033efe46), BlockOwnerDeletion:(*bool)(0xc0033efe47)}}
    Feb 24 11:04:13.320: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ec88bbc1-fdc1-49a7-bcdd-e456517352ed", Controller:(*bool)(0xc0035cc0d6), BlockOwnerDeletion:(*bool)(0xc0035cc0d7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 11:04:18.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2566" for this suite. 02/24/23 11:04:18.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:18.371
Feb 24 11:04:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sysctl 02/24/23 11:04:18.372
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:18.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:18.413
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/24/23 11:04:18.417
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 11:04:18.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9017" for this suite. 02/24/23 11:04:18.431
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":25,"skipped":394,"failed":0}
------------------------------
• [0.068 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:18.371
    Feb 24 11:04:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sysctl 02/24/23 11:04:18.372
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:18.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:18.413
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/24/23 11:04:18.417
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 11:04:18.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-9017" for this suite. 02/24/23 11:04:18.431
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:18.441
Feb 24 11:04:18.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 11:04:18.442
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:18.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:18.482
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/24/23 11:04:18.485
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_tcp@PTR;sleep 1; done
 02/24/23 11:04:18.522
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_tcp@PTR;sleep 1; done
 02/24/23 11:04:18.522
STEP: creating a pod to probe DNS 02/24/23 11:04:18.522
STEP: submitting the pod to kubernetes 02/24/23 11:04:18.522
Feb 24 11:04:18.542: INFO: Waiting up to 15m0s for pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a" in namespace "dns-4528" to be "running"
Feb 24 11:04:18.564: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.576129ms
Feb 24 11:04:20.571: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028676641s
Feb 24 11:04:22.572: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030007852s
Feb 24 11:04:24.571: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029012528s
Feb 24 11:04:26.574: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Running", Reason="", readiness=true. Elapsed: 8.032575375s
Feb 24 11:04:26.575: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:04:26.575
STEP: looking for the results for each expected name from probers 02/24/23 11:04:26.58
Feb 24 11:04:26.591: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.602: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.675: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.689: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.695: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:26.716: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

Feb 24 11:04:31.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.729: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.740: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.776: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.782: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.788: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.794: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:31.814: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

Feb 24 11:04:36.726: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.743: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.774: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.785: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.791: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.799: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:36.831: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

Feb 24 11:04:41.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.742: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.770: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.779: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.789: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.795: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:41.824: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

Feb 24 11:04:46.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.744: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.772: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.777: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.783: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:46.809: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

Feb 24 11:04:51.725: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.745: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.774: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.790: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.807: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
Feb 24 11:04:51.846: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

Feb 24 11:04:56.831: INFO: DNS probes using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a succeeded

STEP: deleting the pod 02/24/23 11:04:56.831
STEP: deleting the test service 02/24/23 11:04:56.85
STEP: deleting the test headless service 02/24/23 11:04:56.89
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 11:04:56.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4528" for this suite. 02/24/23 11:04:56.93
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":26,"skipped":398,"failed":0}
------------------------------
• [SLOW TEST] [38.498 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:18.441
    Feb 24 11:04:18.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 11:04:18.442
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:18.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:18.482
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/24/23 11:04:18.485
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_tcp@PTR;sleep 1; done
     02/24/23 11:04:18.522
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4528.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4528.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4528.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.184.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.184.0_tcp@PTR;sleep 1; done
     02/24/23 11:04:18.522
    STEP: creating a pod to probe DNS 02/24/23 11:04:18.522
    STEP: submitting the pod to kubernetes 02/24/23 11:04:18.522
    Feb 24 11:04:18.542: INFO: Waiting up to 15m0s for pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a" in namespace "dns-4528" to be "running"
    Feb 24 11:04:18.564: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.576129ms
    Feb 24 11:04:20.571: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028676641s
    Feb 24 11:04:22.572: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030007852s
    Feb 24 11:04:24.571: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029012528s
    Feb 24 11:04:26.574: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a": Phase="Running", Reason="", readiness=true. Elapsed: 8.032575375s
    Feb 24 11:04:26.575: INFO: Pod "dns-test-61e69760-54e0-489b-959f-e0ea57210b5a" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:04:26.575
    STEP: looking for the results for each expected name from probers 02/24/23 11:04:26.58
    Feb 24 11:04:26.591: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.602: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.675: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.689: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.695: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:26.716: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

    Feb 24 11:04:31.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.729: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.740: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.776: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.782: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.788: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.794: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:31.814: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

    Feb 24 11:04:36.726: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.743: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.774: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.785: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.791: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.799: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:36.831: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

    Feb 24 11:04:41.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.742: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.770: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.779: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.789: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.795: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:41.824: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

    Feb 24 11:04:46.724: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.735: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.744: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.772: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.777: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.783: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.788: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:46.809: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

    Feb 24 11:04:51.725: INFO: Unable to read wheezy_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.731: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.736: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.745: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.774: INFO: Unable to read jessie_udp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.790: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.807: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local from pod dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a: the server could not find the requested resource (get pods dns-test-61e69760-54e0-489b-959f-e0ea57210b5a)
    Feb 24 11:04:51.846: INFO: Lookups using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a failed for: [wheezy_udp@dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@dns-test-service.dns-4528.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_udp@dns-test-service.dns-4528.svc.cluster.local jessie_tcp@dns-test-service.dns-4528.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4528.svc.cluster.local]

    Feb 24 11:04:56.831: INFO: DNS probes using dns-4528/dns-test-61e69760-54e0-489b-959f-e0ea57210b5a succeeded

    STEP: deleting the pod 02/24/23 11:04:56.831
    STEP: deleting the test service 02/24/23 11:04:56.85
    STEP: deleting the test headless service 02/24/23 11:04:56.89
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 11:04:56.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4528" for this suite. 02/24/23 11:04:56.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:04:56.946
Feb 24 11:04:56.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:04:56.948
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:56.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:57.005
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 02/24/23 11:04:57.054
Feb 24 11:04:57.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 create -f -'
Feb 24 11:04:58.697: INFO: stderr: ""
Feb 24 11:04:58.697: INFO: stdout: "pod/pause created\n"
Feb 24 11:04:58.697: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 24 11:04:58.697: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6246" to be "running and ready"
Feb 24 11:04:58.706: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.035652ms
Feb 24 11:04:58.706: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-216-47.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
Feb 24 11:05:00.712: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.01527235s
Feb 24 11:05:00.712: INFO: Pod "pause" satisfied condition "running and ready"
Feb 24 11:05:00.712: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 02/24/23 11:05:00.712
Feb 24 11:05:00.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 label pods pause testing-label=testing-label-value'
Feb 24 11:05:00.837: INFO: stderr: ""
Feb 24 11:05:00.837: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/24/23 11:05:00.837
Feb 24 11:05:00.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get pod pause -L testing-label'
Feb 24 11:05:00.941: INFO: stderr: ""
Feb 24 11:05:00.941: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/24/23 11:05:00.941
Feb 24 11:05:00.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 label pods pause testing-label-'
Feb 24 11:05:01.108: INFO: stderr: ""
Feb 24 11:05:01.108: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/24/23 11:05:01.108
Feb 24 11:05:01.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get pod pause -L testing-label'
Feb 24 11:05:01.216: INFO: stderr: ""
Feb 24 11:05:01.216: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 02/24/23 11:05:01.216
Feb 24 11:05:01.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 delete --grace-period=0 --force -f -'
Feb 24 11:05:01.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:05:01.439: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 24 11:05:01.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get rc,svc -l name=pause --no-headers'
Feb 24 11:05:01.603: INFO: stderr: "No resources found in kubectl-6246 namespace.\n"
Feb 24 11:05:01.603: INFO: stdout: ""
Feb 24 11:05:01.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 11:05:01.822: INFO: stderr: ""
Feb 24 11:05:01.822: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:05:01.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6246" for this suite. 02/24/23 11:05:01.831
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":27,"skipped":418,"failed":0}
------------------------------
• [4.895 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:04:56.946
    Feb 24 11:04:56.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:04:56.948
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:04:56.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:04:57.005
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 02/24/23 11:04:57.054
    Feb 24 11:04:57.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 create -f -'
    Feb 24 11:04:58.697: INFO: stderr: ""
    Feb 24 11:04:58.697: INFO: stdout: "pod/pause created\n"
    Feb 24 11:04:58.697: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 24 11:04:58.697: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6246" to be "running and ready"
    Feb 24 11:04:58.706: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.035652ms
    Feb 24 11:04:58.706: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-172-31-216-47.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
    Feb 24 11:05:00.712: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.01527235s
    Feb 24 11:05:00.712: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 24 11:05:00.712: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 02/24/23 11:05:00.712
    Feb 24 11:05:00.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 label pods pause testing-label=testing-label-value'
    Feb 24 11:05:00.837: INFO: stderr: ""
    Feb 24 11:05:00.837: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/24/23 11:05:00.837
    Feb 24 11:05:00.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get pod pause -L testing-label'
    Feb 24 11:05:00.941: INFO: stderr: ""
    Feb 24 11:05:00.941: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/24/23 11:05:00.941
    Feb 24 11:05:00.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 label pods pause testing-label-'
    Feb 24 11:05:01.108: INFO: stderr: ""
    Feb 24 11:05:01.108: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/24/23 11:05:01.108
    Feb 24 11:05:01.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get pod pause -L testing-label'
    Feb 24 11:05:01.216: INFO: stderr: ""
    Feb 24 11:05:01.216: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 02/24/23 11:05:01.216
    Feb 24 11:05:01.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 delete --grace-period=0 --force -f -'
    Feb 24 11:05:01.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:05:01.439: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 24 11:05:01.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get rc,svc -l name=pause --no-headers'
    Feb 24 11:05:01.603: INFO: stderr: "No resources found in kubectl-6246 namespace.\n"
    Feb 24 11:05:01.603: INFO: stdout: ""
    Feb 24 11:05:01.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6246 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 24 11:05:01.822: INFO: stderr: ""
    Feb 24 11:05:01.822: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:05:01.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6246" for this suite. 02/24/23 11:05:01.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:05:01.842
Feb 24 11:05:01.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:05:01.85
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:01.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:01.911
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-74276797-8a88-417f-bac6-10c9be1c8813 02/24/23 11:05:01.916
STEP: Creating a pod to test consume secrets 02/24/23 11:05:01.926
Feb 24 11:05:01.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484" in namespace "projected-7226" to be "Succeeded or Failed"
Feb 24 11:05:01.951: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484": Phase="Pending", Reason="", readiness=false. Elapsed: 5.825473ms
Feb 24 11:05:03.956: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011203568s
Feb 24 11:05:05.955: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010596744s
STEP: Saw pod success 02/24/23 11:05:05.955
Feb 24 11:05:05.955: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484" satisfied condition "Succeeded or Failed"
Feb 24 11:05:05.960: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:05:05.975
Feb 24 11:05:05.992: INFO: Waiting for pod pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484 to disappear
Feb 24 11:05:06.000: INFO: Pod pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 11:05:06.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7226" for this suite. 02/24/23 11:05:06.012
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":28,"skipped":446,"failed":0}
------------------------------
• [4.179 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:05:01.842
    Feb 24 11:05:01.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:05:01.85
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:01.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:01.911
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-74276797-8a88-417f-bac6-10c9be1c8813 02/24/23 11:05:01.916
    STEP: Creating a pod to test consume secrets 02/24/23 11:05:01.926
    Feb 24 11:05:01.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484" in namespace "projected-7226" to be "Succeeded or Failed"
    Feb 24 11:05:01.951: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484": Phase="Pending", Reason="", readiness=false. Elapsed: 5.825473ms
    Feb 24 11:05:03.956: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011203568s
    Feb 24 11:05:05.955: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010596744s
    STEP: Saw pod success 02/24/23 11:05:05.955
    Feb 24 11:05:05.955: INFO: Pod "pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484" satisfied condition "Succeeded or Failed"
    Feb 24 11:05:05.960: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:05:05.975
    Feb 24 11:05:05.992: INFO: Waiting for pod pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484 to disappear
    Feb 24 11:05:06.000: INFO: Pod pod-projected-secrets-c672894d-6e75-4d92-b726-9348dfbf4484 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 11:05:06.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7226" for this suite. 02/24/23 11:05:06.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:05:06.025
Feb 24 11:05:06.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:05:06.026
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:06.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:06.059
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 02/24/23 11:05:06.096
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:05:06.104
Feb 24 11:05:06.111: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:06.111: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:06.111: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:06.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:05:06.116: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:05:07.132: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:07.132: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:07.132: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:07.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:05:07.138: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:05:08.125: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:08.125: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:08.125: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:08.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:05:08.130: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/24/23 11:05:08.134
Feb 24 11:05:08.163: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:08.164: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:08.165: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:08.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:05:08.173: INFO: Node ip-172-31-217-191.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:05:09.182: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:09.182: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:09.182: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:09.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:05:09.188: INFO: Node ip-172-31-217-191.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:05:10.181: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:10.181: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:10.181: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:05:10.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:05:10.187: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/24/23 11:05:10.187
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:05:10.197
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7835, will wait for the garbage collector to delete the pods 02/24/23 11:05:10.197
Feb 24 11:05:10.262: INFO: Deleting DaemonSet.extensions daemon-set took: 9.737019ms
Feb 24 11:05:10.363: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.449846ms
Feb 24 11:05:12.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:05:12.569: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:05:12.575: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6326"},"items":null}

Feb 24 11:05:12.580: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6326"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:05:12.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7835" for this suite. 02/24/23 11:05:12.608
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":29,"skipped":451,"failed":0}
------------------------------
• [SLOW TEST] [6.591 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:05:06.025
    Feb 24 11:05:06.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:05:06.026
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:06.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:06.059
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 02/24/23 11:05:06.096
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:05:06.104
    Feb 24 11:05:06.111: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:06.111: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:06.111: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:06.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:05:06.116: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:05:07.132: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:07.132: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:07.132: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:07.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:05:07.138: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:05:08.125: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:08.125: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:08.125: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:08.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:05:08.130: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/24/23 11:05:08.134
    Feb 24 11:05:08.163: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:08.164: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:08.165: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:08.172: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:05:08.173: INFO: Node ip-172-31-217-191.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:05:09.182: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:09.182: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:09.182: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:09.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:05:09.188: INFO: Node ip-172-31-217-191.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:05:10.181: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:10.181: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:10.181: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:05:10.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:05:10.187: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/24/23 11:05:10.187
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:05:10.197
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7835, will wait for the garbage collector to delete the pods 02/24/23 11:05:10.197
    Feb 24 11:05:10.262: INFO: Deleting DaemonSet.extensions daemon-set took: 9.737019ms
    Feb 24 11:05:10.363: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.449846ms
    Feb 24 11:05:12.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:05:12.569: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:05:12.575: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6326"},"items":null}

    Feb 24 11:05:12.580: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6326"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:05:12.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7835" for this suite. 02/24/23 11:05:12.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:05:12.619
Feb 24 11:05:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:05:12.62
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:12.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:12.649
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/24/23 11:05:12.653
Feb 24 11:05:12.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/24/23 11:05:29.599
Feb 24 11:05:29.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:05:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:05:49.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5860" for this suite. 02/24/23 11:05:49.842
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":30,"skipped":464,"failed":0}
------------------------------
• [SLOW TEST] [37.232 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:05:12.619
    Feb 24 11:05:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:05:12.62
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:12.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:12.649
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/24/23 11:05:12.653
    Feb 24 11:05:12.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/24/23 11:05:29.599
    Feb 24 11:05:29.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:05:33.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:05:49.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5860" for this suite. 02/24/23 11:05:49.842
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:05:49.853
Feb 24 11:05:49.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename job 02/24/23 11:05:49.854
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:49.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:49.881
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 02/24/23 11:05:49.883
STEP: Ensuring active pods == parallelism 02/24/23 11:05:49.891
STEP: delete a job 02/24/23 11:05:51.898
STEP: deleting Job.batch foo in namespace job-2202, will wait for the garbage collector to delete the pods 02/24/23 11:05:51.898
Feb 24 11:05:51.968: INFO: Deleting Job.batch foo took: 12.517055ms
Feb 24 11:05:52.070: INFO: Terminating Job.batch foo pods took: 101.605364ms
STEP: Ensuring job was deleted 02/24/23 11:06:24.971
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 24 11:06:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2202" for this suite. 02/24/23 11:06:24.983
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":31,"skipped":467,"failed":0}
------------------------------
• [SLOW TEST] [35.143 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:05:49.853
    Feb 24 11:05:49.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename job 02/24/23 11:05:49.854
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:05:49.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:05:49.881
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 02/24/23 11:05:49.883
    STEP: Ensuring active pods == parallelism 02/24/23 11:05:49.891
    STEP: delete a job 02/24/23 11:05:51.898
    STEP: deleting Job.batch foo in namespace job-2202, will wait for the garbage collector to delete the pods 02/24/23 11:05:51.898
    Feb 24 11:05:51.968: INFO: Deleting Job.batch foo took: 12.517055ms
    Feb 24 11:05:52.070: INFO: Terminating Job.batch foo pods took: 101.605364ms
    STEP: Ensuring job was deleted 02/24/23 11:06:24.971
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 24 11:06:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2202" for this suite. 02/24/23 11:06:24.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:06:25.003
Feb 24 11:06:25.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:06:25.003
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:25.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:25.032
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 11:06:25.035
Feb 24 11:06:25.049: INFO: Waiting up to 5m0s for pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da" in namespace "emptydir-9340" to be "Succeeded or Failed"
Feb 24 11:06:25.056: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.430761ms
Feb 24 11:06:27.062: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012983642s
Feb 24 11:06:29.067: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017094925s
STEP: Saw pod success 02/24/23 11:06:29.067
Feb 24 11:06:29.067: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da" satisfied condition "Succeeded or Failed"
Feb 24 11:06:29.071: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da container test-container: <nil>
STEP: delete the pod 02/24/23 11:06:29.095
Feb 24 11:06:29.121: INFO: Waiting for pod pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da to disappear
Feb 24 11:06:29.127: INFO: Pod pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:06:29.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9340" for this suite. 02/24/23 11:06:29.137
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":493,"failed":0}
------------------------------
• [4.148 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:06:25.003
    Feb 24 11:06:25.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:06:25.003
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:25.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:25.032
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 11:06:25.035
    Feb 24 11:06:25.049: INFO: Waiting up to 5m0s for pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da" in namespace "emptydir-9340" to be "Succeeded or Failed"
    Feb 24 11:06:25.056: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da": Phase="Pending", Reason="", readiness=false. Elapsed: 6.430761ms
    Feb 24 11:06:27.062: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012983642s
    Feb 24 11:06:29.067: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017094925s
    STEP: Saw pod success 02/24/23 11:06:29.067
    Feb 24 11:06:29.067: INFO: Pod "pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da" satisfied condition "Succeeded or Failed"
    Feb 24 11:06:29.071: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da container test-container: <nil>
    STEP: delete the pod 02/24/23 11:06:29.095
    Feb 24 11:06:29.121: INFO: Waiting for pod pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da to disappear
    Feb 24 11:06:29.127: INFO: Pod pod-e61b3d43-106d-4ec4-aaf1-47d7b7bc68da no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:06:29.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9340" for this suite. 02/24/23 11:06:29.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:06:29.153
Feb 24 11:06:29.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename init-container 02/24/23 11:06:29.154
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:29.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:29.187
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 02/24/23 11:06:29.19
Feb 24 11:06:29.190: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 11:06:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9675" for this suite. 02/24/23 11:06:34.841
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":33,"skipped":499,"failed":0}
------------------------------
• [SLOW TEST] [5.742 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:06:29.153
    Feb 24 11:06:29.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename init-container 02/24/23 11:06:29.154
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:29.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:29.187
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 02/24/23 11:06:29.19
    Feb 24 11:06:29.190: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 11:06:34.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9675" for this suite. 02/24/23 11:06:34.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:06:34.896
Feb 24 11:06:34.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:06:34.897
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:34.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:34.978
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-a2808c17-e84c-42c5-952d-3063e999cec4 02/24/23 11:06:34.98
STEP: Creating a pod to test consume configMaps 02/24/23 11:06:34.987
Feb 24 11:06:34.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5" in namespace "configmap-8978" to be "Succeeded or Failed"
Feb 24 11:06:35.024: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.225051ms
Feb 24 11:06:37.032: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5": Phase="Running", Reason="", readiness=false. Elapsed: 2.032197711s
Feb 24 11:06:39.031: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031545736s
STEP: Saw pod success 02/24/23 11:06:39.031
Feb 24 11:06:39.031: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5" satisfied condition "Succeeded or Failed"
Feb 24 11:06:39.039: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:06:39.057
Feb 24 11:06:39.075: INFO: Waiting for pod pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5 to disappear
Feb 24 11:06:39.081: INFO: Pod pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:06:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8978" for this suite. 02/24/23 11:06:39.089
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":34,"skipped":511,"failed":0}
------------------------------
• [4.203 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:06:34.896
    Feb 24 11:06:34.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:06:34.897
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:34.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:34.978
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-a2808c17-e84c-42c5-952d-3063e999cec4 02/24/23 11:06:34.98
    STEP: Creating a pod to test consume configMaps 02/24/23 11:06:34.987
    Feb 24 11:06:34.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5" in namespace "configmap-8978" to be "Succeeded or Failed"
    Feb 24 11:06:35.024: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.225051ms
    Feb 24 11:06:37.032: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5": Phase="Running", Reason="", readiness=false. Elapsed: 2.032197711s
    Feb 24 11:06:39.031: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031545736s
    STEP: Saw pod success 02/24/23 11:06:39.031
    Feb 24 11:06:39.031: INFO: Pod "pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5" satisfied condition "Succeeded or Failed"
    Feb 24 11:06:39.039: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:06:39.057
    Feb 24 11:06:39.075: INFO: Waiting for pod pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5 to disappear
    Feb 24 11:06:39.081: INFO: Pod pod-configmaps-b47b7b49-c319-4d8a-854b-903bdc7687e5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:06:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8978" for this suite. 02/24/23 11:06:39.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:06:39.102
Feb 24 11:06:39.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename endpointslice 02/24/23 11:06:39.103
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:39.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:39.147
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 02/24/23 11:06:44.389
STEP: referencing matching pods with named port 02/24/23 11:06:49.403
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/24/23 11:06:54.419
STEP: recreating EndpointSlices after they've been deleted 02/24/23 11:06:59.43
Feb 24 11:06:59.460: INFO: EndpointSlice for Service endpointslice-6460/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 24 11:07:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6460" for this suite. 02/24/23 11:07:09.48
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":35,"skipped":536,"failed":0}
------------------------------
• [SLOW TEST] [30.389 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:06:39.102
    Feb 24 11:06:39.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename endpointslice 02/24/23 11:06:39.103
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:06:39.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:06:39.147
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 02/24/23 11:06:44.389
    STEP: referencing matching pods with named port 02/24/23 11:06:49.403
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/24/23 11:06:54.419
    STEP: recreating EndpointSlices after they've been deleted 02/24/23 11:06:59.43
    Feb 24 11:06:59.460: INFO: EndpointSlice for Service endpointslice-6460/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 24 11:07:09.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6460" for this suite. 02/24/23 11:07:09.48
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:07:09.493
Feb 24 11:07:09.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:07:09.495
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:07:09.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:07:09.518
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 02/24/23 11:07:09.521
Feb 24 11:07:09.535: INFO: Waiting up to 5m0s for pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c" in namespace "projected-6480" to be "running and ready"
Feb 24 11:07:09.554: INFO: Pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.435975ms
Feb 24 11:07:09.555: INFO: The phase of Pod labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:07:11.561: INFO: Pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c": Phase="Running", Reason="", readiness=true. Elapsed: 2.026232231s
Feb 24 11:07:11.561: INFO: The phase of Pod labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c is Running (Ready = true)
Feb 24 11:07:11.561: INFO: Pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c" satisfied condition "running and ready"
Feb 24 11:07:12.103: INFO: Successfully updated pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 11:07:16.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6480" for this suite. 02/24/23 11:07:16.145
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":36,"skipped":537,"failed":0}
------------------------------
• [SLOW TEST] [6.665 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:07:09.493
    Feb 24 11:07:09.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:07:09.495
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:07:09.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:07:09.518
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 02/24/23 11:07:09.521
    Feb 24 11:07:09.535: INFO: Waiting up to 5m0s for pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c" in namespace "projected-6480" to be "running and ready"
    Feb 24 11:07:09.554: INFO: Pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.435975ms
    Feb 24 11:07:09.555: INFO: The phase of Pod labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:07:11.561: INFO: Pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c": Phase="Running", Reason="", readiness=true. Elapsed: 2.026232231s
    Feb 24 11:07:11.561: INFO: The phase of Pod labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c is Running (Ready = true)
    Feb 24 11:07:11.561: INFO: Pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c" satisfied condition "running and ready"
    Feb 24 11:07:12.103: INFO: Successfully updated pod "labelsupdate0c9ce60d-c473-4db9-9ee5-79342cecd69c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 11:07:16.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6480" for this suite. 02/24/23 11:07:16.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:07:16.161
Feb 24 11:07:16.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 11:07:16.162
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:07:16.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:07:16.187
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6495 02/24/23 11:07:16.189
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 02/24/23 11:07:16.197
Feb 24 11:07:16.220: INFO: Found 0 stateful pods, waiting for 3
Feb 24 11:07:26.226: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:07:26.226: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:07:26.226: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:07:26.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:07:26.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:07:26.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:07:26.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/24/23 11:07:36.45
Feb 24 11:07:36.477: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/24/23 11:07:36.477
STEP: Updating Pods in reverse ordinal order 02/24/23 11:07:46.504
Feb 24 11:07:46.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:07:46.712: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:07:46.712: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:07:46.712: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 02/24/23 11:08:06.741
Feb 24 11:08:06.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:08:06.991: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:08:06.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:08:06.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:08:17.085: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/24/23 11:08:27.127
Feb 24 11:08:27.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:08:27.436: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:08:27.436: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:08:27.436: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 11:08:37.468: INFO: Deleting all statefulset in ns statefulset-6495
Feb 24 11:08:37.479: INFO: Scaling statefulset ss2 to 0
Feb 24 11:08:47.505: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:08:47.510: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 11:08:47.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6495" for this suite. 02/24/23 11:08:47.556
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":37,"skipped":543,"failed":0}
------------------------------
• [SLOW TEST] [91.408 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:07:16.161
    Feb 24 11:07:16.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:07:16.162
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:07:16.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:07:16.187
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6495 02/24/23 11:07:16.189
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 02/24/23 11:07:16.197
    Feb 24 11:07:16.220: INFO: Found 0 stateful pods, waiting for 3
    Feb 24 11:07:26.226: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:07:26.226: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:07:26.226: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:07:26.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:07:26.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:07:26.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:07:26.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/24/23 11:07:36.45
    Feb 24 11:07:36.477: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/24/23 11:07:36.477
    STEP: Updating Pods in reverse ordinal order 02/24/23 11:07:46.504
    Feb 24 11:07:46.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:07:46.712: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:07:46.712: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:07:46.712: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 02/24/23 11:08:06.741
    Feb 24 11:08:06.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:08:06.991: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:08:06.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:08:06.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:08:17.085: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/24/23 11:08:27.127
    Feb 24 11:08:27.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-6495 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:08:27.436: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:08:27.436: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:08:27.436: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 11:08:37.468: INFO: Deleting all statefulset in ns statefulset-6495
    Feb 24 11:08:37.479: INFO: Scaling statefulset ss2 to 0
    Feb 24 11:08:47.505: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:08:47.510: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 11:08:47.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6495" for this suite. 02/24/23 11:08:47.556
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:08:47.571
Feb 24 11:08:47.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:08:47.572
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:47.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:47.64
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 11:08:47.651
Feb 24 11:08:47.673: INFO: Waiting up to 5m0s for pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6" in namespace "emptydir-7995" to be "Succeeded or Failed"
Feb 24 11:08:47.692: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.223659ms
Feb 24 11:08:49.699: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02497236s
Feb 24 11:08:51.701: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027455265s
STEP: Saw pod success 02/24/23 11:08:51.701
Feb 24 11:08:51.701: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6" satisfied condition "Succeeded or Failed"
Feb 24 11:08:51.709: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-93981695-fcd1-4061-8f99-c48dc154b5e6 container test-container: <nil>
STEP: delete the pod 02/24/23 11:08:51.725
Feb 24 11:08:51.745: INFO: Waiting for pod pod-93981695-fcd1-4061-8f99-c48dc154b5e6 to disappear
Feb 24 11:08:51.751: INFO: Pod pod-93981695-fcd1-4061-8f99-c48dc154b5e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:08:51.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7995" for this suite. 02/24/23 11:08:51.764
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":38,"skipped":546,"failed":0}
------------------------------
• [4.202 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:08:47.571
    Feb 24 11:08:47.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:08:47.572
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:47.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:47.64
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/24/23 11:08:47.651
    Feb 24 11:08:47.673: INFO: Waiting up to 5m0s for pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6" in namespace "emptydir-7995" to be "Succeeded or Failed"
    Feb 24 11:08:47.692: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.223659ms
    Feb 24 11:08:49.699: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02497236s
    Feb 24 11:08:51.701: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027455265s
    STEP: Saw pod success 02/24/23 11:08:51.701
    Feb 24 11:08:51.701: INFO: Pod "pod-93981695-fcd1-4061-8f99-c48dc154b5e6" satisfied condition "Succeeded or Failed"
    Feb 24 11:08:51.709: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-93981695-fcd1-4061-8f99-c48dc154b5e6 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:08:51.725
    Feb 24 11:08:51.745: INFO: Waiting for pod pod-93981695-fcd1-4061-8f99-c48dc154b5e6 to disappear
    Feb 24 11:08:51.751: INFO: Pod pod-93981695-fcd1-4061-8f99-c48dc154b5e6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:08:51.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7995" for this suite. 02/24/23 11:08:51.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:08:51.779
Feb 24 11:08:51.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svc-latency 02/24/23 11:08:51.781
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:51.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:51.805
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 24 11:08:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7292 02/24/23 11:08:51.808
I0224 11:08:51.815928      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7292, replica count: 1
I0224 11:08:52.867371      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 11:08:53.868394      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:08:53.995: INFO: Created: latency-svc-vmmd6
Feb 24 11:08:53.997: INFO: Got endpoints: latency-svc-vmmd6 [28.504845ms]
Feb 24 11:08:54.064: INFO: Created: latency-svc-ct74p
Feb 24 11:08:54.107: INFO: Got endpoints: latency-svc-ct74p [109.177764ms]
Feb 24 11:08:54.118: INFO: Created: latency-svc-2mhc9
Feb 24 11:08:54.138: INFO: Got endpoints: latency-svc-2mhc9 [139.364468ms]
Feb 24 11:08:54.144: INFO: Created: latency-svc-cjdpx
Feb 24 11:08:54.155: INFO: Got endpoints: latency-svc-cjdpx [155.809777ms]
Feb 24 11:08:54.161: INFO: Created: latency-svc-fthrj
Feb 24 11:08:54.189: INFO: Got endpoints: latency-svc-fthrj [190.753804ms]
Feb 24 11:08:54.227: INFO: Created: latency-svc-4wmqf
Feb 24 11:08:54.305: INFO: Got endpoints: latency-svc-4wmqf [306.758842ms]
Feb 24 11:08:54.316: INFO: Created: latency-svc-2mvt8
Feb 24 11:08:54.329: INFO: Got endpoints: latency-svc-2mvt8 [330.79078ms]
Feb 24 11:08:54.337: INFO: Created: latency-svc-kxzrf
Feb 24 11:08:54.369: INFO: Got endpoints: latency-svc-kxzrf [370.321785ms]
Feb 24 11:08:54.380: INFO: Created: latency-svc-dnngg
Feb 24 11:08:54.435: INFO: Got endpoints: latency-svc-dnngg [436.181648ms]
Feb 24 11:08:54.448: INFO: Created: latency-svc-pw2g5
Feb 24 11:08:54.490: INFO: Got endpoints: latency-svc-pw2g5 [490.286171ms]
Feb 24 11:08:54.496: INFO: Created: latency-svc-trrqg
Feb 24 11:08:54.514: INFO: Got endpoints: latency-svc-trrqg [514.620264ms]
Feb 24 11:08:54.526: INFO: Created: latency-svc-b2vw5
Feb 24 11:08:54.540: INFO: Got endpoints: latency-svc-b2vw5 [540.351215ms]
Feb 24 11:08:54.554: INFO: Created: latency-svc-7qnk2
Feb 24 11:08:54.611: INFO: Got endpoints: latency-svc-7qnk2 [610.920682ms]
Feb 24 11:08:54.632: INFO: Created: latency-svc-rgg6f
Feb 24 11:08:54.646: INFO: Got endpoints: latency-svc-rgg6f [645.557964ms]
Feb 24 11:08:54.721: INFO: Created: latency-svc-67t6r
Feb 24 11:08:54.731: INFO: Got endpoints: latency-svc-67t6r [730.634196ms]
Feb 24 11:08:54.742: INFO: Created: latency-svc-9nwnd
Feb 24 11:08:54.754: INFO: Got endpoints: latency-svc-9nwnd [753.410629ms]
Feb 24 11:08:54.761: INFO: Created: latency-svc-dg5d5
Feb 24 11:08:54.823: INFO: Got endpoints: latency-svc-dg5d5 [715.919275ms]
Feb 24 11:08:54.831: INFO: Created: latency-svc-dpq25
Feb 24 11:08:54.867: INFO: Got endpoints: latency-svc-dpq25 [728.825805ms]
Feb 24 11:08:54.875: INFO: Created: latency-svc-ddfw9
Feb 24 11:08:54.887: INFO: Got endpoints: latency-svc-ddfw9 [731.851473ms]
Feb 24 11:08:54.899: INFO: Created: latency-svc-n25b6
Feb 24 11:08:54.909: INFO: Got endpoints: latency-svc-n25b6 [720.12233ms]
Feb 24 11:08:54.920: INFO: Created: latency-svc-vnsm7
Feb 24 11:08:54.963: INFO: Got endpoints: latency-svc-vnsm7 [657.572633ms]
Feb 24 11:08:54.970: INFO: Created: latency-svc-n2d45
Feb 24 11:08:55.040: INFO: Got endpoints: latency-svc-n2d45 [711.052606ms]
Feb 24 11:08:55.051: INFO: Created: latency-svc-7m27s
Feb 24 11:08:55.062: INFO: Got endpoints: latency-svc-7m27s [693.159396ms]
Feb 24 11:08:55.070: INFO: Created: latency-svc-jrts2
Feb 24 11:08:55.082: INFO: Got endpoints: latency-svc-jrts2 [646.317117ms]
Feb 24 11:08:55.089: INFO: Created: latency-svc-9lvcg
Feb 24 11:08:55.162: INFO: Got endpoints: latency-svc-9lvcg [672.143139ms]
Feb 24 11:08:55.184: INFO: Created: latency-svc-p8j78
Feb 24 11:08:55.196: INFO: Got endpoints: latency-svc-p8j78 [682.276166ms]
Feb 24 11:08:55.293: INFO: Created: latency-svc-rp7r8
Feb 24 11:08:55.307: INFO: Got endpoints: latency-svc-rp7r8 [766.930997ms]
Feb 24 11:08:55.321: INFO: Created: latency-svc-lf2nx
Feb 24 11:08:55.337: INFO: Got endpoints: latency-svc-lf2nx [725.684043ms]
Feb 24 11:08:55.339: INFO: Created: latency-svc-gf299
Feb 24 11:08:55.384: INFO: Got endpoints: latency-svc-gf299 [737.352186ms]
Feb 24 11:08:55.394: INFO: Created: latency-svc-mv8m2
Feb 24 11:08:55.408: INFO: Got endpoints: latency-svc-mv8m2 [676.477252ms]
Feb 24 11:08:55.472: INFO: Created: latency-svc-kmfp6
Feb 24 11:08:55.478: INFO: Got endpoints: latency-svc-kmfp6 [723.306391ms]
Feb 24 11:08:55.489: INFO: Created: latency-svc-8dg2c
Feb 24 11:08:55.506: INFO: Got endpoints: latency-svc-8dg2c [682.102667ms]
Feb 24 11:08:55.512: INFO: Created: latency-svc-9tlvz
Feb 24 11:08:55.573: INFO: Got endpoints: latency-svc-9tlvz [705.576316ms]
Feb 24 11:08:55.592: INFO: Created: latency-svc-76j7s
Feb 24 11:08:55.648: INFO: Got endpoints: latency-svc-76j7s [760.989222ms]
Feb 24 11:08:55.665: INFO: Created: latency-svc-zvjv8
Feb 24 11:08:55.671: INFO: Got endpoints: latency-svc-zvjv8 [761.422668ms]
Feb 24 11:08:55.678: INFO: Created: latency-svc-cks8l
Feb 24 11:08:55.694: INFO: Got endpoints: latency-svc-cks8l [730.737385ms]
Feb 24 11:08:55.702: INFO: Created: latency-svc-bggqw
Feb 24 11:08:55.742: INFO: Got endpoints: latency-svc-bggqw [701.21192ms]
Feb 24 11:08:55.751: INFO: Created: latency-svc-852bz
Feb 24 11:08:55.813: INFO: Got endpoints: latency-svc-852bz [750.831991ms]
Feb 24 11:08:55.826: INFO: Created: latency-svc-5jr5r
Feb 24 11:08:55.838: INFO: Got endpoints: latency-svc-5jr5r [755.698532ms]
Feb 24 11:08:55.849: INFO: Created: latency-svc-7hrrs
Feb 24 11:08:55.861: INFO: Got endpoints: latency-svc-7hrrs [699.229726ms]
Feb 24 11:08:55.871: INFO: Created: latency-svc-27l7d
Feb 24 11:08:55.922: INFO: Got endpoints: latency-svc-27l7d [725.267929ms]
Feb 24 11:08:55.929: INFO: Created: latency-svc-ccmfc
Feb 24 11:08:55.998: INFO: Got endpoints: latency-svc-ccmfc [690.329452ms]
Feb 24 11:08:56.036: INFO: Created: latency-svc-7hjtf
Feb 24 11:08:56.037: INFO: Created: latency-svc-5sz26
Feb 24 11:08:56.037: INFO: Got endpoints: latency-svc-7hjtf [700.247444ms]
Feb 24 11:08:56.045: INFO: Got endpoints: latency-svc-5sz26 [661.195422ms]
Feb 24 11:08:56.098: INFO: Created: latency-svc-vzbnv
Feb 24 11:08:56.139: INFO: Got endpoints: latency-svc-vzbnv [730.706657ms]
Feb 24 11:08:56.167: INFO: Created: latency-svc-87qs5
Feb 24 11:08:56.240: INFO: Got endpoints: latency-svc-87qs5 [762.556791ms]
Feb 24 11:08:56.255: INFO: Created: latency-svc-mn2rt
Feb 24 11:08:56.274: INFO: Got endpoints: latency-svc-mn2rt [768.242925ms]
Feb 24 11:08:56.281: INFO: Created: latency-svc-mmkx8
Feb 24 11:08:56.294: INFO: Got endpoints: latency-svc-mmkx8 [721.096522ms]
Feb 24 11:08:56.303: INFO: Created: latency-svc-ln6x2
Feb 24 11:08:56.343: INFO: Got endpoints: latency-svc-ln6x2 [694.303521ms]
Feb 24 11:08:56.352: INFO: Created: latency-svc-p7fgf
Feb 24 11:08:56.426: INFO: Got endpoints: latency-svc-p7fgf [754.570627ms]
Feb 24 11:08:56.429: INFO: Created: latency-svc-xhnc8
Feb 24 11:08:56.443: INFO: Got endpoints: latency-svc-xhnc8 [749.547073ms]
Feb 24 11:08:56.453: INFO: Created: latency-svc-4r7w5
Feb 24 11:08:56.466: INFO: Got endpoints: latency-svc-4r7w5 [724.008984ms]
Feb 24 11:08:56.486: INFO: Created: latency-svc-cpkjs
Feb 24 11:08:56.501: INFO: Got endpoints: latency-svc-cpkjs [687.490673ms]
Feb 24 11:08:56.594: INFO: Created: latency-svc-tm9mk
Feb 24 11:08:56.670: INFO: Got endpoints: latency-svc-tm9mk [831.658329ms]
Feb 24 11:08:56.679: INFO: Created: latency-svc-xzb8l
Feb 24 11:08:56.696: INFO: Got endpoints: latency-svc-xzb8l [834.356948ms]
Feb 24 11:08:56.706: INFO: Created: latency-svc-54lfg
Feb 24 11:08:56.717: INFO: Got endpoints: latency-svc-54lfg [795.178723ms]
Feb 24 11:08:56.724: INFO: Created: latency-svc-bggzd
Feb 24 11:08:56.766: INFO: Got endpoints: latency-svc-bggzd [767.964909ms]
Feb 24 11:08:56.778: INFO: Created: latency-svc-spzzx
Feb 24 11:08:56.835: INFO: Got endpoints: latency-svc-spzzx [797.833293ms]
Feb 24 11:08:56.844: INFO: Created: latency-svc-mgnx5
Feb 24 11:08:56.869: INFO: Got endpoints: latency-svc-mgnx5 [823.787749ms]
Feb 24 11:08:56.878: INFO: Created: latency-svc-5k52m
Feb 24 11:08:56.886: INFO: Got endpoints: latency-svc-5k52m [746.66418ms]
Feb 24 11:08:56.902: INFO: Created: latency-svc-d24gv
Feb 24 11:08:56.964: INFO: Got endpoints: latency-svc-d24gv [723.354362ms]
Feb 24 11:08:56.981: INFO: Created: latency-svc-6fww7
Feb 24 11:08:57.014: INFO: Got endpoints: latency-svc-6fww7 [739.825398ms]
Feb 24 11:08:57.026: INFO: Created: latency-svc-t6lvc
Feb 24 11:08:57.044: INFO: Got endpoints: latency-svc-t6lvc [749.527275ms]
Feb 24 11:08:57.067: INFO: Created: latency-svc-jvdd4
Feb 24 11:08:57.080: INFO: Got endpoints: latency-svc-jvdd4 [737.485865ms]
Feb 24 11:08:57.094: INFO: Created: latency-svc-ncdlb
Feb 24 11:08:57.163: INFO: Got endpoints: latency-svc-ncdlb [736.93669ms]
Feb 24 11:08:57.169: INFO: Created: latency-svc-f5b6r
Feb 24 11:08:57.228: INFO: Got endpoints: latency-svc-f5b6r [784.825416ms]
Feb 24 11:08:57.240: INFO: Created: latency-svc-nvkl6
Feb 24 11:08:57.251: INFO: Got endpoints: latency-svc-nvkl6 [784.072954ms]
Feb 24 11:08:57.258: INFO: Created: latency-svc-fn9sv
Feb 24 11:08:57.268: INFO: Got endpoints: latency-svc-fn9sv [767.270431ms]
Feb 24 11:08:57.281: INFO: Created: latency-svc-xnmwj
Feb 24 11:08:57.317: INFO: Got endpoints: latency-svc-xnmwj [647.546952ms]
Feb 24 11:08:57.323: INFO: Created: latency-svc-h5jtb
Feb 24 11:08:57.341: INFO: Got endpoints: latency-svc-h5jtb [644.843629ms]
Feb 24 11:08:57.418: INFO: Created: latency-svc-g25wj
Feb 24 11:08:57.431: INFO: Got endpoints: latency-svc-g25wj [713.590681ms]
Feb 24 11:08:57.440: INFO: Created: latency-svc-l2r22
Feb 24 11:08:57.450: INFO: Got endpoints: latency-svc-l2r22 [684.177615ms]
Feb 24 11:08:57.467: INFO: Created: latency-svc-6rt6k
Feb 24 11:08:57.505: INFO: Got endpoints: latency-svc-6rt6k [669.954605ms]
Feb 24 11:08:57.522: INFO: Created: latency-svc-bhdk9
Feb 24 11:08:57.576: INFO: Got endpoints: latency-svc-bhdk9 [706.410479ms]
Feb 24 11:08:57.579: INFO: Created: latency-svc-9ss2v
Feb 24 11:08:57.597: INFO: Got endpoints: latency-svc-9ss2v [711.532932ms]
Feb 24 11:08:57.603: INFO: Created: latency-svc-phqbq
Feb 24 11:08:57.621: INFO: Got endpoints: latency-svc-phqbq [657.244129ms]
Feb 24 11:08:57.641: INFO: Created: latency-svc-pph5k
Feb 24 11:08:57.714: INFO: Got endpoints: latency-svc-pph5k [699.755051ms]
Feb 24 11:08:57.722: INFO: Created: latency-svc-9gfk9
Feb 24 11:08:57.773: INFO: Got endpoints: latency-svc-9gfk9 [728.457426ms]
Feb 24 11:08:57.774: INFO: Created: latency-svc-khrpk
Feb 24 11:08:57.784: INFO: Got endpoints: latency-svc-khrpk [701.922804ms]
Feb 24 11:08:57.807: INFO: Created: latency-svc-484gp
Feb 24 11:08:57.812: INFO: Got endpoints: latency-svc-484gp [648.45262ms]
Feb 24 11:08:57.820: INFO: Created: latency-svc-5sjbf
Feb 24 11:08:57.865: INFO: Got endpoints: latency-svc-5sjbf [636.716339ms]
Feb 24 11:08:57.875: INFO: Created: latency-svc-ftgwn
Feb 24 11:08:57.942: INFO: Got endpoints: latency-svc-ftgwn [691.840236ms]
Feb 24 11:08:57.957: INFO: Created: latency-svc-x4r8p
Feb 24 11:08:57.974: INFO: Got endpoints: latency-svc-x4r8p [705.321458ms]
Feb 24 11:08:57.982: INFO: Created: latency-svc-qrzpp
Feb 24 11:08:58.003: INFO: Got endpoints: latency-svc-qrzpp [685.091062ms]
Feb 24 11:08:58.027: INFO: Created: latency-svc-rw4hv
Feb 24 11:08:58.031: INFO: Got endpoints: latency-svc-rw4hv [690.283565ms]
Feb 24 11:08:58.100: INFO: Created: latency-svc-pbqdx
Feb 24 11:08:58.112: INFO: Got endpoints: latency-svc-pbqdx [680.66774ms]
Feb 24 11:08:58.150: INFO: Created: latency-svc-lstxm
Feb 24 11:08:58.169: INFO: Got endpoints: latency-svc-lstxm [718.517909ms]
Feb 24 11:08:58.181: INFO: Created: latency-svc-lxxkj
Feb 24 11:08:58.194: INFO: Got endpoints: latency-svc-lxxkj [689.071815ms]
Feb 24 11:08:58.205: INFO: Created: latency-svc-xkhzc
Feb 24 11:08:58.278: INFO: Got endpoints: latency-svc-xkhzc [702.331873ms]
Feb 24 11:08:58.285: INFO: Created: latency-svc-8n9dt
Feb 24 11:08:58.291: INFO: Got endpoints: latency-svc-8n9dt [693.922616ms]
Feb 24 11:08:58.337: INFO: Created: latency-svc-4tcst
Feb 24 11:08:58.341: INFO: Got endpoints: latency-svc-4tcst [719.857235ms]
Feb 24 11:08:58.363: INFO: Created: latency-svc-7vs2v
Feb 24 11:08:58.381: INFO: Got endpoints: latency-svc-7vs2v [667.153032ms]
Feb 24 11:08:58.392: INFO: Created: latency-svc-rqsxp
Feb 24 11:08:58.469: INFO: Got endpoints: latency-svc-rqsxp [696.663057ms]
Feb 24 11:08:58.479: INFO: Created: latency-svc-xlx4g
Feb 24 11:08:58.555: INFO: Got endpoints: latency-svc-xlx4g [770.762829ms]
Feb 24 11:08:58.564: INFO: Created: latency-svc-94fdx
Feb 24 11:08:58.575: INFO: Got endpoints: latency-svc-94fdx [762.353769ms]
Feb 24 11:08:58.588: INFO: Created: latency-svc-9pkl7
Feb 24 11:08:58.606: INFO: Got endpoints: latency-svc-9pkl7 [740.506831ms]
Feb 24 11:08:58.614: INFO: Created: latency-svc-4tlz5
Feb 24 11:08:58.623: INFO: Got endpoints: latency-svc-4tlz5 [680.265851ms]
Feb 24 11:08:58.693: INFO: Created: latency-svc-5xzqk
Feb 24 11:08:58.774: INFO: Got endpoints: latency-svc-5xzqk [800.252511ms]
Feb 24 11:08:58.786: INFO: Created: latency-svc-852dk
Feb 24 11:08:58.800: INFO: Got endpoints: latency-svc-852dk [797.642289ms]
Feb 24 11:08:58.813: INFO: Created: latency-svc-s76nt
Feb 24 11:08:58.829: INFO: Got endpoints: latency-svc-s76nt [797.861741ms]
Feb 24 11:08:58.835: INFO: Created: latency-svc-26l5g
Feb 24 11:08:58.894: INFO: Got endpoints: latency-svc-26l5g [781.624773ms]
Feb 24 11:08:58.906: INFO: Created: latency-svc-b85dn
Feb 24 11:08:58.916: INFO: Got endpoints: latency-svc-b85dn [747.492265ms]
Feb 24 11:08:58.973: INFO: Created: latency-svc-mr4ng
Feb 24 11:08:58.995: INFO: Got endpoints: latency-svc-mr4ng [800.757178ms]
Feb 24 11:08:59.001: INFO: Created: latency-svc-bszbn
Feb 24 11:08:59.016: INFO: Got endpoints: latency-svc-bszbn [737.880678ms]
Feb 24 11:08:59.029: INFO: Created: latency-svc-xd6qx
Feb 24 11:08:59.111: INFO: Got endpoints: latency-svc-xd6qx [818.924072ms]
Feb 24 11:08:59.117: INFO: Created: latency-svc-c2x49
Feb 24 11:08:59.197: INFO: Got endpoints: latency-svc-c2x49 [855.642054ms]
Feb 24 11:08:59.209: INFO: Created: latency-svc-pj44s
Feb 24 11:08:59.222: INFO: Got endpoints: latency-svc-pj44s [840.624709ms]
Feb 24 11:08:59.716: INFO: Created: latency-svc-bm6tx
Feb 24 11:08:59.720: INFO: Created: latency-svc-hnq7t
Feb 24 11:08:59.721: INFO: Created: latency-svc-2m5vs
Feb 24 11:08:59.732: INFO: Created: latency-svc-s9m78
Feb 24 11:08:59.732: INFO: Created: latency-svc-wjsbh
Feb 24 11:08:59.732: INFO: Created: latency-svc-ptltp
Feb 24 11:08:59.732: INFO: Created: latency-svc-fvfvf
Feb 24 11:08:59.732: INFO: Created: latency-svc-km658
Feb 24 11:08:59.744: INFO: Got endpoints: latency-svc-bm6tx [546.864912ms]
Feb 24 11:08:59.754: INFO: Created: latency-svc-rmtb9
Feb 24 11:08:59.754: INFO: Created: latency-svc-gz9vr
Feb 24 11:08:59.754: INFO: Created: latency-svc-bk55h
Feb 24 11:08:59.757: INFO: Created: latency-svc-rn5pq
Feb 24 11:08:59.757: INFO: Created: latency-svc-9bbnq
Feb 24 11:08:59.759: INFO: Created: latency-svc-9xhbx
Feb 24 11:08:59.759: INFO: Created: latency-svc-rsz6s
Feb 24 11:08:59.763: INFO: Got endpoints: latency-svc-ptltp [1.15731132s]
Feb 24 11:08:59.764: INFO: Got endpoints: latency-svc-hnq7t [541.132599ms]
Feb 24 11:08:59.764: INFO: Got endpoints: latency-svc-2m5vs [1.294053342s]
Feb 24 11:08:59.769: INFO: Got endpoints: latency-svc-wjsbh [1.21388322s]
Feb 24 11:08:59.776: INFO: Got endpoints: latency-svc-fvfvf [1.001683127s]
Feb 24 11:08:59.840: INFO: Got endpoints: latency-svc-km658 [1.264842834s]
Feb 24 11:08:59.845: INFO: Got endpoints: latency-svc-s9m78 [1.221867694s]
Feb 24 11:08:59.846: INFO: Got endpoints: latency-svc-bk55h [1.044929928s]
Feb 24 11:08:59.855: INFO: Created: latency-svc-4mk6d
Feb 24 11:08:59.856: INFO: Got endpoints: latency-svc-gz9vr [1.026096978s]
Feb 24 11:08:59.856: INFO: Got endpoints: latency-svc-9bbnq [962.063497ms]
Feb 24 11:08:59.863: INFO: Got endpoints: latency-svc-rn5pq [946.560037ms]
Feb 24 11:08:59.876: INFO: Got endpoints: latency-svc-rsz6s [765.457793ms]
Feb 24 11:08:59.888: INFO: Created: latency-svc-9sscn
Feb 24 11:08:59.896: INFO: Got endpoints: latency-svc-rmtb9 [879.233999ms]
Feb 24 11:08:59.896: INFO: Got endpoints: latency-svc-4mk6d [152.143352ms]
Feb 24 11:08:59.896: INFO: Got endpoints: latency-svc-9xhbx [900.437819ms]
Feb 24 11:08:59.904: INFO: Got endpoints: latency-svc-9sscn [141.058781ms]
Feb 24 11:08:59.947: INFO: Created: latency-svc-8vmsc
Feb 24 11:08:59.959: INFO: Got endpoints: latency-svc-8vmsc [195.777372ms]
Feb 24 11:09:00.051: INFO: Created: latency-svc-zbsc7
Feb 24 11:09:00.060: INFO: Got endpoints: latency-svc-zbsc7 [296.034581ms]
Feb 24 11:09:00.086: INFO: Created: latency-svc-h2v4x
Feb 24 11:09:00.096: INFO: Got endpoints: latency-svc-h2v4x [326.708205ms]
Feb 24 11:09:00.106: INFO: Created: latency-svc-tcr29
Feb 24 11:09:00.184: INFO: Got endpoints: latency-svc-tcr29 [406.954981ms]
Feb 24 11:09:00.297: INFO: Created: latency-svc-w2snq
Feb 24 11:09:00.342: INFO: Got endpoints: latency-svc-w2snq [501.437819ms]
Feb 24 11:09:00.346: INFO: Created: latency-svc-h5bnj
Feb 24 11:09:00.368: INFO: Got endpoints: latency-svc-h5bnj [522.705452ms]
Feb 24 11:09:00.392: INFO: Created: latency-svc-4vc2h
Feb 24 11:09:00.417: INFO: Got endpoints: latency-svc-4vc2h [571.246306ms]
Feb 24 11:09:00.432: INFO: Created: latency-svc-rkdjq
Feb 24 11:09:00.449: INFO: Got endpoints: latency-svc-rkdjq [593.181436ms]
Feb 24 11:09:00.500: INFO: Created: latency-svc-rzmlt
Feb 24 11:09:00.558: INFO: Got endpoints: latency-svc-rzmlt [702.073596ms]
Feb 24 11:09:00.570: INFO: Created: latency-svc-tf4zw
Feb 24 11:09:00.586: INFO: Got endpoints: latency-svc-tf4zw [722.125884ms]
Feb 24 11:09:00.597: INFO: Created: latency-svc-xrxpl
Feb 24 11:09:00.610: INFO: Got endpoints: latency-svc-xrxpl [733.578128ms]
Feb 24 11:09:00.619: INFO: Created: latency-svc-b58cq
Feb 24 11:09:00.697: INFO: Got endpoints: latency-svc-b58cq [801.583804ms]
Feb 24 11:09:00.711: INFO: Created: latency-svc-6szzh
Feb 24 11:09:00.715: INFO: Got endpoints: latency-svc-6szzh [819.202578ms]
Feb 24 11:09:00.755: INFO: Created: latency-svc-46k9z
Feb 24 11:09:00.768: INFO: Got endpoints: latency-svc-46k9z [871.375093ms]
Feb 24 11:09:00.779: INFO: Created: latency-svc-69pvr
Feb 24 11:09:00.787: INFO: Got endpoints: latency-svc-69pvr [882.652551ms]
Feb 24 11:09:00.794: INFO: Created: latency-svc-zmvhd
Feb 24 11:09:00.865: INFO: Got endpoints: latency-svc-zmvhd [904.761408ms]
Feb 24 11:09:00.873: INFO: Created: latency-svc-2wdmw
Feb 24 11:09:00.955: INFO: Got endpoints: latency-svc-2wdmw [894.749949ms]
Feb 24 11:09:00.974: INFO: Created: latency-svc-m6lzp
Feb 24 11:09:00.990: INFO: Got endpoints: latency-svc-m6lzp [893.074418ms]
Feb 24 11:09:00.994: INFO: Created: latency-svc-55rp9
Feb 24 11:09:01.003: INFO: Got endpoints: latency-svc-55rp9 [818.931473ms]
Feb 24 11:09:01.031: INFO: Created: latency-svc-cnnn8
Feb 24 11:09:01.086: INFO: Got endpoints: latency-svc-cnnn8 [743.798642ms]
Feb 24 11:09:01.091: INFO: Created: latency-svc-74z85
Feb 24 11:09:01.152: INFO: Got endpoints: latency-svc-74z85 [783.445353ms]
Feb 24 11:09:01.165: INFO: Created: latency-svc-cqj97
Feb 24 11:09:01.187: INFO: Got endpoints: latency-svc-cqj97 [769.811931ms]
Feb 24 11:09:01.203: INFO: Created: latency-svc-2rtv4
Feb 24 11:09:01.216: INFO: Got endpoints: latency-svc-2rtv4 [766.635431ms]
Feb 24 11:09:01.229: INFO: Created: latency-svc-kmvcs
Feb 24 11:09:01.288: INFO: Got endpoints: latency-svc-kmvcs [729.279924ms]
Feb 24 11:09:01.308: INFO: Created: latency-svc-fqldx
Feb 24 11:09:01.332: INFO: Got endpoints: latency-svc-fqldx [746.157941ms]
Feb 24 11:09:01.383: INFO: Created: latency-svc-gpgqs
Feb 24 11:09:01.399: INFO: Got endpoints: latency-svc-gpgqs [788.68637ms]
Feb 24 11:09:01.408: INFO: Created: latency-svc-wkdf2
Feb 24 11:09:01.423: INFO: Got endpoints: latency-svc-wkdf2 [725.186372ms]
Feb 24 11:09:01.428: INFO: Created: latency-svc-sfmzq
Feb 24 11:09:01.517: INFO: Got endpoints: latency-svc-sfmzq [802.044317ms]
Feb 24 11:09:01.527: INFO: Created: latency-svc-dgr7k
Feb 24 11:09:01.589: INFO: Got endpoints: latency-svc-dgr7k [820.978762ms]
Feb 24 11:09:01.594: INFO: Created: latency-svc-k6qfr
Feb 24 11:09:01.618: INFO: Got endpoints: latency-svc-k6qfr [830.306772ms]
Feb 24 11:09:01.646: INFO: Created: latency-svc-sslpn
Feb 24 11:09:01.656: INFO: Got endpoints: latency-svc-sslpn [791.638224ms]
Feb 24 11:09:01.671: INFO: Created: latency-svc-q7qqk
Feb 24 11:09:01.745: INFO: Got endpoints: latency-svc-q7qqk [789.15672ms]
Feb 24 11:09:01.754: INFO: Created: latency-svc-57pck
Feb 24 11:09:01.787: INFO: Got endpoints: latency-svc-57pck [797.007924ms]
Feb 24 11:09:01.798: INFO: Created: latency-svc-8wwdq
Feb 24 11:09:01.814: INFO: Got endpoints: latency-svc-8wwdq [811.322628ms]
Feb 24 11:09:02.587: INFO: Created: latency-svc-jfptt
Feb 24 11:09:02.593: INFO: Created: latency-svc-5895x
Feb 24 11:09:02.626: INFO: Created: latency-svc-fbh92
Feb 24 11:09:02.629: INFO: Created: latency-svc-b6fv7
Feb 24 11:09:02.731: INFO: Got endpoints: latency-svc-jfptt [943.811262ms]
Feb 24 11:09:02.731: INFO: Got endpoints: latency-svc-5895x [1.643409932s]
Feb 24 11:09:02.738: INFO: Created: latency-svc-6rp4t
Feb 24 11:09:02.738: INFO: Created: latency-svc-pqqpx
Feb 24 11:09:02.739: INFO: Created: latency-svc-lrnjh
Feb 24 11:09:02.739: INFO: Created: latency-svc-t5twx
Feb 24 11:09:02.740: INFO: Created: latency-svc-hrtcx
Feb 24 11:09:02.740: INFO: Created: latency-svc-8dlvc
Feb 24 11:09:02.741: INFO: Created: latency-svc-9c22s
Feb 24 11:09:02.741: INFO: Created: latency-svc-d7w2j
Feb 24 11:09:02.741: INFO: Created: latency-svc-t8pwd
Feb 24 11:09:02.742: INFO: Created: latency-svc-h8bjb
Feb 24 11:09:02.742: INFO: Created: latency-svc-zshvr
Feb 24 11:09:02.765: INFO: Got endpoints: latency-svc-t8pwd [1.577362155s]
Feb 24 11:09:02.766: INFO: Got endpoints: latency-svc-b6fv7 [950.298569ms]
Feb 24 11:09:02.776: INFO: Got endpoints: latency-svc-8dlvc [1.56008827s]
Feb 24 11:09:02.776: INFO: Got endpoints: latency-svc-9c22s [1.444249461s]
Feb 24 11:09:02.797: INFO: Got endpoints: latency-svc-fbh92 [1.644720222s]
Feb 24 11:09:02.857: INFO: Got endpoints: latency-svc-d7w2j [1.458364241s]
Feb 24 11:09:02.875: INFO: Got endpoints: latency-svc-h8bjb [1.218779094s]
Feb 24 11:09:02.876: INFO: Got endpoints: latency-svc-zshvr [1.358332713s]
Feb 24 11:09:02.876: INFO: Got endpoints: latency-svc-hrtcx [1.588561224s]
Feb 24 11:09:02.877: INFO: Got endpoints: latency-svc-pqqpx [1.453720519s]
Feb 24 11:09:02.893: INFO: Got endpoints: latency-svc-lrnjh [1.30425492s]
Feb 24 11:09:02.893: INFO: Created: latency-svc-zvk6h
Feb 24 11:09:02.903: INFO: Got endpoints: latency-svc-t5twx [1.158444033s]
Feb 24 11:09:02.904: INFO: Got endpoints: latency-svc-6rp4t [1.285896807s]
Feb 24 11:09:02.931: INFO: Got endpoints: latency-svc-zvk6h [200.009194ms]
Feb 24 11:09:02.932: INFO: Created: latency-svc-cggjk
Feb 24 11:09:02.947: INFO: Got endpoints: latency-svc-cggjk [215.99128ms]
Feb 24 11:09:02.954: INFO: Created: latency-svc-rm2fx
Feb 24 11:09:03.059: INFO: Got endpoints: latency-svc-rm2fx [294.292174ms]
Feb 24 11:09:03.078: INFO: Created: latency-svc-m8qwz
Feb 24 11:09:03.105: INFO: Created: latency-svc-d76cn
Feb 24 11:09:03.105: INFO: Got endpoints: latency-svc-m8qwz [339.710863ms]
Feb 24 11:09:03.117: INFO: Got endpoints: latency-svc-d76cn [340.662361ms]
Feb 24 11:09:03.135: INFO: Created: latency-svc-mlxxx
Feb 24 11:09:03.142: INFO: Got endpoints: latency-svc-mlxxx [365.448882ms]
Feb 24 11:09:03.161: INFO: Created: latency-svc-kkrgd
Feb 24 11:09:03.173: INFO: Got endpoints: latency-svc-kkrgd [375.815847ms]
Feb 24 11:09:03.223: INFO: Created: latency-svc-56qsp
Feb 24 11:09:03.246: INFO: Got endpoints: latency-svc-56qsp [388.863401ms]
Feb 24 11:09:03.258: INFO: Created: latency-svc-ffcfg
Feb 24 11:09:03.276: INFO: Got endpoints: latency-svc-ffcfg [399.964721ms]
Feb 24 11:09:03.284: INFO: Created: latency-svc-jw9z7
Feb 24 11:09:03.348: INFO: Got endpoints: latency-svc-jw9z7 [471.715516ms]
Feb 24 11:09:03.368: INFO: Created: latency-svc-vkrqk
Feb 24 11:09:03.417: INFO: Got endpoints: latency-svc-vkrqk [540.385343ms]
Feb 24 11:09:03.430: INFO: Created: latency-svc-pz8zb
Feb 24 11:09:03.451: INFO: Got endpoints: latency-svc-pz8zb [574.839691ms]
Feb 24 11:09:03.470: INFO: Created: latency-svc-tkpkb
Feb 24 11:09:03.482: INFO: Got endpoints: latency-svc-tkpkb [588.575902ms]
Feb 24 11:09:03.493: INFO: Created: latency-svc-dh8mh
Feb 24 11:09:03.636: INFO: Got endpoints: latency-svc-dh8mh [732.854749ms]
Feb 24 11:09:03.648: INFO: Created: latency-svc-wb6jm
Feb 24 11:09:03.681: INFO: Got endpoints: latency-svc-wb6jm [777.224769ms]
Feb 24 11:09:03.689: INFO: Created: latency-svc-srjmm
Feb 24 11:09:03.704: INFO: Got endpoints: latency-svc-srjmm [773.347218ms]
Feb 24 11:09:03.709: INFO: Created: latency-svc-fw557
Feb 24 11:09:03.722: INFO: Got endpoints: latency-svc-fw557 [774.003419ms]
Feb 24 11:09:03.726: INFO: Created: latency-svc-bq6kg
Feb 24 11:09:03.800: INFO: Got endpoints: latency-svc-bq6kg [740.701459ms]
Feb 24 11:09:03.805: INFO: Created: latency-svc-rb9gk
Feb 24 11:09:03.859: INFO: Got endpoints: latency-svc-rb9gk [753.777933ms]
Feb 24 11:09:03.903: INFO: Created: latency-svc-qc2bg
Feb 24 11:09:03.918: INFO: Got endpoints: latency-svc-qc2bg [800.093778ms]
Feb 24 11:09:03.940: INFO: Created: latency-svc-s9wfn
Feb 24 11:09:03.967: INFO: Got endpoints: latency-svc-s9wfn [824.932413ms]
Feb 24 11:09:03.979: INFO: Created: latency-svc-rmdn5
Feb 24 11:09:04.059: INFO: Got endpoints: latency-svc-rmdn5 [885.85059ms]
Feb 24 11:09:04.067: INFO: Created: latency-svc-g48s4
Feb 24 11:09:04.150: INFO: Got endpoints: latency-svc-g48s4 [903.938686ms]
Feb 24 11:09:04.171: INFO: Created: latency-svc-hgf5m
Feb 24 11:09:04.215: INFO: Got endpoints: latency-svc-hgf5m [938.66581ms]
Feb 24 11:09:04.215: INFO: Created: latency-svc-v5j6m
Feb 24 11:09:04.255: INFO: Got endpoints: latency-svc-v5j6m [904.53723ms]
Feb 24 11:09:04.269: INFO: Created: latency-svc-6d9g4
Feb 24 11:09:04.335: INFO: Got endpoints: latency-svc-6d9g4 [918.210733ms]
Feb 24 11:09:04.341: INFO: Created: latency-svc-pntsm
Feb 24 11:09:04.351: INFO: Got endpoints: latency-svc-pntsm [899.78562ms]
Feb 24 11:09:04.420: INFO: Created: latency-svc-flkdw
Feb 24 11:09:04.431: INFO: Got endpoints: latency-svc-flkdw [948.87396ms]
Feb 24 11:09:04.431: INFO: Latencies: [109.177764ms 139.364468ms 141.058781ms 152.143352ms 155.809777ms 190.753804ms 195.777372ms 200.009194ms 215.99128ms 294.292174ms 296.034581ms 306.758842ms 326.708205ms 330.79078ms 339.710863ms 340.662361ms 365.448882ms 370.321785ms 375.815847ms 388.863401ms 399.964721ms 406.954981ms 436.181648ms 471.715516ms 490.286171ms 501.437819ms 514.620264ms 522.705452ms 540.351215ms 540.385343ms 541.132599ms 546.864912ms 571.246306ms 574.839691ms 588.575902ms 593.181436ms 610.920682ms 636.716339ms 644.843629ms 645.557964ms 646.317117ms 647.546952ms 648.45262ms 657.244129ms 657.572633ms 661.195422ms 667.153032ms 669.954605ms 672.143139ms 676.477252ms 680.265851ms 680.66774ms 682.102667ms 682.276166ms 684.177615ms 685.091062ms 687.490673ms 689.071815ms 690.283565ms 690.329452ms 691.840236ms 693.159396ms 693.922616ms 694.303521ms 696.663057ms 699.229726ms 699.755051ms 700.247444ms 701.21192ms 701.922804ms 702.073596ms 702.331873ms 705.321458ms 705.576316ms 706.410479ms 711.052606ms 711.532932ms 713.590681ms 715.919275ms 718.517909ms 719.857235ms 720.12233ms 721.096522ms 722.125884ms 723.306391ms 723.354362ms 724.008984ms 725.186372ms 725.267929ms 725.684043ms 728.457426ms 728.825805ms 729.279924ms 730.634196ms 730.706657ms 730.737385ms 731.851473ms 732.854749ms 733.578128ms 736.93669ms 737.352186ms 737.485865ms 737.880678ms 739.825398ms 740.506831ms 740.701459ms 743.798642ms 746.157941ms 746.66418ms 747.492265ms 749.527275ms 749.547073ms 750.831991ms 753.410629ms 753.777933ms 754.570627ms 755.698532ms 760.989222ms 761.422668ms 762.353769ms 762.556791ms 765.457793ms 766.635431ms 766.930997ms 767.270431ms 767.964909ms 768.242925ms 769.811931ms 770.762829ms 773.347218ms 774.003419ms 777.224769ms 781.624773ms 783.445353ms 784.072954ms 784.825416ms 788.68637ms 789.15672ms 791.638224ms 795.178723ms 797.007924ms 797.642289ms 797.833293ms 797.861741ms 800.093778ms 800.252511ms 800.757178ms 801.583804ms 802.044317ms 811.322628ms 818.924072ms 818.931473ms 819.202578ms 820.978762ms 823.787749ms 824.932413ms 830.306772ms 831.658329ms 834.356948ms 840.624709ms 855.642054ms 871.375093ms 879.233999ms 882.652551ms 885.85059ms 893.074418ms 894.749949ms 899.78562ms 900.437819ms 903.938686ms 904.53723ms 904.761408ms 918.210733ms 938.66581ms 943.811262ms 946.560037ms 948.87396ms 950.298569ms 962.063497ms 1.001683127s 1.026096978s 1.044929928s 1.15731132s 1.158444033s 1.21388322s 1.218779094s 1.221867694s 1.264842834s 1.285896807s 1.294053342s 1.30425492s 1.358332713s 1.444249461s 1.453720519s 1.458364241s 1.56008827s 1.577362155s 1.588561224s 1.643409932s 1.644720222s]
Feb 24 11:09:04.431: INFO: 50 %ile: 737.352186ms
Feb 24 11:09:04.431: INFO: 90 %ile: 1.026096978s
Feb 24 11:09:04.431: INFO: 99 %ile: 1.643409932s
Feb 24 11:09:04.431: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Feb 24 11:09:04.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7292" for this suite. 02/24/23 11:09:04.452
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":39,"skipped":565,"failed":0}
------------------------------
• [SLOW TEST] [12.691 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:08:51.779
    Feb 24 11:08:51.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svc-latency 02/24/23 11:08:51.781
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:08:51.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:08:51.805
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 24 11:08:51.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7292 02/24/23 11:08:51.808
    I0224 11:08:51.815928      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7292, replica count: 1
    I0224 11:08:52.867371      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0224 11:08:53.868394      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:08:53.995: INFO: Created: latency-svc-vmmd6
    Feb 24 11:08:53.997: INFO: Got endpoints: latency-svc-vmmd6 [28.504845ms]
    Feb 24 11:08:54.064: INFO: Created: latency-svc-ct74p
    Feb 24 11:08:54.107: INFO: Got endpoints: latency-svc-ct74p [109.177764ms]
    Feb 24 11:08:54.118: INFO: Created: latency-svc-2mhc9
    Feb 24 11:08:54.138: INFO: Got endpoints: latency-svc-2mhc9 [139.364468ms]
    Feb 24 11:08:54.144: INFO: Created: latency-svc-cjdpx
    Feb 24 11:08:54.155: INFO: Got endpoints: latency-svc-cjdpx [155.809777ms]
    Feb 24 11:08:54.161: INFO: Created: latency-svc-fthrj
    Feb 24 11:08:54.189: INFO: Got endpoints: latency-svc-fthrj [190.753804ms]
    Feb 24 11:08:54.227: INFO: Created: latency-svc-4wmqf
    Feb 24 11:08:54.305: INFO: Got endpoints: latency-svc-4wmqf [306.758842ms]
    Feb 24 11:08:54.316: INFO: Created: latency-svc-2mvt8
    Feb 24 11:08:54.329: INFO: Got endpoints: latency-svc-2mvt8 [330.79078ms]
    Feb 24 11:08:54.337: INFO: Created: latency-svc-kxzrf
    Feb 24 11:08:54.369: INFO: Got endpoints: latency-svc-kxzrf [370.321785ms]
    Feb 24 11:08:54.380: INFO: Created: latency-svc-dnngg
    Feb 24 11:08:54.435: INFO: Got endpoints: latency-svc-dnngg [436.181648ms]
    Feb 24 11:08:54.448: INFO: Created: latency-svc-pw2g5
    Feb 24 11:08:54.490: INFO: Got endpoints: latency-svc-pw2g5 [490.286171ms]
    Feb 24 11:08:54.496: INFO: Created: latency-svc-trrqg
    Feb 24 11:08:54.514: INFO: Got endpoints: latency-svc-trrqg [514.620264ms]
    Feb 24 11:08:54.526: INFO: Created: latency-svc-b2vw5
    Feb 24 11:08:54.540: INFO: Got endpoints: latency-svc-b2vw5 [540.351215ms]
    Feb 24 11:08:54.554: INFO: Created: latency-svc-7qnk2
    Feb 24 11:08:54.611: INFO: Got endpoints: latency-svc-7qnk2 [610.920682ms]
    Feb 24 11:08:54.632: INFO: Created: latency-svc-rgg6f
    Feb 24 11:08:54.646: INFO: Got endpoints: latency-svc-rgg6f [645.557964ms]
    Feb 24 11:08:54.721: INFO: Created: latency-svc-67t6r
    Feb 24 11:08:54.731: INFO: Got endpoints: latency-svc-67t6r [730.634196ms]
    Feb 24 11:08:54.742: INFO: Created: latency-svc-9nwnd
    Feb 24 11:08:54.754: INFO: Got endpoints: latency-svc-9nwnd [753.410629ms]
    Feb 24 11:08:54.761: INFO: Created: latency-svc-dg5d5
    Feb 24 11:08:54.823: INFO: Got endpoints: latency-svc-dg5d5 [715.919275ms]
    Feb 24 11:08:54.831: INFO: Created: latency-svc-dpq25
    Feb 24 11:08:54.867: INFO: Got endpoints: latency-svc-dpq25 [728.825805ms]
    Feb 24 11:08:54.875: INFO: Created: latency-svc-ddfw9
    Feb 24 11:08:54.887: INFO: Got endpoints: latency-svc-ddfw9 [731.851473ms]
    Feb 24 11:08:54.899: INFO: Created: latency-svc-n25b6
    Feb 24 11:08:54.909: INFO: Got endpoints: latency-svc-n25b6 [720.12233ms]
    Feb 24 11:08:54.920: INFO: Created: latency-svc-vnsm7
    Feb 24 11:08:54.963: INFO: Got endpoints: latency-svc-vnsm7 [657.572633ms]
    Feb 24 11:08:54.970: INFO: Created: latency-svc-n2d45
    Feb 24 11:08:55.040: INFO: Got endpoints: latency-svc-n2d45 [711.052606ms]
    Feb 24 11:08:55.051: INFO: Created: latency-svc-7m27s
    Feb 24 11:08:55.062: INFO: Got endpoints: latency-svc-7m27s [693.159396ms]
    Feb 24 11:08:55.070: INFO: Created: latency-svc-jrts2
    Feb 24 11:08:55.082: INFO: Got endpoints: latency-svc-jrts2 [646.317117ms]
    Feb 24 11:08:55.089: INFO: Created: latency-svc-9lvcg
    Feb 24 11:08:55.162: INFO: Got endpoints: latency-svc-9lvcg [672.143139ms]
    Feb 24 11:08:55.184: INFO: Created: latency-svc-p8j78
    Feb 24 11:08:55.196: INFO: Got endpoints: latency-svc-p8j78 [682.276166ms]
    Feb 24 11:08:55.293: INFO: Created: latency-svc-rp7r8
    Feb 24 11:08:55.307: INFO: Got endpoints: latency-svc-rp7r8 [766.930997ms]
    Feb 24 11:08:55.321: INFO: Created: latency-svc-lf2nx
    Feb 24 11:08:55.337: INFO: Got endpoints: latency-svc-lf2nx [725.684043ms]
    Feb 24 11:08:55.339: INFO: Created: latency-svc-gf299
    Feb 24 11:08:55.384: INFO: Got endpoints: latency-svc-gf299 [737.352186ms]
    Feb 24 11:08:55.394: INFO: Created: latency-svc-mv8m2
    Feb 24 11:08:55.408: INFO: Got endpoints: latency-svc-mv8m2 [676.477252ms]
    Feb 24 11:08:55.472: INFO: Created: latency-svc-kmfp6
    Feb 24 11:08:55.478: INFO: Got endpoints: latency-svc-kmfp6 [723.306391ms]
    Feb 24 11:08:55.489: INFO: Created: latency-svc-8dg2c
    Feb 24 11:08:55.506: INFO: Got endpoints: latency-svc-8dg2c [682.102667ms]
    Feb 24 11:08:55.512: INFO: Created: latency-svc-9tlvz
    Feb 24 11:08:55.573: INFO: Got endpoints: latency-svc-9tlvz [705.576316ms]
    Feb 24 11:08:55.592: INFO: Created: latency-svc-76j7s
    Feb 24 11:08:55.648: INFO: Got endpoints: latency-svc-76j7s [760.989222ms]
    Feb 24 11:08:55.665: INFO: Created: latency-svc-zvjv8
    Feb 24 11:08:55.671: INFO: Got endpoints: latency-svc-zvjv8 [761.422668ms]
    Feb 24 11:08:55.678: INFO: Created: latency-svc-cks8l
    Feb 24 11:08:55.694: INFO: Got endpoints: latency-svc-cks8l [730.737385ms]
    Feb 24 11:08:55.702: INFO: Created: latency-svc-bggqw
    Feb 24 11:08:55.742: INFO: Got endpoints: latency-svc-bggqw [701.21192ms]
    Feb 24 11:08:55.751: INFO: Created: latency-svc-852bz
    Feb 24 11:08:55.813: INFO: Got endpoints: latency-svc-852bz [750.831991ms]
    Feb 24 11:08:55.826: INFO: Created: latency-svc-5jr5r
    Feb 24 11:08:55.838: INFO: Got endpoints: latency-svc-5jr5r [755.698532ms]
    Feb 24 11:08:55.849: INFO: Created: latency-svc-7hrrs
    Feb 24 11:08:55.861: INFO: Got endpoints: latency-svc-7hrrs [699.229726ms]
    Feb 24 11:08:55.871: INFO: Created: latency-svc-27l7d
    Feb 24 11:08:55.922: INFO: Got endpoints: latency-svc-27l7d [725.267929ms]
    Feb 24 11:08:55.929: INFO: Created: latency-svc-ccmfc
    Feb 24 11:08:55.998: INFO: Got endpoints: latency-svc-ccmfc [690.329452ms]
    Feb 24 11:08:56.036: INFO: Created: latency-svc-7hjtf
    Feb 24 11:08:56.037: INFO: Created: latency-svc-5sz26
    Feb 24 11:08:56.037: INFO: Got endpoints: latency-svc-7hjtf [700.247444ms]
    Feb 24 11:08:56.045: INFO: Got endpoints: latency-svc-5sz26 [661.195422ms]
    Feb 24 11:08:56.098: INFO: Created: latency-svc-vzbnv
    Feb 24 11:08:56.139: INFO: Got endpoints: latency-svc-vzbnv [730.706657ms]
    Feb 24 11:08:56.167: INFO: Created: latency-svc-87qs5
    Feb 24 11:08:56.240: INFO: Got endpoints: latency-svc-87qs5 [762.556791ms]
    Feb 24 11:08:56.255: INFO: Created: latency-svc-mn2rt
    Feb 24 11:08:56.274: INFO: Got endpoints: latency-svc-mn2rt [768.242925ms]
    Feb 24 11:08:56.281: INFO: Created: latency-svc-mmkx8
    Feb 24 11:08:56.294: INFO: Got endpoints: latency-svc-mmkx8 [721.096522ms]
    Feb 24 11:08:56.303: INFO: Created: latency-svc-ln6x2
    Feb 24 11:08:56.343: INFO: Got endpoints: latency-svc-ln6x2 [694.303521ms]
    Feb 24 11:08:56.352: INFO: Created: latency-svc-p7fgf
    Feb 24 11:08:56.426: INFO: Got endpoints: latency-svc-p7fgf [754.570627ms]
    Feb 24 11:08:56.429: INFO: Created: latency-svc-xhnc8
    Feb 24 11:08:56.443: INFO: Got endpoints: latency-svc-xhnc8 [749.547073ms]
    Feb 24 11:08:56.453: INFO: Created: latency-svc-4r7w5
    Feb 24 11:08:56.466: INFO: Got endpoints: latency-svc-4r7w5 [724.008984ms]
    Feb 24 11:08:56.486: INFO: Created: latency-svc-cpkjs
    Feb 24 11:08:56.501: INFO: Got endpoints: latency-svc-cpkjs [687.490673ms]
    Feb 24 11:08:56.594: INFO: Created: latency-svc-tm9mk
    Feb 24 11:08:56.670: INFO: Got endpoints: latency-svc-tm9mk [831.658329ms]
    Feb 24 11:08:56.679: INFO: Created: latency-svc-xzb8l
    Feb 24 11:08:56.696: INFO: Got endpoints: latency-svc-xzb8l [834.356948ms]
    Feb 24 11:08:56.706: INFO: Created: latency-svc-54lfg
    Feb 24 11:08:56.717: INFO: Got endpoints: latency-svc-54lfg [795.178723ms]
    Feb 24 11:08:56.724: INFO: Created: latency-svc-bggzd
    Feb 24 11:08:56.766: INFO: Got endpoints: latency-svc-bggzd [767.964909ms]
    Feb 24 11:08:56.778: INFO: Created: latency-svc-spzzx
    Feb 24 11:08:56.835: INFO: Got endpoints: latency-svc-spzzx [797.833293ms]
    Feb 24 11:08:56.844: INFO: Created: latency-svc-mgnx5
    Feb 24 11:08:56.869: INFO: Got endpoints: latency-svc-mgnx5 [823.787749ms]
    Feb 24 11:08:56.878: INFO: Created: latency-svc-5k52m
    Feb 24 11:08:56.886: INFO: Got endpoints: latency-svc-5k52m [746.66418ms]
    Feb 24 11:08:56.902: INFO: Created: latency-svc-d24gv
    Feb 24 11:08:56.964: INFO: Got endpoints: latency-svc-d24gv [723.354362ms]
    Feb 24 11:08:56.981: INFO: Created: latency-svc-6fww7
    Feb 24 11:08:57.014: INFO: Got endpoints: latency-svc-6fww7 [739.825398ms]
    Feb 24 11:08:57.026: INFO: Created: latency-svc-t6lvc
    Feb 24 11:08:57.044: INFO: Got endpoints: latency-svc-t6lvc [749.527275ms]
    Feb 24 11:08:57.067: INFO: Created: latency-svc-jvdd4
    Feb 24 11:08:57.080: INFO: Got endpoints: latency-svc-jvdd4 [737.485865ms]
    Feb 24 11:08:57.094: INFO: Created: latency-svc-ncdlb
    Feb 24 11:08:57.163: INFO: Got endpoints: latency-svc-ncdlb [736.93669ms]
    Feb 24 11:08:57.169: INFO: Created: latency-svc-f5b6r
    Feb 24 11:08:57.228: INFO: Got endpoints: latency-svc-f5b6r [784.825416ms]
    Feb 24 11:08:57.240: INFO: Created: latency-svc-nvkl6
    Feb 24 11:08:57.251: INFO: Got endpoints: latency-svc-nvkl6 [784.072954ms]
    Feb 24 11:08:57.258: INFO: Created: latency-svc-fn9sv
    Feb 24 11:08:57.268: INFO: Got endpoints: latency-svc-fn9sv [767.270431ms]
    Feb 24 11:08:57.281: INFO: Created: latency-svc-xnmwj
    Feb 24 11:08:57.317: INFO: Got endpoints: latency-svc-xnmwj [647.546952ms]
    Feb 24 11:08:57.323: INFO: Created: latency-svc-h5jtb
    Feb 24 11:08:57.341: INFO: Got endpoints: latency-svc-h5jtb [644.843629ms]
    Feb 24 11:08:57.418: INFO: Created: latency-svc-g25wj
    Feb 24 11:08:57.431: INFO: Got endpoints: latency-svc-g25wj [713.590681ms]
    Feb 24 11:08:57.440: INFO: Created: latency-svc-l2r22
    Feb 24 11:08:57.450: INFO: Got endpoints: latency-svc-l2r22 [684.177615ms]
    Feb 24 11:08:57.467: INFO: Created: latency-svc-6rt6k
    Feb 24 11:08:57.505: INFO: Got endpoints: latency-svc-6rt6k [669.954605ms]
    Feb 24 11:08:57.522: INFO: Created: latency-svc-bhdk9
    Feb 24 11:08:57.576: INFO: Got endpoints: latency-svc-bhdk9 [706.410479ms]
    Feb 24 11:08:57.579: INFO: Created: latency-svc-9ss2v
    Feb 24 11:08:57.597: INFO: Got endpoints: latency-svc-9ss2v [711.532932ms]
    Feb 24 11:08:57.603: INFO: Created: latency-svc-phqbq
    Feb 24 11:08:57.621: INFO: Got endpoints: latency-svc-phqbq [657.244129ms]
    Feb 24 11:08:57.641: INFO: Created: latency-svc-pph5k
    Feb 24 11:08:57.714: INFO: Got endpoints: latency-svc-pph5k [699.755051ms]
    Feb 24 11:08:57.722: INFO: Created: latency-svc-9gfk9
    Feb 24 11:08:57.773: INFO: Got endpoints: latency-svc-9gfk9 [728.457426ms]
    Feb 24 11:08:57.774: INFO: Created: latency-svc-khrpk
    Feb 24 11:08:57.784: INFO: Got endpoints: latency-svc-khrpk [701.922804ms]
    Feb 24 11:08:57.807: INFO: Created: latency-svc-484gp
    Feb 24 11:08:57.812: INFO: Got endpoints: latency-svc-484gp [648.45262ms]
    Feb 24 11:08:57.820: INFO: Created: latency-svc-5sjbf
    Feb 24 11:08:57.865: INFO: Got endpoints: latency-svc-5sjbf [636.716339ms]
    Feb 24 11:08:57.875: INFO: Created: latency-svc-ftgwn
    Feb 24 11:08:57.942: INFO: Got endpoints: latency-svc-ftgwn [691.840236ms]
    Feb 24 11:08:57.957: INFO: Created: latency-svc-x4r8p
    Feb 24 11:08:57.974: INFO: Got endpoints: latency-svc-x4r8p [705.321458ms]
    Feb 24 11:08:57.982: INFO: Created: latency-svc-qrzpp
    Feb 24 11:08:58.003: INFO: Got endpoints: latency-svc-qrzpp [685.091062ms]
    Feb 24 11:08:58.027: INFO: Created: latency-svc-rw4hv
    Feb 24 11:08:58.031: INFO: Got endpoints: latency-svc-rw4hv [690.283565ms]
    Feb 24 11:08:58.100: INFO: Created: latency-svc-pbqdx
    Feb 24 11:08:58.112: INFO: Got endpoints: latency-svc-pbqdx [680.66774ms]
    Feb 24 11:08:58.150: INFO: Created: latency-svc-lstxm
    Feb 24 11:08:58.169: INFO: Got endpoints: latency-svc-lstxm [718.517909ms]
    Feb 24 11:08:58.181: INFO: Created: latency-svc-lxxkj
    Feb 24 11:08:58.194: INFO: Got endpoints: latency-svc-lxxkj [689.071815ms]
    Feb 24 11:08:58.205: INFO: Created: latency-svc-xkhzc
    Feb 24 11:08:58.278: INFO: Got endpoints: latency-svc-xkhzc [702.331873ms]
    Feb 24 11:08:58.285: INFO: Created: latency-svc-8n9dt
    Feb 24 11:08:58.291: INFO: Got endpoints: latency-svc-8n9dt [693.922616ms]
    Feb 24 11:08:58.337: INFO: Created: latency-svc-4tcst
    Feb 24 11:08:58.341: INFO: Got endpoints: latency-svc-4tcst [719.857235ms]
    Feb 24 11:08:58.363: INFO: Created: latency-svc-7vs2v
    Feb 24 11:08:58.381: INFO: Got endpoints: latency-svc-7vs2v [667.153032ms]
    Feb 24 11:08:58.392: INFO: Created: latency-svc-rqsxp
    Feb 24 11:08:58.469: INFO: Got endpoints: latency-svc-rqsxp [696.663057ms]
    Feb 24 11:08:58.479: INFO: Created: latency-svc-xlx4g
    Feb 24 11:08:58.555: INFO: Got endpoints: latency-svc-xlx4g [770.762829ms]
    Feb 24 11:08:58.564: INFO: Created: latency-svc-94fdx
    Feb 24 11:08:58.575: INFO: Got endpoints: latency-svc-94fdx [762.353769ms]
    Feb 24 11:08:58.588: INFO: Created: latency-svc-9pkl7
    Feb 24 11:08:58.606: INFO: Got endpoints: latency-svc-9pkl7 [740.506831ms]
    Feb 24 11:08:58.614: INFO: Created: latency-svc-4tlz5
    Feb 24 11:08:58.623: INFO: Got endpoints: latency-svc-4tlz5 [680.265851ms]
    Feb 24 11:08:58.693: INFO: Created: latency-svc-5xzqk
    Feb 24 11:08:58.774: INFO: Got endpoints: latency-svc-5xzqk [800.252511ms]
    Feb 24 11:08:58.786: INFO: Created: latency-svc-852dk
    Feb 24 11:08:58.800: INFO: Got endpoints: latency-svc-852dk [797.642289ms]
    Feb 24 11:08:58.813: INFO: Created: latency-svc-s76nt
    Feb 24 11:08:58.829: INFO: Got endpoints: latency-svc-s76nt [797.861741ms]
    Feb 24 11:08:58.835: INFO: Created: latency-svc-26l5g
    Feb 24 11:08:58.894: INFO: Got endpoints: latency-svc-26l5g [781.624773ms]
    Feb 24 11:08:58.906: INFO: Created: latency-svc-b85dn
    Feb 24 11:08:58.916: INFO: Got endpoints: latency-svc-b85dn [747.492265ms]
    Feb 24 11:08:58.973: INFO: Created: latency-svc-mr4ng
    Feb 24 11:08:58.995: INFO: Got endpoints: latency-svc-mr4ng [800.757178ms]
    Feb 24 11:08:59.001: INFO: Created: latency-svc-bszbn
    Feb 24 11:08:59.016: INFO: Got endpoints: latency-svc-bszbn [737.880678ms]
    Feb 24 11:08:59.029: INFO: Created: latency-svc-xd6qx
    Feb 24 11:08:59.111: INFO: Got endpoints: latency-svc-xd6qx [818.924072ms]
    Feb 24 11:08:59.117: INFO: Created: latency-svc-c2x49
    Feb 24 11:08:59.197: INFO: Got endpoints: latency-svc-c2x49 [855.642054ms]
    Feb 24 11:08:59.209: INFO: Created: latency-svc-pj44s
    Feb 24 11:08:59.222: INFO: Got endpoints: latency-svc-pj44s [840.624709ms]
    Feb 24 11:08:59.716: INFO: Created: latency-svc-bm6tx
    Feb 24 11:08:59.720: INFO: Created: latency-svc-hnq7t
    Feb 24 11:08:59.721: INFO: Created: latency-svc-2m5vs
    Feb 24 11:08:59.732: INFO: Created: latency-svc-s9m78
    Feb 24 11:08:59.732: INFO: Created: latency-svc-wjsbh
    Feb 24 11:08:59.732: INFO: Created: latency-svc-ptltp
    Feb 24 11:08:59.732: INFO: Created: latency-svc-fvfvf
    Feb 24 11:08:59.732: INFO: Created: latency-svc-km658
    Feb 24 11:08:59.744: INFO: Got endpoints: latency-svc-bm6tx [546.864912ms]
    Feb 24 11:08:59.754: INFO: Created: latency-svc-rmtb9
    Feb 24 11:08:59.754: INFO: Created: latency-svc-gz9vr
    Feb 24 11:08:59.754: INFO: Created: latency-svc-bk55h
    Feb 24 11:08:59.757: INFO: Created: latency-svc-rn5pq
    Feb 24 11:08:59.757: INFO: Created: latency-svc-9bbnq
    Feb 24 11:08:59.759: INFO: Created: latency-svc-9xhbx
    Feb 24 11:08:59.759: INFO: Created: latency-svc-rsz6s
    Feb 24 11:08:59.763: INFO: Got endpoints: latency-svc-ptltp [1.15731132s]
    Feb 24 11:08:59.764: INFO: Got endpoints: latency-svc-hnq7t [541.132599ms]
    Feb 24 11:08:59.764: INFO: Got endpoints: latency-svc-2m5vs [1.294053342s]
    Feb 24 11:08:59.769: INFO: Got endpoints: latency-svc-wjsbh [1.21388322s]
    Feb 24 11:08:59.776: INFO: Got endpoints: latency-svc-fvfvf [1.001683127s]
    Feb 24 11:08:59.840: INFO: Got endpoints: latency-svc-km658 [1.264842834s]
    Feb 24 11:08:59.845: INFO: Got endpoints: latency-svc-s9m78 [1.221867694s]
    Feb 24 11:08:59.846: INFO: Got endpoints: latency-svc-bk55h [1.044929928s]
    Feb 24 11:08:59.855: INFO: Created: latency-svc-4mk6d
    Feb 24 11:08:59.856: INFO: Got endpoints: latency-svc-gz9vr [1.026096978s]
    Feb 24 11:08:59.856: INFO: Got endpoints: latency-svc-9bbnq [962.063497ms]
    Feb 24 11:08:59.863: INFO: Got endpoints: latency-svc-rn5pq [946.560037ms]
    Feb 24 11:08:59.876: INFO: Got endpoints: latency-svc-rsz6s [765.457793ms]
    Feb 24 11:08:59.888: INFO: Created: latency-svc-9sscn
    Feb 24 11:08:59.896: INFO: Got endpoints: latency-svc-rmtb9 [879.233999ms]
    Feb 24 11:08:59.896: INFO: Got endpoints: latency-svc-4mk6d [152.143352ms]
    Feb 24 11:08:59.896: INFO: Got endpoints: latency-svc-9xhbx [900.437819ms]
    Feb 24 11:08:59.904: INFO: Got endpoints: latency-svc-9sscn [141.058781ms]
    Feb 24 11:08:59.947: INFO: Created: latency-svc-8vmsc
    Feb 24 11:08:59.959: INFO: Got endpoints: latency-svc-8vmsc [195.777372ms]
    Feb 24 11:09:00.051: INFO: Created: latency-svc-zbsc7
    Feb 24 11:09:00.060: INFO: Got endpoints: latency-svc-zbsc7 [296.034581ms]
    Feb 24 11:09:00.086: INFO: Created: latency-svc-h2v4x
    Feb 24 11:09:00.096: INFO: Got endpoints: latency-svc-h2v4x [326.708205ms]
    Feb 24 11:09:00.106: INFO: Created: latency-svc-tcr29
    Feb 24 11:09:00.184: INFO: Got endpoints: latency-svc-tcr29 [406.954981ms]
    Feb 24 11:09:00.297: INFO: Created: latency-svc-w2snq
    Feb 24 11:09:00.342: INFO: Got endpoints: latency-svc-w2snq [501.437819ms]
    Feb 24 11:09:00.346: INFO: Created: latency-svc-h5bnj
    Feb 24 11:09:00.368: INFO: Got endpoints: latency-svc-h5bnj [522.705452ms]
    Feb 24 11:09:00.392: INFO: Created: latency-svc-4vc2h
    Feb 24 11:09:00.417: INFO: Got endpoints: latency-svc-4vc2h [571.246306ms]
    Feb 24 11:09:00.432: INFO: Created: latency-svc-rkdjq
    Feb 24 11:09:00.449: INFO: Got endpoints: latency-svc-rkdjq [593.181436ms]
    Feb 24 11:09:00.500: INFO: Created: latency-svc-rzmlt
    Feb 24 11:09:00.558: INFO: Got endpoints: latency-svc-rzmlt [702.073596ms]
    Feb 24 11:09:00.570: INFO: Created: latency-svc-tf4zw
    Feb 24 11:09:00.586: INFO: Got endpoints: latency-svc-tf4zw [722.125884ms]
    Feb 24 11:09:00.597: INFO: Created: latency-svc-xrxpl
    Feb 24 11:09:00.610: INFO: Got endpoints: latency-svc-xrxpl [733.578128ms]
    Feb 24 11:09:00.619: INFO: Created: latency-svc-b58cq
    Feb 24 11:09:00.697: INFO: Got endpoints: latency-svc-b58cq [801.583804ms]
    Feb 24 11:09:00.711: INFO: Created: latency-svc-6szzh
    Feb 24 11:09:00.715: INFO: Got endpoints: latency-svc-6szzh [819.202578ms]
    Feb 24 11:09:00.755: INFO: Created: latency-svc-46k9z
    Feb 24 11:09:00.768: INFO: Got endpoints: latency-svc-46k9z [871.375093ms]
    Feb 24 11:09:00.779: INFO: Created: latency-svc-69pvr
    Feb 24 11:09:00.787: INFO: Got endpoints: latency-svc-69pvr [882.652551ms]
    Feb 24 11:09:00.794: INFO: Created: latency-svc-zmvhd
    Feb 24 11:09:00.865: INFO: Got endpoints: latency-svc-zmvhd [904.761408ms]
    Feb 24 11:09:00.873: INFO: Created: latency-svc-2wdmw
    Feb 24 11:09:00.955: INFO: Got endpoints: latency-svc-2wdmw [894.749949ms]
    Feb 24 11:09:00.974: INFO: Created: latency-svc-m6lzp
    Feb 24 11:09:00.990: INFO: Got endpoints: latency-svc-m6lzp [893.074418ms]
    Feb 24 11:09:00.994: INFO: Created: latency-svc-55rp9
    Feb 24 11:09:01.003: INFO: Got endpoints: latency-svc-55rp9 [818.931473ms]
    Feb 24 11:09:01.031: INFO: Created: latency-svc-cnnn8
    Feb 24 11:09:01.086: INFO: Got endpoints: latency-svc-cnnn8 [743.798642ms]
    Feb 24 11:09:01.091: INFO: Created: latency-svc-74z85
    Feb 24 11:09:01.152: INFO: Got endpoints: latency-svc-74z85 [783.445353ms]
    Feb 24 11:09:01.165: INFO: Created: latency-svc-cqj97
    Feb 24 11:09:01.187: INFO: Got endpoints: latency-svc-cqj97 [769.811931ms]
    Feb 24 11:09:01.203: INFO: Created: latency-svc-2rtv4
    Feb 24 11:09:01.216: INFO: Got endpoints: latency-svc-2rtv4 [766.635431ms]
    Feb 24 11:09:01.229: INFO: Created: latency-svc-kmvcs
    Feb 24 11:09:01.288: INFO: Got endpoints: latency-svc-kmvcs [729.279924ms]
    Feb 24 11:09:01.308: INFO: Created: latency-svc-fqldx
    Feb 24 11:09:01.332: INFO: Got endpoints: latency-svc-fqldx [746.157941ms]
    Feb 24 11:09:01.383: INFO: Created: latency-svc-gpgqs
    Feb 24 11:09:01.399: INFO: Got endpoints: latency-svc-gpgqs [788.68637ms]
    Feb 24 11:09:01.408: INFO: Created: latency-svc-wkdf2
    Feb 24 11:09:01.423: INFO: Got endpoints: latency-svc-wkdf2 [725.186372ms]
    Feb 24 11:09:01.428: INFO: Created: latency-svc-sfmzq
    Feb 24 11:09:01.517: INFO: Got endpoints: latency-svc-sfmzq [802.044317ms]
    Feb 24 11:09:01.527: INFO: Created: latency-svc-dgr7k
    Feb 24 11:09:01.589: INFO: Got endpoints: latency-svc-dgr7k [820.978762ms]
    Feb 24 11:09:01.594: INFO: Created: latency-svc-k6qfr
    Feb 24 11:09:01.618: INFO: Got endpoints: latency-svc-k6qfr [830.306772ms]
    Feb 24 11:09:01.646: INFO: Created: latency-svc-sslpn
    Feb 24 11:09:01.656: INFO: Got endpoints: latency-svc-sslpn [791.638224ms]
    Feb 24 11:09:01.671: INFO: Created: latency-svc-q7qqk
    Feb 24 11:09:01.745: INFO: Got endpoints: latency-svc-q7qqk [789.15672ms]
    Feb 24 11:09:01.754: INFO: Created: latency-svc-57pck
    Feb 24 11:09:01.787: INFO: Got endpoints: latency-svc-57pck [797.007924ms]
    Feb 24 11:09:01.798: INFO: Created: latency-svc-8wwdq
    Feb 24 11:09:01.814: INFO: Got endpoints: latency-svc-8wwdq [811.322628ms]
    Feb 24 11:09:02.587: INFO: Created: latency-svc-jfptt
    Feb 24 11:09:02.593: INFO: Created: latency-svc-5895x
    Feb 24 11:09:02.626: INFO: Created: latency-svc-fbh92
    Feb 24 11:09:02.629: INFO: Created: latency-svc-b6fv7
    Feb 24 11:09:02.731: INFO: Got endpoints: latency-svc-jfptt [943.811262ms]
    Feb 24 11:09:02.731: INFO: Got endpoints: latency-svc-5895x [1.643409932s]
    Feb 24 11:09:02.738: INFO: Created: latency-svc-6rp4t
    Feb 24 11:09:02.738: INFO: Created: latency-svc-pqqpx
    Feb 24 11:09:02.739: INFO: Created: latency-svc-lrnjh
    Feb 24 11:09:02.739: INFO: Created: latency-svc-t5twx
    Feb 24 11:09:02.740: INFO: Created: latency-svc-hrtcx
    Feb 24 11:09:02.740: INFO: Created: latency-svc-8dlvc
    Feb 24 11:09:02.741: INFO: Created: latency-svc-9c22s
    Feb 24 11:09:02.741: INFO: Created: latency-svc-d7w2j
    Feb 24 11:09:02.741: INFO: Created: latency-svc-t8pwd
    Feb 24 11:09:02.742: INFO: Created: latency-svc-h8bjb
    Feb 24 11:09:02.742: INFO: Created: latency-svc-zshvr
    Feb 24 11:09:02.765: INFO: Got endpoints: latency-svc-t8pwd [1.577362155s]
    Feb 24 11:09:02.766: INFO: Got endpoints: latency-svc-b6fv7 [950.298569ms]
    Feb 24 11:09:02.776: INFO: Got endpoints: latency-svc-8dlvc [1.56008827s]
    Feb 24 11:09:02.776: INFO: Got endpoints: latency-svc-9c22s [1.444249461s]
    Feb 24 11:09:02.797: INFO: Got endpoints: latency-svc-fbh92 [1.644720222s]
    Feb 24 11:09:02.857: INFO: Got endpoints: latency-svc-d7w2j [1.458364241s]
    Feb 24 11:09:02.875: INFO: Got endpoints: latency-svc-h8bjb [1.218779094s]
    Feb 24 11:09:02.876: INFO: Got endpoints: latency-svc-zshvr [1.358332713s]
    Feb 24 11:09:02.876: INFO: Got endpoints: latency-svc-hrtcx [1.588561224s]
    Feb 24 11:09:02.877: INFO: Got endpoints: latency-svc-pqqpx [1.453720519s]
    Feb 24 11:09:02.893: INFO: Got endpoints: latency-svc-lrnjh [1.30425492s]
    Feb 24 11:09:02.893: INFO: Created: latency-svc-zvk6h
    Feb 24 11:09:02.903: INFO: Got endpoints: latency-svc-t5twx [1.158444033s]
    Feb 24 11:09:02.904: INFO: Got endpoints: latency-svc-6rp4t [1.285896807s]
    Feb 24 11:09:02.931: INFO: Got endpoints: latency-svc-zvk6h [200.009194ms]
    Feb 24 11:09:02.932: INFO: Created: latency-svc-cggjk
    Feb 24 11:09:02.947: INFO: Got endpoints: latency-svc-cggjk [215.99128ms]
    Feb 24 11:09:02.954: INFO: Created: latency-svc-rm2fx
    Feb 24 11:09:03.059: INFO: Got endpoints: latency-svc-rm2fx [294.292174ms]
    Feb 24 11:09:03.078: INFO: Created: latency-svc-m8qwz
    Feb 24 11:09:03.105: INFO: Created: latency-svc-d76cn
    Feb 24 11:09:03.105: INFO: Got endpoints: latency-svc-m8qwz [339.710863ms]
    Feb 24 11:09:03.117: INFO: Got endpoints: latency-svc-d76cn [340.662361ms]
    Feb 24 11:09:03.135: INFO: Created: latency-svc-mlxxx
    Feb 24 11:09:03.142: INFO: Got endpoints: latency-svc-mlxxx [365.448882ms]
    Feb 24 11:09:03.161: INFO: Created: latency-svc-kkrgd
    Feb 24 11:09:03.173: INFO: Got endpoints: latency-svc-kkrgd [375.815847ms]
    Feb 24 11:09:03.223: INFO: Created: latency-svc-56qsp
    Feb 24 11:09:03.246: INFO: Got endpoints: latency-svc-56qsp [388.863401ms]
    Feb 24 11:09:03.258: INFO: Created: latency-svc-ffcfg
    Feb 24 11:09:03.276: INFO: Got endpoints: latency-svc-ffcfg [399.964721ms]
    Feb 24 11:09:03.284: INFO: Created: latency-svc-jw9z7
    Feb 24 11:09:03.348: INFO: Got endpoints: latency-svc-jw9z7 [471.715516ms]
    Feb 24 11:09:03.368: INFO: Created: latency-svc-vkrqk
    Feb 24 11:09:03.417: INFO: Got endpoints: latency-svc-vkrqk [540.385343ms]
    Feb 24 11:09:03.430: INFO: Created: latency-svc-pz8zb
    Feb 24 11:09:03.451: INFO: Got endpoints: latency-svc-pz8zb [574.839691ms]
    Feb 24 11:09:03.470: INFO: Created: latency-svc-tkpkb
    Feb 24 11:09:03.482: INFO: Got endpoints: latency-svc-tkpkb [588.575902ms]
    Feb 24 11:09:03.493: INFO: Created: latency-svc-dh8mh
    Feb 24 11:09:03.636: INFO: Got endpoints: latency-svc-dh8mh [732.854749ms]
    Feb 24 11:09:03.648: INFO: Created: latency-svc-wb6jm
    Feb 24 11:09:03.681: INFO: Got endpoints: latency-svc-wb6jm [777.224769ms]
    Feb 24 11:09:03.689: INFO: Created: latency-svc-srjmm
    Feb 24 11:09:03.704: INFO: Got endpoints: latency-svc-srjmm [773.347218ms]
    Feb 24 11:09:03.709: INFO: Created: latency-svc-fw557
    Feb 24 11:09:03.722: INFO: Got endpoints: latency-svc-fw557 [774.003419ms]
    Feb 24 11:09:03.726: INFO: Created: latency-svc-bq6kg
    Feb 24 11:09:03.800: INFO: Got endpoints: latency-svc-bq6kg [740.701459ms]
    Feb 24 11:09:03.805: INFO: Created: latency-svc-rb9gk
    Feb 24 11:09:03.859: INFO: Got endpoints: latency-svc-rb9gk [753.777933ms]
    Feb 24 11:09:03.903: INFO: Created: latency-svc-qc2bg
    Feb 24 11:09:03.918: INFO: Got endpoints: latency-svc-qc2bg [800.093778ms]
    Feb 24 11:09:03.940: INFO: Created: latency-svc-s9wfn
    Feb 24 11:09:03.967: INFO: Got endpoints: latency-svc-s9wfn [824.932413ms]
    Feb 24 11:09:03.979: INFO: Created: latency-svc-rmdn5
    Feb 24 11:09:04.059: INFO: Got endpoints: latency-svc-rmdn5 [885.85059ms]
    Feb 24 11:09:04.067: INFO: Created: latency-svc-g48s4
    Feb 24 11:09:04.150: INFO: Got endpoints: latency-svc-g48s4 [903.938686ms]
    Feb 24 11:09:04.171: INFO: Created: latency-svc-hgf5m
    Feb 24 11:09:04.215: INFO: Got endpoints: latency-svc-hgf5m [938.66581ms]
    Feb 24 11:09:04.215: INFO: Created: latency-svc-v5j6m
    Feb 24 11:09:04.255: INFO: Got endpoints: latency-svc-v5j6m [904.53723ms]
    Feb 24 11:09:04.269: INFO: Created: latency-svc-6d9g4
    Feb 24 11:09:04.335: INFO: Got endpoints: latency-svc-6d9g4 [918.210733ms]
    Feb 24 11:09:04.341: INFO: Created: latency-svc-pntsm
    Feb 24 11:09:04.351: INFO: Got endpoints: latency-svc-pntsm [899.78562ms]
    Feb 24 11:09:04.420: INFO: Created: latency-svc-flkdw
    Feb 24 11:09:04.431: INFO: Got endpoints: latency-svc-flkdw [948.87396ms]
    Feb 24 11:09:04.431: INFO: Latencies: [109.177764ms 139.364468ms 141.058781ms 152.143352ms 155.809777ms 190.753804ms 195.777372ms 200.009194ms 215.99128ms 294.292174ms 296.034581ms 306.758842ms 326.708205ms 330.79078ms 339.710863ms 340.662361ms 365.448882ms 370.321785ms 375.815847ms 388.863401ms 399.964721ms 406.954981ms 436.181648ms 471.715516ms 490.286171ms 501.437819ms 514.620264ms 522.705452ms 540.351215ms 540.385343ms 541.132599ms 546.864912ms 571.246306ms 574.839691ms 588.575902ms 593.181436ms 610.920682ms 636.716339ms 644.843629ms 645.557964ms 646.317117ms 647.546952ms 648.45262ms 657.244129ms 657.572633ms 661.195422ms 667.153032ms 669.954605ms 672.143139ms 676.477252ms 680.265851ms 680.66774ms 682.102667ms 682.276166ms 684.177615ms 685.091062ms 687.490673ms 689.071815ms 690.283565ms 690.329452ms 691.840236ms 693.159396ms 693.922616ms 694.303521ms 696.663057ms 699.229726ms 699.755051ms 700.247444ms 701.21192ms 701.922804ms 702.073596ms 702.331873ms 705.321458ms 705.576316ms 706.410479ms 711.052606ms 711.532932ms 713.590681ms 715.919275ms 718.517909ms 719.857235ms 720.12233ms 721.096522ms 722.125884ms 723.306391ms 723.354362ms 724.008984ms 725.186372ms 725.267929ms 725.684043ms 728.457426ms 728.825805ms 729.279924ms 730.634196ms 730.706657ms 730.737385ms 731.851473ms 732.854749ms 733.578128ms 736.93669ms 737.352186ms 737.485865ms 737.880678ms 739.825398ms 740.506831ms 740.701459ms 743.798642ms 746.157941ms 746.66418ms 747.492265ms 749.527275ms 749.547073ms 750.831991ms 753.410629ms 753.777933ms 754.570627ms 755.698532ms 760.989222ms 761.422668ms 762.353769ms 762.556791ms 765.457793ms 766.635431ms 766.930997ms 767.270431ms 767.964909ms 768.242925ms 769.811931ms 770.762829ms 773.347218ms 774.003419ms 777.224769ms 781.624773ms 783.445353ms 784.072954ms 784.825416ms 788.68637ms 789.15672ms 791.638224ms 795.178723ms 797.007924ms 797.642289ms 797.833293ms 797.861741ms 800.093778ms 800.252511ms 800.757178ms 801.583804ms 802.044317ms 811.322628ms 818.924072ms 818.931473ms 819.202578ms 820.978762ms 823.787749ms 824.932413ms 830.306772ms 831.658329ms 834.356948ms 840.624709ms 855.642054ms 871.375093ms 879.233999ms 882.652551ms 885.85059ms 893.074418ms 894.749949ms 899.78562ms 900.437819ms 903.938686ms 904.53723ms 904.761408ms 918.210733ms 938.66581ms 943.811262ms 946.560037ms 948.87396ms 950.298569ms 962.063497ms 1.001683127s 1.026096978s 1.044929928s 1.15731132s 1.158444033s 1.21388322s 1.218779094s 1.221867694s 1.264842834s 1.285896807s 1.294053342s 1.30425492s 1.358332713s 1.444249461s 1.453720519s 1.458364241s 1.56008827s 1.577362155s 1.588561224s 1.643409932s 1.644720222s]
    Feb 24 11:09:04.431: INFO: 50 %ile: 737.352186ms
    Feb 24 11:09:04.431: INFO: 90 %ile: 1.026096978s
    Feb 24 11:09:04.431: INFO: 99 %ile: 1.643409932s
    Feb 24 11:09:04.431: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Feb 24 11:09:04.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-7292" for this suite. 02/24/23 11:09:04.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:04.48
Feb 24 11:09:04.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:09:04.481
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:04.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:04.507
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:09:04.51
Feb 24 11:09:04.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6" in namespace "projected-4327" to be "Succeeded or Failed"
Feb 24 11:09:04.551: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.659607ms
Feb 24 11:09:06.561: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039601718s
Feb 24 11:09:08.557: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035524959s
Feb 24 11:09:10.559: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037244085s
STEP: Saw pod success 02/24/23 11:09:10.559
Feb 24 11:09:10.559: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6" satisfied condition "Succeeded or Failed"
Feb 24 11:09:10.565: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6 container client-container: <nil>
STEP: delete the pod 02/24/23 11:09:10.589
Feb 24 11:09:10.610: INFO: Waiting for pod downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6 to disappear
Feb 24 11:09:10.621: INFO: Pod downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 11:09:10.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4327" for this suite. 02/24/23 11:09:10.63
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":40,"skipped":598,"failed":0}
------------------------------
• [SLOW TEST] [6.160 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:04.48
    Feb 24 11:09:04.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:09:04.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:04.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:04.507
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:09:04.51
    Feb 24 11:09:04.522: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6" in namespace "projected-4327" to be "Succeeded or Failed"
    Feb 24 11:09:04.551: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.659607ms
    Feb 24 11:09:06.561: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039601718s
    Feb 24 11:09:08.557: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035524959s
    Feb 24 11:09:10.559: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037244085s
    STEP: Saw pod success 02/24/23 11:09:10.559
    Feb 24 11:09:10.559: INFO: Pod "downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6" satisfied condition "Succeeded or Failed"
    Feb 24 11:09:10.565: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:09:10.589
    Feb 24 11:09:10.610: INFO: Waiting for pod downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6 to disappear
    Feb 24 11:09:10.621: INFO: Pod downwardapi-volume-a9d830c1-6e19-435c-aeb9-dbabdc2605c6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 11:09:10.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4327" for this suite. 02/24/23 11:09:10.63
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:10.642
Feb 24 11:09:10.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:09:10.643
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:10.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:10.668
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 02/24/23 11:09:10.67
Feb 24 11:09:10.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 create -f -'
Feb 24 11:09:11.655: INFO: stderr: ""
Feb 24 11:09:11.655: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:09:11.655
Feb 24 11:09:11.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:09:11.876: INFO: stderr: ""
Feb 24 11:09:11.876: INFO: stdout: "update-demo-nautilus-6jsfv update-demo-nautilus-bhhm6 "
Feb 24 11:09:11.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-6jsfv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:12.063: INFO: stderr: ""
Feb 24 11:09:12.063: INFO: stdout: ""
Feb 24 11:09:12.063: INFO: update-demo-nautilus-6jsfv is created but not running
Feb 24 11:09:17.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:09:17.166: INFO: stderr: ""
Feb 24 11:09:17.166: INFO: stdout: "update-demo-nautilus-6jsfv update-demo-nautilus-bhhm6 "
Feb 24 11:09:17.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-6jsfv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:17.282: INFO: stderr: ""
Feb 24 11:09:17.282: INFO: stdout: "true"
Feb 24 11:09:17.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-6jsfv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:09:17.383: INFO: stderr: ""
Feb 24 11:09:17.383: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 11:09:17.383: INFO: validating pod update-demo-nautilus-6jsfv
Feb 24 11:09:17.454: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:09:17.454: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:09:17.454: INFO: update-demo-nautilus-6jsfv is verified up and running
Feb 24 11:09:17.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:17.577: INFO: stderr: ""
Feb 24 11:09:17.577: INFO: stdout: "true"
Feb 24 11:09:17.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:09:17.684: INFO: stderr: ""
Feb 24 11:09:17.684: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 11:09:17.684: INFO: validating pod update-demo-nautilus-bhhm6
Feb 24 11:09:17.706: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:09:17.706: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:09:17.706: INFO: update-demo-nautilus-bhhm6 is verified up and running
STEP: scaling down the replication controller 02/24/23 11:09:17.706
Feb 24 11:09:17.711: INFO: scanned /root for discovery docs: <nil>
Feb 24 11:09:17.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 24 11:09:18.860: INFO: stderr: ""
Feb 24 11:09:18.860: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:09:18.86
Feb 24 11:09:18.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:09:18.940: INFO: stderr: ""
Feb 24 11:09:18.940: INFO: stdout: "update-demo-nautilus-6jsfv update-demo-nautilus-bhhm6 "
STEP: Replicas for name=update-demo: expected=1 actual=2 02/24/23 11:09:18.94
Feb 24 11:09:23.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:09:24.084: INFO: stderr: ""
Feb 24 11:09:24.084: INFO: stdout: "update-demo-nautilus-bhhm6 "
Feb 24 11:09:24.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:24.169: INFO: stderr: ""
Feb 24 11:09:24.169: INFO: stdout: "true"
Feb 24 11:09:24.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:09:24.280: INFO: stderr: ""
Feb 24 11:09:24.280: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 11:09:24.280: INFO: validating pod update-demo-nautilus-bhhm6
Feb 24 11:09:24.288: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:09:24.288: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:09:24.288: INFO: update-demo-nautilus-bhhm6 is verified up and running
STEP: scaling up the replication controller 02/24/23 11:09:24.288
Feb 24 11:09:24.289: INFO: scanned /root for discovery docs: <nil>
Feb 24 11:09:24.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 24 11:09:25.433: INFO: stderr: ""
Feb 24 11:09:25.433: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:09:25.433
Feb 24 11:09:25.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:09:25.518: INFO: stderr: ""
Feb 24 11:09:25.518: INFO: stdout: "update-demo-nautilus-bhhm6 update-demo-nautilus-s6rn2 "
Feb 24 11:09:25.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:25.734: INFO: stderr: ""
Feb 24 11:09:25.734: INFO: stdout: "true"
Feb 24 11:09:25.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:09:25.938: INFO: stderr: ""
Feb 24 11:09:25.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 11:09:25.938: INFO: validating pod update-demo-nautilus-bhhm6
Feb 24 11:09:25.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:09:25.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:09:25.951: INFO: update-demo-nautilus-bhhm6 is verified up and running
Feb 24 11:09:25.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-s6rn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:26.031: INFO: stderr: ""
Feb 24 11:09:26.031: INFO: stdout: ""
Feb 24 11:09:26.031: INFO: update-demo-nautilus-s6rn2 is created but not running
Feb 24 11:09:31.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 11:09:31.151: INFO: stderr: ""
Feb 24 11:09:31.151: INFO: stdout: "update-demo-nautilus-bhhm6 update-demo-nautilus-s6rn2 "
Feb 24 11:09:31.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:31.318: INFO: stderr: ""
Feb 24 11:09:31.318: INFO: stdout: "true"
Feb 24 11:09:31.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:09:31.415: INFO: stderr: ""
Feb 24 11:09:31.415: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 11:09:31.415: INFO: validating pod update-demo-nautilus-bhhm6
Feb 24 11:09:31.431: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:09:31.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:09:31.431: INFO: update-demo-nautilus-bhhm6 is verified up and running
Feb 24 11:09:31.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-s6rn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 11:09:31.509: INFO: stderr: ""
Feb 24 11:09:31.509: INFO: stdout: "true"
Feb 24 11:09:31.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-s6rn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 11:09:31.592: INFO: stderr: ""
Feb 24 11:09:31.593: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 11:09:31.593: INFO: validating pod update-demo-nautilus-s6rn2
Feb 24 11:09:31.603: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 11:09:31.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 11:09:31.603: INFO: update-demo-nautilus-s6rn2 is verified up and running
STEP: using delete to clean up resources 02/24/23 11:09:31.603
Feb 24 11:09:31.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 delete --grace-period=0 --force -f -'
Feb 24 11:09:31.718: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:09:31.718: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 24 11:09:31.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get rc,svc -l name=update-demo --no-headers'
Feb 24 11:09:31.881: INFO: stderr: "No resources found in kubectl-579 namespace.\n"
Feb 24 11:09:31.881: INFO: stdout: ""
Feb 24 11:09:31.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 11:09:31.964: INFO: stderr: ""
Feb 24 11:09:31.964: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:09:31.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-579" for this suite. 02/24/23 11:09:31.974
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":41,"skipped":598,"failed":0}
------------------------------
• [SLOW TEST] [21.351 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:10.642
    Feb 24 11:09:10.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:09:10.643
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:10.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:10.668
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 02/24/23 11:09:10.67
    Feb 24 11:09:10.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 create -f -'
    Feb 24 11:09:11.655: INFO: stderr: ""
    Feb 24 11:09:11.655: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:09:11.655
    Feb 24 11:09:11.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:09:11.876: INFO: stderr: ""
    Feb 24 11:09:11.876: INFO: stdout: "update-demo-nautilus-6jsfv update-demo-nautilus-bhhm6 "
    Feb 24 11:09:11.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-6jsfv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:12.063: INFO: stderr: ""
    Feb 24 11:09:12.063: INFO: stdout: ""
    Feb 24 11:09:12.063: INFO: update-demo-nautilus-6jsfv is created but not running
    Feb 24 11:09:17.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:09:17.166: INFO: stderr: ""
    Feb 24 11:09:17.166: INFO: stdout: "update-demo-nautilus-6jsfv update-demo-nautilus-bhhm6 "
    Feb 24 11:09:17.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-6jsfv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:17.282: INFO: stderr: ""
    Feb 24 11:09:17.282: INFO: stdout: "true"
    Feb 24 11:09:17.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-6jsfv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:09:17.383: INFO: stderr: ""
    Feb 24 11:09:17.383: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 11:09:17.383: INFO: validating pod update-demo-nautilus-6jsfv
    Feb 24 11:09:17.454: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:09:17.454: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:09:17.454: INFO: update-demo-nautilus-6jsfv is verified up and running
    Feb 24 11:09:17.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:17.577: INFO: stderr: ""
    Feb 24 11:09:17.577: INFO: stdout: "true"
    Feb 24 11:09:17.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:09:17.684: INFO: stderr: ""
    Feb 24 11:09:17.684: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 11:09:17.684: INFO: validating pod update-demo-nautilus-bhhm6
    Feb 24 11:09:17.706: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:09:17.706: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:09:17.706: INFO: update-demo-nautilus-bhhm6 is verified up and running
    STEP: scaling down the replication controller 02/24/23 11:09:17.706
    Feb 24 11:09:17.711: INFO: scanned /root for discovery docs: <nil>
    Feb 24 11:09:17.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 24 11:09:18.860: INFO: stderr: ""
    Feb 24 11:09:18.860: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:09:18.86
    Feb 24 11:09:18.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:09:18.940: INFO: stderr: ""
    Feb 24 11:09:18.940: INFO: stdout: "update-demo-nautilus-6jsfv update-demo-nautilus-bhhm6 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 02/24/23 11:09:18.94
    Feb 24 11:09:23.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:09:24.084: INFO: stderr: ""
    Feb 24 11:09:24.084: INFO: stdout: "update-demo-nautilus-bhhm6 "
    Feb 24 11:09:24.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:24.169: INFO: stderr: ""
    Feb 24 11:09:24.169: INFO: stdout: "true"
    Feb 24 11:09:24.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:09:24.280: INFO: stderr: ""
    Feb 24 11:09:24.280: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 11:09:24.280: INFO: validating pod update-demo-nautilus-bhhm6
    Feb 24 11:09:24.288: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:09:24.288: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:09:24.288: INFO: update-demo-nautilus-bhhm6 is verified up and running
    STEP: scaling up the replication controller 02/24/23 11:09:24.288
    Feb 24 11:09:24.289: INFO: scanned /root for discovery docs: <nil>
    Feb 24 11:09:24.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 24 11:09:25.433: INFO: stderr: ""
    Feb 24 11:09:25.433: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 11:09:25.433
    Feb 24 11:09:25.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:09:25.518: INFO: stderr: ""
    Feb 24 11:09:25.518: INFO: stdout: "update-demo-nautilus-bhhm6 update-demo-nautilus-s6rn2 "
    Feb 24 11:09:25.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:25.734: INFO: stderr: ""
    Feb 24 11:09:25.734: INFO: stdout: "true"
    Feb 24 11:09:25.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:09:25.938: INFO: stderr: ""
    Feb 24 11:09:25.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 11:09:25.938: INFO: validating pod update-demo-nautilus-bhhm6
    Feb 24 11:09:25.951: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:09:25.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:09:25.951: INFO: update-demo-nautilus-bhhm6 is verified up and running
    Feb 24 11:09:25.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-s6rn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:26.031: INFO: stderr: ""
    Feb 24 11:09:26.031: INFO: stdout: ""
    Feb 24 11:09:26.031: INFO: update-demo-nautilus-s6rn2 is created but not running
    Feb 24 11:09:31.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 11:09:31.151: INFO: stderr: ""
    Feb 24 11:09:31.151: INFO: stdout: "update-demo-nautilus-bhhm6 update-demo-nautilus-s6rn2 "
    Feb 24 11:09:31.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:31.318: INFO: stderr: ""
    Feb 24 11:09:31.318: INFO: stdout: "true"
    Feb 24 11:09:31.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-bhhm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:09:31.415: INFO: stderr: ""
    Feb 24 11:09:31.415: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 11:09:31.415: INFO: validating pod update-demo-nautilus-bhhm6
    Feb 24 11:09:31.431: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:09:31.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:09:31.431: INFO: update-demo-nautilus-bhhm6 is verified up and running
    Feb 24 11:09:31.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-s6rn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 11:09:31.509: INFO: stderr: ""
    Feb 24 11:09:31.509: INFO: stdout: "true"
    Feb 24 11:09:31.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods update-demo-nautilus-s6rn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 11:09:31.592: INFO: stderr: ""
    Feb 24 11:09:31.593: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 11:09:31.593: INFO: validating pod update-demo-nautilus-s6rn2
    Feb 24 11:09:31.603: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 11:09:31.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 11:09:31.603: INFO: update-demo-nautilus-s6rn2 is verified up and running
    STEP: using delete to clean up resources 02/24/23 11:09:31.603
    Feb 24 11:09:31.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 delete --grace-period=0 --force -f -'
    Feb 24 11:09:31.718: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:09:31.718: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 24 11:09:31.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get rc,svc -l name=update-demo --no-headers'
    Feb 24 11:09:31.881: INFO: stderr: "No resources found in kubectl-579 namespace.\n"
    Feb 24 11:09:31.881: INFO: stdout: ""
    Feb 24 11:09:31.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-579 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 24 11:09:31.964: INFO: stderr: ""
    Feb 24 11:09:31.964: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:09:31.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-579" for this suite. 02/24/23 11:09:31.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:31.994
Feb 24 11:09:31.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:09:32.001
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:32.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:32.095
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:09:32.132
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:09:32.989
STEP: Deploying the webhook pod 02/24/23 11:09:33.001
STEP: Wait for the deployment to be ready 02/24/23 11:09:33.083
Feb 24 11:09:33.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:09:35.114
STEP: Verifying the service has paired with the endpoint 02/24/23 11:09:35.147
Feb 24 11:09:36.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 02/24/23 11:09:36.273
STEP: Creating a configMap that should be mutated 02/24/23 11:09:36.29
STEP: Deleting the collection of validation webhooks 02/24/23 11:09:36.344
STEP: Creating a configMap that should not be mutated 02/24/23 11:09:36.442
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:09:36.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-153" for this suite. 02/24/23 11:09:36.464
STEP: Destroying namespace "webhook-153-markers" for this suite. 02/24/23 11:09:36.481
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":42,"skipped":634,"failed":0}
------------------------------
• [4.707 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:31.994
    Feb 24 11:09:31.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:09:32.001
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:32.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:32.095
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:09:32.132
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:09:32.989
    STEP: Deploying the webhook pod 02/24/23 11:09:33.001
    STEP: Wait for the deployment to be ready 02/24/23 11:09:33.083
    Feb 24 11:09:33.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:09:35.114
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:09:35.147
    Feb 24 11:09:36.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 02/24/23 11:09:36.273
    STEP: Creating a configMap that should be mutated 02/24/23 11:09:36.29
    STEP: Deleting the collection of validation webhooks 02/24/23 11:09:36.344
    STEP: Creating a configMap that should not be mutated 02/24/23 11:09:36.442
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:09:36.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-153" for this suite. 02/24/23 11:09:36.464
    STEP: Destroying namespace "webhook-153-markers" for this suite. 02/24/23 11:09:36.481
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:36.703
Feb 24 11:09:36.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:09:36.704
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:36.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:36.739
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:09:36.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3849" for this suite. 02/24/23 11:09:36.759
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":43,"skipped":654,"failed":0}
------------------------------
• [0.068 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:36.703
    Feb 24 11:09:36.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:09:36.704
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:36.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:36.739
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:09:36.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3849" for this suite. 02/24/23 11:09:36.759
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:36.772
Feb 24 11:09:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:09:36.773
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:36.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:36.805
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 24 11:09:36.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5688" for this suite. 02/24/23 11:09:36.854
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":44,"skipped":663,"failed":0}
------------------------------
• [0.093 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:36.772
    Feb 24 11:09:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:09:36.773
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:36.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:36.805
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 24 11:09:36.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5688" for this suite. 02/24/23 11:09:36.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:36.868
Feb 24 11:09:36.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:09:36.869
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:36.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:36.894
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/24/23 11:09:36.897
Feb 24 11:09:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:09:39.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:09:54.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6620" for this suite. 02/24/23 11:09:54.243
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":45,"skipped":688,"failed":0}
------------------------------
• [SLOW TEST] [17.387 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:36.868
    Feb 24 11:09:36.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:09:36.869
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:36.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:36.894
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/24/23 11:09:36.897
    Feb 24 11:09:36.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:09:39.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:09:54.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6620" for this suite. 02/24/23 11:09:54.243
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:54.256
Feb 24 11:09:54.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:09:54.261
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:54.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:54.287
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-a75bfb97-9d7c-4dfc-a7ee-4cd475946747 02/24/23 11:09:54.289
STEP: Creating a pod to test consume configMaps 02/24/23 11:09:54.295
Feb 24 11:09:54.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011" in namespace "configmap-4855" to be "Succeeded or Failed"
Feb 24 11:09:54.321: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011": Phase="Pending", Reason="", readiness=false. Elapsed: 12.718775ms
Feb 24 11:09:56.328: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01910719s
Feb 24 11:09:58.327: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018423492s
STEP: Saw pod success 02/24/23 11:09:58.327
Feb 24 11:09:58.327: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011" satisfied condition "Succeeded or Failed"
Feb 24 11:09:58.332: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:09:58.348
Feb 24 11:09:58.371: INFO: Waiting for pod pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011 to disappear
Feb 24 11:09:58.381: INFO: Pod pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:09:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4855" for this suite. 02/24/23 11:09:58.389
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":46,"skipped":692,"failed":0}
------------------------------
• [4.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:54.256
    Feb 24 11:09:54.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:09:54.261
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:54.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:54.287
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-a75bfb97-9d7c-4dfc-a7ee-4cd475946747 02/24/23 11:09:54.289
    STEP: Creating a pod to test consume configMaps 02/24/23 11:09:54.295
    Feb 24 11:09:54.309: INFO: Waiting up to 5m0s for pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011" in namespace "configmap-4855" to be "Succeeded or Failed"
    Feb 24 11:09:54.321: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011": Phase="Pending", Reason="", readiness=false. Elapsed: 12.718775ms
    Feb 24 11:09:56.328: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01910719s
    Feb 24 11:09:58.327: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018423492s
    STEP: Saw pod success 02/24/23 11:09:58.327
    Feb 24 11:09:58.327: INFO: Pod "pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011" satisfied condition "Succeeded or Failed"
    Feb 24 11:09:58.332: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:09:58.348
    Feb 24 11:09:58.371: INFO: Waiting for pod pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011 to disappear
    Feb 24 11:09:58.381: INFO: Pod pod-configmaps-06f7ad78-27e0-4531-9ad8-ad5a8950c011 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:09:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4855" for this suite. 02/24/23 11:09:58.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:09:58.406
Feb 24 11:09:58.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:09:58.407
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:58.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:58.441
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 11:09:58.447
Feb 24 11:09:58.468: INFO: Waiting up to 5m0s for pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae" in namespace "emptydir-1422" to be "Succeeded or Failed"
Feb 24 11:09:58.477: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae": Phase="Pending", Reason="", readiness=false. Elapsed: 9.752774ms
Feb 24 11:10:00.487: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae": Phase="Running", Reason="", readiness=false. Elapsed: 2.019568783s
Feb 24 11:10:02.487: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01899316s
STEP: Saw pod success 02/24/23 11:10:02.487
Feb 24 11:10:02.487: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae" satisfied condition "Succeeded or Failed"
Feb 24 11:10:02.493: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-9c01f122-c01a-4227-895d-7bf48cc180ae container test-container: <nil>
STEP: delete the pod 02/24/23 11:10:02.504
Feb 24 11:10:02.524: INFO: Waiting for pod pod-9c01f122-c01a-4227-895d-7bf48cc180ae to disappear
Feb 24 11:10:02.533: INFO: Pod pod-9c01f122-c01a-4227-895d-7bf48cc180ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:10:02.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1422" for this suite. 02/24/23 11:10:02.552
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":47,"skipped":738,"failed":0}
------------------------------
• [4.161 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:09:58.406
    Feb 24 11:09:58.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:09:58.407
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:09:58.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:09:58.441
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 11:09:58.447
    Feb 24 11:09:58.468: INFO: Waiting up to 5m0s for pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae" in namespace "emptydir-1422" to be "Succeeded or Failed"
    Feb 24 11:09:58.477: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae": Phase="Pending", Reason="", readiness=false. Elapsed: 9.752774ms
    Feb 24 11:10:00.487: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae": Phase="Running", Reason="", readiness=false. Elapsed: 2.019568783s
    Feb 24 11:10:02.487: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01899316s
    STEP: Saw pod success 02/24/23 11:10:02.487
    Feb 24 11:10:02.487: INFO: Pod "pod-9c01f122-c01a-4227-895d-7bf48cc180ae" satisfied condition "Succeeded or Failed"
    Feb 24 11:10:02.493: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-9c01f122-c01a-4227-895d-7bf48cc180ae container test-container: <nil>
    STEP: delete the pod 02/24/23 11:10:02.504
    Feb 24 11:10:02.524: INFO: Waiting for pod pod-9c01f122-c01a-4227-895d-7bf48cc180ae to disappear
    Feb 24 11:10:02.533: INFO: Pod pod-9c01f122-c01a-4227-895d-7bf48cc180ae no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:10:02.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1422" for this suite. 02/24/23 11:10:02.552
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:02.567
Feb 24 11:10:02.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:10:02.569
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:02.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:02.601
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:10:02.634
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:10:03.505
STEP: Deploying the webhook pod 02/24/23 11:10:03.525
STEP: Wait for the deployment to be ready 02/24/23 11:10:03.546
Feb 24 11:10:03.573: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:10:05.594
STEP: Verifying the service has paired with the endpoint 02/24/23 11:10:05.66
Feb 24 11:10:06.661: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 02/24/23 11:10:06.673
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/24/23 11:10:06.701
STEP: Creating a configMap that should not be mutated 02/24/23 11:10:06.712
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/24/23 11:10:06.731
STEP: Creating a configMap that should be mutated 02/24/23 11:10:06.741
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:10:06.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8600" for this suite. 02/24/23 11:10:06.783
STEP: Destroying namespace "webhook-8600-markers" for this suite. 02/24/23 11:10:06.794
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":48,"skipped":740,"failed":0}
------------------------------
• [4.371 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:02.567
    Feb 24 11:10:02.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:10:02.569
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:02.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:02.601
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:10:02.634
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:10:03.505
    STEP: Deploying the webhook pod 02/24/23 11:10:03.525
    STEP: Wait for the deployment to be ready 02/24/23 11:10:03.546
    Feb 24 11:10:03.573: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:10:05.594
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:10:05.66
    Feb 24 11:10:06.661: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 02/24/23 11:10:06.673
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/24/23 11:10:06.701
    STEP: Creating a configMap that should not be mutated 02/24/23 11:10:06.712
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/24/23 11:10:06.731
    STEP: Creating a configMap that should be mutated 02/24/23 11:10:06.741
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:10:06.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8600" for this suite. 02/24/23 11:10:06.783
    STEP: Destroying namespace "webhook-8600-markers" for this suite. 02/24/23 11:10:06.794
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:06.944
Feb 24 11:10:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:10:06.945
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:06.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:06.992
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-f8964bd0-ca0f-48eb-ab0e-6654257d311a 02/24/23 11:10:07
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:10:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-18" for this suite. 02/24/23 11:10:07.012
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":49,"skipped":777,"failed":0}
------------------------------
• [0.081 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:06.944
    Feb 24 11:10:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:10:06.945
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:06.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:06.992
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-f8964bd0-ca0f-48eb-ab0e-6654257d311a 02/24/23 11:10:07
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:10:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-18" for this suite. 02/24/23 11:10:07.012
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:07.025
Feb 24 11:10:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename namespaces 02/24/23 11:10:07.026
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.057
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 02/24/23 11:10:07.06
STEP: patching the Namespace 02/24/23 11:10:07.082
STEP: get the Namespace and ensuring it has the label 02/24/23 11:10:07.092
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:10:07.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3953" for this suite. 02/24/23 11:10:07.107
STEP: Destroying namespace "nspatchtest-57c81584-b17f-4a55-bf93-d932b7a8d035-2652" for this suite. 02/24/23 11:10:07.118
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":50,"skipped":779,"failed":0}
------------------------------
• [0.106 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:07.025
    Feb 24 11:10:07.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:10:07.026
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.057
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 02/24/23 11:10:07.06
    STEP: patching the Namespace 02/24/23 11:10:07.082
    STEP: get the Namespace and ensuring it has the label 02/24/23 11:10:07.092
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:10:07.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3953" for this suite. 02/24/23 11:10:07.107
    STEP: Destroying namespace "nspatchtest-57c81584-b17f-4a55-bf93-d932b7a8d035-2652" for this suite. 02/24/23 11:10:07.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:07.134
Feb 24 11:10:07.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename events 02/24/23 11:10:07.135
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.177
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/24/23 11:10:07.185
Feb 24 11:10:07.191: INFO: created test-event-1
Feb 24 11:10:07.199: INFO: created test-event-2
Feb 24 11:10:07.205: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/24/23 11:10:07.205
STEP: delete collection of events 02/24/23 11:10:07.22
Feb 24 11:10:07.221: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/24/23 11:10:07.271
Feb 24 11:10:07.271: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 24 11:10:07.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4359" for this suite. 02/24/23 11:10:07.282
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":51,"skipped":787,"failed":0}
------------------------------
• [0.160 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:07.134
    Feb 24 11:10:07.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename events 02/24/23 11:10:07.135
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.177
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/24/23 11:10:07.185
    Feb 24 11:10:07.191: INFO: created test-event-1
    Feb 24 11:10:07.199: INFO: created test-event-2
    Feb 24 11:10:07.205: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/24/23 11:10:07.205
    STEP: delete collection of events 02/24/23 11:10:07.22
    Feb 24 11:10:07.221: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/24/23 11:10:07.271
    Feb 24 11:10:07.271: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 24 11:10:07.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4359" for this suite. 02/24/23 11:10:07.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:07.296
Feb 24 11:10:07.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename tables 02/24/23 11:10:07.297
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.322
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Feb 24 11:10:07.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2774" for this suite. 02/24/23 11:10:07.332
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":52,"skipped":796,"failed":0}
------------------------------
• [0.046 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:07.296
    Feb 24 11:10:07.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename tables 02/24/23 11:10:07.297
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.322
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Feb 24 11:10:07.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-2774" for this suite. 02/24/23 11:10:07.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:07.349
Feb 24 11:10:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename disruption 02/24/23 11:10:07.35
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.376
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 02/24/23 11:10:07.379
STEP: Waiting for the pdb to be processed 02/24/23 11:10:07.385
STEP: First trying to evict a pod which shouldn't be evictable 02/24/23 11:10:07.407
STEP: Waiting for all pods to be running 02/24/23 11:10:07.407
Feb 24 11:10:07.413: INFO: pods: 0 < 3
STEP: locating a running pod 02/24/23 11:10:09.419
STEP: Updating the pdb to allow a pod to be evicted 02/24/23 11:10:09.434
STEP: Waiting for the pdb to be processed 02/24/23 11:10:09.448
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:10:09.455
STEP: Waiting for all pods to be running 02/24/23 11:10:09.455
STEP: Waiting for the pdb to observed all healthy pods 02/24/23 11:10:09.462
STEP: Patching the pdb to disallow a pod to be evicted 02/24/23 11:10:09.5
STEP: Waiting for the pdb to be processed 02/24/23 11:10:09.556
STEP: Waiting for all pods to be running 02/24/23 11:10:11.572
STEP: locating a running pod 02/24/23 11:10:11.58
STEP: Deleting the pdb to allow a pod to be evicted 02/24/23 11:10:11.599
STEP: Waiting for the pdb to be deleted 02/24/23 11:10:11.615
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:10:11.62
STEP: Waiting for all pods to be running 02/24/23 11:10:11.621
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 24 11:10:11.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6572" for this suite. 02/24/23 11:10:11.68
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":53,"skipped":869,"failed":0}
------------------------------
• [4.368 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:07.349
    Feb 24 11:10:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename disruption 02/24/23 11:10:07.35
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:07.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:07.376
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 02/24/23 11:10:07.379
    STEP: Waiting for the pdb to be processed 02/24/23 11:10:07.385
    STEP: First trying to evict a pod which shouldn't be evictable 02/24/23 11:10:07.407
    STEP: Waiting for all pods to be running 02/24/23 11:10:07.407
    Feb 24 11:10:07.413: INFO: pods: 0 < 3
    STEP: locating a running pod 02/24/23 11:10:09.419
    STEP: Updating the pdb to allow a pod to be evicted 02/24/23 11:10:09.434
    STEP: Waiting for the pdb to be processed 02/24/23 11:10:09.448
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:10:09.455
    STEP: Waiting for all pods to be running 02/24/23 11:10:09.455
    STEP: Waiting for the pdb to observed all healthy pods 02/24/23 11:10:09.462
    STEP: Patching the pdb to disallow a pod to be evicted 02/24/23 11:10:09.5
    STEP: Waiting for the pdb to be processed 02/24/23 11:10:09.556
    STEP: Waiting for all pods to be running 02/24/23 11:10:11.572
    STEP: locating a running pod 02/24/23 11:10:11.58
    STEP: Deleting the pdb to allow a pod to be evicted 02/24/23 11:10:11.599
    STEP: Waiting for the pdb to be deleted 02/24/23 11:10:11.615
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/24/23 11:10:11.62
    STEP: Waiting for all pods to be running 02/24/23 11:10:11.621
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 24 11:10:11.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6572" for this suite. 02/24/23 11:10:11.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:11.721
Feb 24 11:10:11.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:10:11.722
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:11.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:11.749
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-4800 02/24/23 11:10:11.754
STEP: creating replication controller nodeport-test in namespace services-4800 02/24/23 11:10:11.842
I0224 11:10:11.860198      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4800, replica count: 2
I0224 11:10:14.911948      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:10:14.912: INFO: Creating new exec pod
Feb 24 11:10:14.920: INFO: Waiting up to 5m0s for pod "execpodp6pfh" in namespace "services-4800" to be "running"
Feb 24 11:10:14.925: INFO: Pod "execpodp6pfh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.119422ms
Feb 24 11:10:16.933: INFO: Pod "execpodp6pfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.013717296s
Feb 24 11:10:16.934: INFO: Pod "execpodp6pfh" satisfied condition "running"
Feb 24 11:10:17.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 24 11:10:18.106: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 24 11:10:18.106: INFO: stdout: "nodeport-test-nnpk9"
Feb 24 11:10:18.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.99.190 80'
Feb 24 11:10:18.257: INFO: stderr: "+ nc -v -t -w 2 10.109.99.190 80\nConnection to 10.109.99.190 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Feb 24 11:10:18.258: INFO: stdout: "nodeport-test-nnpk9"
Feb 24 11:10:18.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 30225'
Feb 24 11:10:18.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 30225\nConnection to 172.31.217.191 30225 port [tcp/*] succeeded!\n"
Feb 24 11:10:18.438: INFO: stdout: ""
Feb 24 11:10:19.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 30225'
Feb 24 11:10:19.663: INFO: stderr: "+ nc -v -t -w 2 172.31.217.191 30225\n+ echo hostName\nConnection to 172.31.217.191 30225 port [tcp/*] succeeded!\n"
Feb 24 11:10:19.663: INFO: stdout: ""
Feb 24 11:10:20.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 30225'
Feb 24 11:10:20.678: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 172.31.217.191 30225\nConnection to 172.31.217.191 30225 port [tcp/*] succeeded!\n"
Feb 24 11:10:20.678: INFO: stdout: "nodeport-test-n4pz5"
Feb 24 11:10:20.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.215.124 30225'
Feb 24 11:10:20.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.215.124 30225\nConnection to 172.31.215.124 30225 port [tcp/*] succeeded!\n"
Feb 24 11:10:20.851: INFO: stdout: "nodeport-test-n4pz5"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:10:20.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4800" for this suite. 02/24/23 11:10:20.861
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":54,"skipped":890,"failed":0}
------------------------------
• [SLOW TEST] [9.175 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:11.721
    Feb 24 11:10:11.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:10:11.722
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:11.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:11.749
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-4800 02/24/23 11:10:11.754
    STEP: creating replication controller nodeport-test in namespace services-4800 02/24/23 11:10:11.842
    I0224 11:10:11.860198      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4800, replica count: 2
    I0224 11:10:14.911948      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:10:14.912: INFO: Creating new exec pod
    Feb 24 11:10:14.920: INFO: Waiting up to 5m0s for pod "execpodp6pfh" in namespace "services-4800" to be "running"
    Feb 24 11:10:14.925: INFO: Pod "execpodp6pfh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.119422ms
    Feb 24 11:10:16.933: INFO: Pod "execpodp6pfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.013717296s
    Feb 24 11:10:16.934: INFO: Pod "execpodp6pfh" satisfied condition "running"
    Feb 24 11:10:17.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 24 11:10:18.106: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 24 11:10:18.106: INFO: stdout: "nodeport-test-nnpk9"
    Feb 24 11:10:18.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.99.190 80'
    Feb 24 11:10:18.257: INFO: stderr: "+ nc -v -t -w 2 10.109.99.190 80\nConnection to 10.109.99.190 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Feb 24 11:10:18.258: INFO: stdout: "nodeport-test-nnpk9"
    Feb 24 11:10:18.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 30225'
    Feb 24 11:10:18.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 30225\nConnection to 172.31.217.191 30225 port [tcp/*] succeeded!\n"
    Feb 24 11:10:18.438: INFO: stdout: ""
    Feb 24 11:10:19.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 30225'
    Feb 24 11:10:19.663: INFO: stderr: "+ nc -v -t -w 2 172.31.217.191 30225\n+ echo hostName\nConnection to 172.31.217.191 30225 port [tcp/*] succeeded!\n"
    Feb 24 11:10:19.663: INFO: stdout: ""
    Feb 24 11:10:20.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 30225'
    Feb 24 11:10:20.678: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 172.31.217.191 30225\nConnection to 172.31.217.191 30225 port [tcp/*] succeeded!\n"
    Feb 24 11:10:20.678: INFO: stdout: "nodeport-test-n4pz5"
    Feb 24 11:10:20.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4800 exec execpodp6pfh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.215.124 30225'
    Feb 24 11:10:20.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.215.124 30225\nConnection to 172.31.215.124 30225 port [tcp/*] succeeded!\n"
    Feb 24 11:10:20.851: INFO: stdout: "nodeport-test-n4pz5"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:10:20.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4800" for this suite. 02/24/23 11:10:20.861
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:20.897
Feb 24 11:10:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:10:20.898
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:20.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:20.981
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/24/23 11:10:20.988
Feb 24 11:10:20.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:10:23.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:10:38.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3467" for this suite. 02/24/23 11:10:38.351
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":55,"skipped":891,"failed":0}
------------------------------
• [SLOW TEST] [17.465 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:20.897
    Feb 24 11:10:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:10:20.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:20.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:20.981
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/24/23 11:10:20.988
    Feb 24 11:10:20.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:10:23.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:10:38.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3467" for this suite. 02/24/23 11:10:38.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:38.365
Feb 24 11:10:38.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:10:38.366
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:38.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:38.394
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 02/24/23 11:10:38.402
STEP: waiting for available Endpoint 02/24/23 11:10:38.41
STEP: listing all Endpoints 02/24/23 11:10:38.412
STEP: updating the Endpoint 02/24/23 11:10:38.417
STEP: fetching the Endpoint 02/24/23 11:10:38.425
STEP: patching the Endpoint 02/24/23 11:10:38.429
STEP: fetching the Endpoint 02/24/23 11:10:38.438
STEP: deleting the Endpoint by Collection 02/24/23 11:10:38.442
STEP: waiting for Endpoint deletion 02/24/23 11:10:38.455
STEP: fetching the Endpoint 02/24/23 11:10:38.457
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:10:38.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9223" for this suite. 02/24/23 11:10:38.467
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":56,"skipped":913,"failed":0}
------------------------------
• [0.109 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:38.365
    Feb 24 11:10:38.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:10:38.366
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:38.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:38.394
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 02/24/23 11:10:38.402
    STEP: waiting for available Endpoint 02/24/23 11:10:38.41
    STEP: listing all Endpoints 02/24/23 11:10:38.412
    STEP: updating the Endpoint 02/24/23 11:10:38.417
    STEP: fetching the Endpoint 02/24/23 11:10:38.425
    STEP: patching the Endpoint 02/24/23 11:10:38.429
    STEP: fetching the Endpoint 02/24/23 11:10:38.438
    STEP: deleting the Endpoint by Collection 02/24/23 11:10:38.442
    STEP: waiting for Endpoint deletion 02/24/23 11:10:38.455
    STEP: fetching the Endpoint 02/24/23 11:10:38.457
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:10:38.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9223" for this suite. 02/24/23 11:10:38.467
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:38.476
Feb 24 11:10:38.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:10:38.478
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:38.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:38.518
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 11:10:38.522
Feb 24 11:10:38.531: INFO: Waiting up to 5m0s for pod "pod-937ba40c-deec-4a06-8892-b5793491d696" in namespace "emptydir-2274" to be "Succeeded or Failed"
Feb 24 11:10:38.539: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696": Phase="Pending", Reason="", readiness=false. Elapsed: 7.724876ms
Feb 24 11:10:40.546: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014470488s
Feb 24 11:10:42.545: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013336638s
STEP: Saw pod success 02/24/23 11:10:42.545
Feb 24 11:10:42.545: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696" satisfied condition "Succeeded or Failed"
Feb 24 11:10:42.550: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-937ba40c-deec-4a06-8892-b5793491d696 container test-container: <nil>
STEP: delete the pod 02/24/23 11:10:42.568
Feb 24 11:10:42.586: INFO: Waiting for pod pod-937ba40c-deec-4a06-8892-b5793491d696 to disappear
Feb 24 11:10:42.594: INFO: Pod pod-937ba40c-deec-4a06-8892-b5793491d696 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:10:42.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2274" for this suite. 02/24/23 11:10:42.602
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":57,"skipped":919,"failed":0}
------------------------------
• [4.135 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:38.476
    Feb 24 11:10:38.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:10:38.478
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:38.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:38.518
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 11:10:38.522
    Feb 24 11:10:38.531: INFO: Waiting up to 5m0s for pod "pod-937ba40c-deec-4a06-8892-b5793491d696" in namespace "emptydir-2274" to be "Succeeded or Failed"
    Feb 24 11:10:38.539: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696": Phase="Pending", Reason="", readiness=false. Elapsed: 7.724876ms
    Feb 24 11:10:40.546: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014470488s
    Feb 24 11:10:42.545: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013336638s
    STEP: Saw pod success 02/24/23 11:10:42.545
    Feb 24 11:10:42.545: INFO: Pod "pod-937ba40c-deec-4a06-8892-b5793491d696" satisfied condition "Succeeded or Failed"
    Feb 24 11:10:42.550: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-937ba40c-deec-4a06-8892-b5793491d696 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:10:42.568
    Feb 24 11:10:42.586: INFO: Waiting for pod pod-937ba40c-deec-4a06-8892-b5793491d696 to disappear
    Feb 24 11:10:42.594: INFO: Pod pod-937ba40c-deec-4a06-8892-b5793491d696 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:10:42.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2274" for this suite. 02/24/23 11:10:42.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:42.613
Feb 24 11:10:42.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-runtime 02/24/23 11:10:42.617
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:42.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:42.65
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 02/24/23 11:10:42.655
STEP: wait for the container to reach Succeeded 02/24/23 11:10:42.664
STEP: get the container status 02/24/23 11:10:45.687
STEP: the container should be terminated 02/24/23 11:10:45.691
STEP: the termination message should be set 02/24/23 11:10:45.691
Feb 24 11:10:45.692: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/24/23 11:10:45.692
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 24 11:10:45.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-107" for this suite. 02/24/23 11:10:45.722
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":58,"skipped":947,"failed":0}
------------------------------
• [3.120 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:42.613
    Feb 24 11:10:42.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-runtime 02/24/23 11:10:42.617
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:42.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:42.65
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 02/24/23 11:10:42.655
    STEP: wait for the container to reach Succeeded 02/24/23 11:10:42.664
    STEP: get the container status 02/24/23 11:10:45.687
    STEP: the container should be terminated 02/24/23 11:10:45.691
    STEP: the termination message should be set 02/24/23 11:10:45.691
    Feb 24 11:10:45.692: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/24/23 11:10:45.692
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 24 11:10:45.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-107" for this suite. 02/24/23 11:10:45.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:45.742
Feb 24 11:10:45.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:10:45.743
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:45.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:45.775
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 24 11:10:45.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:10:52.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1762" for this suite. 02/24/23 11:10:52.157
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":59,"skipped":1010,"failed":0}
------------------------------
• [SLOW TEST] [6.424 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:45.742
    Feb 24 11:10:45.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:10:45.743
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:45.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:45.775
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 24 11:10:45.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:10:52.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1762" for this suite. 02/24/23 11:10:52.157
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:10:52.167
Feb 24 11:10:52.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 11:10:52.169
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:52.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:52.195
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2528 02/24/23 11:10:52.198
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 02/24/23 11:10:52.205
STEP: Creating stateful set ss in namespace statefulset-2528 02/24/23 11:10:52.21
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2528 02/24/23 11:10:52.218
Feb 24 11:10:52.223: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:11:02.229: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/24/23 11:11:02.229
Feb 24 11:11:02.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:11:02.434: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:11:02.434: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:11:02.434: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:11:02.440: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 24 11:11:12.445: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:11:12.445: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:11:12.465: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999761s
Feb 24 11:11:13.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995337746s
Feb 24 11:11:14.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990016857s
Feb 24 11:11:15.481: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984506808s
Feb 24 11:11:16.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978522342s
Feb 24 11:11:17.492: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973290371s
Feb 24 11:11:18.501: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968155844s
Feb 24 11:11:19.506: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958806201s
Feb 24 11:11:20.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953760674s
Feb 24 11:11:21.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.172735ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2528 02/24/23 11:11:22.517
Feb 24 11:11:22.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:11:22.687: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:11:22.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:11:22.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:11:22.692: INFO: Found 1 stateful pods, waiting for 3
Feb 24 11:11:32.700: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:11:32.700: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:11:32.700: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/24/23 11:11:32.7
STEP: Scale down will halt with unhealthy stateful pod 02/24/23 11:11:32.7
Feb 24 11:11:32.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:11:32.911: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:11:32.911: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:11:32.911: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:11:32.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:11:33.218: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:11:33.218: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:11:33.218: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:11:33.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:11:33.418: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:11:33.419: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:11:33.419: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:11:33.419: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:11:33.423: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 24 11:11:43.435: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:11:43.435: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:11:43.435: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:11:43.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999972s
Feb 24 11:11:44.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99557528s
Feb 24 11:11:45.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989963593s
Feb 24 11:11:46.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98355628s
Feb 24 11:11:47.475: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977913054s
Feb 24 11:11:48.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971933244s
Feb 24 11:11:49.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965437385s
Feb 24 11:11:50.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959658083s
Feb 24 11:11:51.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954445516s
Feb 24 11:11:52.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.074397ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2528 02/24/23 11:11:53.504
Feb 24 11:11:53.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:11:53.729: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:11:53.729: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:11:53.729: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:11:53.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:11:53.945: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:11:53.945: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:11:53.945: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:11:53.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:11:54.159: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:11:54.159: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:11:54.159: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:11:54.159: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/24/23 11:12:04.185
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 11:12:04.186: INFO: Deleting all statefulset in ns statefulset-2528
Feb 24 11:12:04.195: INFO: Scaling statefulset ss to 0
Feb 24 11:12:04.219: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:12:04.224: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 11:12:04.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2528" for this suite. 02/24/23 11:12:04.26
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":60,"skipped":1013,"failed":0}
------------------------------
• [SLOW TEST] [72.104 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:10:52.167
    Feb 24 11:10:52.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:10:52.169
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:10:52.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:10:52.195
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2528 02/24/23 11:10:52.198
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/24/23 11:10:52.205
    STEP: Creating stateful set ss in namespace statefulset-2528 02/24/23 11:10:52.21
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2528 02/24/23 11:10:52.218
    Feb 24 11:10:52.223: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:11:02.229: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/24/23 11:11:02.229
    Feb 24 11:11:02.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:11:02.434: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:11:02.434: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:11:02.434: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:11:02.440: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 24 11:11:12.445: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:11:12.445: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:11:12.465: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999761s
    Feb 24 11:11:13.470: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995337746s
    Feb 24 11:11:14.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990016857s
    Feb 24 11:11:15.481: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984506808s
    Feb 24 11:11:16.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978522342s
    Feb 24 11:11:17.492: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973290371s
    Feb 24 11:11:18.501: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968155844s
    Feb 24 11:11:19.506: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958806201s
    Feb 24 11:11:20.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953760674s
    Feb 24 11:11:21.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.172735ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2528 02/24/23 11:11:22.517
    Feb 24 11:11:22.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:11:22.687: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:11:22.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:11:22.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:11:22.692: INFO: Found 1 stateful pods, waiting for 3
    Feb 24 11:11:32.700: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:11:32.700: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:11:32.700: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/24/23 11:11:32.7
    STEP: Scale down will halt with unhealthy stateful pod 02/24/23 11:11:32.7
    Feb 24 11:11:32.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:11:32.911: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:11:32.911: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:11:32.911: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:11:32.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:11:33.218: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:11:33.218: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:11:33.218: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:11:33.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:11:33.418: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:11:33.419: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:11:33.419: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:11:33.419: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:11:33.423: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 24 11:11:43.435: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:11:43.435: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:11:43.435: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:11:43.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999972s
    Feb 24 11:11:44.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99557528s
    Feb 24 11:11:45.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989963593s
    Feb 24 11:11:46.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98355628s
    Feb 24 11:11:47.475: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977913054s
    Feb 24 11:11:48.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971933244s
    Feb 24 11:11:49.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965437385s
    Feb 24 11:11:50.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959658083s
    Feb 24 11:11:51.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954445516s
    Feb 24 11:11:52.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.074397ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2528 02/24/23 11:11:53.504
    Feb 24 11:11:53.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:11:53.729: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:11:53.729: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:11:53.729: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:11:53.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:11:53.945: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:11:53.945: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:11:53.945: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:11:53.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-2528 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:11:54.159: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:11:54.159: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:11:54.159: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:11:54.159: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/24/23 11:12:04.185
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 11:12:04.186: INFO: Deleting all statefulset in ns statefulset-2528
    Feb 24 11:12:04.195: INFO: Scaling statefulset ss to 0
    Feb 24 11:12:04.219: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:12:04.224: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 11:12:04.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2528" for this suite. 02/24/23 11:12:04.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:04.292
Feb 24 11:12:04.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replicaset 02/24/23 11:12:04.301
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:04.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:04.338
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 24 11:12:04.359: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 11:12:09.376: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:12:09.376
STEP: Scaling up "test-rs" replicaset  02/24/23 11:12:09.376
Feb 24 11:12:09.392: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/24/23 11:12:09.392
W0224 11:12:09.451581      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 24 11:12:09.454: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:12:09.499: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:12:09.525: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:12:09.572: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
Feb 24 11:12:10.567: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 2, AvailableReplicas 2
Feb 24 11:12:10.844: INFO: observed Replicaset test-rs in namespace replicaset-6981 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 24 11:12:10.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6981" for this suite. 02/24/23 11:12:10.854
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":61,"skipped":1045,"failed":0}
------------------------------
• [SLOW TEST] [6.572 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:04.292
    Feb 24 11:12:04.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:12:04.301
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:04.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:04.338
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 24 11:12:04.359: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 11:12:09.376: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:12:09.376
    STEP: Scaling up "test-rs" replicaset  02/24/23 11:12:09.376
    Feb 24 11:12:09.392: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/24/23 11:12:09.392
    W0224 11:12:09.451581      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 24 11:12:09.454: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:12:09.499: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:12:09.525: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:12:09.572: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 1, AvailableReplicas 1
    Feb 24 11:12:10.567: INFO: observed ReplicaSet test-rs in namespace replicaset-6981 with ReadyReplicas 2, AvailableReplicas 2
    Feb 24 11:12:10.844: INFO: observed Replicaset test-rs in namespace replicaset-6981 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 24 11:12:10.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6981" for this suite. 02/24/23 11:12:10.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:10.88
Feb 24 11:12:10.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:12:10.88
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:10.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:10.906
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:12:10.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5190" for this suite. 02/24/23 11:12:10.962
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":62,"skipped":1069,"failed":0}
------------------------------
• [0.090 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:10.88
    Feb 24 11:12:10.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:12:10.88
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:10.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:10.906
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:12:10.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5190" for this suite. 02/24/23 11:12:10.962
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:10.972
Feb 24 11:12:10.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename events 02/24/23 11:12:10.973
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:10.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:11.007
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/24/23 11:12:11.011
STEP: listing all events in all namespaces 02/24/23 11:12:11.016
STEP: patching the test event 02/24/23 11:12:11.031
STEP: fetching the test event 02/24/23 11:12:11.041
STEP: updating the test event 02/24/23 11:12:11.045
STEP: getting the test event 02/24/23 11:12:11.058
STEP: deleting the test event 02/24/23 11:12:11.063
STEP: listing all events in all namespaces 02/24/23 11:12:11.074
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 24 11:12:11.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5169" for this suite. 02/24/23 11:12:11.095
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":63,"skipped":1070,"failed":0}
------------------------------
• [0.131 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:10.972
    Feb 24 11:12:10.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename events 02/24/23 11:12:10.973
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:10.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:11.007
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/24/23 11:12:11.011
    STEP: listing all events in all namespaces 02/24/23 11:12:11.016
    STEP: patching the test event 02/24/23 11:12:11.031
    STEP: fetching the test event 02/24/23 11:12:11.041
    STEP: updating the test event 02/24/23 11:12:11.045
    STEP: getting the test event 02/24/23 11:12:11.058
    STEP: deleting the test event 02/24/23 11:12:11.063
    STEP: listing all events in all namespaces 02/24/23 11:12:11.074
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 24 11:12:11.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5169" for this suite. 02/24/23 11:12:11.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:11.109
Feb 24 11:12:11.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:12:11.11
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:11.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:11.143
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-02c97574-a104-491b-86b4-6677f1bc56ab 02/24/23 11:12:11.148
STEP: Creating a pod to test consume secrets 02/24/23 11:12:11.157
Feb 24 11:12:11.169: INFO: Waiting up to 5m0s for pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7" in namespace "secrets-8560" to be "Succeeded or Failed"
Feb 24 11:12:11.175: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.567275ms
Feb 24 11:12:13.180: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011739152s
Feb 24 11:12:15.181: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012061871s
STEP: Saw pod success 02/24/23 11:12:15.181
Feb 24 11:12:15.181: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7" satisfied condition "Succeeded or Failed"
Feb 24 11:12:15.186: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:12:15.205
Feb 24 11:12:15.218: INFO: Waiting for pod pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7 to disappear
Feb 24 11:12:15.224: INFO: Pod pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:12:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8560" for this suite. 02/24/23 11:12:15.233
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":64,"skipped":1097,"failed":0}
------------------------------
• [4.133 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:11.109
    Feb 24 11:12:11.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:12:11.11
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:11.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:11.143
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-02c97574-a104-491b-86b4-6677f1bc56ab 02/24/23 11:12:11.148
    STEP: Creating a pod to test consume secrets 02/24/23 11:12:11.157
    Feb 24 11:12:11.169: INFO: Waiting up to 5m0s for pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7" in namespace "secrets-8560" to be "Succeeded or Failed"
    Feb 24 11:12:11.175: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.567275ms
    Feb 24 11:12:13.180: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011739152s
    Feb 24 11:12:15.181: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012061871s
    STEP: Saw pod success 02/24/23 11:12:15.181
    Feb 24 11:12:15.181: INFO: Pod "pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7" satisfied condition "Succeeded or Failed"
    Feb 24 11:12:15.186: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:12:15.205
    Feb 24 11:12:15.218: INFO: Waiting for pod pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7 to disappear
    Feb 24 11:12:15.224: INFO: Pod pod-secrets-3a659fb9-da8f-4092-b22d-cb4d3b31b9d7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:12:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8560" for this suite. 02/24/23 11:12:15.233
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:15.243
Feb 24 11:12:15.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 11:12:15.244
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:15.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:15.277
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 24 11:12:15.282: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 24 11:12:15.293: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 11:12:20.299: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:12:20.299
Feb 24 11:12:20.299: INFO: Creating deployment "test-rolling-update-deployment"
Feb 24 11:12:20.310: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 24 11:12:20.322: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 24 11:12:22.334: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 24 11:12:22.340: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:12:22.355: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4751  9516175c-8dd1-4774-a397-78a9f07b93ad 11716 1 2023-02-24 11:12:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049a7208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:12:20 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-24 11:12:21 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 11:12:22.360: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4751  4578ea33-9758-4ed4-bd2f-c4d04e21d22c 11706 1 2023-02-24 11:12:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9516175c-8dd1-4774-a397-78a9f07b93ad 0xc0048e4e17 0xc0048e4e18}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9516175c-8dd1-4774-a397-78a9f07b93ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048e4ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:12:22.361: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 24 11:12:22.361: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4751  533103c3-98d2-4950-b04f-525569b95312 11715 2 2023-02-24 11:12:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9516175c-8dd1-4774-a397-78a9f07b93ad 0xc0048e4cb7 0xc0048e4cb8}] [] [{e2e.test Update apps/v1 2023-02-24 11:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9516175c-8dd1-4774-a397-78a9f07b93ad\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048e4d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:12:22.365: INFO: Pod "test-rolling-update-deployment-78f575d8ff-4brjv" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-4brjv test-rolling-update-deployment-78f575d8ff- deployment-4751  d119af15-440b-491b-a940-793721968a2a 11705 0 2023-02-24 11:12:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:a74de3cec707654ed0a0cf47faadf81572018ff424adb6c2d4a9a9f7ac305992 cni.projectcalico.org/podIP:10.244.4.26/32 cni.projectcalico.org/podIPs:10.244.4.26/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 4578ea33-9758-4ed4-bd2f-c4d04e21d22c 0xc0048e5477 0xc0048e5478}] [] [{Go-http-client Update v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4578ea33-9758-4ed4-bd2f-c4d04e21d22c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztwlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztwlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.26,StartTime:2023-02-24 11:12:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:12:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://28c2b935f4326e71b366a46b39850c5d1762157d5fe8ccbc026da8412882ba95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 11:12:22.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4751" for this suite. 02/24/23 11:12:22.373
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":65,"skipped":1099,"failed":0}
------------------------------
• [SLOW TEST] [7.142 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:15.243
    Feb 24 11:12:15.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 11:12:15.244
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:15.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:15.277
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 24 11:12:15.282: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 24 11:12:15.293: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 11:12:20.299: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:12:20.299
    Feb 24 11:12:20.299: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 24 11:12:20.310: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 24 11:12:20.322: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb 24 11:12:22.334: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 24 11:12:22.340: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:12:22.355: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4751  9516175c-8dd1-4774-a397-78a9f07b93ad 11716 1 2023-02-24 11:12:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049a7208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:12:20 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-24 11:12:21 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 11:12:22.360: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-4751  4578ea33-9758-4ed4-bd2f-c4d04e21d22c 11706 1 2023-02-24 11:12:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9516175c-8dd1-4774-a397-78a9f07b93ad 0xc0048e4e17 0xc0048e4e18}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9516175c-8dd1-4774-a397-78a9f07b93ad\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048e4ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:12:22.361: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 24 11:12:22.361: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4751  533103c3-98d2-4950-b04f-525569b95312 11715 2 2023-02-24 11:12:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9516175c-8dd1-4774-a397-78a9f07b93ad 0xc0048e4cb7 0xc0048e4cb8}] [] [{e2e.test Update apps/v1 2023-02-24 11:12:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9516175c-8dd1-4774-a397-78a9f07b93ad\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048e4d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:12:22.365: INFO: Pod "test-rolling-update-deployment-78f575d8ff-4brjv" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-4brjv test-rolling-update-deployment-78f575d8ff- deployment-4751  d119af15-440b-491b-a940-793721968a2a 11705 0 2023-02-24 11:12:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:a74de3cec707654ed0a0cf47faadf81572018ff424adb6c2d4a9a9f7ac305992 cni.projectcalico.org/podIP:10.244.4.26/32 cni.projectcalico.org/podIPs:10.244.4.26/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 4578ea33-9758-4ed4-bd2f-c4d04e21d22c 0xc0048e5477 0xc0048e5478}] [] [{Go-http-client Update v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:12:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4578ea33-9758-4ed4-bd2f-c4d04e21d22c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:12:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztwlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztwlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:12:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.26,StartTime:2023-02-24 11:12:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:12:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://28c2b935f4326e71b366a46b39850c5d1762157d5fe8ccbc026da8412882ba95,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 11:12:22.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4751" for this suite. 02/24/23 11:12:22.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:22.393
Feb 24 11:12:22.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:12:22.394
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:22.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:22.424
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 02/24/23 11:12:22.427
Feb 24 11:12:22.428: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 24 11:12:22.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
Feb 24 11:12:23.500: INFO: stderr: ""
Feb 24 11:12:23.500: INFO: stdout: "service/agnhost-replica created\n"
Feb 24 11:12:23.500: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 24 11:12:23.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
Feb 24 11:12:23.902: INFO: stderr: ""
Feb 24 11:12:23.902: INFO: stdout: "service/agnhost-primary created\n"
Feb 24 11:12:23.902: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 24 11:12:23.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
Feb 24 11:12:25.008: INFO: stderr: ""
Feb 24 11:12:25.008: INFO: stdout: "service/frontend created\n"
Feb 24 11:12:25.008: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 24 11:12:25.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
Feb 24 11:12:25.304: INFO: stderr: ""
Feb 24 11:12:25.304: INFO: stdout: "deployment.apps/frontend created\n"
Feb 24 11:12:25.304: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 24 11:12:25.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
Feb 24 11:12:26.398: INFO: stderr: ""
Feb 24 11:12:26.398: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 24 11:12:26.398: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 24 11:12:26.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
Feb 24 11:12:26.702: INFO: stderr: ""
Feb 24 11:12:26.702: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/24/23 11:12:26.702
Feb 24 11:12:26.702: INFO: Waiting for all frontend pods to be Running.
Feb 24 11:12:31.753: INFO: Waiting for frontend to serve content.
Feb 24 11:12:31.772: INFO: Trying to add a new entry to the guestbook.
Feb 24 11:12:31.788: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/24/23 11:12:31.812
Feb 24 11:12:31.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
Feb 24 11:12:31.997: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:31.997: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:31.997
Feb 24 11:12:31.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
Feb 24 11:12:32.214: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:32.214: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:32.214
Feb 24 11:12:32.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
Feb 24 11:12:32.315: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:32.315: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:32.315
Feb 24 11:12:32.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
Feb 24 11:12:32.402: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:32.402: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:32.402
Feb 24 11:12:32.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
Feb 24 11:12:32.528: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:32.528: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/24/23 11:12:32.528
Feb 24 11:12:32.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
Feb 24 11:12:32.663: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 11:12:32.663: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:12:32.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2533" for this suite. 02/24/23 11:12:32.672
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":66,"skipped":1169,"failed":0}
------------------------------
• [SLOW TEST] [10.385 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:22.393
    Feb 24 11:12:22.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:12:22.394
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:22.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:22.424
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 02/24/23 11:12:22.427
    Feb 24 11:12:22.428: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 24 11:12:22.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
    Feb 24 11:12:23.500: INFO: stderr: ""
    Feb 24 11:12:23.500: INFO: stdout: "service/agnhost-replica created\n"
    Feb 24 11:12:23.500: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 24 11:12:23.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
    Feb 24 11:12:23.902: INFO: stderr: ""
    Feb 24 11:12:23.902: INFO: stdout: "service/agnhost-primary created\n"
    Feb 24 11:12:23.902: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 24 11:12:23.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
    Feb 24 11:12:25.008: INFO: stderr: ""
    Feb 24 11:12:25.008: INFO: stdout: "service/frontend created\n"
    Feb 24 11:12:25.008: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 24 11:12:25.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
    Feb 24 11:12:25.304: INFO: stderr: ""
    Feb 24 11:12:25.304: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 24 11:12:25.304: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 24 11:12:25.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
    Feb 24 11:12:26.398: INFO: stderr: ""
    Feb 24 11:12:26.398: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 24 11:12:26.398: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 24 11:12:26.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 create -f -'
    Feb 24 11:12:26.702: INFO: stderr: ""
    Feb 24 11:12:26.702: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/24/23 11:12:26.702
    Feb 24 11:12:26.702: INFO: Waiting for all frontend pods to be Running.
    Feb 24 11:12:31.753: INFO: Waiting for frontend to serve content.
    Feb 24 11:12:31.772: INFO: Trying to add a new entry to the guestbook.
    Feb 24 11:12:31.788: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/24/23 11:12:31.812
    Feb 24 11:12:31.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
    Feb 24 11:12:31.997: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:31.997: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:31.997
    Feb 24 11:12:31.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
    Feb 24 11:12:32.214: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:32.214: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:32.214
    Feb 24 11:12:32.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
    Feb 24 11:12:32.315: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:32.315: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:32.315
    Feb 24 11:12:32.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
    Feb 24 11:12:32.402: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:32.402: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:32.402
    Feb 24 11:12:32.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
    Feb 24 11:12:32.528: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:32.528: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/24/23 11:12:32.528
    Feb 24 11:12:32.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2533 delete --grace-period=0 --force -f -'
    Feb 24 11:12:32.663: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 11:12:32.663: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:12:32.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2533" for this suite. 02/24/23 11:12:32.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:32.779
Feb 24 11:12:32.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:12:32.799
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:32.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:32.84
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-6f221acb-45c9-4bc4-8543-5cd9524a7f03 02/24/23 11:12:32.844
STEP: Creating a pod to test consume configMaps 02/24/23 11:12:32.851
Feb 24 11:12:32.860: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7" in namespace "projected-5461" to be "Succeeded or Failed"
Feb 24 11:12:32.871: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.521808ms
Feb 24 11:12:34.878: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017127634s
Feb 24 11:12:36.877: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016160893s
STEP: Saw pod success 02/24/23 11:12:36.877
Feb 24 11:12:36.877: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7" satisfied condition "Succeeded or Failed"
Feb 24 11:12:36.881: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:12:36.892
Feb 24 11:12:36.916: INFO: Waiting for pod pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7 to disappear
Feb 24 11:12:36.920: INFO: Pod pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 11:12:36.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5461" for this suite. 02/24/23 11:12:36.928
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":67,"skipped":1198,"failed":0}
------------------------------
• [4.160 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:32.779
    Feb 24 11:12:32.780: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:12:32.799
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:32.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:32.84
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-6f221acb-45c9-4bc4-8543-5cd9524a7f03 02/24/23 11:12:32.844
    STEP: Creating a pod to test consume configMaps 02/24/23 11:12:32.851
    Feb 24 11:12:32.860: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7" in namespace "projected-5461" to be "Succeeded or Failed"
    Feb 24 11:12:32.871: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.521808ms
    Feb 24 11:12:34.878: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017127634s
    Feb 24 11:12:36.877: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016160893s
    STEP: Saw pod success 02/24/23 11:12:36.877
    Feb 24 11:12:36.877: INFO: Pod "pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7" satisfied condition "Succeeded or Failed"
    Feb 24 11:12:36.881: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:12:36.892
    Feb 24 11:12:36.916: INFO: Waiting for pod pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7 to disappear
    Feb 24 11:12:36.920: INFO: Pod pod-projected-configmaps-451cbbe5-e961-4939-a65c-6352aacf75c7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 11:12:36.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5461" for this suite. 02/24/23 11:12:36.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:36.941
Feb 24 11:12:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:12:36.942
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:36.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:36.978
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-1209 02/24/23 11:12:36.981
STEP: creating service affinity-clusterip-transition in namespace services-1209 02/24/23 11:12:36.981
STEP: creating replication controller affinity-clusterip-transition in namespace services-1209 02/24/23 11:12:37.006
I0224 11:12:37.017960      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1209, replica count: 3
I0224 11:12:40.069181      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:12:40.079: INFO: Creating new exec pod
Feb 24 11:12:40.094: INFO: Waiting up to 5m0s for pod "execpod-affinitykqbzg" in namespace "services-1209" to be "running"
Feb 24 11:12:40.101: INFO: Pod "execpod-affinitykqbzg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.594182ms
Feb 24 11:12:42.108: INFO: Pod "execpod-affinitykqbzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.013875394s
Feb 24 11:12:42.108: INFO: Pod "execpod-affinitykqbzg" satisfied condition "running"
Feb 24 11:12:43.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 24 11:12:43.353: INFO: stderr: "+ nc -v+ echo hostName\n -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 24 11:12:43.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:12:43.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.12.33 80'
Feb 24 11:12:43.518: INFO: stderr: "+ nc -v -t -w 2 10.97.12.33 80\n+ echo hostName\nConnection to 10.97.12.33 80 port [tcp/http] succeeded!\n"
Feb 24 11:12:43.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:12:43.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.12.33:80/ ; done'
Feb 24 11:12:43.849: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n"
Feb 24 11:12:43.849: INFO: stdout: "\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf"
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
Feb 24 11:12:43.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.12.33:80/ ; done'
Feb 24 11:12:44.228: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n"
Feb 24 11:12:44.228: INFO: stdout: "\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w"
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
Feb 24 11:12:44.228: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1209, will wait for the garbage collector to delete the pods 02/24/23 11:12:44.251
Feb 24 11:12:44.315: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.988565ms
Feb 24 11:12:44.416: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.639912ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:12:46.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1209" for this suite. 02/24/23 11:12:46.762
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":68,"skipped":1219,"failed":0}
------------------------------
• [SLOW TEST] [9.831 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:36.941
    Feb 24 11:12:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:12:36.942
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:36.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:36.978
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-1209 02/24/23 11:12:36.981
    STEP: creating service affinity-clusterip-transition in namespace services-1209 02/24/23 11:12:36.981
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1209 02/24/23 11:12:37.006
    I0224 11:12:37.017960      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1209, replica count: 3
    I0224 11:12:40.069181      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:12:40.079: INFO: Creating new exec pod
    Feb 24 11:12:40.094: INFO: Waiting up to 5m0s for pod "execpod-affinitykqbzg" in namespace "services-1209" to be "running"
    Feb 24 11:12:40.101: INFO: Pod "execpod-affinitykqbzg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.594182ms
    Feb 24 11:12:42.108: INFO: Pod "execpod-affinitykqbzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.013875394s
    Feb 24 11:12:42.108: INFO: Pod "execpod-affinitykqbzg" satisfied condition "running"
    Feb 24 11:12:43.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Feb 24 11:12:43.353: INFO: stderr: "+ nc -v+ echo hostName\n -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 24 11:12:43.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:12:43.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.12.33 80'
    Feb 24 11:12:43.518: INFO: stderr: "+ nc -v -t -w 2 10.97.12.33 80\n+ echo hostName\nConnection to 10.97.12.33 80 port [tcp/http] succeeded!\n"
    Feb 24 11:12:43.518: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:12:43.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.12.33:80/ ; done'
    Feb 24 11:12:43.849: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n"
    Feb 24 11:12:43.849: INFO: stdout: "\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-zshn2\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf\naffinity-clusterip-transition-8kjjf"
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-zshn2
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
    Feb 24 11:12:43.849: INFO: Received response from host: affinity-clusterip-transition-8kjjf
    Feb 24 11:12:43.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1209 exec execpod-affinitykqbzg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.97.12.33:80/ ; done'
    Feb 24 11:12:44.228: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.97.12.33:80/\n"
    Feb 24 11:12:44.228: INFO: stdout: "\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w\naffinity-clusterip-transition-x858w"
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Received response from host: affinity-clusterip-transition-x858w
    Feb 24 11:12:44.228: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1209, will wait for the garbage collector to delete the pods 02/24/23 11:12:44.251
    Feb 24 11:12:44.315: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.988565ms
    Feb 24 11:12:44.416: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.639912ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:12:46.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1209" for this suite. 02/24/23 11:12:46.762
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:12:46.774
Feb 24 11:12:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 11:12:46.775
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:46.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:46.806
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1743 02/24/23 11:12:46.81
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-1743 02/24/23 11:12:46.824
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1743 02/24/23 11:12:46.838
Feb 24 11:12:46.844: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:12:56.850: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/24/23 11:12:56.85
Feb 24 11:12:56.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:12:57.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:12:57.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:12:57.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:12:57.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 24 11:13:07.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:13:07.141: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:13:07.165: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 24 11:13:07.166: INFO: ss-0  ip-172-31-216-47.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  }]
Feb 24 11:13:07.167: INFO: 
Feb 24 11:13:07.167: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 24 11:13:08.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992990599s
Feb 24 11:13:09.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987577907s
Feb 24 11:13:10.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9808449s
Feb 24 11:13:11.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975727197s
Feb 24 11:13:12.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97036805s
Feb 24 11:13:13.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964345408s
Feb 24 11:13:14.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95825031s
Feb 24 11:13:15.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948762246s
Feb 24 11:13:16.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 935.516516ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1743 02/24/23 11:13:17.237
Feb 24 11:13:17.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:13:17.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 24 11:13:17.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:13:17.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:13:17.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:13:17.551: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 24 11:13:17.551: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:13:17.551: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:13:17.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 24 11:13:17.750: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 24 11:13:17.750: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 24 11:13:17.750: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 24 11:13:17.755: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 24 11:13:27.764: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:13:27.764: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 11:13:27.764: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/24/23 11:13:27.764
Feb 24 11:13:27.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:13:27.989: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:13:27.989: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:13:27.989: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:13:27.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:13:28.153: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:13:28.153: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:13:28.153: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:13:28.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 24 11:13:28.341: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 24 11:13:28.341: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 24 11:13:28.341: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 24 11:13:28.341: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:13:28.346: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 24 11:13:38.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:13:38.360: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:13:38.360: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 24 11:13:38.376: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Feb 24 11:13:38.376: INFO: ss-0  ip-172-31-216-47.eu-west-3.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  }]
Feb 24 11:13:38.376: INFO: ss-1  ip-172-31-215-124.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  }]
Feb 24 11:13:38.376: INFO: ss-2  ip-172-31-217-191.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  }]
Feb 24 11:13:38.376: INFO: 
Feb 24 11:13:38.376: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 24 11:13:39.381: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.995118831s
Feb 24 11:13:40.387: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989929349s
Feb 24 11:13:41.393: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984565708s
Feb 24 11:13:42.398: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.97835535s
Feb 24 11:13:43.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.973449703s
Feb 24 11:13:44.408: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.968376945s
Feb 24 11:13:45.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.963628032s
Feb 24 11:13:46.420: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.957587104s
Feb 24 11:13:47.428: INFO: Verifying statefulset ss doesn't scale past 0 for another 950.57758ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1743 02/24/23 11:13:48.428
Feb 24 11:13:48.434: INFO: Scaling statefulset ss to 0
Feb 24 11:13:48.450: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 11:13:48.459: INFO: Deleting all statefulset in ns statefulset-1743
Feb 24 11:13:48.464: INFO: Scaling statefulset ss to 0
Feb 24 11:13:48.489: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:13:48.494: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 11:13:48.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1743" for this suite. 02/24/23 11:13:48.518
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":69,"skipped":1225,"failed":0}
------------------------------
• [SLOW TEST] [61.753 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:12:46.774
    Feb 24 11:12:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:12:46.775
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:12:46.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:12:46.806
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1743 02/24/23 11:12:46.81
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-1743 02/24/23 11:12:46.824
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1743 02/24/23 11:12:46.838
    Feb 24 11:12:46.844: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:12:56.850: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/24/23 11:12:56.85
    Feb 24 11:12:56.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:12:57.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:12:57.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:12:57.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:12:57.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 24 11:13:07.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:13:07.141: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:13:07.165: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 24 11:13:07.166: INFO: ss-0  ip-172-31-216-47.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:57 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  }]
    Feb 24 11:13:07.167: INFO: 
    Feb 24 11:13:07.167: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb 24 11:13:08.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992990599s
    Feb 24 11:13:09.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987577907s
    Feb 24 11:13:10.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9808449s
    Feb 24 11:13:11.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975727197s
    Feb 24 11:13:12.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97036805s
    Feb 24 11:13:13.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964345408s
    Feb 24 11:13:14.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95825031s
    Feb 24 11:13:15.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948762246s
    Feb 24 11:13:16.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 935.516516ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1743 02/24/23 11:13:17.237
    Feb 24 11:13:17.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:13:17.401: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 24 11:13:17.401: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:13:17.401: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:13:17.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:13:17.551: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 24 11:13:17.551: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:13:17.551: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:13:17.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 24 11:13:17.750: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 24 11:13:17.750: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 24 11:13:17.750: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 24 11:13:17.755: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Feb 24 11:13:27.764: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:13:27.764: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 11:13:27.764: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/24/23 11:13:27.764
    Feb 24 11:13:27.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:13:27.989: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:13:27.989: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:13:27.989: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:13:27.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:13:28.153: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:13:28.153: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:13:28.153: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:13:28.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=statefulset-1743 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 24 11:13:28.341: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 24 11:13:28.341: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 24 11:13:28.341: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 24 11:13:28.341: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:13:28.346: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Feb 24 11:13:38.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:13:38.360: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:13:38.360: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 24 11:13:38.376: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
    Feb 24 11:13:38.376: INFO: ss-0  ip-172-31-216-47.eu-west-3.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:12:46 +0000 UTC  }]
    Feb 24 11:13:38.376: INFO: ss-1  ip-172-31-215-124.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  }]
    Feb 24 11:13:38.376: INFO: ss-2  ip-172-31-217-191.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 11:13:07 +0000 UTC  }]
    Feb 24 11:13:38.376: INFO: 
    Feb 24 11:13:38.376: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 24 11:13:39.381: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.995118831s
    Feb 24 11:13:40.387: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989929349s
    Feb 24 11:13:41.393: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984565708s
    Feb 24 11:13:42.398: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.97835535s
    Feb 24 11:13:43.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.973449703s
    Feb 24 11:13:44.408: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.968376945s
    Feb 24 11:13:45.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.963628032s
    Feb 24 11:13:46.420: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.957587104s
    Feb 24 11:13:47.428: INFO: Verifying statefulset ss doesn't scale past 0 for another 950.57758ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1743 02/24/23 11:13:48.428
    Feb 24 11:13:48.434: INFO: Scaling statefulset ss to 0
    Feb 24 11:13:48.450: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 11:13:48.459: INFO: Deleting all statefulset in ns statefulset-1743
    Feb 24 11:13:48.464: INFO: Scaling statefulset ss to 0
    Feb 24 11:13:48.489: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:13:48.494: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 11:13:48.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1743" for this suite. 02/24/23 11:13:48.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:13:48.536
Feb 24 11:13:48.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:13:48.537
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:13:48.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:13:48.58
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:13:48.584
Feb 24 11:13:48.595: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5" in namespace "downward-api-7388" to be "Succeeded or Failed"
Feb 24 11:13:48.602: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07469ms
Feb 24 11:13:50.608: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013243847s
Feb 24 11:13:52.610: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539687s
STEP: Saw pod success 02/24/23 11:13:52.61
Feb 24 11:13:52.610: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5" satisfied condition "Succeeded or Failed"
Feb 24 11:13:52.615: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5 container client-container: <nil>
STEP: delete the pod 02/24/23 11:13:52.631
Feb 24 11:13:52.647: INFO: Waiting for pod downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5 to disappear
Feb 24 11:13:52.651: INFO: Pod downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:13:52.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7388" for this suite. 02/24/23 11:13:52.66
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":70,"skipped":1295,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:13:48.536
    Feb 24 11:13:48.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:13:48.537
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:13:48.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:13:48.58
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:13:48.584
    Feb 24 11:13:48.595: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5" in namespace "downward-api-7388" to be "Succeeded or Failed"
    Feb 24 11:13:48.602: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07469ms
    Feb 24 11:13:50.608: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013243847s
    Feb 24 11:13:52.610: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015539687s
    STEP: Saw pod success 02/24/23 11:13:52.61
    Feb 24 11:13:52.610: INFO: Pod "downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5" satisfied condition "Succeeded or Failed"
    Feb 24 11:13:52.615: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:13:52.631
    Feb 24 11:13:52.647: INFO: Waiting for pod downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5 to disappear
    Feb 24 11:13:52.651: INFO: Pod downwardapi-volume-1796573b-d504-491a-a71d-9d9cdf7e67f5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:13:52.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7388" for this suite. 02/24/23 11:13:52.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:13:52.68
Feb 24 11:13:52.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:13:52.681
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:13:52.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:13:52.708
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3544 02/24/23 11:13:52.712
STEP: creating a selector 02/24/23 11:13:52.712
STEP: Creating the service pods in kubernetes 02/24/23 11:13:52.712
Feb 24 11:13:52.712: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 11:13:52.754: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3544" to be "running and ready"
Feb 24 11:13:52.764: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.201057ms
Feb 24 11:13:52.764: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:13:54.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015094685s
Feb 24 11:13:54.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:13:56.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015523795s
Feb 24 11:13:56.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:13:58.777: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023471778s
Feb 24 11:13:58.777: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:14:00.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015567383s
Feb 24 11:14:00.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:14:02.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015528346s
Feb 24 11:14:02.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:14:04.770: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016009162s
Feb 24 11:14:04.770: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 11:14:04.770: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 11:14:04.777: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3544" to be "running and ready"
Feb 24 11:14:04.782: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.046623ms
Feb 24 11:14:04.783: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 11:14:04.783: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 11:14:04.789: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3544" to be "running and ready"
Feb 24 11:14:04.794: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.046338ms
Feb 24 11:14:04.794: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 11:14:04.794: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 11:14:04.799
Feb 24 11:14:04.820: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3544" to be "running"
Feb 24 11:14:04.828: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.932633ms
Feb 24 11:14:06.834: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013312994s
Feb 24 11:14:06.834: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 11:14:06.841: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3544" to be "running"
Feb 24 11:14:06.846: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.802874ms
Feb 24 11:14:06.846: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 24 11:14:06.852: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 11:14:06.852: INFO: Going to poll 10.244.4.32 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 11:14:06.857: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:14:06.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:14:06.857: INFO: ExecWithOptions: Clientset creation
Feb 24 11:14:06.858: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.4.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:14:07.959: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 24 11:14:07.959: INFO: Going to poll 10.244.3.57 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 11:14:07.964: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:14:07.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:14:07.965: INFO: ExecWithOptions: Clientset creation
Feb 24 11:14:07.965: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.57+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:14:09.059: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 24 11:14:09.059: INFO: Going to poll 10.244.5.21 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 24 11:14:09.064: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:14:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:14:09.065: INFO: ExecWithOptions: Clientset creation
Feb 24 11:14:09.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.5.21+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 11:14:10.205: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 24 11:14:10.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3544" for this suite. 02/24/23 11:14:10.214
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":71,"skipped":1306,"failed":0}
------------------------------
• [SLOW TEST] [17.543 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:13:52.68
    Feb 24 11:13:52.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:13:52.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:13:52.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:13:52.708
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3544 02/24/23 11:13:52.712
    STEP: creating a selector 02/24/23 11:13:52.712
    STEP: Creating the service pods in kubernetes 02/24/23 11:13:52.712
    Feb 24 11:13:52.712: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 11:13:52.754: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3544" to be "running and ready"
    Feb 24 11:13:52.764: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.201057ms
    Feb 24 11:13:52.764: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:13:54.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015094685s
    Feb 24 11:13:54.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:13:56.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.015523795s
    Feb 24 11:13:56.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:13:58.777: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023471778s
    Feb 24 11:13:58.777: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:14:00.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.015567383s
    Feb 24 11:14:00.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:14:02.769: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015528346s
    Feb 24 11:14:02.769: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:14:04.770: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016009162s
    Feb 24 11:14:04.770: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 11:14:04.770: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 11:14:04.777: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3544" to be "running and ready"
    Feb 24 11:14:04.782: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.046623ms
    Feb 24 11:14:04.783: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 11:14:04.783: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 11:14:04.789: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3544" to be "running and ready"
    Feb 24 11:14:04.794: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.046338ms
    Feb 24 11:14:04.794: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 11:14:04.794: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 11:14:04.799
    Feb 24 11:14:04.820: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3544" to be "running"
    Feb 24 11:14:04.828: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.932633ms
    Feb 24 11:14:06.834: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013312994s
    Feb 24 11:14:06.834: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 11:14:06.841: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3544" to be "running"
    Feb 24 11:14:06.846: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.802874ms
    Feb 24 11:14:06.846: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 24 11:14:06.852: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 11:14:06.852: INFO: Going to poll 10.244.4.32 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 11:14:06.857: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:14:06.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:14:06.857: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:14:06.858: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.4.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:14:07.959: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 24 11:14:07.959: INFO: Going to poll 10.244.3.57 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 11:14:07.964: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:14:07.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:14:07.965: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:14:07.965: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.3.57+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:14:09.059: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 24 11:14:09.059: INFO: Going to poll 10.244.5.21 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 11:14:09.064: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3544 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:14:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:14:09.065: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:14:09.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3544/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.5.21+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 11:14:10.205: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 24 11:14:10.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3544" for this suite. 02/24/23 11:14:10.214
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:14:10.223
Feb 24 11:14:10.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:14:10.224
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:10.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:10.296
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 02/24/23 11:14:27.305
STEP: Creating a ResourceQuota 02/24/23 11:14:32.312
STEP: Ensuring resource quota status is calculated 02/24/23 11:14:32.318
STEP: Creating a ConfigMap 02/24/23 11:14:34.324
STEP: Ensuring resource quota status captures configMap creation 02/24/23 11:14:34.339
STEP: Deleting a ConfigMap 02/24/23 11:14:36.345
STEP: Ensuring resource quota status released usage 02/24/23 11:14:36.353
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 11:14:38.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7980" for this suite. 02/24/23 11:14:38.365
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":72,"skipped":1309,"failed":0}
------------------------------
• [SLOW TEST] [28.152 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:14:10.223
    Feb 24 11:14:10.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:14:10.224
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:10.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:10.296
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 02/24/23 11:14:27.305
    STEP: Creating a ResourceQuota 02/24/23 11:14:32.312
    STEP: Ensuring resource quota status is calculated 02/24/23 11:14:32.318
    STEP: Creating a ConfigMap 02/24/23 11:14:34.324
    STEP: Ensuring resource quota status captures configMap creation 02/24/23 11:14:34.339
    STEP: Deleting a ConfigMap 02/24/23 11:14:36.345
    STEP: Ensuring resource quota status released usage 02/24/23 11:14:36.353
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 11:14:38.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7980" for this suite. 02/24/23 11:14:38.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:14:38.381
Feb 24 11:14:38.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:14:38.383
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:38.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:38.411
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Feb 24 11:14:38.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: creating the pod 02/24/23 11:14:38.415
STEP: submitting the pod to kubernetes 02/24/23 11:14:38.415
Feb 24 11:14:38.427: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560" in namespace "pods-2023" to be "running and ready"
Feb 24 11:14:38.432: INFO: Pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024771ms
Feb 24 11:14:38.432: INFO: The phase of Pod pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:14:40.437: INFO: Pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560": Phase="Running", Reason="", readiness=true. Elapsed: 2.010690471s
Feb 24 11:14:40.437: INFO: The phase of Pod pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560 is Running (Ready = true)
Feb 24 11:14:40.437: INFO: Pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 11:14:40.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2023" for this suite. 02/24/23 11:14:40.471
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":73,"skipped":1347,"failed":0}
------------------------------
• [2.099 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:14:38.381
    Feb 24 11:14:38.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:14:38.383
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:38.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:38.411
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Feb 24 11:14:38.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: creating the pod 02/24/23 11:14:38.415
    STEP: submitting the pod to kubernetes 02/24/23 11:14:38.415
    Feb 24 11:14:38.427: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560" in namespace "pods-2023" to be "running and ready"
    Feb 24 11:14:38.432: INFO: Pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024771ms
    Feb 24 11:14:38.432: INFO: The phase of Pod pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:14:40.437: INFO: Pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560": Phase="Running", Reason="", readiness=true. Elapsed: 2.010690471s
    Feb 24 11:14:40.437: INFO: The phase of Pod pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560 is Running (Ready = true)
    Feb 24 11:14:40.437: INFO: Pod "pod-logs-websocket-04105651-6f0b-4490-82c4-a42bb0ddb560" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 11:14:40.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2023" for this suite. 02/24/23 11:14:40.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:14:40.485
Feb 24 11:14:40.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:14:40.486
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:40.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:40.526
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 11:14:40.529
Feb 24 11:14:40.539: INFO: Waiting up to 5m0s for pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29" in namespace "emptydir-7280" to be "Succeeded or Failed"
Feb 24 11:14:40.548: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.40428ms
Feb 24 11:14:42.553: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014264609s
Feb 24 11:14:44.554: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014877028s
STEP: Saw pod success 02/24/23 11:14:44.554
Feb 24 11:14:44.554: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29" satisfied condition "Succeeded or Failed"
Feb 24 11:14:44.559: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-28f334d4-4bd7-4f25-828c-4b92e2550f29 container test-container: <nil>
STEP: delete the pod 02/24/23 11:14:44.576
Feb 24 11:14:44.588: INFO: Waiting for pod pod-28f334d4-4bd7-4f25-828c-4b92e2550f29 to disappear
Feb 24 11:14:44.592: INFO: Pod pod-28f334d4-4bd7-4f25-828c-4b92e2550f29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:14:44.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7280" for this suite. 02/24/23 11:14:44.601
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":74,"skipped":1407,"failed":0}
------------------------------
• [4.128 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:14:40.485
    Feb 24 11:14:40.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:14:40.486
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:40.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:40.526
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 11:14:40.529
    Feb 24 11:14:40.539: INFO: Waiting up to 5m0s for pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29" in namespace "emptydir-7280" to be "Succeeded or Failed"
    Feb 24 11:14:40.548: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29": Phase="Pending", Reason="", readiness=false. Elapsed: 9.40428ms
    Feb 24 11:14:42.553: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014264609s
    Feb 24 11:14:44.554: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014877028s
    STEP: Saw pod success 02/24/23 11:14:44.554
    Feb 24 11:14:44.554: INFO: Pod "pod-28f334d4-4bd7-4f25-828c-4b92e2550f29" satisfied condition "Succeeded or Failed"
    Feb 24 11:14:44.559: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-28f334d4-4bd7-4f25-828c-4b92e2550f29 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:14:44.576
    Feb 24 11:14:44.588: INFO: Waiting for pod pod-28f334d4-4bd7-4f25-828c-4b92e2550f29 to disappear
    Feb 24 11:14:44.592: INFO: Pod pod-28f334d4-4bd7-4f25-828c-4b92e2550f29 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:14:44.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7280" for this suite. 02/24/23 11:14:44.601
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:14:44.615
Feb 24 11:14:44.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:14:44.616
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:44.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:44.642
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-3881 02/24/23 11:14:44.646
Feb 24 11:14:44.655: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3881" to be "running and ready"
Feb 24 11:14:44.664: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.986445ms
Feb 24 11:14:44.664: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:14:46.670: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014597789s
Feb 24 11:14:46.670: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 24 11:14:46.670: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 24 11:14:46.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 24 11:14:47.015: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 24 11:14:47.015: INFO: stdout: "iptables"
Feb 24 11:14:47.015: INFO: proxyMode: iptables
Feb 24 11:14:47.028: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 24 11:14:47.032: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3881 02/24/23 11:14:47.032
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3881 02/24/23 11:14:47.149
I0224 11:14:47.169701      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3881, replica count: 3
I0224 11:14:50.221051      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:14:50.250: INFO: Creating new exec pod
Feb 24 11:14:50.257: INFO: Waiting up to 5m0s for pod "execpod-affinityzc9p6" in namespace "services-3881" to be "running"
Feb 24 11:14:50.263: INFO: Pod "execpod-affinityzc9p6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.975247ms
Feb 24 11:14:52.268: INFO: Pod "execpod-affinityzc9p6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011426849s
Feb 24 11:14:52.268: INFO: Pod "execpod-affinityzc9p6" satisfied condition "running"
Feb 24 11:14:53.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 24 11:14:53.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 24 11:14:53.512: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:14:53.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.216.254 80'
Feb 24 11:14:53.792: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.216.254 80\nConnection to 10.101.216.254 80 port [tcp/http] succeeded!\n"
Feb 24 11:14:53.792: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:14:53.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 31045'
Feb 24 11:14:53.999: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 31045\nConnection to 172.31.217.191 31045 port [tcp/*] succeeded!\n"
Feb 24 11:14:53.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:14:53.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31045'
Feb 24 11:14:54.188: INFO: stderr: "+ nc -v -t -w 2 172.31.216.47 31045\n+ echo hostName\nConnection to 172.31.216.47 31045 port [tcp/*] succeeded!\n"
Feb 24 11:14:54.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:14:54.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:31045/ ; done'
Feb 24 11:14:54.546: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n"
Feb 24 11:14:54.546: INFO: stdout: "\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88"
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
Feb 24 11:14:54.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.215.124:31045/'
Feb 24 11:14:54.741: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n"
Feb 24 11:14:54.741: INFO: stdout: "affinity-nodeport-timeout-tqv88"
Feb 24 11:15:14.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.215.124:31045/'
Feb 24 11:15:14.992: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n"
Feb 24 11:15:14.992: INFO: stdout: "affinity-nodeport-timeout-bbxwg"
Feb 24 11:15:14.992: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3881, will wait for the garbage collector to delete the pods 02/24/23 11:15:15.011
Feb 24 11:15:15.078: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.500949ms
Feb 24 11:15:15.279: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.908352ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:15:17.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3881" for this suite. 02/24/23 11:15:17.452
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":75,"skipped":1411,"failed":0}
------------------------------
• [SLOW TEST] [32.853 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:14:44.615
    Feb 24 11:14:44.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:14:44.616
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:14:44.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:14:44.642
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-3881 02/24/23 11:14:44.646
    Feb 24 11:14:44.655: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3881" to be "running and ready"
    Feb 24 11:14:44.664: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 8.986445ms
    Feb 24 11:14:44.664: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:14:46.670: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014597789s
    Feb 24 11:14:46.670: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 24 11:14:46.670: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 24 11:14:46.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 24 11:14:47.015: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb 24 11:14:47.015: INFO: stdout: "iptables"
    Feb 24 11:14:47.015: INFO: proxyMode: iptables
    Feb 24 11:14:47.028: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 24 11:14:47.032: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-3881 02/24/23 11:14:47.032
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-3881 02/24/23 11:14:47.149
    I0224 11:14:47.169701      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3881, replica count: 3
    I0224 11:14:50.221051      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:14:50.250: INFO: Creating new exec pod
    Feb 24 11:14:50.257: INFO: Waiting up to 5m0s for pod "execpod-affinityzc9p6" in namespace "services-3881" to be "running"
    Feb 24 11:14:50.263: INFO: Pod "execpod-affinityzc9p6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.975247ms
    Feb 24 11:14:52.268: INFO: Pod "execpod-affinityzc9p6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011426849s
    Feb 24 11:14:52.268: INFO: Pod "execpod-affinityzc9p6" satisfied condition "running"
    Feb 24 11:14:53.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Feb 24 11:14:53.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Feb 24 11:14:53.512: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:14:53.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.216.254 80'
    Feb 24 11:14:53.792: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.216.254 80\nConnection to 10.101.216.254 80 port [tcp/http] succeeded!\n"
    Feb 24 11:14:53.792: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:14:53.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 31045'
    Feb 24 11:14:53.999: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 31045\nConnection to 172.31.217.191 31045 port [tcp/*] succeeded!\n"
    Feb 24 11:14:53.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:14:53.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31045'
    Feb 24 11:14:54.188: INFO: stderr: "+ nc -v -t -w 2 172.31.216.47 31045\n+ echo hostName\nConnection to 172.31.216.47 31045 port [tcp/*] succeeded!\n"
    Feb 24 11:14:54.188: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:14:54.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:31045/ ; done'
    Feb 24 11:14:54.546: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n"
    Feb 24 11:14:54.546: INFO: stdout: "\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88\naffinity-nodeport-timeout-tqv88"
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Received response from host: affinity-nodeport-timeout-tqv88
    Feb 24 11:14:54.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.215.124:31045/'
    Feb 24 11:14:54.741: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n"
    Feb 24 11:14:54.741: INFO: stdout: "affinity-nodeport-timeout-tqv88"
    Feb 24 11:15:14.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3881 exec execpod-affinityzc9p6 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.215.124:31045/'
    Feb 24 11:15:14.992: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.215.124:31045/\n"
    Feb 24 11:15:14.992: INFO: stdout: "affinity-nodeport-timeout-bbxwg"
    Feb 24 11:15:14.992: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3881, will wait for the garbage collector to delete the pods 02/24/23 11:15:15.011
    Feb 24 11:15:15.078: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.500949ms
    Feb 24 11:15:15.279: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.908352ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:15:17.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3881" for this suite. 02/24/23 11:15:17.452
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:15:17.481
Feb 24 11:15:17.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename server-version 02/24/23 11:15:17.497
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:17.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:17.558
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/24/23 11:15:17.565
STEP: Confirm major version 02/24/23 11:15:17.579
Feb 24 11:15:17.580: INFO: Major version: 1
STEP: Confirm minor version 02/24/23 11:15:17.58
Feb 24 11:15:17.580: INFO: cleanMinorVersion: 25
Feb 24 11:15:17.580: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Feb 24 11:15:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3780" for this suite. 02/24/23 11:15:17.588
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":76,"skipped":1415,"failed":0}
------------------------------
• [0.118 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:15:17.481
    Feb 24 11:15:17.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename server-version 02/24/23 11:15:17.497
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:17.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:17.558
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/24/23 11:15:17.565
    STEP: Confirm major version 02/24/23 11:15:17.579
    Feb 24 11:15:17.580: INFO: Major version: 1
    STEP: Confirm minor version 02/24/23 11:15:17.58
    Feb 24 11:15:17.580: INFO: cleanMinorVersion: 25
    Feb 24 11:15:17.580: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Feb 24 11:15:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-3780" for this suite. 02/24/23 11:15:17.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:15:17.619
Feb 24 11:15:17.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-runtime 02/24/23 11:15:17.62
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:17.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:17.648
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/24/23 11:15:17.663
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/24/23 11:15:35.775
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/24/23 11:15:35.782
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/24/23 11:15:35.794
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/24/23 11:15:35.795
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/24/23 11:15:35.821
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/24/23 11:15:38.845
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/24/23 11:15:40.86
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/24/23 11:15:40.87
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/24/23 11:15:40.87
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/24/23 11:15:40.9
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/24/23 11:15:41.91
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/24/23 11:15:44.935
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/24/23 11:15:44.944
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/24/23 11:15:44.944
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 24 11:15:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5755" for this suite. 02/24/23 11:15:44.988
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":77,"skipped":1443,"failed":0}
------------------------------
• [SLOW TEST] [27.378 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:15:17.619
    Feb 24 11:15:17.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-runtime 02/24/23 11:15:17.62
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:17.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:17.648
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/24/23 11:15:17.663
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/24/23 11:15:35.775
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/24/23 11:15:35.782
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/24/23 11:15:35.794
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/24/23 11:15:35.795
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/24/23 11:15:35.821
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/24/23 11:15:38.845
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/24/23 11:15:40.86
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/24/23 11:15:40.87
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/24/23 11:15:40.87
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/24/23 11:15:40.9
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/24/23 11:15:41.91
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/24/23 11:15:44.935
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/24/23 11:15:44.944
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/24/23 11:15:44.944
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 24 11:15:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5755" for this suite. 02/24/23 11:15:44.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:15:44.999
Feb 24 11:15:45.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:15:45.006
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:45.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:45.051
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Feb 24 11:15:45.068: INFO: Waiting up to 2m0s for pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" in namespace "var-expansion-3111" to be "container 0 failed with reason CreateContainerConfigError"
Feb 24 11:15:45.078: INFO: Pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.58665ms
Feb 24 11:15:47.083: INFO: Pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015119395s
Feb 24 11:15:47.084: INFO: Pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 24 11:15:47.084: INFO: Deleting pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" in namespace "var-expansion-3111"
Feb 24 11:15:47.093: INFO: Wait up to 5m0s for pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 11:15:51.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3111" for this suite. 02/24/23 11:15:51.113
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":78,"skipped":1483,"failed":0}
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:15:44.999
    Feb 24 11:15:45.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:15:45.006
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:45.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:45.051
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Feb 24 11:15:45.068: INFO: Waiting up to 2m0s for pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" in namespace "var-expansion-3111" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 24 11:15:45.078: INFO: Pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.58665ms
    Feb 24 11:15:47.083: INFO: Pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015119395s
    Feb 24 11:15:47.084: INFO: Pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 24 11:15:47.084: INFO: Deleting pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" in namespace "var-expansion-3111"
    Feb 24 11:15:47.093: INFO: Wait up to 5m0s for pod "var-expansion-a26509ef-5322-42ab-a1ed-e2fb7a7ad14d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 11:15:51.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3111" for this suite. 02/24/23 11:15:51.113
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:15:51.132
Feb 24 11:15:51.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 11:15:51.133
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:51.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:51.162
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 24 11:15:51.189: INFO: Waiting up to 5m0s for pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78" in namespace "emptydir-wrapper-9825" to be "running and ready"
Feb 24 11:15:51.197: INFO: Pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.374754ms
Feb 24 11:15:51.197: INFO: The phase of Pod pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:15:53.203: INFO: Pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78": Phase="Running", Reason="", readiness=true. Elapsed: 2.014019408s
Feb 24 11:15:53.203: INFO: The phase of Pod pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78 is Running (Ready = true)
Feb 24 11:15:53.203: INFO: Pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/24/23 11:15:53.207
STEP: Cleaning up the configmap 02/24/23 11:15:53.215
STEP: Cleaning up the pod 02/24/23 11:15:53.223
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 24 11:15:53.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9825" for this suite. 02/24/23 11:15:53.247
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":79,"skipped":1485,"failed":0}
------------------------------
• [2.133 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:15:51.132
    Feb 24 11:15:51.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 11:15:51.133
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:51.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:51.162
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 24 11:15:51.189: INFO: Waiting up to 5m0s for pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78" in namespace "emptydir-wrapper-9825" to be "running and ready"
    Feb 24 11:15:51.197: INFO: Pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.374754ms
    Feb 24 11:15:51.197: INFO: The phase of Pod pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:15:53.203: INFO: Pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78": Phase="Running", Reason="", readiness=true. Elapsed: 2.014019408s
    Feb 24 11:15:53.203: INFO: The phase of Pod pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78 is Running (Ready = true)
    Feb 24 11:15:53.203: INFO: Pod "pod-secrets-dda3ab9e-d393-4660-8d0d-201779773b78" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/24/23 11:15:53.207
    STEP: Cleaning up the configmap 02/24/23 11:15:53.215
    STEP: Cleaning up the pod 02/24/23 11:15:53.223
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:15:53.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-9825" for this suite. 02/24/23 11:15:53.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:15:53.27
Feb 24 11:15:53.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:15:53.271
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:53.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:53.3
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-8aa15f11-d97b-42bb-8d6f-eaa1034ca77f 02/24/23 11:15:53.304
STEP: Creating a pod to test consume configMaps 02/24/23 11:15:53.314
Feb 24 11:15:53.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6" in namespace "projected-3772" to be "Succeeded or Failed"
Feb 24 11:15:53.332: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145941ms
Feb 24 11:15:55.337: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6": Phase="Running", Reason="", readiness=false. Elapsed: 2.012596994s
Feb 24 11:15:57.337: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012200625s
STEP: Saw pod success 02/24/23 11:15:57.337
Feb 24 11:15:57.337: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6" satisfied condition "Succeeded or Failed"
Feb 24 11:15:57.342: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:15:57.355
Feb 24 11:15:57.373: INFO: Waiting for pod pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6 to disappear
Feb 24 11:15:57.378: INFO: Pod pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 11:15:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3772" for this suite. 02/24/23 11:15:57.386
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":80,"skipped":1508,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:15:53.27
    Feb 24 11:15:53.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:15:53.271
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:53.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:53.3
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-8aa15f11-d97b-42bb-8d6f-eaa1034ca77f 02/24/23 11:15:53.304
    STEP: Creating a pod to test consume configMaps 02/24/23 11:15:53.314
    Feb 24 11:15:53.325: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6" in namespace "projected-3772" to be "Succeeded or Failed"
    Feb 24 11:15:53.332: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145941ms
    Feb 24 11:15:55.337: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6": Phase="Running", Reason="", readiness=false. Elapsed: 2.012596994s
    Feb 24 11:15:57.337: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012200625s
    STEP: Saw pod success 02/24/23 11:15:57.337
    Feb 24 11:15:57.337: INFO: Pod "pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6" satisfied condition "Succeeded or Failed"
    Feb 24 11:15:57.342: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:15:57.355
    Feb 24 11:15:57.373: INFO: Waiting for pod pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6 to disappear
    Feb 24 11:15:57.378: INFO: Pod pod-projected-configmaps-1b96f382-81a1-43f8-a537-22d3be07eee6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 11:15:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3772" for this suite. 02/24/23 11:15:57.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:15:57.396
Feb 24 11:15:57.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:15:57.397
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:57.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:57.425
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Feb 24 11:15:57.455: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:15:57.461
Feb 24 11:15:57.466: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:57.466: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:57.466: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:57.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:15:57.471: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:15:58.480: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:58.480: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:58.480: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:58.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:15:58.492: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:15:59.480: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:59.480: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:59.480: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:59.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:15:59.485: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/24/23 11:15:59.501
STEP: Check that daemon pods images are updated. 02/24/23 11:15:59.516
Feb 24 11:15:59.522: INFO: Wrong image for pod: daemon-set-4mkl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:15:59.522: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:15:59.522: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:15:59.527: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:59.527: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:15:59.527: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:00.533: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:00.533: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:00.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:00.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:00.540: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:01.536: INFO: Pod daemon-set-2nk7n is not available
Feb 24 11:16:01.536: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:01.537: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:01.544: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:01.545: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:01.545: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:02.533: INFO: Pod daemon-set-2nk7n is not available
Feb 24 11:16:02.533: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:02.533: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:02.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:02.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:02.540: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:03.535: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:03.545: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:03.545: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:03.546: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:04.532: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 24 11:16:04.533: INFO: Pod daemon-set-vf4gm is not available
Feb 24 11:16:04.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:04.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:04.540: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:05.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:05.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:05.541: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:06.532: INFO: Pod daemon-set-8j66q is not available
Feb 24 11:16:06.539: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:06.539: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:06.539: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 02/24/23 11:16:06.539
Feb 24 11:16:06.546: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:06.546: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:06.546: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:06.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:16:06.551: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:16:07.560: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:07.560: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:07.560: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:16:07.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:16:07.565: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:16:07.589
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3094, will wait for the garbage collector to delete the pods 02/24/23 11:16:07.589
Feb 24 11:16:07.653: INFO: Deleting DaemonSet.extensions daemon-set took: 9.183307ms
Feb 24 11:16:07.754: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.989073ms
Feb 24 11:16:10.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:16:10.260: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:16:10.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13789"},"items":null}

Feb 24 11:16:10.269: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13789"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:16:10.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3094" for this suite. 02/24/23 11:16:10.298
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":81,"skipped":1523,"failed":0}
------------------------------
• [SLOW TEST] [12.918 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:15:57.396
    Feb 24 11:15:57.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:15:57.397
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:15:57.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:15:57.425
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Feb 24 11:15:57.455: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:15:57.461
    Feb 24 11:15:57.466: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:57.466: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:57.466: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:57.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:15:57.471: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:15:58.480: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:58.480: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:58.480: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:58.492: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:15:58.492: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:15:59.480: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:59.480: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:59.480: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:59.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:15:59.485: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/24/23 11:15:59.501
    STEP: Check that daemon pods images are updated. 02/24/23 11:15:59.516
    Feb 24 11:15:59.522: INFO: Wrong image for pod: daemon-set-4mkl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:15:59.522: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:15:59.522: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:15:59.527: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:59.527: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:15:59.527: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:00.533: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:00.533: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:00.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:00.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:00.540: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:01.536: INFO: Pod daemon-set-2nk7n is not available
    Feb 24 11:16:01.536: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:01.537: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:01.544: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:01.545: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:01.545: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:02.533: INFO: Pod daemon-set-2nk7n is not available
    Feb 24 11:16:02.533: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:02.533: INFO: Wrong image for pod: daemon-set-6njhr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:02.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:02.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:02.540: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:03.535: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:03.545: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:03.545: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:03.546: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:04.532: INFO: Wrong image for pod: daemon-set-4np6s. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 24 11:16:04.533: INFO: Pod daemon-set-vf4gm is not available
    Feb 24 11:16:04.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:04.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:04.540: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:05.540: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:05.540: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:05.541: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:06.532: INFO: Pod daemon-set-8j66q is not available
    Feb 24 11:16:06.539: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:06.539: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:06.539: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 02/24/23 11:16:06.539
    Feb 24 11:16:06.546: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:06.546: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:06.546: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:06.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:16:06.551: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:16:07.560: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:07.560: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:07.560: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:16:07.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:16:07.565: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:16:07.589
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3094, will wait for the garbage collector to delete the pods 02/24/23 11:16:07.589
    Feb 24 11:16:07.653: INFO: Deleting DaemonSet.extensions daemon-set took: 9.183307ms
    Feb 24 11:16:07.754: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.989073ms
    Feb 24 11:16:10.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:16:10.260: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:16:10.265: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13789"},"items":null}

    Feb 24 11:16:10.269: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13789"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:16:10.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3094" for this suite. 02/24/23 11:16:10.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:16:10.319
Feb 24 11:16:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename containers 02/24/23 11:16:10.32
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:10.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:10.356
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Feb 24 11:16:10.371: INFO: Waiting up to 5m0s for pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f" in namespace "containers-6739" to be "running"
Feb 24 11:16:10.378: INFO: Pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.486462ms
Feb 24 11:16:12.383: INFO: Pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011440885s
Feb 24 11:16:12.383: INFO: Pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 24 11:16:12.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6739" for this suite. 02/24/23 11:16:12.402
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":82,"skipped":1530,"failed":0}
------------------------------
• [2.094 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:16:10.319
    Feb 24 11:16:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename containers 02/24/23 11:16:10.32
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:10.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:10.356
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Feb 24 11:16:10.371: INFO: Waiting up to 5m0s for pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f" in namespace "containers-6739" to be "running"
    Feb 24 11:16:10.378: INFO: Pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.486462ms
    Feb 24 11:16:12.383: INFO: Pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011440885s
    Feb 24 11:16:12.383: INFO: Pod "client-containers-9f611da1-7b43-478a-849a-d9709ff6e81f" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 24 11:16:12.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-6739" for this suite. 02/24/23 11:16:12.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:16:12.421
Feb 24 11:16:12.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:16:12.424
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:12.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:12.45
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:16:12.471
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:16:12.861
STEP: Deploying the webhook pod 02/24/23 11:16:12.871
STEP: Wait for the deployment to be ready 02/24/23 11:16:12.889
Feb 24 11:16:12.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:16:14.916
STEP: Verifying the service has paired with the endpoint 02/24/23 11:16:14.936
Feb 24 11:16:15.937: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/24/23 11:16:15.943
STEP: create a namespace for the webhook 02/24/23 11:16:15.958
STEP: create a configmap should be unconditionally rejected by the webhook 02/24/23 11:16:15.97
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:16:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3669" for this suite. 02/24/23 11:16:16.03
STEP: Destroying namespace "webhook-3669-markers" for this suite. 02/24/23 11:16:16.039
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":83,"skipped":1542,"failed":0}
------------------------------
• [3.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:16:12.421
    Feb 24 11:16:12.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:16:12.424
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:12.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:12.45
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:16:12.471
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:16:12.861
    STEP: Deploying the webhook pod 02/24/23 11:16:12.871
    STEP: Wait for the deployment to be ready 02/24/23 11:16:12.889
    Feb 24 11:16:12.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:16:14.916
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:16:14.936
    Feb 24 11:16:15.937: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/24/23 11:16:15.943
    STEP: create a namespace for the webhook 02/24/23 11:16:15.958
    STEP: create a configmap should be unconditionally rejected by the webhook 02/24/23 11:16:15.97
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:16:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3669" for this suite. 02/24/23 11:16:16.03
    STEP: Destroying namespace "webhook-3669-markers" for this suite. 02/24/23 11:16:16.039
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:16:16.124
Feb 24 11:16:16.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replicaset 02/24/23 11:16:16.127
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:16.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:16.166
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/24/23 11:16:16.17
Feb 24 11:16:16.181: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 11:16:21.187: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:16:21.187
STEP: getting scale subresource 02/24/23 11:16:21.187
STEP: updating a scale subresource 02/24/23 11:16:21.195
STEP: verifying the replicaset Spec.Replicas was modified 02/24/23 11:16:21.206
STEP: Patch a scale subresource 02/24/23 11:16:21.215
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 24 11:16:21.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8703" for this suite. 02/24/23 11:16:21.268
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":84,"skipped":1560,"failed":0}
------------------------------
• [SLOW TEST] [5.179 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:16:16.124
    Feb 24 11:16:16.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:16:16.127
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:16.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:16.166
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/24/23 11:16:16.17
    Feb 24 11:16:16.181: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 11:16:21.187: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:16:21.187
    STEP: getting scale subresource 02/24/23 11:16:21.187
    STEP: updating a scale subresource 02/24/23 11:16:21.195
    STEP: verifying the replicaset Spec.Replicas was modified 02/24/23 11:16:21.206
    STEP: Patch a scale subresource 02/24/23 11:16:21.215
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 24 11:16:21.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8703" for this suite. 02/24/23 11:16:21.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:16:21.306
Feb 24 11:16:21.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:16:21.308
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:21.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:21.383
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:16:21.389
Feb 24 11:16:21.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca" in namespace "downward-api-2054" to be "Succeeded or Failed"
Feb 24 11:16:21.422: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca": Phase="Pending", Reason="", readiness=false. Elapsed: 13.612775ms
Feb 24 11:16:23.428: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020028292s
Feb 24 11:16:25.427: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018829746s
STEP: Saw pod success 02/24/23 11:16:25.427
Feb 24 11:16:25.427: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca" satisfied condition "Succeeded or Failed"
Feb 24 11:16:25.432: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca container client-container: <nil>
STEP: delete the pod 02/24/23 11:16:25.44
Feb 24 11:16:25.453: INFO: Waiting for pod downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca to disappear
Feb 24 11:16:25.456: INFO: Pod downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:16:25.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2054" for this suite. 02/24/23 11:16:25.463
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":85,"skipped":1586,"failed":0}
------------------------------
• [4.171 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:16:21.306
    Feb 24 11:16:21.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:16:21.308
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:21.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:21.383
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:16:21.389
    Feb 24 11:16:21.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca" in namespace "downward-api-2054" to be "Succeeded or Failed"
    Feb 24 11:16:21.422: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca": Phase="Pending", Reason="", readiness=false. Elapsed: 13.612775ms
    Feb 24 11:16:23.428: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020028292s
    Feb 24 11:16:25.427: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018829746s
    STEP: Saw pod success 02/24/23 11:16:25.427
    Feb 24 11:16:25.427: INFO: Pod "downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca" satisfied condition "Succeeded or Failed"
    Feb 24 11:16:25.432: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca container client-container: <nil>
    STEP: delete the pod 02/24/23 11:16:25.44
    Feb 24 11:16:25.453: INFO: Waiting for pod downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca to disappear
    Feb 24 11:16:25.456: INFO: Pod downwardapi-volume-646d2dce-f38e-4e3b-b9cb-2d74988172ca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:16:25.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2054" for this suite. 02/24/23 11:16:25.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:16:25.479
Feb 24 11:16:25.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:16:25.48
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:25.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:25.509
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 24 11:16:25.536: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:17:25.607: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:17:25.614
Feb 24 11:17:25.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:17:25.615
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:25.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:25.648
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 02/24/23 11:17:25.657
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:17:25.657
Feb 24 11:17:25.673: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4579" to be "running"
Feb 24 11:17:25.682: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.322932ms
Feb 24 11:17:27.689: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016333844s
Feb 24 11:17:27.689: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:17:27.693
Feb 24 11:17:27.709: INFO: found a healthy node: ip-172-31-216-47.eu-west-3.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Feb 24 11:17:37.804: INFO: pods created so far: [1 1 1]
Feb 24 11:17:37.804: INFO: length of pods created so far: 3
Feb 24 11:17:39.817: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Feb 24 11:17:46.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4579" for this suite. 02/24/23 11:17:46.831
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:17:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5069" for this suite. 02/24/23 11:17:46.886
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":86,"skipped":1593,"failed":0}
------------------------------
• [SLOW TEST] [81.477 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:16:25.479
    Feb 24 11:16:25.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:16:25.48
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:16:25.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:16:25.509
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 24 11:16:25.536: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:17:25.607: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:17:25.614
    Feb 24 11:17:25.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-preemption-path 02/24/23 11:17:25.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:25.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:25.648
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 02/24/23 11:17:25.657
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:17:25.657
    Feb 24 11:17:25.673: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4579" to be "running"
    Feb 24 11:17:25.682: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.322932ms
    Feb 24 11:17:27.689: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016333844s
    Feb 24 11:17:27.689: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:17:27.693
    Feb 24 11:17:27.709: INFO: found a healthy node: ip-172-31-216-47.eu-west-3.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Feb 24 11:17:37.804: INFO: pods created so far: [1 1 1]
    Feb 24 11:17:37.804: INFO: length of pods created so far: 3
    Feb 24 11:17:39.817: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Feb 24 11:17:46.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-4579" for this suite. 02/24/23 11:17:46.831
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:17:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5069" for this suite. 02/24/23 11:17:46.886
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:17:46.957
Feb 24 11:17:46.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 11:17:46.958
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:46.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:46.987
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 11:18:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8644" for this suite. 02/24/23 11:18:47.016
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":87,"skipped":1605,"failed":0}
------------------------------
• [SLOW TEST] [60.068 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:17:46.957
    Feb 24 11:17:46.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:17:46.958
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:17:46.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:17:46.987
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 11:18:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8644" for this suite. 02/24/23 11:18:47.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:18:47.025
Feb 24 11:18:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 11:18:47.026
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:18:47.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:18:47.063
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 24 11:18:47.081: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 24 11:18:52.088: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:18:52.088
Feb 24 11:18:52.088: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/24/23 11:18:52.104
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:18:52.127: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3101  4c19547d-c99e-455d-a3ac-653e73615acc 14829 1 2023-02-24 11:18:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-24 11:18:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051ada88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 24 11:18:52.134: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb 24 11:18:52.134: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 24 11:18:52.135: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3101  e7009a1b-1aad-4cf2-ac6a-0baf7578c2bb 14830 1 2023-02-24 11:18:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4c19547d-c99e-455d-a3ac-653e73615acc 0xc0051addd7 0xc0051addd8}] [] [{e2e.test Update apps/v1 2023-02-24 11:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:18:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:18:52 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4c19547d-c99e-455d-a3ac-653e73615acc\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0051ade98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:18:52.143: INFO: Pod "test-cleanup-controller-49n6z" is available:
&Pod{ObjectMeta:{test-cleanup-controller-49n6z test-cleanup-controller- deployment-3101  cf12be2c-cf3b-40af-a16f-14cc68c8c928 14817 0 2023-02-24 11:18:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:533681ed8072f8739ada25dd39e0c0196bf062434254cbcea618491824cbe675 cni.projectcalico.org/podIP:10.244.3.76/32 cni.projectcalico.org/podIPs:10.244.3.76/32] [{apps/v1 ReplicaSet test-cleanup-controller e7009a1b-1aad-4cf2-ac6a-0baf7578c2bb 0xc0041fb9b7 0xc0041fb9b8}] [] [{Go-http-client Update v1 2023-02-24 11:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7009a1b-1aad-4cf2-ac6a-0baf7578c2bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:18:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78kbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78kbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.76,StartTime:2023-02-24 11:18:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:18:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d4371b3ebe292854aee995161cb30483df3ec206ef1beb8ca1b6c94371b2ca40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 11:18:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3101" for this suite. 02/24/23 11:18:52.154
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":88,"skipped":1612,"failed":0}
------------------------------
• [SLOW TEST] [5.148 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:18:47.025
    Feb 24 11:18:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 11:18:47.026
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:18:47.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:18:47.063
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 24 11:18:47.081: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 24 11:18:52.088: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:18:52.088
    Feb 24 11:18:52.088: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/24/23 11:18:52.104
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:18:52.127: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3101  4c19547d-c99e-455d-a3ac-653e73615acc 14829 1 2023-02-24 11:18:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-24 11:18:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051ada88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 24 11:18:52.134: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Feb 24 11:18:52.134: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Feb 24 11:18:52.135: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3101  e7009a1b-1aad-4cf2-ac6a-0baf7578c2bb 14830 1 2023-02-24 11:18:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4c19547d-c99e-455d-a3ac-653e73615acc 0xc0051addd7 0xc0051addd8}] [] [{e2e.test Update apps/v1 2023-02-24 11:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:18:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-24 11:18:52 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4c19547d-c99e-455d-a3ac-653e73615acc\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0051ade98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:18:52.143: INFO: Pod "test-cleanup-controller-49n6z" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-49n6z test-cleanup-controller- deployment-3101  cf12be2c-cf3b-40af-a16f-14cc68c8c928 14817 0 2023-02-24 11:18:47 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:533681ed8072f8739ada25dd39e0c0196bf062434254cbcea618491824cbe675 cni.projectcalico.org/podIP:10.244.3.76/32 cni.projectcalico.org/podIPs:10.244.3.76/32] [{apps/v1 ReplicaSet test-cleanup-controller e7009a1b-1aad-4cf2-ac6a-0baf7578c2bb 0xc0041fb9b7 0xc0041fb9b8}] [] [{Go-http-client Update v1 2023-02-24 11:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 11:18:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7009a1b-1aad-4cf2-ac6a-0baf7578c2bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:18:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78kbj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78kbj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:18:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.76,StartTime:2023-02-24 11:18:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:18:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d4371b3ebe292854aee995161cb30483df3ec206ef1beb8ca1b6c94371b2ca40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 11:18:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3101" for this suite. 02/24/23 11:18:52.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:18:52.177
Feb 24 11:18:52.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename job 02/24/23 11:18:52.178
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:18:52.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:18:52.22
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 02/24/23 11:18:52.225
STEP: Ensuring job reaches completions 02/24/23 11:18:52.232
STEP: Ensuring pods with index for job exist 02/24/23 11:19:00.241
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 24 11:19:00.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7563" for this suite. 02/24/23 11:19:00.265
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":89,"skipped":1624,"failed":0}
------------------------------
• [SLOW TEST] [8.105 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:18:52.177
    Feb 24 11:18:52.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename job 02/24/23 11:18:52.178
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:18:52.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:18:52.22
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 02/24/23 11:18:52.225
    STEP: Ensuring job reaches completions 02/24/23 11:18:52.232
    STEP: Ensuring pods with index for job exist 02/24/23 11:19:00.241
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 24 11:19:00.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7563" for this suite. 02/24/23 11:19:00.265
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:00.283
Feb 24 11:19:00.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename containers 02/24/23 11:19:00.284
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:00.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:00.322
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 02/24/23 11:19:00.327
Feb 24 11:19:00.350: INFO: Waiting up to 5m0s for pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a" in namespace "containers-9083" to be "Succeeded or Failed"
Feb 24 11:19:00.359: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.938355ms
Feb 24 11:19:02.366: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015621171s
Feb 24 11:19:04.368: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017806966s
STEP: Saw pod success 02/24/23 11:19:04.368
Feb 24 11:19:04.368: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a" satisfied condition "Succeeded or Failed"
Feb 24 11:19:04.375: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:19:04.395
Feb 24 11:19:04.411: INFO: Waiting for pod client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a to disappear
Feb 24 11:19:04.420: INFO: Pod client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 24 11:19:04.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9083" for this suite. 02/24/23 11:19:04.429
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":90,"skipped":1625,"failed":0}
------------------------------
• [4.158 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:00.283
    Feb 24 11:19:00.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename containers 02/24/23 11:19:00.284
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:00.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:00.322
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 02/24/23 11:19:00.327
    Feb 24 11:19:00.350: INFO: Waiting up to 5m0s for pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a" in namespace "containers-9083" to be "Succeeded or Failed"
    Feb 24 11:19:00.359: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.938355ms
    Feb 24 11:19:02.366: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015621171s
    Feb 24 11:19:04.368: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017806966s
    STEP: Saw pod success 02/24/23 11:19:04.368
    Feb 24 11:19:04.368: INFO: Pod "client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a" satisfied condition "Succeeded or Failed"
    Feb 24 11:19:04.375: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:19:04.395
    Feb 24 11:19:04.411: INFO: Waiting for pod client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a to disappear
    Feb 24 11:19:04.420: INFO: Pod client-containers-6a8b5d71-df4a-4477-9b65-efdf05e06c8a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 24 11:19:04.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9083" for this suite. 02/24/23 11:19:04.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:04.441
Feb 24 11:19:04.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:19:04.442
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:04.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:04.481
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 02/24/23 11:19:04.485
Feb 24 11:19:04.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 24 11:19:04.595: INFO: stderr: ""
Feb 24 11:19:04.595: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 02/24/23 11:19:04.595
Feb 24 11:19:04.596: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 24 11:19:04.596: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6662" to be "running and ready, or succeeded"
Feb 24 11:19:04.603: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.028232ms
Feb 24 11:19:04.603: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-216-47.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
Feb 24 11:19:06.610: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014365202s
Feb 24 11:19:06.610: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 24 11:19:06.610: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/24/23 11:19:06.61
Feb 24 11:19:06.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator'
Feb 24 11:19:06.742: INFO: stderr: ""
Feb 24 11:19:06.742: INFO: stdout: "I0224 11:19:05.358624       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/fxn 272\nI0224 11:19:05.558948       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hpqd 239\nI0224 11:19:05.759140       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/4wv 590\nI0224 11:19:05.959630       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/6nc 218\nI0224 11:19:06.158754       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/44hd 307\nI0224 11:19:06.359171       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/bdr 377\nI0224 11:19:06.559518       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/6zb 422\n"
STEP: limiting log lines 02/24/23 11:19:06.742
Feb 24 11:19:06.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --tail=1'
Feb 24 11:19:06.884: INFO: stderr: ""
Feb 24 11:19:06.884: INFO: stdout: "I0224 11:19:06.758715       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/lf7 576\n"
Feb 24 11:19:06.884: INFO: got output "I0224 11:19:06.758715       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/lf7 576\n"
STEP: limiting log bytes 02/24/23 11:19:06.884
Feb 24 11:19:06.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --limit-bytes=1'
Feb 24 11:19:07.030: INFO: stderr: ""
Feb 24 11:19:07.031: INFO: stdout: "I"
Feb 24 11:19:07.031: INFO: got output "I"
STEP: exposing timestamps 02/24/23 11:19:07.031
Feb 24 11:19:07.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 24 11:19:07.126: INFO: stderr: ""
Feb 24 11:19:07.126: INFO: stdout: "2023-02-24T11:19:06.959128106Z I0224 11:19:06.958966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/fgl 545\n"
Feb 24 11:19:07.126: INFO: got output "2023-02-24T11:19:06.959128106Z I0224 11:19:06.958966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/fgl 545\n"
STEP: restricting to a time range 02/24/23 11:19:07.126
Feb 24 11:19:09.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --since=1s'
Feb 24 11:19:09.710: INFO: stderr: ""
Feb 24 11:19:09.710: INFO: stdout: "I0224 11:19:08.758749       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bjdd 451\nI0224 11:19:08.958871       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/64jb 345\nI0224 11:19:09.158713       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/m46j 391\nI0224 11:19:09.359138       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/n4vj 321\nI0224 11:19:09.559406       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/cscm 490\n"
Feb 24 11:19:09.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --since=24h'
Feb 24 11:19:09.824: INFO: stderr: ""
Feb 24 11:19:09.824: INFO: stdout: "I0224 11:19:05.358624       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/fxn 272\nI0224 11:19:05.558948       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hpqd 239\nI0224 11:19:05.759140       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/4wv 590\nI0224 11:19:05.959630       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/6nc 218\nI0224 11:19:06.158754       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/44hd 307\nI0224 11:19:06.359171       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/bdr 377\nI0224 11:19:06.559518       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/6zb 422\nI0224 11:19:06.758715       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/lf7 576\nI0224 11:19:06.958966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/fgl 545\nI0224 11:19:07.159402       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/9cmz 405\nI0224 11:19:07.358641       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/xfq 262\nI0224 11:19:07.559037       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/ctm9 232\nI0224 11:19:07.759432       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/tmwc 265\nI0224 11:19:07.958642       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/5b6l 403\nI0224 11:19:08.159052       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/nljh 462\nI0224 11:19:08.359283       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/65n 558\nI0224 11:19:08.559461       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/8mw 320\nI0224 11:19:08.758749       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bjdd 451\nI0224 11:19:08.958871       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/64jb 345\nI0224 11:19:09.158713       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/m46j 391\nI0224 11:19:09.359138       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/n4vj 321\nI0224 11:19:09.559406       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/cscm 490\nI0224 11:19:09.758687       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/dgx 518\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Feb 24 11:19:09.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 delete pod logs-generator'
Feb 24 11:19:10.622: INFO: stderr: ""
Feb 24 11:19:10.622: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:19:10.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6662" for this suite. 02/24/23 11:19:10.63
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":91,"skipped":1640,"failed":0}
------------------------------
• [SLOW TEST] [6.198 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:04.441
    Feb 24 11:19:04.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:19:04.442
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:04.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:04.481
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 02/24/23 11:19:04.485
    Feb 24 11:19:04.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 24 11:19:04.595: INFO: stderr: ""
    Feb 24 11:19:04.595: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 02/24/23 11:19:04.595
    Feb 24 11:19:04.596: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 24 11:19:04.596: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6662" to be "running and ready, or succeeded"
    Feb 24 11:19:04.603: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.028232ms
    Feb 24 11:19:04.603: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-172-31-216-47.eu-west-3.compute.internal' to be 'Running' but was 'Pending'
    Feb 24 11:19:06.610: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014365202s
    Feb 24 11:19:06.610: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 24 11:19:06.610: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/24/23 11:19:06.61
    Feb 24 11:19:06.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator'
    Feb 24 11:19:06.742: INFO: stderr: ""
    Feb 24 11:19:06.742: INFO: stdout: "I0224 11:19:05.358624       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/fxn 272\nI0224 11:19:05.558948       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hpqd 239\nI0224 11:19:05.759140       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/4wv 590\nI0224 11:19:05.959630       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/6nc 218\nI0224 11:19:06.158754       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/44hd 307\nI0224 11:19:06.359171       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/bdr 377\nI0224 11:19:06.559518       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/6zb 422\n"
    STEP: limiting log lines 02/24/23 11:19:06.742
    Feb 24 11:19:06.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --tail=1'
    Feb 24 11:19:06.884: INFO: stderr: ""
    Feb 24 11:19:06.884: INFO: stdout: "I0224 11:19:06.758715       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/lf7 576\n"
    Feb 24 11:19:06.884: INFO: got output "I0224 11:19:06.758715       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/lf7 576\n"
    STEP: limiting log bytes 02/24/23 11:19:06.884
    Feb 24 11:19:06.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --limit-bytes=1'
    Feb 24 11:19:07.030: INFO: stderr: ""
    Feb 24 11:19:07.031: INFO: stdout: "I"
    Feb 24 11:19:07.031: INFO: got output "I"
    STEP: exposing timestamps 02/24/23 11:19:07.031
    Feb 24 11:19:07.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 24 11:19:07.126: INFO: stderr: ""
    Feb 24 11:19:07.126: INFO: stdout: "2023-02-24T11:19:06.959128106Z I0224 11:19:06.958966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/fgl 545\n"
    Feb 24 11:19:07.126: INFO: got output "2023-02-24T11:19:06.959128106Z I0224 11:19:06.958966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/fgl 545\n"
    STEP: restricting to a time range 02/24/23 11:19:07.126
    Feb 24 11:19:09.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --since=1s'
    Feb 24 11:19:09.710: INFO: stderr: ""
    Feb 24 11:19:09.710: INFO: stdout: "I0224 11:19:08.758749       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bjdd 451\nI0224 11:19:08.958871       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/64jb 345\nI0224 11:19:09.158713       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/m46j 391\nI0224 11:19:09.359138       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/n4vj 321\nI0224 11:19:09.559406       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/cscm 490\n"
    Feb 24 11:19:09.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 logs logs-generator logs-generator --since=24h'
    Feb 24 11:19:09.824: INFO: stderr: ""
    Feb 24 11:19:09.824: INFO: stdout: "I0224 11:19:05.358624       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/fxn 272\nI0224 11:19:05.558948       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/hpqd 239\nI0224 11:19:05.759140       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/4wv 590\nI0224 11:19:05.959630       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/6nc 218\nI0224 11:19:06.158754       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/44hd 307\nI0224 11:19:06.359171       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/bdr 377\nI0224 11:19:06.559518       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/6zb 422\nI0224 11:19:06.758715       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/lf7 576\nI0224 11:19:06.958966       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/fgl 545\nI0224 11:19:07.159402       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/9cmz 405\nI0224 11:19:07.358641       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/xfq 262\nI0224 11:19:07.559037       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/ctm9 232\nI0224 11:19:07.759432       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/tmwc 265\nI0224 11:19:07.958642       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/5b6l 403\nI0224 11:19:08.159052       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/nljh 462\nI0224 11:19:08.359283       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/65n 558\nI0224 11:19:08.559461       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/8mw 320\nI0224 11:19:08.758749       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bjdd 451\nI0224 11:19:08.958871       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/64jb 345\nI0224 11:19:09.158713       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/m46j 391\nI0224 11:19:09.359138       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/n4vj 321\nI0224 11:19:09.559406       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/cscm 490\nI0224 11:19:09.758687       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/dgx 518\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Feb 24 11:19:09.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6662 delete pod logs-generator'
    Feb 24 11:19:10.622: INFO: stderr: ""
    Feb 24 11:19:10.622: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:19:10.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6662" for this suite. 02/24/23 11:19:10.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:10.639
Feb 24 11:19:10.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename namespaces 02/24/23 11:19:10.64
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:10.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:10.668
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 02/24/23 11:19:10.672
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:10.699
STEP: Creating a service in the namespace 02/24/23 11:19:10.704
STEP: Deleting the namespace 02/24/23 11:19:10.724
STEP: Waiting for the namespace to be removed. 02/24/23 11:19:10.744
STEP: Recreating the namespace 02/24/23 11:19:16.749
STEP: Verifying there is no service in the namespace 02/24/23 11:19:16.769
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:19:16.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3547" for this suite. 02/24/23 11:19:16.786
STEP: Destroying namespace "nsdeletetest-7530" for this suite. 02/24/23 11:19:16.794
Feb 24 11:19:16.799: INFO: Namespace nsdeletetest-7530 was already deleted
STEP: Destroying namespace "nsdeletetest-9546" for this suite. 02/24/23 11:19:16.799
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":92,"skipped":1653,"failed":0}
------------------------------
• [SLOW TEST] [6.167 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:10.639
    Feb 24 11:19:10.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:19:10.64
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:10.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:10.668
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 02/24/23 11:19:10.672
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:10.699
    STEP: Creating a service in the namespace 02/24/23 11:19:10.704
    STEP: Deleting the namespace 02/24/23 11:19:10.724
    STEP: Waiting for the namespace to be removed. 02/24/23 11:19:10.744
    STEP: Recreating the namespace 02/24/23 11:19:16.749
    STEP: Verifying there is no service in the namespace 02/24/23 11:19:16.769
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:19:16.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3547" for this suite. 02/24/23 11:19:16.786
    STEP: Destroying namespace "nsdeletetest-7530" for this suite. 02/24/23 11:19:16.794
    Feb 24 11:19:16.799: INFO: Namespace nsdeletetest-7530 was already deleted
    STEP: Destroying namespace "nsdeletetest-9546" for this suite. 02/24/23 11:19:16.799
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:16.809
Feb 24 11:19:16.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:19:16.816
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:16.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:16.845
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  02/24/23 11:19:16.849
Feb 24 11:19:16.859: INFO: Waiting up to 5m0s for pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8" in namespace "svcaccounts-2276" to be "Succeeded or Failed"
Feb 24 11:19:16.866: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.590007ms
Feb 24 11:19:18.871: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8": Phase="Running", Reason="", readiness=false. Elapsed: 2.012460219s
Feb 24 11:19:20.872: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013068945s
STEP: Saw pod success 02/24/23 11:19:20.872
Feb 24 11:19:20.872: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8" satisfied condition "Succeeded or Failed"
Feb 24 11:19:20.877: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:19:20.885
Feb 24 11:19:20.899: INFO: Waiting for pod test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8 to disappear
Feb 24 11:19:20.904: INFO: Pod test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 24 11:19:20.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2276" for this suite. 02/24/23 11:19:20.916
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":93,"skipped":1654,"failed":0}
------------------------------
• [4.122 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:16.809
    Feb 24 11:19:16.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:19:16.816
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:16.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:16.845
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  02/24/23 11:19:16.849
    Feb 24 11:19:16.859: INFO: Waiting up to 5m0s for pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8" in namespace "svcaccounts-2276" to be "Succeeded or Failed"
    Feb 24 11:19:16.866: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.590007ms
    Feb 24 11:19:18.871: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8": Phase="Running", Reason="", readiness=false. Elapsed: 2.012460219s
    Feb 24 11:19:20.872: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013068945s
    STEP: Saw pod success 02/24/23 11:19:20.872
    Feb 24 11:19:20.872: INFO: Pod "test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8" satisfied condition "Succeeded or Failed"
    Feb 24 11:19:20.877: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:19:20.885
    Feb 24 11:19:20.899: INFO: Waiting for pod test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8 to disappear
    Feb 24 11:19:20.904: INFO: Pod test-pod-7b9792f7-e473-4607-9e10-82ada373d0a8 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 24 11:19:20.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2276" for this suite. 02/24/23 11:19:20.916
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:20.933
Feb 24 11:19:20.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:19:20.934
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:20.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:20.969
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:19:20.972
Feb 24 11:19:20.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011" in namespace "projected-44" to be "Succeeded or Failed"
Feb 24 11:19:20.989: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011": Phase="Pending", Reason="", readiness=false. Elapsed: 5.340079ms
Feb 24 11:19:22.995: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01118423s
Feb 24 11:19:24.995: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011424742s
STEP: Saw pod success 02/24/23 11:19:24.995
Feb 24 11:19:24.995: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011" satisfied condition "Succeeded or Failed"
Feb 24 11:19:25.000: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011 container client-container: <nil>
STEP: delete the pod 02/24/23 11:19:25.015
Feb 24 11:19:25.033: INFO: Waiting for pod downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011 to disappear
Feb 24 11:19:25.037: INFO: Pod downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 11:19:25.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-44" for this suite. 02/24/23 11:19:25.045
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":94,"skipped":1654,"failed":0}
------------------------------
• [4.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:20.933
    Feb 24 11:19:20.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:19:20.934
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:20.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:20.969
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:19:20.972
    Feb 24 11:19:20.984: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011" in namespace "projected-44" to be "Succeeded or Failed"
    Feb 24 11:19:20.989: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011": Phase="Pending", Reason="", readiness=false. Elapsed: 5.340079ms
    Feb 24 11:19:22.995: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01118423s
    Feb 24 11:19:24.995: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011424742s
    STEP: Saw pod success 02/24/23 11:19:24.995
    Feb 24 11:19:24.995: INFO: Pod "downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011" satisfied condition "Succeeded or Failed"
    Feb 24 11:19:25.000: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:19:25.015
    Feb 24 11:19:25.033: INFO: Waiting for pod downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011 to disappear
    Feb 24 11:19:25.037: INFO: Pod downwardapi-volume-b860099c-daf5-4bdc-9d20-7ab510844011 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 11:19:25.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-44" for this suite. 02/24/23 11:19:25.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:25.071
Feb 24 11:19:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:19:25.078
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:25.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:25.112
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 02/24/23 11:19:25.117
STEP: watching for the ServiceAccount to be added 02/24/23 11:19:25.13
STEP: patching the ServiceAccount 02/24/23 11:19:25.132
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/24/23 11:19:25.14
STEP: deleting the ServiceAccount 02/24/23 11:19:25.146
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 24 11:19:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9924" for this suite. 02/24/23 11:19:25.169
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":95,"skipped":1673,"failed":0}
------------------------------
• [0.107 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:25.071
    Feb 24 11:19:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 11:19:25.078
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:25.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:25.112
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 02/24/23 11:19:25.117
    STEP: watching for the ServiceAccount to be added 02/24/23 11:19:25.13
    STEP: patching the ServiceAccount 02/24/23 11:19:25.132
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/24/23 11:19:25.14
    STEP: deleting the ServiceAccount 02/24/23 11:19:25.146
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 24 11:19:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9924" for this suite. 02/24/23 11:19:25.169
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:25.18
Feb 24 11:19:25.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:19:25.184
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:25.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:25.226
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 02/24/23 11:19:25.23
Feb 24 11:19:25.239: INFO: Waiting up to 5m0s for pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85" in namespace "downward-api-7738" to be "running and ready"
Feb 24 11:19:25.247: INFO: Pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280515ms
Feb 24 11:19:25.247: INFO: The phase of Pod annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:27.253: INFO: Pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85": Phase="Running", Reason="", readiness=true. Elapsed: 2.013658514s
Feb 24 11:19:27.253: INFO: The phase of Pod annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85 is Running (Ready = true)
Feb 24 11:19:27.253: INFO: Pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85" satisfied condition "running and ready"
Feb 24 11:19:27.780: INFO: Successfully updated pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:19:31.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7738" for this suite. 02/24/23 11:19:31.814
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":96,"skipped":1676,"failed":0}
------------------------------
• [SLOW TEST] [6.644 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:25.18
    Feb 24 11:19:25.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:19:25.184
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:25.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:25.226
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 02/24/23 11:19:25.23
    Feb 24 11:19:25.239: INFO: Waiting up to 5m0s for pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85" in namespace "downward-api-7738" to be "running and ready"
    Feb 24 11:19:25.247: INFO: Pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280515ms
    Feb 24 11:19:25.247: INFO: The phase of Pod annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:27.253: INFO: Pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85": Phase="Running", Reason="", readiness=true. Elapsed: 2.013658514s
    Feb 24 11:19:27.253: INFO: The phase of Pod annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85 is Running (Ready = true)
    Feb 24 11:19:27.253: INFO: Pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85" satisfied condition "running and ready"
    Feb 24 11:19:27.780: INFO: Successfully updated pod "annotationupdate02c5c2d2-46bc-4366-a0f7-8da38b546d85"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:19:31.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7738" for this suite. 02/24/23 11:19:31.814
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:31.83
Feb 24 11:19:31.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename watch 02/24/23 11:19:31.832
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:31.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:31.897
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/24/23 11:19:31.901
STEP: creating a new configmap 02/24/23 11:19:31.903
STEP: modifying the configmap once 02/24/23 11:19:31.909
STEP: closing the watch once it receives two notifications 02/24/23 11:19:31.918
Feb 24 11:19:31.918: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15288 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:19:31.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15289 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/24/23 11:19:31.919
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/24/23 11:19:31.928
STEP: deleting the configmap 02/24/23 11:19:31.93
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/24/23 11:19:31.937
Feb 24 11:19:31.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15290 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:19:31.937: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15291 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 24 11:19:31.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5783" for this suite. 02/24/23 11:19:31.944
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":97,"skipped":1678,"failed":0}
------------------------------
• [0.124 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:31.83
    Feb 24 11:19:31.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename watch 02/24/23 11:19:31.832
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:31.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:31.897
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/24/23 11:19:31.901
    STEP: creating a new configmap 02/24/23 11:19:31.903
    STEP: modifying the configmap once 02/24/23 11:19:31.909
    STEP: closing the watch once it receives two notifications 02/24/23 11:19:31.918
    Feb 24 11:19:31.918: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15288 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:19:31.918: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15289 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/24/23 11:19:31.919
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/24/23 11:19:31.928
    STEP: deleting the configmap 02/24/23 11:19:31.93
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/24/23 11:19:31.937
    Feb 24 11:19:31.937: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15290 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:19:31.937: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5783  46f596e3-ae14-42aa-a0cf-86dfa5c360ab 15291 0 2023-02-24 11:19:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-24 11:19:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 24 11:19:31.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5783" for this suite. 02/24/23 11:19:31.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:31.956
Feb 24 11:19:31.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename job 02/24/23 11:19:31.957
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:31.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:31.989
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 02/24/23 11:19:31.995
STEP: Ensure pods equal to paralellism count is attached to the job 02/24/23 11:19:32.005
STEP: patching /status 02/24/23 11:19:36.013
STEP: updating /status 02/24/23 11:19:36.031
STEP: get /status 02/24/23 11:19:36.043
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 24 11:19:36.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8418" for this suite. 02/24/23 11:19:36.054
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":98,"skipped":1686,"failed":0}
------------------------------
• [4.107 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:31.956
    Feb 24 11:19:31.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename job 02/24/23 11:19:31.957
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:31.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:31.989
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 02/24/23 11:19:31.995
    STEP: Ensure pods equal to paralellism count is attached to the job 02/24/23 11:19:32.005
    STEP: patching /status 02/24/23 11:19:36.013
    STEP: updating /status 02/24/23 11:19:36.031
    STEP: get /status 02/24/23 11:19:36.043
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 24 11:19:36.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8418" for this suite. 02/24/23 11:19:36.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:36.064
Feb 24 11:19:36.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename hostport 02/24/23 11:19:36.065
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:36.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:36.094
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/24/23 11:19:36.105
Feb 24 11:19:36.114: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9391" to be "running and ready"
Feb 24 11:19:36.119: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77424ms
Feb 24 11:19:36.119: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:38.124: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009754049s
Feb 24 11:19:38.124: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 24 11:19:38.124: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.216.47 on the node which pod1 resides and expect scheduled 02/24/23 11:19:38.124
Feb 24 11:19:38.133: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9391" to be "running and ready"
Feb 24 11:19:38.138: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650564ms
Feb 24 11:19:38.138: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:40.144: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.010907455s
Feb 24 11:19:40.144: INFO: The phase of Pod pod2 is Running (Ready = false)
Feb 24 11:19:42.143: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009677462s
Feb 24 11:19:42.143: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 24 11:19:42.143: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.216.47 but use UDP protocol on the node which pod2 resides 02/24/23 11:19:42.143
Feb 24 11:19:42.152: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9391" to be "running and ready"
Feb 24 11:19:42.156: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.213457ms
Feb 24 11:19:42.156: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:44.162: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009964284s
Feb 24 11:19:44.162: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 24 11:19:44.162: INFO: Pod "pod3" satisfied condition "running and ready"
Feb 24 11:19:44.168: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9391" to be "running and ready"
Feb 24 11:19:44.173: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138711ms
Feb 24 11:19:44.173: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:19:46.178: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009805962s
Feb 24 11:19:46.178: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 24 11:19:46.178: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/24/23 11:19:46.183
Feb 24 11:19:46.183: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.216.47 http://127.0.0.1:54323/hostname] Namespace:hostport-9391 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:19:46.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:19:46.184: INFO: ExecWithOptions: Clientset creation
Feb 24 11:19:46.184: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9391/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.216.47+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.216.47, port: 54323 02/24/23 11:19:46.259
Feb 24 11:19:46.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.216.47:54323/hostname] Namespace:hostport-9391 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:19:46.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:19:46.260: INFO: ExecWithOptions: Clientset creation
Feb 24 11:19:46.260: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9391/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.216.47%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.216.47, port: 54323 UDP 02/24/23 11:19:46.342
Feb 24 11:19:46.342: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.216.47 54323] Namespace:hostport-9391 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:19:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:19:46.343: INFO: ExecWithOptions: Clientset creation
Feb 24 11:19:46.343: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9391/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.216.47+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Feb 24 11:19:51.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9391" for this suite. 02/24/23 11:19:51.437
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":99,"skipped":1692,"failed":0}
------------------------------
• [SLOW TEST] [15.383 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:36.064
    Feb 24 11:19:36.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename hostport 02/24/23 11:19:36.065
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:36.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:36.094
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/24/23 11:19:36.105
    Feb 24 11:19:36.114: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9391" to be "running and ready"
    Feb 24 11:19:36.119: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77424ms
    Feb 24 11:19:36.119: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:38.124: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009754049s
    Feb 24 11:19:38.124: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 24 11:19:38.124: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.216.47 on the node which pod1 resides and expect scheduled 02/24/23 11:19:38.124
    Feb 24 11:19:38.133: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9391" to be "running and ready"
    Feb 24 11:19:38.138: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650564ms
    Feb 24 11:19:38.138: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:40.144: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.010907455s
    Feb 24 11:19:40.144: INFO: The phase of Pod pod2 is Running (Ready = false)
    Feb 24 11:19:42.143: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.009677462s
    Feb 24 11:19:42.143: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 24 11:19:42.143: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.216.47 but use UDP protocol on the node which pod2 resides 02/24/23 11:19:42.143
    Feb 24 11:19:42.152: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9391" to be "running and ready"
    Feb 24 11:19:42.156: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.213457ms
    Feb 24 11:19:42.156: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:44.162: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009964284s
    Feb 24 11:19:44.162: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 24 11:19:44.162: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb 24 11:19:44.168: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9391" to be "running and ready"
    Feb 24 11:19:44.173: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138711ms
    Feb 24 11:19:44.173: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:19:46.178: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009805962s
    Feb 24 11:19:46.178: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 24 11:19:46.178: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/24/23 11:19:46.183
    Feb 24 11:19:46.183: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.216.47 http://127.0.0.1:54323/hostname] Namespace:hostport-9391 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:19:46.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:19:46.184: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:19:46.184: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9391/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.216.47+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.216.47, port: 54323 02/24/23 11:19:46.259
    Feb 24 11:19:46.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.216.47:54323/hostname] Namespace:hostport-9391 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:19:46.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:19:46.260: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:19:46.260: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9391/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.216.47%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.216.47, port: 54323 UDP 02/24/23 11:19:46.342
    Feb 24 11:19:46.342: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.216.47 54323] Namespace:hostport-9391 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:19:46.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:19:46.343: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:19:46.343: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9391/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.216.47+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Feb 24 11:19:51.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-9391" for this suite. 02/24/23 11:19:51.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:51.45
Feb 24 11:19:51.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:19:51.451
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:51.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:51.492
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-1de0d233-0524-45fb-a4d5-69a39c5a87be 02/24/23 11:19:51.496
STEP: Creating a pod to test consume secrets 02/24/23 11:19:51.502
Feb 24 11:19:51.513: INFO: Waiting up to 5m0s for pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225" in namespace "secrets-8707" to be "Succeeded or Failed"
Feb 24 11:19:51.520: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225": Phase="Pending", Reason="", readiness=false. Elapsed: 6.821731ms
Feb 24 11:19:53.525: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011847621s
Feb 24 11:19:55.525: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012026096s
STEP: Saw pod success 02/24/23 11:19:55.525
Feb 24 11:19:55.526: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225" satisfied condition "Succeeded or Failed"
Feb 24 11:19:55.530: INFO: Trying to get logs from node ip-172-31-217-191.eu-west-3.compute.internal pod pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:19:55.547
Feb 24 11:19:55.561: INFO: Waiting for pod pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225 to disappear
Feb 24 11:19:55.566: INFO: Pod pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:19:55.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8707" for this suite. 02/24/23 11:19:55.573
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":100,"skipped":1704,"failed":0}
------------------------------
• [4.133 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:51.45
    Feb 24 11:19:51.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:19:51.451
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:51.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:51.492
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-1de0d233-0524-45fb-a4d5-69a39c5a87be 02/24/23 11:19:51.496
    STEP: Creating a pod to test consume secrets 02/24/23 11:19:51.502
    Feb 24 11:19:51.513: INFO: Waiting up to 5m0s for pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225" in namespace "secrets-8707" to be "Succeeded or Failed"
    Feb 24 11:19:51.520: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225": Phase="Pending", Reason="", readiness=false. Elapsed: 6.821731ms
    Feb 24 11:19:53.525: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011847621s
    Feb 24 11:19:55.525: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012026096s
    STEP: Saw pod success 02/24/23 11:19:55.525
    Feb 24 11:19:55.526: INFO: Pod "pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225" satisfied condition "Succeeded or Failed"
    Feb 24 11:19:55.530: INFO: Trying to get logs from node ip-172-31-217-191.eu-west-3.compute.internal pod pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:19:55.547
    Feb 24 11:19:55.561: INFO: Waiting for pod pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225 to disappear
    Feb 24 11:19:55.566: INFO: Pod pod-secrets-234582a3-b34f-4d8b-ac51-84ffe6d56225 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:19:55.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8707" for this suite. 02/24/23 11:19:55.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:19:55.588
Feb 24 11:19:55.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:19:55.589
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:55.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:55.619
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 02/24/23 11:19:55.623
Feb 24 11:19:55.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: rename a version 02/24/23 11:20:04.317
STEP: check the new version name is served 02/24/23 11:20:04.338
STEP: check the old version name is removed 02/24/23 11:20:08.199
STEP: check the other version is not changed 02/24/23 11:20:10.049
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:20:17.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4389" for this suite. 02/24/23 11:20:17.363
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":101,"skipped":1735,"failed":0}
------------------------------
• [SLOW TEST] [21.797 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:19:55.588
    Feb 24 11:19:55.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:19:55.589
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:19:55.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:19:55.619
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 02/24/23 11:19:55.623
    Feb 24 11:19:55.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: rename a version 02/24/23 11:20:04.317
    STEP: check the new version name is served 02/24/23 11:20:04.338
    STEP: check the old version name is removed 02/24/23 11:20:08.199
    STEP: check the other version is not changed 02/24/23 11:20:10.049
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:20:17.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4389" for this suite. 02/24/23 11:20:17.363
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:20:17.386
Feb 24 11:20:17.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 11:20:17.387
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:17.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:17.478
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/24/23 11:20:17.487
Feb 24 11:20:17.499: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-423" to be "running and ready"
Feb 24 11:20:17.505: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351704ms
Feb 24 11:20:17.506: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:20:19.512: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013276748s
Feb 24 11:20:19.513: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 11:20:19.513: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 02/24/23 11:20:19.52
Feb 24 11:20:19.526: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-423" to be "running and ready"
Feb 24 11:20:19.534: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.435551ms
Feb 24 11:20:19.534: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:20:21.539: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012786547s
Feb 24 11:20:21.539: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 24 11:20:21.539: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/24/23 11:20:21.544
Feb 24 11:20:21.556: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 11:20:21.562: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 11:20:23.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 11:20:23.568: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 24 11:20:25.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 24 11:20:25.568: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/24/23 11:20:25.568
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 24 11:20:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-423" for this suite. 02/24/23 11:20:25.585
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":102,"skipped":1737,"failed":0}
------------------------------
• [SLOW TEST] [8.211 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:20:17.386
    Feb 24 11:20:17.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 11:20:17.387
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:17.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:17.478
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 11:20:17.487
    Feb 24 11:20:17.499: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-423" to be "running and ready"
    Feb 24 11:20:17.505: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351704ms
    Feb 24 11:20:17.506: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:20:19.512: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013276748s
    Feb 24 11:20:19.513: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 11:20:19.513: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 02/24/23 11:20:19.52
    Feb 24 11:20:19.526: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-423" to be "running and ready"
    Feb 24 11:20:19.534: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.435551ms
    Feb 24 11:20:19.534: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:20:21.539: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012786547s
    Feb 24 11:20:21.539: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 24 11:20:21.539: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/24/23 11:20:21.544
    Feb 24 11:20:21.556: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 24 11:20:21.562: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 24 11:20:23.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 24 11:20:23.568: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 24 11:20:25.563: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 24 11:20:25.568: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/24/23 11:20:25.568
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 24 11:20:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-423" for this suite. 02/24/23 11:20:25.585
  << End Captured GinkgoWriter Output
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:20:25.599
Feb 24 11:20:25.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename conformance-tests 02/24/23 11:20:25.6
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:25.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:25.634
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/24/23 11:20:25.638
Feb 24 11:20:25.639: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Feb 24 11:20:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-2353" for this suite. 02/24/23 11:20:25.653
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":103,"skipped":1737,"failed":0}
------------------------------
• [0.064 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:20:25.599
    Feb 24 11:20:25.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename conformance-tests 02/24/23 11:20:25.6
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:25.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:25.634
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/24/23 11:20:25.638
    Feb 24 11:20:25.639: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Feb 24 11:20:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-2353" for this suite. 02/24/23 11:20:25.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:20:25.669
Feb 24 11:20:25.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename subpath 02/24/23 11:20:25.67
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:25.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:25.692
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 11:20:25.694
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-s6rn 02/24/23 11:20:25.709
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:20:25.709
Feb 24 11:20:25.730: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-s6rn" in namespace "subpath-1414" to be "Succeeded or Failed"
Feb 24 11:20:25.741: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Pending", Reason="", readiness=false. Elapsed: 11.315565ms
Feb 24 11:20:27.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017656866s
Feb 24 11:20:29.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 4.017479538s
Feb 24 11:20:31.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 6.018265847s
Feb 24 11:20:33.750: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 8.019676381s
Feb 24 11:20:35.749: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 10.019074516s
Feb 24 11:20:37.751: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 12.021155635s
Feb 24 11:20:39.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 14.018338382s
Feb 24 11:20:41.757: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 16.027303486s
Feb 24 11:20:43.747: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 18.017279104s
Feb 24 11:20:45.750: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 20.019469045s
Feb 24 11:20:47.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=false. Elapsed: 22.017492769s
Feb 24 11:20:49.750: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019583822s
STEP: Saw pod success 02/24/23 11:20:49.75
Feb 24 11:20:49.750: INFO: Pod "pod-subpath-test-projected-s6rn" satisfied condition "Succeeded or Failed"
Feb 24 11:20:49.756: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-projected-s6rn container test-container-subpath-projected-s6rn: <nil>
STEP: delete the pod 02/24/23 11:20:49.766
Feb 24 11:20:49.795: INFO: Waiting for pod pod-subpath-test-projected-s6rn to disappear
Feb 24 11:20:49.801: INFO: Pod pod-subpath-test-projected-s6rn no longer exists
STEP: Deleting pod pod-subpath-test-projected-s6rn 02/24/23 11:20:49.801
Feb 24 11:20:49.801: INFO: Deleting pod "pod-subpath-test-projected-s6rn" in namespace "subpath-1414"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 24 11:20:49.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1414" for this suite. 02/24/23 11:20:49.813
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":104,"skipped":1766,"failed":0}
------------------------------
• [SLOW TEST] [24.157 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:20:25.669
    Feb 24 11:20:25.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename subpath 02/24/23 11:20:25.67
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:25.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:25.692
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 11:20:25.694
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-s6rn 02/24/23 11:20:25.709
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:20:25.709
    Feb 24 11:20:25.730: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-s6rn" in namespace "subpath-1414" to be "Succeeded or Failed"
    Feb 24 11:20:25.741: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Pending", Reason="", readiness=false. Elapsed: 11.315565ms
    Feb 24 11:20:27.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017656866s
    Feb 24 11:20:29.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 4.017479538s
    Feb 24 11:20:31.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 6.018265847s
    Feb 24 11:20:33.750: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 8.019676381s
    Feb 24 11:20:35.749: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 10.019074516s
    Feb 24 11:20:37.751: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 12.021155635s
    Feb 24 11:20:39.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 14.018338382s
    Feb 24 11:20:41.757: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 16.027303486s
    Feb 24 11:20:43.747: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 18.017279104s
    Feb 24 11:20:45.750: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=true. Elapsed: 20.019469045s
    Feb 24 11:20:47.748: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Running", Reason="", readiness=false. Elapsed: 22.017492769s
    Feb 24 11:20:49.750: INFO: Pod "pod-subpath-test-projected-s6rn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019583822s
    STEP: Saw pod success 02/24/23 11:20:49.75
    Feb 24 11:20:49.750: INFO: Pod "pod-subpath-test-projected-s6rn" satisfied condition "Succeeded or Failed"
    Feb 24 11:20:49.756: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-projected-s6rn container test-container-subpath-projected-s6rn: <nil>
    STEP: delete the pod 02/24/23 11:20:49.766
    Feb 24 11:20:49.795: INFO: Waiting for pod pod-subpath-test-projected-s6rn to disappear
    Feb 24 11:20:49.801: INFO: Pod pod-subpath-test-projected-s6rn no longer exists
    STEP: Deleting pod pod-subpath-test-projected-s6rn 02/24/23 11:20:49.801
    Feb 24 11:20:49.801: INFO: Deleting pod "pod-subpath-test-projected-s6rn" in namespace "subpath-1414"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 24 11:20:49.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1414" for this suite. 02/24/23 11:20:49.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:20:49.828
Feb 24 11:20:49.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:20:49.829
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:49.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:49.858
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:20:49.862
Feb 24 11:20:49.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e" in namespace "downward-api-3820" to be "Succeeded or Failed"
Feb 24 11:20:49.887: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.505966ms
Feb 24 11:20:51.894: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Running", Reason="", readiness=true. Elapsed: 2.019298035s
Feb 24 11:20:53.896: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021421461s
Feb 24 11:20:55.914: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039609843s
STEP: Saw pod success 02/24/23 11:20:55.914
Feb 24 11:20:55.914: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e" satisfied condition "Succeeded or Failed"
Feb 24 11:20:55.920: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e container client-container: <nil>
STEP: delete the pod 02/24/23 11:20:55.932
Feb 24 11:20:55.954: INFO: Waiting for pod downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e to disappear
Feb 24 11:20:55.959: INFO: Pod downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:20:55.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3820" for this suite. 02/24/23 11:20:55.974
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":105,"skipped":1774,"failed":0}
------------------------------
• [SLOW TEST] [6.162 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:20:49.828
    Feb 24 11:20:49.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:20:49.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:49.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:49.858
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:20:49.862
    Feb 24 11:20:49.874: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e" in namespace "downward-api-3820" to be "Succeeded or Failed"
    Feb 24 11:20:49.887: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.505966ms
    Feb 24 11:20:51.894: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Running", Reason="", readiness=true. Elapsed: 2.019298035s
    Feb 24 11:20:53.896: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021421461s
    Feb 24 11:20:55.914: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039609843s
    STEP: Saw pod success 02/24/23 11:20:55.914
    Feb 24 11:20:55.914: INFO: Pod "downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e" satisfied condition "Succeeded or Failed"
    Feb 24 11:20:55.920: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e container client-container: <nil>
    STEP: delete the pod 02/24/23 11:20:55.932
    Feb 24 11:20:55.954: INFO: Waiting for pod downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e to disappear
    Feb 24 11:20:55.959: INFO: Pod downwardapi-volume-7d6c7e3c-9250-4bb9-995e-53137aa1f84e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:20:55.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3820" for this suite. 02/24/23 11:20:55.974
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:20:55.994
Feb 24 11:20:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replicaset 02/24/23 11:20:55.997
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:56.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:56.047
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 24 11:20:56.050: INFO: Creating ReplicaSet my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab
Feb 24 11:20:56.067: INFO: Pod name my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab: Found 0 pods out of 1
Feb 24 11:21:01.077: INFO: Pod name my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab: Found 1 pods out of 1
Feb 24 11:21:01.077: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab" is running
Feb 24 11:21:01.077: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km" in namespace "replicaset-9797" to be "running"
Feb 24 11:21:01.084: INFO: Pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km": Phase="Running", Reason="", readiness=true. Elapsed: 6.673633ms
Feb 24 11:21:01.084: INFO: Pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km" satisfied condition "running"
Feb 24 11:21:01.084: INFO: Pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:}])
Feb 24 11:21:01.084: INFO: Trying to dial the pod
Feb 24 11:21:06.103: INFO: Controller my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab: Got expected result from replica 1 [my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km]: "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 24 11:21:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9797" for this suite. 02/24/23 11:21:06.109
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":106,"skipped":1778,"failed":0}
------------------------------
• [SLOW TEST] [10.128 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:20:55.994
    Feb 24 11:20:55.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:20:55.997
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:20:56.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:20:56.047
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 24 11:20:56.050: INFO: Creating ReplicaSet my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab
    Feb 24 11:20:56.067: INFO: Pod name my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab: Found 0 pods out of 1
    Feb 24 11:21:01.077: INFO: Pod name my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab: Found 1 pods out of 1
    Feb 24 11:21:01.077: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab" is running
    Feb 24 11:21:01.077: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km" in namespace "replicaset-9797" to be "running"
    Feb 24 11:21:01.084: INFO: Pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km": Phase="Running", Reason="", readiness=true. Elapsed: 6.673633ms
    Feb 24 11:21:01.084: INFO: Pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km" satisfied condition "running"
    Feb 24 11:21:01.084: INFO: Pod "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:20:56 +0000 UTC Reason: Message:}])
    Feb 24 11:21:01.084: INFO: Trying to dial the pod
    Feb 24 11:21:06.103: INFO: Controller my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab: Got expected result from replica 1 [my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km]: "my-hostname-basic-3eb2ec47-7946-4c6b-8183-32fa10099bab-7n6km", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 24 11:21:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9797" for this suite. 02/24/23 11:21:06.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:06.126
Feb 24 11:21:06.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:21:06.127
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:06.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:06.155
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Feb 24 11:21:06.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: creating the pod 02/24/23 11:21:06.159
STEP: submitting the pod to kubernetes 02/24/23 11:21:06.159
Feb 24 11:21:06.171: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead" in namespace "pods-9358" to be "running and ready"
Feb 24 11:21:06.182: INFO: Pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead": Phase="Pending", Reason="", readiness=false. Elapsed: 10.827276ms
Feb 24 11:21:06.182: INFO: The phase of Pod pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:21:08.188: INFO: Pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead": Phase="Running", Reason="", readiness=true. Elapsed: 2.016996851s
Feb 24 11:21:08.188: INFO: The phase of Pod pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead is Running (Ready = true)
Feb 24 11:21:08.188: INFO: Pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 11:21:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9358" for this suite. 02/24/23 11:21:08.321
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":107,"skipped":1806,"failed":0}
------------------------------
• [2.208 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:06.126
    Feb 24 11:21:06.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:21:06.127
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:06.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:06.155
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Feb 24 11:21:06.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: creating the pod 02/24/23 11:21:06.159
    STEP: submitting the pod to kubernetes 02/24/23 11:21:06.159
    Feb 24 11:21:06.171: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead" in namespace "pods-9358" to be "running and ready"
    Feb 24 11:21:06.182: INFO: Pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead": Phase="Pending", Reason="", readiness=false. Elapsed: 10.827276ms
    Feb 24 11:21:06.182: INFO: The phase of Pod pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:21:08.188: INFO: Pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead": Phase="Running", Reason="", readiness=true. Elapsed: 2.016996851s
    Feb 24 11:21:08.188: INFO: The phase of Pod pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead is Running (Ready = true)
    Feb 24 11:21:08.188: INFO: Pod "pod-exec-websocket-7cef2769-f2f0-4f31-9640-2d85817d5ead" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 11:21:08.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9358" for this suite. 02/24/23 11:21:08.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:08.337
Feb 24 11:21:08.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:21:08.339
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:08.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:08.39
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 24 11:21:08.418: INFO: Waiting up to 5m0s for pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4" in namespace "kubelet-test-848" to be "running and ready"
Feb 24 11:21:08.437: INFO: Pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.902005ms
Feb 24 11:21:08.437: INFO: The phase of Pod busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:21:10.447: INFO: Pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028673694s
Feb 24 11:21:10.447: INFO: The phase of Pod busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4 is Running (Ready = true)
Feb 24 11:21:10.447: INFO: Pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 24 11:21:10.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-848" for this suite. 02/24/23 11:21:10.472
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":108,"skipped":1837,"failed":0}
------------------------------
• [2.145 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:08.337
    Feb 24 11:21:08.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 11:21:08.339
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:08.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:08.39
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 24 11:21:08.418: INFO: Waiting up to 5m0s for pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4" in namespace "kubelet-test-848" to be "running and ready"
    Feb 24 11:21:08.437: INFO: Pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.902005ms
    Feb 24 11:21:08.437: INFO: The phase of Pod busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:21:10.447: INFO: Pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028673694s
    Feb 24 11:21:10.447: INFO: The phase of Pod busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4 is Running (Ready = true)
    Feb 24 11:21:10.447: INFO: Pod "busybox-scheduling-47c77eaa-c625-4731-8ec1-71ce475bc4f4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 24 11:21:10.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-848" for this suite. 02/24/23 11:21:10.472
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:10.484
Feb 24 11:21:10.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:21:10.487
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:10.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:10.508
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3423 02/24/23 11:21:10.511
STEP: changing the ExternalName service to type=NodePort 02/24/23 11:21:10.519
STEP: creating replication controller externalname-service in namespace services-3423 02/24/23 11:21:10.647
I0224 11:21:10.666499      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3423, replica count: 2
I0224 11:21:13.718204      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:21:13.718: INFO: Creating new exec pod
Feb 24 11:21:13.726: INFO: Waiting up to 5m0s for pod "execpod2dj8d" in namespace "services-3423" to be "running"
Feb 24 11:21:13.734: INFO: Pod "execpod2dj8d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.822052ms
Feb 24 11:21:15.745: INFO: Pod "execpod2dj8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019162805s
Feb 24 11:21:15.745: INFO: Pod "execpod2dj8d" satisfied condition "running"
Feb 24 11:21:16.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:16.932: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:16.932: INFO: stdout: ""
Feb 24 11:21:17.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:18.133: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:18.133: INFO: stdout: ""
Feb 24 11:21:18.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:19.198: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:19.198: INFO: stdout: ""
Feb 24 11:21:19.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:20.103: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:20.104: INFO: stdout: ""
Feb 24 11:21:20.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:21.108: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:21.108: INFO: stdout: ""
Feb 24 11:21:21.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:22.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:22.104: INFO: stdout: ""
Feb 24 11:21:22.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:23.114: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:23.114: INFO: stdout: ""
Feb 24 11:21:23.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 11:21:24.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:24.100: INFO: stdout: "externalname-service-znwnb"
Feb 24 11:21:24.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.37.232 80'
Feb 24 11:21:24.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.37.232 80\nConnection to 10.110.37.232 80 port [tcp/http] succeeded!\n"
Feb 24 11:21:24.284: INFO: stdout: "externalname-service-pm7rt"
Feb 24 11:21:24.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 31324'
Feb 24 11:21:24.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 31324\nConnection to 172.31.217.191 31324 port [tcp/*] succeeded!\n"
Feb 24 11:21:24.449: INFO: stdout: "externalname-service-pm7rt"
Feb 24 11:21:24.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31324'
Feb 24 11:21:24.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.216.47 31324\nConnection to 172.31.216.47 31324 port [tcp/*] succeeded!\n"
Feb 24 11:21:24.626: INFO: stdout: ""
Feb 24 11:21:25.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31324'
Feb 24 11:21:25.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.216.47 31324\nConnection to 172.31.216.47 31324 port [tcp/*] succeeded!\n"
Feb 24 11:21:25.942: INFO: stdout: "externalname-service-znwnb"
Feb 24 11:21:25.942: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:21:26.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3423" for this suite. 02/24/23 11:21:26.079
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":109,"skipped":1841,"failed":0}
------------------------------
• [SLOW TEST] [15.607 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:10.484
    Feb 24 11:21:10.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:21:10.487
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:10.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:10.508
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3423 02/24/23 11:21:10.511
    STEP: changing the ExternalName service to type=NodePort 02/24/23 11:21:10.519
    STEP: creating replication controller externalname-service in namespace services-3423 02/24/23 11:21:10.647
    I0224 11:21:10.666499      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3423, replica count: 2
    I0224 11:21:13.718204      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:21:13.718: INFO: Creating new exec pod
    Feb 24 11:21:13.726: INFO: Waiting up to 5m0s for pod "execpod2dj8d" in namespace "services-3423" to be "running"
    Feb 24 11:21:13.734: INFO: Pod "execpod2dj8d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.822052ms
    Feb 24 11:21:15.745: INFO: Pod "execpod2dj8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019162805s
    Feb 24 11:21:15.745: INFO: Pod "execpod2dj8d" satisfied condition "running"
    Feb 24 11:21:16.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:16.932: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:16.932: INFO: stdout: ""
    Feb 24 11:21:17.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:18.133: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:18.133: INFO: stdout: ""
    Feb 24 11:21:18.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:19.198: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:19.198: INFO: stdout: ""
    Feb 24 11:21:19.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:20.103: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:20.104: INFO: stdout: ""
    Feb 24 11:21:20.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:21.108: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:21.108: INFO: stdout: ""
    Feb 24 11:21:21.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:22.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:22.104: INFO: stdout: ""
    Feb 24 11:21:22.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:23.114: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:23.114: INFO: stdout: ""
    Feb 24 11:21:23.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 11:21:24.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:24.100: INFO: stdout: "externalname-service-znwnb"
    Feb 24 11:21:24.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.37.232 80'
    Feb 24 11:21:24.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.37.232 80\nConnection to 10.110.37.232 80 port [tcp/http] succeeded!\n"
    Feb 24 11:21:24.284: INFO: stdout: "externalname-service-pm7rt"
    Feb 24 11:21:24.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 31324'
    Feb 24 11:21:24.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 31324\nConnection to 172.31.217.191 31324 port [tcp/*] succeeded!\n"
    Feb 24 11:21:24.449: INFO: stdout: "externalname-service-pm7rt"
    Feb 24 11:21:24.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31324'
    Feb 24 11:21:24.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.216.47 31324\nConnection to 172.31.216.47 31324 port [tcp/*] succeeded!\n"
    Feb 24 11:21:24.626: INFO: stdout: ""
    Feb 24 11:21:25.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-3423 exec execpod2dj8d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 31324'
    Feb 24 11:21:25.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.216.47 31324\nConnection to 172.31.216.47 31324 port [tcp/*] succeeded!\n"
    Feb 24 11:21:25.942: INFO: stdout: "externalname-service-znwnb"
    Feb 24 11:21:25.942: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:21:26.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3423" for this suite. 02/24/23 11:21:26.079
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:26.092
Feb 24 11:21:26.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename watch 02/24/23 11:21:26.093
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:26.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:26.134
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/24/23 11:21:26.144
STEP: creating a watch on configmaps with label B 02/24/23 11:21:26.146
STEP: creating a watch on configmaps with label A or B 02/24/23 11:21:26.147
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/24/23 11:21:26.149
Feb 24 11:21:26.157: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16156 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:21:26.157: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16156 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/24/23 11:21:26.158
Feb 24 11:21:26.174: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16157 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:21:26.175: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16157 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/24/23 11:21:26.175
Feb 24 11:21:26.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16158 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:21:26.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16158 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/24/23 11:21:26.199
Feb 24 11:21:26.209: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16159 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:21:26.209: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16159 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/24/23 11:21:26.209
Feb 24 11:21:26.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16160 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:21:26.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16160 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/24/23 11:21:36.224
Feb 24 11:21:36.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16234 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 11:21:36.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16234 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 24 11:21:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4272" for this suite. 02/24/23 11:21:46.245
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":110,"skipped":1841,"failed":0}
------------------------------
• [SLOW TEST] [20.163 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:26.092
    Feb 24 11:21:26.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename watch 02/24/23 11:21:26.093
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:26.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:26.134
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/24/23 11:21:26.144
    STEP: creating a watch on configmaps with label B 02/24/23 11:21:26.146
    STEP: creating a watch on configmaps with label A or B 02/24/23 11:21:26.147
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/24/23 11:21:26.149
    Feb 24 11:21:26.157: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16156 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:21:26.157: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16156 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/24/23 11:21:26.158
    Feb 24 11:21:26.174: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16157 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:21:26.175: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16157 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/24/23 11:21:26.175
    Feb 24 11:21:26.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16158 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:21:26.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16158 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/24/23 11:21:26.199
    Feb 24 11:21:26.209: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16159 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:21:26.209: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4272  43e58e8f-9827-4435-8fa8-2bab2e223d8c 16159 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/24/23 11:21:26.209
    Feb 24 11:21:26.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16160 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:21:26.223: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16160 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/24/23 11:21:36.224
    Feb 24 11:21:36.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16234 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 11:21:36.236: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4272  44183286-69f8-4bc5-b6ae-301cc8626d5a 16234 0 2023-02-24 11:21:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-24 11:21:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 24 11:21:46.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4272" for this suite. 02/24/23 11:21:46.245
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:46.258
Feb 24 11:21:46.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:21:46.261
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:46.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:46.287
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-687edd5b-f9d2-4b84-ba9d-b5b15bf69d5b 02/24/23 11:21:46.289
STEP: Creating a pod to test consume secrets 02/24/23 11:21:46.312
Feb 24 11:21:46.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0" in namespace "projected-6604" to be "Succeeded or Failed"
Feb 24 11:21:46.350: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.375645ms
Feb 24 11:21:48.360: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028537371s
Feb 24 11:21:50.355: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023688254s
STEP: Saw pod success 02/24/23 11:21:50.355
Feb 24 11:21:50.355: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0" satisfied condition "Succeeded or Failed"
Feb 24 11:21:50.361: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:21:50.377
Feb 24 11:21:50.409: INFO: Waiting for pod pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0 to disappear
Feb 24 11:21:50.415: INFO: Pod pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 11:21:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6604" for this suite. 02/24/23 11:21:50.426
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":1845,"failed":0}
------------------------------
• [4.182 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:46.258
    Feb 24 11:21:46.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:21:46.261
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:46.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:46.287
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-687edd5b-f9d2-4b84-ba9d-b5b15bf69d5b 02/24/23 11:21:46.289
    STEP: Creating a pod to test consume secrets 02/24/23 11:21:46.312
    Feb 24 11:21:46.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0" in namespace "projected-6604" to be "Succeeded or Failed"
    Feb 24 11:21:46.350: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.375645ms
    Feb 24 11:21:48.360: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028537371s
    Feb 24 11:21:50.355: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023688254s
    STEP: Saw pod success 02/24/23 11:21:50.355
    Feb 24 11:21:50.355: INFO: Pod "pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0" satisfied condition "Succeeded or Failed"
    Feb 24 11:21:50.361: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:21:50.377
    Feb 24 11:21:50.409: INFO: Waiting for pod pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0 to disappear
    Feb 24 11:21:50.415: INFO: Pod pod-projected-secrets-8a352c3c-573f-4d76-a7fe-0025a9cef1a0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 11:21:50.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6604" for this suite. 02/24/23 11:21:50.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:50.444
Feb 24 11:21:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:21:50.445
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:50.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:50.478
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-49da6e93-0ac0-4f3d-9fb3-2bcfad1f5ff3 02/24/23 11:21:50.481
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:21:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-986" for this suite. 02/24/23 11:21:50.49
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":112,"skipped":1860,"failed":0}
------------------------------
• [0.056 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:50.444
    Feb 24 11:21:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:21:50.445
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:50.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:50.478
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-49da6e93-0ac0-4f3d-9fb3-2bcfad1f5ff3 02/24/23 11:21:50.481
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:21:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-986" for this suite. 02/24/23 11:21:50.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:50.505
Feb 24 11:21:50.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:21:50.506
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:50.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:50.532
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 02/24/23 11:21:50.534
STEP: Creating a ResourceQuota 02/24/23 11:21:55.558
STEP: Ensuring resource quota status is calculated 02/24/23 11:21:55.571
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 11:21:57.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1995" for this suite. 02/24/23 11:21:57.585
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":113,"skipped":1866,"failed":0}
------------------------------
• [SLOW TEST] [7.091 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:50.505
    Feb 24 11:21:50.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:21:50.506
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:50.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:50.532
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 02/24/23 11:21:50.534
    STEP: Creating a ResourceQuota 02/24/23 11:21:55.558
    STEP: Ensuring resource quota status is calculated 02/24/23 11:21:55.571
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 11:21:57.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1995" for this suite. 02/24/23 11:21:57.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:57.597
Feb 24 11:21:57.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename ingress 02/24/23 11:21:57.598
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:57.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:57.625
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/24/23 11:21:57.627
STEP: getting /apis/networking.k8s.io 02/24/23 11:21:57.628
STEP: getting /apis/networking.k8s.iov1 02/24/23 11:21:57.63
STEP: creating 02/24/23 11:21:57.631
STEP: getting 02/24/23 11:21:57.653
STEP: listing 02/24/23 11:21:57.658
STEP: watching 02/24/23 11:21:57.662
Feb 24 11:21:57.663: INFO: starting watch
STEP: cluster-wide listing 02/24/23 11:21:57.664
STEP: cluster-wide watching 02/24/23 11:21:57.669
Feb 24 11:21:57.669: INFO: starting watch
STEP: patching 02/24/23 11:21:57.67
STEP: updating 02/24/23 11:21:57.681
Feb 24 11:21:57.696: INFO: waiting for watch events with expected annotations
Feb 24 11:21:57.696: INFO: saw patched and updated annotations
STEP: patching /status 02/24/23 11:21:57.696
STEP: updating /status 02/24/23 11:21:57.705
STEP: get /status 02/24/23 11:21:57.718
STEP: deleting 02/24/23 11:21:57.722
STEP: deleting a collection 02/24/23 11:21:57.74
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Feb 24 11:21:57.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7632" for this suite. 02/24/23 11:21:57.771
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":114,"skipped":1875,"failed":0}
------------------------------
• [0.185 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:57.597
    Feb 24 11:21:57.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename ingress 02/24/23 11:21:57.598
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:57.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:57.625
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/24/23 11:21:57.627
    STEP: getting /apis/networking.k8s.io 02/24/23 11:21:57.628
    STEP: getting /apis/networking.k8s.iov1 02/24/23 11:21:57.63
    STEP: creating 02/24/23 11:21:57.631
    STEP: getting 02/24/23 11:21:57.653
    STEP: listing 02/24/23 11:21:57.658
    STEP: watching 02/24/23 11:21:57.662
    Feb 24 11:21:57.663: INFO: starting watch
    STEP: cluster-wide listing 02/24/23 11:21:57.664
    STEP: cluster-wide watching 02/24/23 11:21:57.669
    Feb 24 11:21:57.669: INFO: starting watch
    STEP: patching 02/24/23 11:21:57.67
    STEP: updating 02/24/23 11:21:57.681
    Feb 24 11:21:57.696: INFO: waiting for watch events with expected annotations
    Feb 24 11:21:57.696: INFO: saw patched and updated annotations
    STEP: patching /status 02/24/23 11:21:57.696
    STEP: updating /status 02/24/23 11:21:57.705
    STEP: get /status 02/24/23 11:21:57.718
    STEP: deleting 02/24/23 11:21:57.722
    STEP: deleting a collection 02/24/23 11:21:57.74
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Feb 24 11:21:57.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-7632" for this suite. 02/24/23 11:21:57.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:21:57.787
Feb 24 11:21:57.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:21:57.788
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:57.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:57.818
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7622 02/24/23 11:21:57.821
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 11:21:57.903
STEP: creating service externalsvc in namespace services-7622 02/24/23 11:21:57.904
STEP: creating replication controller externalsvc in namespace services-7622 02/24/23 11:21:58.047
I0224 11:21:58.123441      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7622, replica count: 2
I0224 11:22:01.175194      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/24/23 11:22:01.182
Feb 24 11:22:01.239: INFO: Creating new exec pod
Feb 24 11:22:01.267: INFO: Waiting up to 5m0s for pod "execpoddpxts" in namespace "services-7622" to be "running"
Feb 24 11:22:01.281: INFO: Pod "execpoddpxts": Phase="Pending", Reason="", readiness=false. Elapsed: 14.260352ms
Feb 24 11:22:03.299: INFO: Pod "execpoddpxts": Phase="Running", Reason="", readiness=true. Elapsed: 2.031764913s
Feb 24 11:22:03.299: INFO: Pod "execpoddpxts" satisfied condition "running"
Feb 24 11:22:03.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7622 exec execpoddpxts -- /bin/sh -x -c nslookup nodeport-service.services-7622.svc.cluster.local'
Feb 24 11:22:03.584: INFO: stderr: "+ nslookup nodeport-service.services-7622.svc.cluster.local\n"
Feb 24 11:22:03.584: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-7622.svc.cluster.local\tcanonical name = externalsvc.services-7622.svc.cluster.local.\nName:\texternalsvc.services-7622.svc.cluster.local\nAddress: 10.98.236.4\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7622, will wait for the garbage collector to delete the pods 02/24/23 11:22:03.584
Feb 24 11:22:03.660: INFO: Deleting ReplicationController externalsvc took: 17.466495ms
Feb 24 11:22:03.760: INFO: Terminating ReplicationController externalsvc pods took: 100.294445ms
Feb 24 11:22:05.685: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:22:05.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7622" for this suite. 02/24/23 11:22:05.756
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":115,"skipped":1899,"failed":0}
------------------------------
• [SLOW TEST] [7.997 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:21:57.787
    Feb 24 11:21:57.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:21:57.788
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:21:57.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:21:57.818
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7622 02/24/23 11:21:57.821
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 11:21:57.903
    STEP: creating service externalsvc in namespace services-7622 02/24/23 11:21:57.904
    STEP: creating replication controller externalsvc in namespace services-7622 02/24/23 11:21:58.047
    I0224 11:21:58.123441      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7622, replica count: 2
    I0224 11:22:01.175194      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/24/23 11:22:01.182
    Feb 24 11:22:01.239: INFO: Creating new exec pod
    Feb 24 11:22:01.267: INFO: Waiting up to 5m0s for pod "execpoddpxts" in namespace "services-7622" to be "running"
    Feb 24 11:22:01.281: INFO: Pod "execpoddpxts": Phase="Pending", Reason="", readiness=false. Elapsed: 14.260352ms
    Feb 24 11:22:03.299: INFO: Pod "execpoddpxts": Phase="Running", Reason="", readiness=true. Elapsed: 2.031764913s
    Feb 24 11:22:03.299: INFO: Pod "execpoddpxts" satisfied condition "running"
    Feb 24 11:22:03.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7622 exec execpoddpxts -- /bin/sh -x -c nslookup nodeport-service.services-7622.svc.cluster.local'
    Feb 24 11:22:03.584: INFO: stderr: "+ nslookup nodeport-service.services-7622.svc.cluster.local\n"
    Feb 24 11:22:03.584: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-7622.svc.cluster.local\tcanonical name = externalsvc.services-7622.svc.cluster.local.\nName:\texternalsvc.services-7622.svc.cluster.local\nAddress: 10.98.236.4\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7622, will wait for the garbage collector to delete the pods 02/24/23 11:22:03.584
    Feb 24 11:22:03.660: INFO: Deleting ReplicationController externalsvc took: 17.466495ms
    Feb 24 11:22:03.760: INFO: Terminating ReplicationController externalsvc pods took: 100.294445ms
    Feb 24 11:22:05.685: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:22:05.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7622" for this suite. 02/24/23 11:22:05.756
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:05.791
Feb 24 11:22:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:22:05.792
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:05.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:05.835
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:22:05.838
Feb 24 11:22:05.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964" in namespace "downward-api-8603" to be "Succeeded or Failed"
Feb 24 11:22:05.869: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964": Phase="Pending", Reason="", readiness=false. Elapsed: 15.005086ms
Feb 24 11:22:07.878: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024798257s
Feb 24 11:22:09.876: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022811347s
STEP: Saw pod success 02/24/23 11:22:09.877
Feb 24 11:22:09.877: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964" satisfied condition "Succeeded or Failed"
Feb 24 11:22:09.884: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964 container client-container: <nil>
STEP: delete the pod 02/24/23 11:22:09.899
Feb 24 11:22:09.922: INFO: Waiting for pod downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964 to disappear
Feb 24 11:22:09.927: INFO: Pod downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:22:09.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8603" for this suite. 02/24/23 11:22:09.933
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":116,"skipped":1903,"failed":0}
------------------------------
• [4.155 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:05.791
    Feb 24 11:22:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:22:05.792
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:05.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:05.835
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:22:05.838
    Feb 24 11:22:05.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964" in namespace "downward-api-8603" to be "Succeeded or Failed"
    Feb 24 11:22:05.869: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964": Phase="Pending", Reason="", readiness=false. Elapsed: 15.005086ms
    Feb 24 11:22:07.878: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024798257s
    Feb 24 11:22:09.876: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022811347s
    STEP: Saw pod success 02/24/23 11:22:09.877
    Feb 24 11:22:09.877: INFO: Pod "downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964" satisfied condition "Succeeded or Failed"
    Feb 24 11:22:09.884: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:22:09.899
    Feb 24 11:22:09.922: INFO: Waiting for pod downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964 to disappear
    Feb 24 11:22:09.927: INFO: Pod downwardapi-volume-cf2c7339-6891-40c0-a1e7-f53b4c9ba964 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:22:09.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8603" for this suite. 02/24/23 11:22:09.933
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:09.947
Feb 24 11:22:09.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename subpath 02/24/23 11:22:09.948
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:09.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:09.988
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 11:22:09.992
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-pl9v 02/24/23 11:22:10.014
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:22:10.014
Feb 24 11:22:10.033: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pl9v" in namespace "subpath-7053" to be "Succeeded or Failed"
Feb 24 11:22:10.053: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Pending", Reason="", readiness=false. Elapsed: 19.11011ms
Feb 24 11:22:12.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.025733629s
Feb 24 11:22:14.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 4.026195285s
Feb 24 11:22:16.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 6.025711773s
Feb 24 11:22:18.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 8.025597573s
Feb 24 11:22:20.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 10.026380637s
Feb 24 11:22:22.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.025336226s
Feb 24 11:22:24.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.024882929s
Feb 24 11:22:26.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.025206933s
Feb 24 11:22:28.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.025065999s
Feb 24 11:22:30.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.024604719s
Feb 24 11:22:32.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=false. Elapsed: 22.026412361s
Feb 24 11:22:34.062: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.027600638s
STEP: Saw pod success 02/24/23 11:22:34.062
Feb 24 11:22:34.062: INFO: Pod "pod-subpath-test-configmap-pl9v" satisfied condition "Succeeded or Failed"
Feb 24 11:22:34.067: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-subpath-test-configmap-pl9v container test-container-subpath-configmap-pl9v: <nil>
STEP: delete the pod 02/24/23 11:22:34.08
Feb 24 11:22:34.108: INFO: Waiting for pod pod-subpath-test-configmap-pl9v to disappear
Feb 24 11:22:34.113: INFO: Pod pod-subpath-test-configmap-pl9v no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pl9v 02/24/23 11:22:34.113
Feb 24 11:22:34.113: INFO: Deleting pod "pod-subpath-test-configmap-pl9v" in namespace "subpath-7053"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 24 11:22:34.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7053" for this suite. 02/24/23 11:22:34.125
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":117,"skipped":1906,"failed":0}
------------------------------
• [SLOW TEST] [24.193 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:09.947
    Feb 24 11:22:09.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename subpath 02/24/23 11:22:09.948
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:09.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:09.988
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 11:22:09.992
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-pl9v 02/24/23 11:22:10.014
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:22:10.014
    Feb 24 11:22:10.033: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pl9v" in namespace "subpath-7053" to be "Succeeded or Failed"
    Feb 24 11:22:10.053: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Pending", Reason="", readiness=false. Elapsed: 19.11011ms
    Feb 24 11:22:12.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 2.025733629s
    Feb 24 11:22:14.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 4.026195285s
    Feb 24 11:22:16.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 6.025711773s
    Feb 24 11:22:18.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 8.025597573s
    Feb 24 11:22:20.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 10.026380637s
    Feb 24 11:22:22.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 12.025336226s
    Feb 24 11:22:24.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 14.024882929s
    Feb 24 11:22:26.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 16.025206933s
    Feb 24 11:22:28.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 18.025065999s
    Feb 24 11:22:30.059: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=true. Elapsed: 20.024604719s
    Feb 24 11:22:32.060: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Running", Reason="", readiness=false. Elapsed: 22.026412361s
    Feb 24 11:22:34.062: INFO: Pod "pod-subpath-test-configmap-pl9v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.027600638s
    STEP: Saw pod success 02/24/23 11:22:34.062
    Feb 24 11:22:34.062: INFO: Pod "pod-subpath-test-configmap-pl9v" satisfied condition "Succeeded or Failed"
    Feb 24 11:22:34.067: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-subpath-test-configmap-pl9v container test-container-subpath-configmap-pl9v: <nil>
    STEP: delete the pod 02/24/23 11:22:34.08
    Feb 24 11:22:34.108: INFO: Waiting for pod pod-subpath-test-configmap-pl9v to disappear
    Feb 24 11:22:34.113: INFO: Pod pod-subpath-test-configmap-pl9v no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-pl9v 02/24/23 11:22:34.113
    Feb 24 11:22:34.113: INFO: Deleting pod "pod-subpath-test-configmap-pl9v" in namespace "subpath-7053"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 24 11:22:34.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7053" for this suite. 02/24/23 11:22:34.125
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:34.14
Feb 24 11:22:34.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:22:34.142
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:34.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:34.168
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 02/24/23 11:22:34.176
STEP: fetching the ConfigMap 02/24/23 11:22:34.187
STEP: patching the ConfigMap 02/24/23 11:22:34.196
STEP: listing all ConfigMaps in all namespaces with a label selector 02/24/23 11:22:34.211
STEP: deleting the ConfigMap by collection with a label selector 02/24/23 11:22:34.216
STEP: listing all ConfigMaps in test namespace 02/24/23 11:22:34.246
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:22:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6199" for this suite. 02/24/23 11:22:34.261
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":118,"skipped":1906,"failed":0}
------------------------------
• [0.131 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:34.14
    Feb 24 11:22:34.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:22:34.142
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:34.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:34.168
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 02/24/23 11:22:34.176
    STEP: fetching the ConfigMap 02/24/23 11:22:34.187
    STEP: patching the ConfigMap 02/24/23 11:22:34.196
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/24/23 11:22:34.211
    STEP: deleting the ConfigMap by collection with a label selector 02/24/23 11:22:34.216
    STEP: listing all ConfigMaps in test namespace 02/24/23 11:22:34.246
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:22:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6199" for this suite. 02/24/23 11:22:34.261
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:34.274
Feb 24 11:22:34.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:22:34.275
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:34.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:34.304
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-6105fd58-60bd-43a9-b20d-ca7fa8ce4d9c 02/24/23 11:22:34.306
STEP: Creating a pod to test consume secrets 02/24/23 11:22:34.318
Feb 24 11:22:34.331: INFO: Waiting up to 5m0s for pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110" in namespace "secrets-726" to be "Succeeded or Failed"
Feb 24 11:22:34.345: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110": Phase="Pending", Reason="", readiness=false. Elapsed: 13.806469ms
Feb 24 11:22:36.353: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110": Phase="Running", Reason="", readiness=false. Elapsed: 2.021543452s
Feb 24 11:22:38.353: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021756906s
STEP: Saw pod success 02/24/23 11:22:38.353
Feb 24 11:22:38.354: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110" satisfied condition "Succeeded or Failed"
Feb 24 11:22:38.358: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-9f4206ab-098e-4149-aa70-752234de8110 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:22:38.369
Feb 24 11:22:38.390: INFO: Waiting for pod pod-secrets-9f4206ab-098e-4149-aa70-752234de8110 to disappear
Feb 24 11:22:38.394: INFO: Pod pod-secrets-9f4206ab-098e-4149-aa70-752234de8110 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:22:38.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-726" for this suite. 02/24/23 11:22:38.402
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":119,"skipped":1908,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:34.274
    Feb 24 11:22:34.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:22:34.275
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:34.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:34.304
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-6105fd58-60bd-43a9-b20d-ca7fa8ce4d9c 02/24/23 11:22:34.306
    STEP: Creating a pod to test consume secrets 02/24/23 11:22:34.318
    Feb 24 11:22:34.331: INFO: Waiting up to 5m0s for pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110" in namespace "secrets-726" to be "Succeeded or Failed"
    Feb 24 11:22:34.345: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110": Phase="Pending", Reason="", readiness=false. Elapsed: 13.806469ms
    Feb 24 11:22:36.353: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110": Phase="Running", Reason="", readiness=false. Elapsed: 2.021543452s
    Feb 24 11:22:38.353: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021756906s
    STEP: Saw pod success 02/24/23 11:22:38.353
    Feb 24 11:22:38.354: INFO: Pod "pod-secrets-9f4206ab-098e-4149-aa70-752234de8110" satisfied condition "Succeeded or Failed"
    Feb 24 11:22:38.358: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-9f4206ab-098e-4149-aa70-752234de8110 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:22:38.369
    Feb 24 11:22:38.390: INFO: Waiting for pod pod-secrets-9f4206ab-098e-4149-aa70-752234de8110 to disappear
    Feb 24 11:22:38.394: INFO: Pod pod-secrets-9f4206ab-098e-4149-aa70-752234de8110 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:22:38.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-726" for this suite. 02/24/23 11:22:38.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:38.416
Feb 24 11:22:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:22:38.417
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:38.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:38.449
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 02/24/23 11:22:38.451
Feb 24 11:22:38.451: INFO: Creating e2e-svc-a-lgmq2
Feb 24 11:22:38.483: INFO: Creating e2e-svc-b-8bthn
Feb 24 11:22:38.515: INFO: Creating e2e-svc-c-8l98v
STEP: deleting service collection 02/24/23 11:22:38.63
Feb 24 11:22:38.823: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:22:38.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5638" for this suite. 02/24/23 11:22:38.829
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":120,"skipped":1917,"failed":0}
------------------------------
• [0.430 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:38.416
    Feb 24 11:22:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:22:38.417
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:38.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:38.449
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 02/24/23 11:22:38.451
    Feb 24 11:22:38.451: INFO: Creating e2e-svc-a-lgmq2
    Feb 24 11:22:38.483: INFO: Creating e2e-svc-b-8bthn
    Feb 24 11:22:38.515: INFO: Creating e2e-svc-c-8l98v
    STEP: deleting service collection 02/24/23 11:22:38.63
    Feb 24 11:22:38.823: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:22:38.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5638" for this suite. 02/24/23 11:22:38.829
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:38.852
Feb 24 11:22:38.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:22:38.853
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:38.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:38.896
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 02/24/23 11:22:38.898
Feb 24 11:22:38.912: INFO: Waiting up to 5m0s for pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c" in namespace "var-expansion-7408" to be "Succeeded or Failed"
Feb 24 11:22:38.922: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.473398ms
Feb 24 11:22:40.928: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015410933s
Feb 24 11:22:42.929: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016515573s
STEP: Saw pod success 02/24/23 11:22:42.929
Feb 24 11:22:42.929: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c" satisfied condition "Succeeded or Failed"
Feb 24 11:22:42.934: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:22:42.947
Feb 24 11:22:42.970: INFO: Waiting for pod var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c to disappear
Feb 24 11:22:42.978: INFO: Pod var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 11:22:42.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7408" for this suite. 02/24/23 11:22:42.986
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":121,"skipped":1989,"failed":0}
------------------------------
• [4.148 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:38.852
    Feb 24 11:22:38.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:22:38.853
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:38.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:38.896
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 02/24/23 11:22:38.898
    Feb 24 11:22:38.912: INFO: Waiting up to 5m0s for pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c" in namespace "var-expansion-7408" to be "Succeeded or Failed"
    Feb 24 11:22:38.922: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.473398ms
    Feb 24 11:22:40.928: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015410933s
    Feb 24 11:22:42.929: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016515573s
    STEP: Saw pod success 02/24/23 11:22:42.929
    Feb 24 11:22:42.929: INFO: Pod "var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c" satisfied condition "Succeeded or Failed"
    Feb 24 11:22:42.934: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:22:42.947
    Feb 24 11:22:42.970: INFO: Waiting for pod var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c to disappear
    Feb 24 11:22:42.978: INFO: Pod var-expansion-6599a4fe-b338-46af-a757-42d9bd18850c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 11:22:42.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7408" for this suite. 02/24/23 11:22:42.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:43.002
Feb 24 11:22:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:22:43.003
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:43.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:43.036
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 02/24/23 11:22:43.04
STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:22:43.048
STEP: Creating a ResourceQuota with not terminating scope 02/24/23 11:22:45.054
STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:22:45.06
STEP: Creating a long running pod 02/24/23 11:22:47.068
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/24/23 11:22:47.103
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/24/23 11:22:49.113
STEP: Deleting the pod 02/24/23 11:22:51.119
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:22:51.142
STEP: Creating a terminating pod 02/24/23 11:22:53.148
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/24/23 11:22:53.162
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/24/23 11:22:55.169
STEP: Deleting the pod 02/24/23 11:22:57.177
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:22:57.225
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 11:22:59.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4624" for this suite. 02/24/23 11:22:59.238
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":122,"skipped":1997,"failed":0}
------------------------------
• [SLOW TEST] [16.321 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:43.002
    Feb 24 11:22:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:22:43.003
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:43.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:43.036
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 02/24/23 11:22:43.04
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:22:43.048
    STEP: Creating a ResourceQuota with not terminating scope 02/24/23 11:22:45.054
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 11:22:45.06
    STEP: Creating a long running pod 02/24/23 11:22:47.068
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/24/23 11:22:47.103
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/24/23 11:22:49.113
    STEP: Deleting the pod 02/24/23 11:22:51.119
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:22:51.142
    STEP: Creating a terminating pod 02/24/23 11:22:53.148
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/24/23 11:22:53.162
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/24/23 11:22:55.169
    STEP: Deleting the pod 02/24/23 11:22:57.177
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:22:57.225
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 11:22:59.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4624" for this suite. 02/24/23 11:22:59.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:22:59.325
Feb 24 11:22:59.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename subpath 02/24/23 11:22:59.326
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:59.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:59.377
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 11:22:59.385
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-6bdz 02/24/23 11:22:59.405
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:22:59.406
Feb 24 11:22:59.430: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6bdz" in namespace "subpath-9800" to be "Succeeded or Failed"
Feb 24 11:22:59.437: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.600178ms
Feb 24 11:23:01.455: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.024729029s
Feb 24 11:23:03.446: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.015445396s
Feb 24 11:23:05.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.013842372s
Feb 24 11:23:07.445: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 8.014519261s
Feb 24 11:23:09.446: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 10.01551545s
Feb 24 11:23:11.443: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 12.013091059s
Feb 24 11:23:13.443: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 14.012471311s
Feb 24 11:23:15.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.014264785s
Feb 24 11:23:17.448: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 18.017521165s
Feb 24 11:23:19.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 20.013403535s
Feb 24 11:23:21.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=false. Elapsed: 22.014025821s
Feb 24 11:23:23.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013802476s
STEP: Saw pod success 02/24/23 11:23:23.444
Feb 24 11:23:23.444: INFO: Pod "pod-subpath-test-configmap-6bdz" satisfied condition "Succeeded or Failed"
Feb 24 11:23:23.449: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-configmap-6bdz container test-container-subpath-configmap-6bdz: <nil>
STEP: delete the pod 02/24/23 11:23:23.46
Feb 24 11:23:23.478: INFO: Waiting for pod pod-subpath-test-configmap-6bdz to disappear
Feb 24 11:23:23.483: INFO: Pod pod-subpath-test-configmap-6bdz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6bdz 02/24/23 11:23:23.483
Feb 24 11:23:23.484: INFO: Deleting pod "pod-subpath-test-configmap-6bdz" in namespace "subpath-9800"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 24 11:23:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9800" for this suite. 02/24/23 11:23:23.496
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":123,"skipped":2003,"failed":0}
------------------------------
• [SLOW TEST] [24.181 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:22:59.325
    Feb 24 11:22:59.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename subpath 02/24/23 11:22:59.326
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:22:59.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:22:59.377
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 11:22:59.385
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-6bdz 02/24/23 11:22:59.405
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:22:59.406
    Feb 24 11:22:59.430: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6bdz" in namespace "subpath-9800" to be "Succeeded or Failed"
    Feb 24 11:22:59.437: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.600178ms
    Feb 24 11:23:01.455: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.024729029s
    Feb 24 11:23:03.446: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.015445396s
    Feb 24 11:23:05.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.013842372s
    Feb 24 11:23:07.445: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 8.014519261s
    Feb 24 11:23:09.446: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 10.01551545s
    Feb 24 11:23:11.443: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 12.013091059s
    Feb 24 11:23:13.443: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 14.012471311s
    Feb 24 11:23:15.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.014264785s
    Feb 24 11:23:17.448: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 18.017521165s
    Feb 24 11:23:19.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=true. Elapsed: 20.013403535s
    Feb 24 11:23:21.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Running", Reason="", readiness=false. Elapsed: 22.014025821s
    Feb 24 11:23:23.444: INFO: Pod "pod-subpath-test-configmap-6bdz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013802476s
    STEP: Saw pod success 02/24/23 11:23:23.444
    Feb 24 11:23:23.444: INFO: Pod "pod-subpath-test-configmap-6bdz" satisfied condition "Succeeded or Failed"
    Feb 24 11:23:23.449: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-configmap-6bdz container test-container-subpath-configmap-6bdz: <nil>
    STEP: delete the pod 02/24/23 11:23:23.46
    Feb 24 11:23:23.478: INFO: Waiting for pod pod-subpath-test-configmap-6bdz to disappear
    Feb 24 11:23:23.483: INFO: Pod pod-subpath-test-configmap-6bdz no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6bdz 02/24/23 11:23:23.483
    Feb 24 11:23:23.484: INFO: Deleting pod "pod-subpath-test-configmap-6bdz" in namespace "subpath-9800"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 24 11:23:23.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9800" for this suite. 02/24/23 11:23:23.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:23:23.508
Feb 24 11:23:23.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:23:23.509
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:23.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:23:23.544
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 02/24/23 11:23:23.548
Feb 24 11:23:23.561: INFO: Waiting up to 5m0s for pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d" in namespace "downward-api-619" to be "running and ready"
Feb 24 11:23:23.569: INFO: Pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497858ms
Feb 24 11:23:23.569: INFO: The phase of Pod labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:23:25.576: INFO: Pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014302942s
Feb 24 11:23:25.576: INFO: The phase of Pod labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d is Running (Ready = true)
Feb 24 11:23:25.576: INFO: Pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d" satisfied condition "running and ready"
Feb 24 11:23:26.124: INFO: Successfully updated pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:23:30.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-619" for this suite. 02/24/23 11:23:30.17
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":124,"skipped":2012,"failed":0}
------------------------------
• [SLOW TEST] [6.675 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:23:23.508
    Feb 24 11:23:23.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:23:23.509
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:23.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:23:23.544
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 02/24/23 11:23:23.548
    Feb 24 11:23:23.561: INFO: Waiting up to 5m0s for pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d" in namespace "downward-api-619" to be "running and ready"
    Feb 24 11:23:23.569: INFO: Pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.497858ms
    Feb 24 11:23:23.569: INFO: The phase of Pod labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:23:25.576: INFO: Pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014302942s
    Feb 24 11:23:25.576: INFO: The phase of Pod labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d is Running (Ready = true)
    Feb 24 11:23:25.576: INFO: Pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d" satisfied condition "running and ready"
    Feb 24 11:23:26.124: INFO: Successfully updated pod "labelsupdatecc150ce3-bafe-415a-b694-abcdecdb9a5d"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:23:30.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-619" for this suite. 02/24/23 11:23:30.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:23:30.188
Feb 24 11:23:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename namespaces 02/24/23 11:23:30.189
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:30.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:23:30.234
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 02/24/23 11:23:30.241
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:30.266
STEP: Creating a pod in the namespace 02/24/23 11:23:30.27
STEP: Waiting for the pod to have running status 02/24/23 11:23:30.29
Feb 24 11:23:30.290: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4645" to be "running"
Feb 24 11:23:30.375: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 84.474189ms
Feb 24 11:23:32.381: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.091050342s
Feb 24 11:23:32.381: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/24/23 11:23:32.381
STEP: Waiting for the namespace to be removed. 02/24/23 11:23:32.395
STEP: Recreating the namespace 02/24/23 11:23:44.401
STEP: Verifying there are no pods in the namespace 02/24/23 11:23:44.426
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:23:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2995" for this suite. 02/24/23 11:23:44.438
STEP: Destroying namespace "nsdeletetest-4645" for this suite. 02/24/23 11:23:44.448
Feb 24 11:23:44.453: INFO: Namespace nsdeletetest-4645 was already deleted
STEP: Destroying namespace "nsdeletetest-3411" for this suite. 02/24/23 11:23:44.453
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":125,"skipped":2039,"failed":0}
------------------------------
• [SLOW TEST] [14.274 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:23:30.188
    Feb 24 11:23:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:23:30.189
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:30.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:23:30.234
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 02/24/23 11:23:30.241
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:30.266
    STEP: Creating a pod in the namespace 02/24/23 11:23:30.27
    STEP: Waiting for the pod to have running status 02/24/23 11:23:30.29
    Feb 24 11:23:30.290: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4645" to be "running"
    Feb 24 11:23:30.375: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 84.474189ms
    Feb 24 11:23:32.381: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.091050342s
    Feb 24 11:23:32.381: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/24/23 11:23:32.381
    STEP: Waiting for the namespace to be removed. 02/24/23 11:23:32.395
    STEP: Recreating the namespace 02/24/23 11:23:44.401
    STEP: Verifying there are no pods in the namespace 02/24/23 11:23:44.426
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:23:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2995" for this suite. 02/24/23 11:23:44.438
    STEP: Destroying namespace "nsdeletetest-4645" for this suite. 02/24/23 11:23:44.448
    Feb 24 11:23:44.453: INFO: Namespace nsdeletetest-4645 was already deleted
    STEP: Destroying namespace "nsdeletetest-3411" for this suite. 02/24/23 11:23:44.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:23:44.465
Feb 24 11:23:44.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:23:44.467
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:44.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:23:44.49
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6680 02/24/23 11:23:44.493
STEP: creating a selector 02/24/23 11:23:44.493
STEP: Creating the service pods in kubernetes 02/24/23 11:23:44.493
Feb 24 11:23:44.494: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 11:23:44.536: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6680" to be "running and ready"
Feb 24 11:23:44.552: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.910424ms
Feb 24 11:23:44.553: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:23:46.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02206909s
Feb 24 11:23:46.558: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:23:48.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023377614s
Feb 24 11:23:48.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:23:50.565: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.028285176s
Feb 24 11:23:50.565: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:23:52.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02214308s
Feb 24 11:23:52.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:23:54.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022392888s
Feb 24 11:23:54.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:23:56.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025993247s
Feb 24 11:23:56.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:23:58.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022680341s
Feb 24 11:23:58.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:24:00.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.024272532s
Feb 24 11:24:00.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:24:02.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027666675s
Feb 24 11:24:02.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:24:04.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.021577957s
Feb 24 11:24:04.558: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 11:24:06.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.027721803s
Feb 24 11:24:06.564: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 11:24:06.564: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 11:24:06.571: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6680" to be "running and ready"
Feb 24 11:24:06.576: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.071743ms
Feb 24 11:24:06.576: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 11:24:06.576: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 11:24:06.597: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6680" to be "running and ready"
Feb 24 11:24:06.607: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.706627ms
Feb 24 11:24:06.608: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 11:24:06.608: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 11:24:06.614
Feb 24 11:24:06.628: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6680" to be "running"
Feb 24 11:24:06.635: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.184808ms
Feb 24 11:24:08.640: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012456989s
Feb 24 11:24:08.641: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 11:24:08.646: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 11:24:08.646: INFO: Breadth first check of 10.244.4.51 on host 172.31.215.124...
Feb 24 11:24:08.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.102:9080/dial?request=hostname&protocol=http&host=10.244.4.51&port=8083&tries=1'] Namespace:pod-network-test-6680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:24:08.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:24:08.652: INFO: ExecWithOptions: Clientset creation
Feb 24 11:24:08.652: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.4.51%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:24:08.745: INFO: Waiting for responses: map[]
Feb 24 11:24:08.745: INFO: reached 10.244.4.51 after 0/1 tries
Feb 24 11:24:08.745: INFO: Breadth first check of 10.244.3.101 on host 172.31.216.47...
Feb 24 11:24:08.750: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.102:9080/dial?request=hostname&protocol=http&host=10.244.3.101&port=8083&tries=1'] Namespace:pod-network-test-6680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:24:08.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:24:08.752: INFO: ExecWithOptions: Clientset creation
Feb 24 11:24:08.752: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.101%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:24:08.835: INFO: Waiting for responses: map[]
Feb 24 11:24:08.835: INFO: reached 10.244.3.101 after 0/1 tries
Feb 24 11:24:08.835: INFO: Breadth first check of 10.244.5.27 on host 172.31.217.191...
Feb 24 11:24:08.844: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.102:9080/dial?request=hostname&protocol=http&host=10.244.5.27&port=8083&tries=1'] Namespace:pod-network-test-6680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:24:08.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:24:08.845: INFO: ExecWithOptions: Clientset creation
Feb 24 11:24:08.845: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.5.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 11:24:08.927: INFO: Waiting for responses: map[]
Feb 24 11:24:08.927: INFO: reached 10.244.5.27 after 0/1 tries
Feb 24 11:24:08.927: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 24 11:24:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6680" for this suite. 02/24/23 11:24:08.934
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":126,"skipped":2049,"failed":0}
------------------------------
• [SLOW TEST] [24.479 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:23:44.465
    Feb 24 11:23:44.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 11:23:44.467
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:23:44.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:23:44.49
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6680 02/24/23 11:23:44.493
    STEP: creating a selector 02/24/23 11:23:44.493
    STEP: Creating the service pods in kubernetes 02/24/23 11:23:44.493
    Feb 24 11:23:44.494: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 11:23:44.536: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6680" to be "running and ready"
    Feb 24 11:23:44.552: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.910424ms
    Feb 24 11:23:44.553: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:23:46.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02206909s
    Feb 24 11:23:46.558: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:23:48.560: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023377614s
    Feb 24 11:23:48.560: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:23:50.565: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.028285176s
    Feb 24 11:23:50.565: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:23:52.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.02214308s
    Feb 24 11:23:52.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:23:54.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.022392888s
    Feb 24 11:23:54.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:23:56.562: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025993247s
    Feb 24 11:23:56.562: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:23:58.559: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.022680341s
    Feb 24 11:23:58.559: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:24:00.561: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.024272532s
    Feb 24 11:24:00.561: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:24:02.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027666675s
    Feb 24 11:24:02.564: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:24:04.558: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.021577957s
    Feb 24 11:24:04.558: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 11:24:06.564: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.027721803s
    Feb 24 11:24:06.564: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 11:24:06.564: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 11:24:06.571: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6680" to be "running and ready"
    Feb 24 11:24:06.576: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.071743ms
    Feb 24 11:24:06.576: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 11:24:06.576: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 11:24:06.597: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6680" to be "running and ready"
    Feb 24 11:24:06.607: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.706627ms
    Feb 24 11:24:06.608: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 11:24:06.608: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 11:24:06.614
    Feb 24 11:24:06.628: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6680" to be "running"
    Feb 24 11:24:06.635: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.184808ms
    Feb 24 11:24:08.640: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012456989s
    Feb 24 11:24:08.641: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 11:24:08.646: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 11:24:08.646: INFO: Breadth first check of 10.244.4.51 on host 172.31.215.124...
    Feb 24 11:24:08.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.102:9080/dial?request=hostname&protocol=http&host=10.244.4.51&port=8083&tries=1'] Namespace:pod-network-test-6680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:24:08.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:24:08.652: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:24:08.652: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.4.51%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:24:08.745: INFO: Waiting for responses: map[]
    Feb 24 11:24:08.745: INFO: reached 10.244.4.51 after 0/1 tries
    Feb 24 11:24:08.745: INFO: Breadth first check of 10.244.3.101 on host 172.31.216.47...
    Feb 24 11:24:08.750: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.102:9080/dial?request=hostname&protocol=http&host=10.244.3.101&port=8083&tries=1'] Namespace:pod-network-test-6680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:24:08.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:24:08.752: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:24:08.752: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.3.101%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:24:08.835: INFO: Waiting for responses: map[]
    Feb 24 11:24:08.835: INFO: reached 10.244.3.101 after 0/1 tries
    Feb 24 11:24:08.835: INFO: Breadth first check of 10.244.5.27 on host 172.31.217.191...
    Feb 24 11:24:08.844: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.102:9080/dial?request=hostname&protocol=http&host=10.244.5.27&port=8083&tries=1'] Namespace:pod-network-test-6680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:24:08.844: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:24:08.845: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:24:08.845: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.102%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.5.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 11:24:08.927: INFO: Waiting for responses: map[]
    Feb 24 11:24:08.927: INFO: reached 10.244.5.27 after 0/1 tries
    Feb 24 11:24:08.927: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 24 11:24:08.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6680" for this suite. 02/24/23 11:24:08.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:24:08.945
Feb 24 11:24:08.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 11:24:08.946
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:08.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:08.973
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/24/23 11:24:08.983
STEP: delete the rc 02/24/23 11:24:13.996
STEP: wait for the rc to be deleted 02/24/23 11:24:14.03
Feb 24 11:24:15.079: INFO: 80 pods remaining
Feb 24 11:24:15.079: INFO: 80 pods has nil DeletionTimestamp
Feb 24 11:24:15.079: INFO: 
Feb 24 11:24:16.110: INFO: 71 pods remaining
Feb 24 11:24:16.111: INFO: 71 pods has nil DeletionTimestamp
Feb 24 11:24:16.112: INFO: 
Feb 24 11:24:17.047: INFO: 60 pods remaining
Feb 24 11:24:17.047: INFO: 60 pods has nil DeletionTimestamp
Feb 24 11:24:17.047: INFO: 
Feb 24 11:24:18.073: INFO: 40 pods remaining
Feb 24 11:24:18.073: INFO: 40 pods has nil DeletionTimestamp
Feb 24 11:24:18.073: INFO: 
Feb 24 11:24:19.075: INFO: 31 pods remaining
Feb 24 11:24:19.075: INFO: 31 pods has nil DeletionTimestamp
Feb 24 11:24:19.075: INFO: 
Feb 24 11:24:20.050: INFO: 20 pods remaining
Feb 24 11:24:20.050: INFO: 20 pods has nil DeletionTimestamp
Feb 24 11:24:20.050: INFO: 
STEP: Gathering metrics 02/24/23 11:24:21.1
Feb 24 11:24:21.148: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:24:21.156: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 7.956136ms
Feb 24 11:24:21.156: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:24:21.156: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:24:21.343: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 11:24:21.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7501" for this suite. 02/24/23 11:24:21.351
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":127,"skipped":2063,"failed":0}
------------------------------
• [SLOW TEST] [12.421 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:24:08.945
    Feb 24 11:24:08.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 11:24:08.946
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:08.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:08.973
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/24/23 11:24:08.983
    STEP: delete the rc 02/24/23 11:24:13.996
    STEP: wait for the rc to be deleted 02/24/23 11:24:14.03
    Feb 24 11:24:15.079: INFO: 80 pods remaining
    Feb 24 11:24:15.079: INFO: 80 pods has nil DeletionTimestamp
    Feb 24 11:24:15.079: INFO: 
    Feb 24 11:24:16.110: INFO: 71 pods remaining
    Feb 24 11:24:16.111: INFO: 71 pods has nil DeletionTimestamp
    Feb 24 11:24:16.112: INFO: 
    Feb 24 11:24:17.047: INFO: 60 pods remaining
    Feb 24 11:24:17.047: INFO: 60 pods has nil DeletionTimestamp
    Feb 24 11:24:17.047: INFO: 
    Feb 24 11:24:18.073: INFO: 40 pods remaining
    Feb 24 11:24:18.073: INFO: 40 pods has nil DeletionTimestamp
    Feb 24 11:24:18.073: INFO: 
    Feb 24 11:24:19.075: INFO: 31 pods remaining
    Feb 24 11:24:19.075: INFO: 31 pods has nil DeletionTimestamp
    Feb 24 11:24:19.075: INFO: 
    Feb 24 11:24:20.050: INFO: 20 pods remaining
    Feb 24 11:24:20.050: INFO: 20 pods has nil DeletionTimestamp
    Feb 24 11:24:20.050: INFO: 
    STEP: Gathering metrics 02/24/23 11:24:21.1
    Feb 24 11:24:21.148: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:24:21.156: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 7.956136ms
    Feb 24 11:24:21.156: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:24:21.156: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:24:21.343: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 11:24:21.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7501" for this suite. 02/24/23 11:24:21.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:24:21.368
Feb 24 11:24:21.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:24:21.369
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:21.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:21.407
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 02/24/23 11:24:21.411
Feb 24 11:24:21.411: INFO: namespace kubectl-182
Feb 24 11:24:21.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 create -f -'
Feb 24 11:24:22.649: INFO: stderr: ""
Feb 24 11:24:22.649: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/24/23 11:24:22.649
Feb 24 11:24:23.656: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:24:23.656: INFO: Found 0 / 1
Feb 24 11:24:24.657: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:24:24.657: INFO: Found 0 / 1
Feb 24 11:24:25.657: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:24:25.657: INFO: Found 0 / 1
Feb 24 11:24:26.659: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:24:26.659: INFO: Found 0 / 1
Feb 24 11:24:27.656: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:24:27.656: INFO: Found 1 / 1
Feb 24 11:24:27.656: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 24 11:24:27.663: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:24:27.663: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 11:24:27.663: INFO: wait on agnhost-primary startup in kubectl-182 
Feb 24 11:24:27.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 logs agnhost-primary-dsfkg agnhost-primary'
Feb 24 11:24:27.817: INFO: stderr: ""
Feb 24 11:24:27.817: INFO: stdout: "Paused\n"
STEP: exposing RC 02/24/23 11:24:27.817
Feb 24 11:24:27.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 24 11:24:27.938: INFO: stderr: ""
Feb 24 11:24:27.938: INFO: stdout: "service/rm2 exposed\n"
Feb 24 11:24:27.948: INFO: Service rm2 in namespace kubectl-182 found.
STEP: exposing service 02/24/23 11:24:29.959
Feb 24 11:24:29.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 24 11:24:30.086: INFO: stderr: ""
Feb 24 11:24:30.086: INFO: stdout: "service/rm3 exposed\n"
Feb 24 11:24:30.098: INFO: Service rm3 in namespace kubectl-182 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:24:32.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-182" for this suite. 02/24/23 11:24:32.116
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":128,"skipped":2080,"failed":0}
------------------------------
• [SLOW TEST] [10.759 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:24:21.368
    Feb 24 11:24:21.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:24:21.369
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:21.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:21.407
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 02/24/23 11:24:21.411
    Feb 24 11:24:21.411: INFO: namespace kubectl-182
    Feb 24 11:24:21.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 create -f -'
    Feb 24 11:24:22.649: INFO: stderr: ""
    Feb 24 11:24:22.649: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/24/23 11:24:22.649
    Feb 24 11:24:23.656: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:24:23.656: INFO: Found 0 / 1
    Feb 24 11:24:24.657: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:24:24.657: INFO: Found 0 / 1
    Feb 24 11:24:25.657: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:24:25.657: INFO: Found 0 / 1
    Feb 24 11:24:26.659: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:24:26.659: INFO: Found 0 / 1
    Feb 24 11:24:27.656: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:24:27.656: INFO: Found 1 / 1
    Feb 24 11:24:27.656: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 24 11:24:27.663: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:24:27.663: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 24 11:24:27.663: INFO: wait on agnhost-primary startup in kubectl-182 
    Feb 24 11:24:27.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 logs agnhost-primary-dsfkg agnhost-primary'
    Feb 24 11:24:27.817: INFO: stderr: ""
    Feb 24 11:24:27.817: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/24/23 11:24:27.817
    Feb 24 11:24:27.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 24 11:24:27.938: INFO: stderr: ""
    Feb 24 11:24:27.938: INFO: stdout: "service/rm2 exposed\n"
    Feb 24 11:24:27.948: INFO: Service rm2 in namespace kubectl-182 found.
    STEP: exposing service 02/24/23 11:24:29.959
    Feb 24 11:24:29.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-182 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 24 11:24:30.086: INFO: stderr: ""
    Feb 24 11:24:30.086: INFO: stdout: "service/rm3 exposed\n"
    Feb 24 11:24:30.098: INFO: Service rm3 in namespace kubectl-182 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:24:32.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-182" for this suite. 02/24/23 11:24:32.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:24:32.127
Feb 24 11:24:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:24:32.129
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:32.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:32.154
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-666ecafe-9c81-4488-90aa-7ccdbc0d0ebc 02/24/23 11:24:32.157
STEP: Creating a pod to test consume configMaps 02/24/23 11:24:32.162
Feb 24 11:24:32.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455" in namespace "configmap-3668" to be "Succeeded or Failed"
Feb 24 11:24:32.187: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121362ms
Feb 24 11:24:34.199: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020054904s
Feb 24 11:24:36.195: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016434232s
STEP: Saw pod success 02/24/23 11:24:36.195
Feb 24 11:24:36.196: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455" satisfied condition "Succeeded or Failed"
Feb 24 11:24:36.204: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:24:36.224
Feb 24 11:24:36.269: INFO: Waiting for pod pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455 to disappear
Feb 24 11:24:36.274: INFO: Pod pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:24:36.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3668" for this suite. 02/24/23 11:24:36.292
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":129,"skipped":2104,"failed":0}
------------------------------
• [4.180 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:24:32.127
    Feb 24 11:24:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:24:32.129
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:32.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:32.154
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-666ecafe-9c81-4488-90aa-7ccdbc0d0ebc 02/24/23 11:24:32.157
    STEP: Creating a pod to test consume configMaps 02/24/23 11:24:32.162
    Feb 24 11:24:32.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455" in namespace "configmap-3668" to be "Succeeded or Failed"
    Feb 24 11:24:32.187: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455": Phase="Pending", Reason="", readiness=false. Elapsed: 8.121362ms
    Feb 24 11:24:34.199: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020054904s
    Feb 24 11:24:36.195: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016434232s
    STEP: Saw pod success 02/24/23 11:24:36.195
    Feb 24 11:24:36.196: INFO: Pod "pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455" satisfied condition "Succeeded or Failed"
    Feb 24 11:24:36.204: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:24:36.224
    Feb 24 11:24:36.269: INFO: Waiting for pod pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455 to disappear
    Feb 24 11:24:36.274: INFO: Pod pod-configmaps-20817c2a-1a4e-4043-940f-0ca8366ee455 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:24:36.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3668" for this suite. 02/24/23 11:24:36.292
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:24:36.308
Feb 24 11:24:36.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 11:24:36.309
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:36.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:36.353
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-014b208f-d19e-4096-9db5-e4a85772e511 in namespace container-probe-9458 02/24/23 11:24:36.359
Feb 24 11:24:36.373: INFO: Waiting up to 5m0s for pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511" in namespace "container-probe-9458" to be "not pending"
Feb 24 11:24:36.391: INFO: Pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511": Phase="Pending", Reason="", readiness=false. Elapsed: 17.607326ms
Feb 24 11:24:38.397: INFO: Pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511": Phase="Running", Reason="", readiness=true. Elapsed: 2.023590676s
Feb 24 11:24:38.397: INFO: Pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511" satisfied condition "not pending"
Feb 24 11:24:38.397: INFO: Started pod liveness-014b208f-d19e-4096-9db5-e4a85772e511 in namespace container-probe-9458
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:24:38.397
Feb 24 11:24:38.402: INFO: Initial restart count of pod liveness-014b208f-d19e-4096-9db5-e4a85772e511 is 0
Feb 24 11:24:58.484: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 1 (20.081558482s elapsed)
Feb 24 11:25:18.561: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 2 (40.158683488s elapsed)
Feb 24 11:25:38.647: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 3 (1m0.245074189s elapsed)
Feb 24 11:25:58.726: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 4 (1m20.323986827s elapsed)
Feb 24 11:27:04.966: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 5 (2m26.563979928s elapsed)
STEP: deleting the pod 02/24/23 11:27:04.966
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 11:27:05.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9458" for this suite. 02/24/23 11:27:05.014
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":130,"skipped":2108,"failed":0}
------------------------------
• [SLOW TEST] [148.720 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:24:36.308
    Feb 24 11:24:36.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:24:36.309
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:24:36.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:24:36.353
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-014b208f-d19e-4096-9db5-e4a85772e511 in namespace container-probe-9458 02/24/23 11:24:36.359
    Feb 24 11:24:36.373: INFO: Waiting up to 5m0s for pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511" in namespace "container-probe-9458" to be "not pending"
    Feb 24 11:24:36.391: INFO: Pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511": Phase="Pending", Reason="", readiness=false. Elapsed: 17.607326ms
    Feb 24 11:24:38.397: INFO: Pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511": Phase="Running", Reason="", readiness=true. Elapsed: 2.023590676s
    Feb 24 11:24:38.397: INFO: Pod "liveness-014b208f-d19e-4096-9db5-e4a85772e511" satisfied condition "not pending"
    Feb 24 11:24:38.397: INFO: Started pod liveness-014b208f-d19e-4096-9db5-e4a85772e511 in namespace container-probe-9458
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:24:38.397
    Feb 24 11:24:38.402: INFO: Initial restart count of pod liveness-014b208f-d19e-4096-9db5-e4a85772e511 is 0
    Feb 24 11:24:58.484: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 1 (20.081558482s elapsed)
    Feb 24 11:25:18.561: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 2 (40.158683488s elapsed)
    Feb 24 11:25:38.647: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 3 (1m0.245074189s elapsed)
    Feb 24 11:25:58.726: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 4 (1m20.323986827s elapsed)
    Feb 24 11:27:04.966: INFO: Restart count of pod container-probe-9458/liveness-014b208f-d19e-4096-9db5-e4a85772e511 is now 5 (2m26.563979928s elapsed)
    STEP: deleting the pod 02/24/23 11:27:04.966
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 11:27:05.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9458" for this suite. 02/24/23 11:27:05.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:27:05.035
Feb 24 11:27:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename cronjob 02/24/23 11:27:05.039
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:05.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:05.078
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/24/23 11:27:05.082
STEP: Ensuring a job is scheduled 02/24/23 11:27:05.092
STEP: Ensuring exactly one is scheduled 02/24/23 11:28:01.098
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 11:28:01.104
STEP: Ensuring the job is replaced with a new one 02/24/23 11:28:01.111
STEP: Removing cronjob 02/24/23 11:29:01.118
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 24 11:29:01.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8070" for this suite. 02/24/23 11:29:01.142
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":131,"skipped":2162,"failed":0}
------------------------------
• [SLOW TEST] [116.123 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:27:05.035
    Feb 24 11:27:05.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename cronjob 02/24/23 11:27:05.039
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:27:05.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:27:05.078
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/24/23 11:27:05.082
    STEP: Ensuring a job is scheduled 02/24/23 11:27:05.092
    STEP: Ensuring exactly one is scheduled 02/24/23 11:28:01.098
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 11:28:01.104
    STEP: Ensuring the job is replaced with a new one 02/24/23 11:28:01.111
    STEP: Removing cronjob 02/24/23 11:29:01.118
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 24 11:29:01.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8070" for this suite. 02/24/23 11:29:01.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:01.162
Feb 24 11:29:01.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename endpointslicemirroring 02/24/23 11:29:01.163
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:01.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:01.194
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/24/23 11:29:01.228
Feb 24 11:29:01.247: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/24/23 11:29:03.253
Feb 24 11:29:03.267: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 02/24/23 11:29:05.273
Feb 24 11:29:05.289: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Feb 24 11:29:07.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3303" for this suite. 02/24/23 11:29:07.312
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":132,"skipped":2170,"failed":0}
------------------------------
• [SLOW TEST] [6.164 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:01.162
    Feb 24 11:29:01.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename endpointslicemirroring 02/24/23 11:29:01.163
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:01.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:01.194
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/24/23 11:29:01.228
    Feb 24 11:29:01.247: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/24/23 11:29:03.253
    Feb 24 11:29:03.267: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 02/24/23 11:29:05.273
    Feb 24 11:29:05.289: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Feb 24 11:29:07.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-3303" for this suite. 02/24/23 11:29:07.312
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:07.328
Feb 24 11:29:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:29:07.329
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:07.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:07.353
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 02/24/23 11:29:07.354
Feb 24 11:29:07.366: INFO: Waiting up to 5m0s for pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b" in namespace "downward-api-1601" to be "Succeeded or Failed"
Feb 24 11:29:07.373: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370367ms
Feb 24 11:29:09.379: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013479976s
Feb 24 11:29:11.381: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015303175s
STEP: Saw pod success 02/24/23 11:29:11.381
Feb 24 11:29:11.382: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b" satisfied condition "Succeeded or Failed"
Feb 24 11:29:11.387: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:29:11.419
Feb 24 11:29:11.463: INFO: Waiting for pod downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b to disappear
Feb 24 11:29:11.470: INFO: Pod downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 24 11:29:11.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1601" for this suite. 02/24/23 11:29:11.476
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":133,"skipped":2173,"failed":0}
------------------------------
• [4.158 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:07.328
    Feb 24 11:29:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:29:07.329
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:07.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:07.353
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 02/24/23 11:29:07.354
    Feb 24 11:29:07.366: INFO: Waiting up to 5m0s for pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b" in namespace "downward-api-1601" to be "Succeeded or Failed"
    Feb 24 11:29:07.373: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.370367ms
    Feb 24 11:29:09.379: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013479976s
    Feb 24 11:29:11.381: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015303175s
    STEP: Saw pod success 02/24/23 11:29:11.381
    Feb 24 11:29:11.382: INFO: Pod "downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b" satisfied condition "Succeeded or Failed"
    Feb 24 11:29:11.387: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:29:11.419
    Feb 24 11:29:11.463: INFO: Waiting for pod downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b to disappear
    Feb 24 11:29:11.470: INFO: Pod downward-api-6d951baf-b67d-40ef-a723-8b8992dda79b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 24 11:29:11.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1601" for this suite. 02/24/23 11:29:11.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:11.488
Feb 24 11:29:11.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename ingressclass 02/24/23 11:29:11.489
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:11.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:11.513
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/24/23 11:29:11.515
STEP: getting /apis/networking.k8s.io 02/24/23 11:29:11.517
STEP: getting /apis/networking.k8s.iov1 02/24/23 11:29:11.517
STEP: creating 02/24/23 11:29:11.518
STEP: getting 02/24/23 11:29:11.536
STEP: listing 02/24/23 11:29:11.542
STEP: watching 02/24/23 11:29:11.552
Feb 24 11:29:11.552: INFO: starting watch
STEP: patching 02/24/23 11:29:11.553
STEP: updating 02/24/23 11:29:11.566
Feb 24 11:29:11.573: INFO: waiting for watch events with expected annotations
Feb 24 11:29:11.573: INFO: saw patched and updated annotations
STEP: deleting 02/24/23 11:29:11.573
STEP: deleting a collection 02/24/23 11:29:11.597
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Feb 24 11:29:11.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2251" for this suite. 02/24/23 11:29:11.647
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":134,"skipped":2183,"failed":0}
------------------------------
• [0.177 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:11.488
    Feb 24 11:29:11.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename ingressclass 02/24/23 11:29:11.489
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:11.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:11.513
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/24/23 11:29:11.515
    STEP: getting /apis/networking.k8s.io 02/24/23 11:29:11.517
    STEP: getting /apis/networking.k8s.iov1 02/24/23 11:29:11.517
    STEP: creating 02/24/23 11:29:11.518
    STEP: getting 02/24/23 11:29:11.536
    STEP: listing 02/24/23 11:29:11.542
    STEP: watching 02/24/23 11:29:11.552
    Feb 24 11:29:11.552: INFO: starting watch
    STEP: patching 02/24/23 11:29:11.553
    STEP: updating 02/24/23 11:29:11.566
    Feb 24 11:29:11.573: INFO: waiting for watch events with expected annotations
    Feb 24 11:29:11.573: INFO: saw patched and updated annotations
    STEP: deleting 02/24/23 11:29:11.573
    STEP: deleting a collection 02/24/23 11:29:11.597
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Feb 24 11:29:11.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-2251" for this suite. 02/24/23 11:29:11.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:11.665
Feb 24 11:29:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:29:11.667
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:11.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:11.712
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:29:11.739
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:29:12.287
STEP: Deploying the webhook pod 02/24/23 11:29:12.299
STEP: Wait for the deployment to be ready 02/24/23 11:29:12.32
Feb 24 11:29:12.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 11:29:14.381
STEP: Verifying the service has paired with the endpoint 02/24/23 11:29:14.45
Feb 24 11:29:15.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Feb 24 11:29:15.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/24/23 11:29:15.973
STEP: Creating a custom resource that should be denied by the webhook 02/24/23 11:29:15.996
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/24/23 11:29:18.033
STEP: Updating the custom resource with disallowed data should be denied 02/24/23 11:29:18.048
STEP: Deleting the custom resource should be denied 02/24/23 11:29:18.061
STEP: Remove the offending key and value from the custom resource data 02/24/23 11:29:18.073
STEP: Deleting the updated custom resource should be successful 02/24/23 11:29:18.09
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:29:18.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8293" for this suite. 02/24/23 11:29:18.641
STEP: Destroying namespace "webhook-8293-markers" for this suite. 02/24/23 11:29:18.652
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":135,"skipped":2198,"failed":0}
------------------------------
• [SLOW TEST] [7.081 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:11.665
    Feb 24 11:29:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:29:11.667
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:11.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:11.712
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:29:11.739
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:29:12.287
    STEP: Deploying the webhook pod 02/24/23 11:29:12.299
    STEP: Wait for the deployment to be ready 02/24/23 11:29:12.32
    Feb 24 11:29:12.358: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 11:29:14.381
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:29:14.45
    Feb 24 11:29:15.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Feb 24 11:29:15.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/24/23 11:29:15.973
    STEP: Creating a custom resource that should be denied by the webhook 02/24/23 11:29:15.996
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/24/23 11:29:18.033
    STEP: Updating the custom resource with disallowed data should be denied 02/24/23 11:29:18.048
    STEP: Deleting the custom resource should be denied 02/24/23 11:29:18.061
    STEP: Remove the offending key and value from the custom resource data 02/24/23 11:29:18.073
    STEP: Deleting the updated custom resource should be successful 02/24/23 11:29:18.09
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:29:18.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8293" for this suite. 02/24/23 11:29:18.641
    STEP: Destroying namespace "webhook-8293-markers" for this suite. 02/24/23 11:29:18.652
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:18.755
Feb 24 11:29:18.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:29:18.756
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:18.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:18.812
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 02/24/23 11:29:18.826
Feb 24 11:29:18.850: INFO: Waiting up to 5m0s for pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc" in namespace "downward-api-7970" to be "Succeeded or Failed"
Feb 24 11:29:18.859: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.509417ms
Feb 24 11:29:20.865: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015398959s
Feb 24 11:29:22.867: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01725153s
STEP: Saw pod success 02/24/23 11:29:22.867
Feb 24 11:29:22.867: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc" satisfied condition "Succeeded or Failed"
Feb 24 11:29:22.872: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downward-api-402199be-6e7f-467f-8dbf-a130298863cc container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:29:22.887
Feb 24 11:29:22.907: INFO: Waiting for pod downward-api-402199be-6e7f-467f-8dbf-a130298863cc to disappear
Feb 24 11:29:22.915: INFO: Pod downward-api-402199be-6e7f-467f-8dbf-a130298863cc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 24 11:29:22.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7970" for this suite. 02/24/23 11:29:22.922
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":136,"skipped":2213,"failed":0}
------------------------------
• [4.178 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:18.755
    Feb 24 11:29:18.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:29:18.756
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:18.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:18.812
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 02/24/23 11:29:18.826
    Feb 24 11:29:18.850: INFO: Waiting up to 5m0s for pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc" in namespace "downward-api-7970" to be "Succeeded or Failed"
    Feb 24 11:29:18.859: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.509417ms
    Feb 24 11:29:20.865: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015398959s
    Feb 24 11:29:22.867: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01725153s
    STEP: Saw pod success 02/24/23 11:29:22.867
    Feb 24 11:29:22.867: INFO: Pod "downward-api-402199be-6e7f-467f-8dbf-a130298863cc" satisfied condition "Succeeded or Failed"
    Feb 24 11:29:22.872: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downward-api-402199be-6e7f-467f-8dbf-a130298863cc container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:29:22.887
    Feb 24 11:29:22.907: INFO: Waiting for pod downward-api-402199be-6e7f-467f-8dbf-a130298863cc to disappear
    Feb 24 11:29:22.915: INFO: Pod downward-api-402199be-6e7f-467f-8dbf-a130298863cc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 24 11:29:22.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7970" for this suite. 02/24/23 11:29:22.922
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:22.935
Feb 24 11:29:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename init-container 02/24/23 11:29:22.936
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:22.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:22.963
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 02/24/23 11:29:22.965
Feb 24 11:29:22.966: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 11:29:27.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9315" for this suite. 02/24/23 11:29:27.375
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":137,"skipped":2217,"failed":0}
------------------------------
• [4.451 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:22.935
    Feb 24 11:29:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename init-container 02/24/23 11:29:22.936
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:22.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:22.963
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 02/24/23 11:29:22.965
    Feb 24 11:29:22.966: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 11:29:27.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9315" for this suite. 02/24/23 11:29:27.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:27.389
Feb 24 11:29:27.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:29:27.392
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:27.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:27.417
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-1447 02/24/23 11:29:27.421
STEP: creating service affinity-nodeport-transition in namespace services-1447 02/24/23 11:29:27.421
STEP: creating replication controller affinity-nodeport-transition in namespace services-1447 02/24/23 11:29:27.456
I0224 11:29:27.469420      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1447, replica count: 3
I0224 11:29:30.521259      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:29:30.538: INFO: Creating new exec pod
Feb 24 11:29:30.554: INFO: Waiting up to 5m0s for pod "execpod-affinityn4rj6" in namespace "services-1447" to be "running"
Feb 24 11:29:30.569: INFO: Pod "execpod-affinityn4rj6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.880088ms
Feb 24 11:29:32.577: INFO: Pod "execpod-affinityn4rj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023203707s
Feb 24 11:29:32.577: INFO: Pod "execpod-affinityn4rj6" satisfied condition "running"
Feb 24 11:29:33.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 24 11:29:33.814: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 24 11:29:33.814: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:29:33.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.254.10 80'
Feb 24 11:29:33.962: INFO: stderr: "+ nc -v -t -w 2 10.99.254.10 80\n+ echo hostName\nConnection to 10.99.254.10 80 port [tcp/http] succeeded!\n"
Feb 24 11:29:33.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:29:33.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 32628'
Feb 24 11:29:34.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 32628\nConnection to 172.31.217.191 32628 port [tcp/*] succeeded!\n"
Feb 24 11:29:34.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:29:34.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 32628'
Feb 24 11:29:34.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.216.47 32628\nConnection to 172.31.216.47 32628 port [tcp/*] succeeded!\n"
Feb 24 11:29:34.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:29:34.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:32628/ ; done'
Feb 24 11:29:34.604: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n"
Feb 24 11:29:34.605: INFO: stdout: "\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-6d8wr"
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:34.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:32628/ ; done'
Feb 24 11:29:35.008: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n"
Feb 24 11:29:35.008: INFO: stdout: "\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr"
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
Feb 24 11:29:35.008: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1447, will wait for the garbage collector to delete the pods 02/24/23 11:29:35.027
Feb 24 11:29:35.104: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.604478ms
Feb 24 11:29:35.204: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.838439ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:29:37.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1447" for this suite. 02/24/23 11:29:37.557
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":138,"skipped":2244,"failed":0}
------------------------------
• [SLOW TEST] [10.186 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:27.389
    Feb 24 11:29:27.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:29:27.392
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:27.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:27.417
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-1447 02/24/23 11:29:27.421
    STEP: creating service affinity-nodeport-transition in namespace services-1447 02/24/23 11:29:27.421
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1447 02/24/23 11:29:27.456
    I0224 11:29:27.469420      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1447, replica count: 3
    I0224 11:29:30.521259      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:29:30.538: INFO: Creating new exec pod
    Feb 24 11:29:30.554: INFO: Waiting up to 5m0s for pod "execpod-affinityn4rj6" in namespace "services-1447" to be "running"
    Feb 24 11:29:30.569: INFO: Pod "execpod-affinityn4rj6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.880088ms
    Feb 24 11:29:32.577: INFO: Pod "execpod-affinityn4rj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023203707s
    Feb 24 11:29:32.577: INFO: Pod "execpod-affinityn4rj6" satisfied condition "running"
    Feb 24 11:29:33.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Feb 24 11:29:33.814: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 24 11:29:33.814: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:29:33.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.254.10 80'
    Feb 24 11:29:33.962: INFO: stderr: "+ nc -v -t -w 2 10.99.254.10 80\n+ echo hostName\nConnection to 10.99.254.10 80 port [tcp/http] succeeded!\n"
    Feb 24 11:29:33.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:29:33.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.217.191 32628'
    Feb 24 11:29:34.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.217.191 32628\nConnection to 172.31.217.191 32628 port [tcp/*] succeeded!\n"
    Feb 24 11:29:34.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:29:34.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.216.47 32628'
    Feb 24 11:29:34.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.216.47 32628\nConnection to 172.31.216.47 32628 port [tcp/*] succeeded!\n"
    Feb 24 11:29:34.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:29:34.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:32628/ ; done'
    Feb 24 11:29:34.604: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n"
    Feb 24 11:29:34.605: INFO: stdout: "\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-hftph\naffinity-nodeport-transition-htkdr\naffinity-nodeport-transition-6d8wr"
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-hftph
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-htkdr
    Feb 24 11:29:34.605: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:34.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-1447 exec execpod-affinityn4rj6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.215.124:32628/ ; done'
    Feb 24 11:29:35.008: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.215.124:32628/\n"
    Feb 24 11:29:35.008: INFO: stdout: "\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr\naffinity-nodeport-transition-6d8wr"
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Received response from host: affinity-nodeport-transition-6d8wr
    Feb 24 11:29:35.008: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1447, will wait for the garbage collector to delete the pods 02/24/23 11:29:35.027
    Feb 24 11:29:35.104: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.604478ms
    Feb 24 11:29:35.204: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.838439ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:29:37.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1447" for this suite. 02/24/23 11:29:37.557
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:37.576
Feb 24 11:29:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename disruption 02/24/23 11:29:37.577
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:37.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:37.603
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 02/24/23 11:29:37.612
STEP: Waiting for all pods to be running 02/24/23 11:29:39.702
Feb 24 11:29:39.713: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 24 11:29:41.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8840" for this suite. 02/24/23 11:29:41.735
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":139,"skipped":2247,"failed":0}
------------------------------
• [4.170 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:37.576
    Feb 24 11:29:37.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename disruption 02/24/23 11:29:37.577
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:37.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:37.603
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 02/24/23 11:29:37.612
    STEP: Waiting for all pods to be running 02/24/23 11:29:39.702
    Feb 24 11:29:39.713: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 24 11:29:41.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8840" for this suite. 02/24/23 11:29:41.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:41.746
Feb 24 11:29:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 11:29:41.748
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:41.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:41.771
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/24/23 11:29:41.78
STEP: create the rc2 02/24/23 11:29:41.787
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/24/23 11:29:46.803
STEP: delete the rc simpletest-rc-to-be-deleted 02/24/23 11:29:48.185
STEP: wait for the rc to be deleted 02/24/23 11:29:48.225
Feb 24 11:29:53.269: INFO: 69 pods remaining
Feb 24 11:29:53.270: INFO: 69 pods has nil DeletionTimestamp
Feb 24 11:29:53.270: INFO: 
STEP: Gathering metrics 02/24/23 11:29:58.269
Feb 24 11:29:58.325: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:29:58.330: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.952513ms
Feb 24 11:29:58.330: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:29:58.330: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:29:58.414: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 24 11:29:58.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-22zpm" in namespace "gc-975"
Feb 24 11:29:58.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qk7z" in namespace "gc-975"
Feb 24 11:29:58.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ss7x" in namespace "gc-975"
Feb 24 11:29:58.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tk95" in namespace "gc-975"
Feb 24 11:29:58.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-42j6r" in namespace "gc-975"
Feb 24 11:29:58.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-47hsl" in namespace "gc-975"
Feb 24 11:29:58.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-47l9n" in namespace "gc-975"
Feb 24 11:29:58.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jfp5" in namespace "gc-975"
Feb 24 11:29:58.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-58796" in namespace "gc-975"
Feb 24 11:29:58.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cgdc" in namespace "gc-975"
Feb 24 11:29:58.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lfrd" in namespace "gc-975"
Feb 24 11:29:58.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w5sb" in namespace "gc-975"
Feb 24 11:29:58.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-78wgc" in namespace "gc-975"
Feb 24 11:29:58.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bcx5" in namespace "gc-975"
Feb 24 11:29:58.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m6sq" in namespace "gc-975"
Feb 24 11:29:58.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-822mg" in namespace "gc-975"
Feb 24 11:29:58.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lbwn" in namespace "gc-975"
Feb 24 11:29:58.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zxbx" in namespace "gc-975"
Feb 24 11:29:58.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-97zkx" in namespace "gc-975"
Feb 24 11:29:58.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gk4c" in namespace "gc-975"
Feb 24 11:29:59.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t6nj" in namespace "gc-975"
Feb 24 11:29:59.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbd5k" in namespace "gc-975"
Feb 24 11:29:59.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-btkm2" in namespace "gc-975"
Feb 24 11:29:59.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-btrvn" in namespace "gc-975"
Feb 24 11:29:59.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfvq8" in namespace "gc-975"
Feb 24 11:29:59.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvhn5" in namespace "gc-975"
Feb 24 11:29:59.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx2px" in namespace "gc-975"
Feb 24 11:29:59.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffnsr" in namespace "gc-975"
Feb 24 11:29:59.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv6tw" in namespace "gc-975"
Feb 24 11:29:59.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxlpw" in namespace "gc-975"
Feb 24 11:29:59.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzkc5" in namespace "gc-975"
Feb 24 11:29:59.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbjps" in namespace "gc-975"
Feb 24 11:29:59.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-hl4w9" in namespace "gc-975"
Feb 24 11:29:59.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrl67" in namespace "gc-975"
Feb 24 11:29:59.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-j24wm" in namespace "gc-975"
Feb 24 11:29:59.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2v9h" in namespace "gc-975"
Feb 24 11:29:59.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7kxw" in namespace "gc-975"
Feb 24 11:29:59.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9mxp" in namespace "gc-975"
Feb 24 11:29:59.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqjsx" in namespace "gc-975"
Feb 24 11:29:59.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-jr6pt" in namespace "gc-975"
Feb 24 11:29:59.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfsnt" in namespace "gc-975"
Feb 24 11:29:59.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-kqq74" in namespace "gc-975"
Feb 24 11:29:59.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-lfmhf" in namespace "gc-975"
Feb 24 11:29:59.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkhm7" in namespace "gc-975"
Feb 24 11:29:59.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkmqr" in namespace "gc-975"
Feb 24 11:29:59.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-lwqpk" in namespace "gc-975"
Feb 24 11:29:59.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-m8wpp" in namespace "gc-975"
Feb 24 11:29:59.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-mjzq8" in namespace "gc-975"
Feb 24 11:29:59.861: INFO: Deleting pod "simpletest-rc-to-be-deleted-mrjzt" in namespace "gc-975"
Feb 24 11:29:59.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-nc2q2" in namespace "gc-975"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 11:29:59.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-975" for this suite. 02/24/23 11:29:59.962
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":140,"skipped":2265,"failed":0}
------------------------------
• [SLOW TEST] [18.227 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:41.746
    Feb 24 11:29:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 11:29:41.748
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:29:41.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:29:41.771
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/24/23 11:29:41.78
    STEP: create the rc2 02/24/23 11:29:41.787
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/24/23 11:29:46.803
    STEP: delete the rc simpletest-rc-to-be-deleted 02/24/23 11:29:48.185
    STEP: wait for the rc to be deleted 02/24/23 11:29:48.225
    Feb 24 11:29:53.269: INFO: 69 pods remaining
    Feb 24 11:29:53.270: INFO: 69 pods has nil DeletionTimestamp
    Feb 24 11:29:53.270: INFO: 
    STEP: Gathering metrics 02/24/23 11:29:58.269
    Feb 24 11:29:58.325: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:29:58.330: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.952513ms
    Feb 24 11:29:58.330: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:29:58.330: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:29:58.414: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 24 11:29:58.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-22zpm" in namespace "gc-975"
    Feb 24 11:29:58.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qk7z" in namespace "gc-975"
    Feb 24 11:29:58.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ss7x" in namespace "gc-975"
    Feb 24 11:29:58.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tk95" in namespace "gc-975"
    Feb 24 11:29:58.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-42j6r" in namespace "gc-975"
    Feb 24 11:29:58.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-47hsl" in namespace "gc-975"
    Feb 24 11:29:58.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-47l9n" in namespace "gc-975"
    Feb 24 11:29:58.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jfp5" in namespace "gc-975"
    Feb 24 11:29:58.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-58796" in namespace "gc-975"
    Feb 24 11:29:58.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cgdc" in namespace "gc-975"
    Feb 24 11:29:58.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lfrd" in namespace "gc-975"
    Feb 24 11:29:58.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w5sb" in namespace "gc-975"
    Feb 24 11:29:58.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-78wgc" in namespace "gc-975"
    Feb 24 11:29:58.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bcx5" in namespace "gc-975"
    Feb 24 11:29:58.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m6sq" in namespace "gc-975"
    Feb 24 11:29:58.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-822mg" in namespace "gc-975"
    Feb 24 11:29:58.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lbwn" in namespace "gc-975"
    Feb 24 11:29:58.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zxbx" in namespace "gc-975"
    Feb 24 11:29:58.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-97zkx" in namespace "gc-975"
    Feb 24 11:29:58.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gk4c" in namespace "gc-975"
    Feb 24 11:29:59.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t6nj" in namespace "gc-975"
    Feb 24 11:29:59.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbd5k" in namespace "gc-975"
    Feb 24 11:29:59.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-btkm2" in namespace "gc-975"
    Feb 24 11:29:59.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-btrvn" in namespace "gc-975"
    Feb 24 11:29:59.109: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfvq8" in namespace "gc-975"
    Feb 24 11:29:59.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvhn5" in namespace "gc-975"
    Feb 24 11:29:59.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx2px" in namespace "gc-975"
    Feb 24 11:29:59.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffnsr" in namespace "gc-975"
    Feb 24 11:29:59.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv6tw" in namespace "gc-975"
    Feb 24 11:29:59.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxlpw" in namespace "gc-975"
    Feb 24 11:29:59.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzkc5" in namespace "gc-975"
    Feb 24 11:29:59.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbjps" in namespace "gc-975"
    Feb 24 11:29:59.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-hl4w9" in namespace "gc-975"
    Feb 24 11:29:59.352: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrl67" in namespace "gc-975"
    Feb 24 11:29:59.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-j24wm" in namespace "gc-975"
    Feb 24 11:29:59.407: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2v9h" in namespace "gc-975"
    Feb 24 11:29:59.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7kxw" in namespace "gc-975"
    Feb 24 11:29:59.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9mxp" in namespace "gc-975"
    Feb 24 11:29:59.480: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqjsx" in namespace "gc-975"
    Feb 24 11:29:59.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-jr6pt" in namespace "gc-975"
    Feb 24 11:29:59.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfsnt" in namespace "gc-975"
    Feb 24 11:29:59.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-kqq74" in namespace "gc-975"
    Feb 24 11:29:59.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-lfmhf" in namespace "gc-975"
    Feb 24 11:29:59.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkhm7" in namespace "gc-975"
    Feb 24 11:29:59.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkmqr" in namespace "gc-975"
    Feb 24 11:29:59.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-lwqpk" in namespace "gc-975"
    Feb 24 11:29:59.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-m8wpp" in namespace "gc-975"
    Feb 24 11:29:59.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-mjzq8" in namespace "gc-975"
    Feb 24 11:29:59.861: INFO: Deleting pod "simpletest-rc-to-be-deleted-mrjzt" in namespace "gc-975"
    Feb 24 11:29:59.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-nc2q2" in namespace "gc-975"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 11:29:59.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-975" for this suite. 02/24/23 11:29:59.962
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:29:59.974
Feb 24 11:29:59.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:29:59.995
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:00.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:00.082
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 02/24/23 11:30:00.085
Feb 24 11:30:00.127: INFO: Waiting up to 5m0s for pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824" in namespace "var-expansion-2456" to be "Succeeded or Failed"
Feb 24 11:30:00.147: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Pending", Reason="", readiness=false. Elapsed: 19.869047ms
Feb 24 11:30:02.262: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135702319s
Feb 24 11:30:04.153: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026200464s
Feb 24 11:30:06.153: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Running", Reason="", readiness=true. Elapsed: 6.026472506s
Feb 24 11:30:08.164: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036852092s
STEP: Saw pod success 02/24/23 11:30:08.164
Feb 24 11:30:08.164: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824" satisfied condition "Succeeded or Failed"
Feb 24 11:30:08.176: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824 container dapi-container: <nil>
STEP: delete the pod 02/24/23 11:30:08.2
Feb 24 11:30:08.221: INFO: Waiting for pod var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824 to disappear
Feb 24 11:30:08.233: INFO: Pod var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 11:30:08.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2456" for this suite. 02/24/23 11:30:08.257
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":141,"skipped":2265,"failed":0}
------------------------------
• [SLOW TEST] [8.326 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:29:59.974
    Feb 24 11:29:59.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:29:59.995
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:00.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:00.082
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 02/24/23 11:30:00.085
    Feb 24 11:30:00.127: INFO: Waiting up to 5m0s for pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824" in namespace "var-expansion-2456" to be "Succeeded or Failed"
    Feb 24 11:30:00.147: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Pending", Reason="", readiness=false. Elapsed: 19.869047ms
    Feb 24 11:30:02.262: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135702319s
    Feb 24 11:30:04.153: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026200464s
    Feb 24 11:30:06.153: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Running", Reason="", readiness=true. Elapsed: 6.026472506s
    Feb 24 11:30:08.164: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036852092s
    STEP: Saw pod success 02/24/23 11:30:08.164
    Feb 24 11:30:08.164: INFO: Pod "var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824" satisfied condition "Succeeded or Failed"
    Feb 24 11:30:08.176: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 11:30:08.2
    Feb 24 11:30:08.221: INFO: Waiting for pod var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824 to disappear
    Feb 24 11:30:08.233: INFO: Pod var-expansion-8cec7945-0d88-4e86-a6c4-057548a28824 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 11:30:08.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2456" for this suite. 02/24/23 11:30:08.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:08.304
Feb 24 11:30:08.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:30:08.305
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:08.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:08.434
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/24/23 11:30:08.446
STEP: submitting the pod to kubernetes 02/24/23 11:30:08.446
STEP: verifying QOS class is set on the pod 02/24/23 11:30:08.459
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Feb 24 11:30:08.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8271" for this suite. 02/24/23 11:30:08.479
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":142,"skipped":2291,"failed":0}
------------------------------
• [0.199 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:08.304
    Feb 24 11:30:08.304: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:30:08.305
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:08.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:08.434
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/24/23 11:30:08.446
    STEP: submitting the pod to kubernetes 02/24/23 11:30:08.446
    STEP: verifying QOS class is set on the pod 02/24/23 11:30:08.459
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Feb 24 11:30:08.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8271" for this suite. 02/24/23 11:30:08.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:08.511
Feb 24 11:30:08.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:30:08.514
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:08.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:08.554
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2014 02/24/23 11:30:08.566
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[] 02/24/23 11:30:08.651
Feb 24 11:30:08.677: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 24 11:30:09.690: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2014 02/24/23 11:30:09.69
Feb 24 11:30:09.702: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2014" to be "running and ready"
Feb 24 11:30:09.716: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041755ms
Feb 24 11:30:09.716: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:30:11.741: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.039221249s
Feb 24 11:30:11.741: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 24 11:30:11.741: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[pod1:[80]] 02/24/23 11:30:11.757
Feb 24 11:30:11.813: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/24/23 11:30:11.813
Feb 24 11:30:11.813: INFO: Creating new exec pod
Feb 24 11:30:11.820: INFO: Waiting up to 5m0s for pod "execpodstlk4" in namespace "services-2014" to be "running"
Feb 24 11:30:11.827: INFO: Pod "execpodstlk4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145322ms
Feb 24 11:30:13.833: INFO: Pod "execpodstlk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013199874s
Feb 24 11:30:13.833: INFO: Pod "execpodstlk4" satisfied condition "running"
Feb 24 11:30:14.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 24 11:30:15.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 24 11:30:15.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:30:15.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.195.54 80'
Feb 24 11:30:15.253: INFO: stderr: "+ nc -v -t -w 2 10.97.195.54 80\n+ echo hostName\nConnection to 10.97.195.54 80 port [tcp/http] succeeded!\n"
Feb 24 11:30:15.253: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2014 02/24/23 11:30:15.253
Feb 24 11:30:15.262: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2014" to be "running and ready"
Feb 24 11:30:15.270: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.630099ms
Feb 24 11:30:15.270: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:30:17.277: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014419149s
Feb 24 11:30:17.277: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 24 11:30:17.277: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[pod1:[80] pod2:[80]] 02/24/23 11:30:17.282
Feb 24 11:30:17.305: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/24/23 11:30:17.305
Feb 24 11:30:18.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 24 11:30:18.496: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 24 11:30:18.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:30:18.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.195.54 80'
Feb 24 11:30:18.681: INFO: stderr: "+ nc -v -t -w 2 10.97.195.54 80\n+ echo hostName\nConnection to 10.97.195.54 80 port [tcp/http] succeeded!\n"
Feb 24 11:30:18.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2014 02/24/23 11:30:18.681
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[pod2:[80]] 02/24/23 11:30:18.73
Feb 24 11:30:18.768: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/24/23 11:30:18.768
Feb 24 11:30:19.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 24 11:30:19.988: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 24 11:30:19.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:30:19.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.195.54 80'
Feb 24 11:30:20.245: INFO: stderr: "+ nc -v -t -w 2 10.97.195.54 80\n+ echo hostName\nConnection to 10.97.195.54 80 port [tcp/http] succeeded!\n"
Feb 24 11:30:20.246: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2014 02/24/23 11:30:20.246
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[] 02/24/23 11:30:20.289
Feb 24 11:30:20.323: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:30:20.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2014" for this suite. 02/24/23 11:30:20.465
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":143,"skipped":2303,"failed":0}
------------------------------
• [SLOW TEST] [11.968 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:08.511
    Feb 24 11:30:08.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:30:08.514
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:08.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:08.554
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2014 02/24/23 11:30:08.566
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[] 02/24/23 11:30:08.651
    Feb 24 11:30:08.677: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Feb 24 11:30:09.690: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2014 02/24/23 11:30:09.69
    Feb 24 11:30:09.702: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2014" to be "running and ready"
    Feb 24 11:30:09.716: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.041755ms
    Feb 24 11:30:09.716: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:30:11.741: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.039221249s
    Feb 24 11:30:11.741: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 24 11:30:11.741: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[pod1:[80]] 02/24/23 11:30:11.757
    Feb 24 11:30:11.813: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/24/23 11:30:11.813
    Feb 24 11:30:11.813: INFO: Creating new exec pod
    Feb 24 11:30:11.820: INFO: Waiting up to 5m0s for pod "execpodstlk4" in namespace "services-2014" to be "running"
    Feb 24 11:30:11.827: INFO: Pod "execpodstlk4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.145322ms
    Feb 24 11:30:13.833: INFO: Pod "execpodstlk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.013199874s
    Feb 24 11:30:13.833: INFO: Pod "execpodstlk4" satisfied condition "running"
    Feb 24 11:30:14.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 24 11:30:15.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 24 11:30:15.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:30:15.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.195.54 80'
    Feb 24 11:30:15.253: INFO: stderr: "+ nc -v -t -w 2 10.97.195.54 80\n+ echo hostName\nConnection to 10.97.195.54 80 port [tcp/http] succeeded!\n"
    Feb 24 11:30:15.253: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2014 02/24/23 11:30:15.253
    Feb 24 11:30:15.262: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2014" to be "running and ready"
    Feb 24 11:30:15.270: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.630099ms
    Feb 24 11:30:15.270: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:30:17.277: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014419149s
    Feb 24 11:30:17.277: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 24 11:30:17.277: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[pod1:[80] pod2:[80]] 02/24/23 11:30:17.282
    Feb 24 11:30:17.305: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/24/23 11:30:17.305
    Feb 24 11:30:18.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 24 11:30:18.496: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 24 11:30:18.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:30:18.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.195.54 80'
    Feb 24 11:30:18.681: INFO: stderr: "+ nc -v -t -w 2 10.97.195.54 80\n+ echo hostName\nConnection to 10.97.195.54 80 port [tcp/http] succeeded!\n"
    Feb 24 11:30:18.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2014 02/24/23 11:30:18.681
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[pod2:[80]] 02/24/23 11:30:18.73
    Feb 24 11:30:18.768: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/24/23 11:30:18.768
    Feb 24 11:30:19.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 24 11:30:19.988: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 24 11:30:19.988: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:30:19.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-2014 exec execpodstlk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.195.54 80'
    Feb 24 11:30:20.245: INFO: stderr: "+ nc -v -t -w 2 10.97.195.54 80\n+ echo hostName\nConnection to 10.97.195.54 80 port [tcp/http] succeeded!\n"
    Feb 24 11:30:20.246: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2014 02/24/23 11:30:20.246
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2014 to expose endpoints map[] 02/24/23 11:30:20.289
    Feb 24 11:30:20.323: INFO: successfully validated that service endpoint-test2 in namespace services-2014 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:30:20.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2014" for this suite. 02/24/23 11:30:20.465
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:20.479
Feb 24 11:30:20.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replication-controller 02/24/23 11:30:20.481
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:20.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:20.546
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c 02/24/23 11:30:20.613
Feb 24 11:30:20.654: INFO: Pod name my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c: Found 0 pods out of 1
Feb 24 11:30:25.662: INFO: Pod name my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c: Found 1 pods out of 1
Feb 24 11:30:25.662: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c" are running
Feb 24 11:30:25.662: INFO: Waiting up to 5m0s for pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8" in namespace "replication-controller-7223" to be "running"
Feb 24 11:30:25.673: INFO: Pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8": Phase="Running", Reason="", readiness=true. Elapsed: 10.736134ms
Feb 24 11:30:25.673: INFO: Pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8" satisfied condition "running"
Feb 24 11:30:25.673: INFO: Pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:20 +0000 UTC Reason: Message:}])
Feb 24 11:30:25.673: INFO: Trying to dial the pod
Feb 24 11:30:30.712: INFO: Controller my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c: Got expected result from replica 1 [my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8]: "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 24 11:30:30.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7223" for this suite. 02/24/23 11:30:30.72
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":144,"skipped":2310,"failed":0}
------------------------------
• [SLOW TEST] [10.255 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:20.479
    Feb 24 11:30:20.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replication-controller 02/24/23 11:30:20.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:20.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:20.546
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c 02/24/23 11:30:20.613
    Feb 24 11:30:20.654: INFO: Pod name my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c: Found 0 pods out of 1
    Feb 24 11:30:25.662: INFO: Pod name my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c: Found 1 pods out of 1
    Feb 24 11:30:25.662: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c" are running
    Feb 24 11:30:25.662: INFO: Waiting up to 5m0s for pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8" in namespace "replication-controller-7223" to be "running"
    Feb 24 11:30:25.673: INFO: Pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8": Phase="Running", Reason="", readiness=true. Elapsed: 10.736134ms
    Feb 24 11:30:25.673: INFO: Pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8" satisfied condition "running"
    Feb 24 11:30:25.673: INFO: Pod "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-24 11:30:20 +0000 UTC Reason: Message:}])
    Feb 24 11:30:25.673: INFO: Trying to dial the pod
    Feb 24 11:30:30.712: INFO: Controller my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c: Got expected result from replica 1 [my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8]: "my-hostname-basic-db803722-aa5d-4b2e-813b-bf0319d0875c-pdwg8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 24 11:30:30.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7223" for this suite. 02/24/23 11:30:30.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:30.741
Feb 24 11:30:30.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sysctl 02/24/23 11:30:30.742
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:30.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:30.771
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/24/23 11:30:30.773
STEP: Watching for error events or started pod 02/24/23 11:30:30.795
STEP: Waiting for pod completion 02/24/23 11:30:32.803
Feb 24 11:30:32.803: INFO: Waiting up to 3m0s for pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643" in namespace "sysctl-1133" to be "completed"
Feb 24 11:30:32.813: INFO: Pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643": Phase="Pending", Reason="", readiness=false. Elapsed: 10.287937ms
Feb 24 11:30:34.820: INFO: Pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016840693s
Feb 24 11:30:34.820: INFO: Pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/24/23 11:30:34.826
STEP: Getting logs from the pod 02/24/23 11:30:34.826
STEP: Checking that the sysctl is actually updated 02/24/23 11:30:34.86
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 11:30:34.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1133" for this suite. 02/24/23 11:30:34.881
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":145,"skipped":2326,"failed":0}
------------------------------
• [4.157 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:30.741
    Feb 24 11:30:30.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sysctl 02/24/23 11:30:30.742
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:30.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:30.771
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/24/23 11:30:30.773
    STEP: Watching for error events or started pod 02/24/23 11:30:30.795
    STEP: Waiting for pod completion 02/24/23 11:30:32.803
    Feb 24 11:30:32.803: INFO: Waiting up to 3m0s for pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643" in namespace "sysctl-1133" to be "completed"
    Feb 24 11:30:32.813: INFO: Pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643": Phase="Pending", Reason="", readiness=false. Elapsed: 10.287937ms
    Feb 24 11:30:34.820: INFO: Pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016840693s
    Feb 24 11:30:34.820: INFO: Pod "sysctl-48e4668c-e200-4ee2-8eed-87e3b4fdf643" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/24/23 11:30:34.826
    STEP: Getting logs from the pod 02/24/23 11:30:34.826
    STEP: Checking that the sysctl is actually updated 02/24/23 11:30:34.86
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 11:30:34.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-1133" for this suite. 02/24/23 11:30:34.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:34.9
Feb 24 11:30:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:30:34.901
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:34.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:34.926
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:30:34.929
Feb 24 11:30:34.943: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169" in namespace "downward-api-8864" to be "Succeeded or Failed"
Feb 24 11:30:34.956: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169": Phase="Pending", Reason="", readiness=false. Elapsed: 12.823908ms
Feb 24 11:30:36.963: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019512451s
Feb 24 11:30:38.962: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019256666s
STEP: Saw pod success 02/24/23 11:30:38.962
Feb 24 11:30:38.962: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169" satisfied condition "Succeeded or Failed"
Feb 24 11:30:38.969: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169 container client-container: <nil>
STEP: delete the pod 02/24/23 11:30:38.981
Feb 24 11:30:39.005: INFO: Waiting for pod downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169 to disappear
Feb 24 11:30:39.011: INFO: Pod downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:30:39.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8864" for this suite. 02/24/23 11:30:39.02
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":146,"skipped":2334,"failed":0}
------------------------------
• [4.130 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:34.9
    Feb 24 11:30:34.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:30:34.901
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:34.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:34.926
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:30:34.929
    Feb 24 11:30:34.943: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169" in namespace "downward-api-8864" to be "Succeeded or Failed"
    Feb 24 11:30:34.956: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169": Phase="Pending", Reason="", readiness=false. Elapsed: 12.823908ms
    Feb 24 11:30:36.963: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019512451s
    Feb 24 11:30:38.962: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019256666s
    STEP: Saw pod success 02/24/23 11:30:38.962
    Feb 24 11:30:38.962: INFO: Pod "downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169" satisfied condition "Succeeded or Failed"
    Feb 24 11:30:38.969: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:30:38.981
    Feb 24 11:30:39.005: INFO: Waiting for pod downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169 to disappear
    Feb 24 11:30:39.011: INFO: Pod downwardapi-volume-84041cf0-8602-462f-9a05-79a61e805169 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:30:39.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8864" for this suite. 02/24/23 11:30:39.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:39.037
Feb 24 11:30:39.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:30:39.039
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:39.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:39.066
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 02/24/23 11:30:39.068
Feb 24 11:30:39.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: mark a version not serverd 02/24/23 11:30:46.803
STEP: check the unserved version gets removed 02/24/23 11:30:46.834
STEP: check the other version is not changed 02/24/23 11:30:51.04
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:30:58.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4424" for this suite. 02/24/23 11:30:58.564
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":147,"skipped":2372,"failed":0}
------------------------------
• [SLOW TEST] [19.536 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:39.037
    Feb 24 11:30:39.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:30:39.039
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:39.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:39.066
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 02/24/23 11:30:39.068
    Feb 24 11:30:39.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: mark a version not serverd 02/24/23 11:30:46.803
    STEP: check the unserved version gets removed 02/24/23 11:30:46.834
    STEP: check the other version is not changed 02/24/23 11:30:51.04
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:30:58.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4424" for this suite. 02/24/23 11:30:58.564
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:58.573
Feb 24 11:30:58.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename csistoragecapacity 02/24/23 11:30:58.574
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:58.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:58.607
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/24/23 11:30:58.615
STEP: getting /apis/storage.k8s.io 02/24/23 11:30:58.62
STEP: getting /apis/storage.k8s.io/v1 02/24/23 11:30:58.622
STEP: creating 02/24/23 11:30:58.624
STEP: watching 02/24/23 11:30:58.647
Feb 24 11:30:58.648: INFO: starting watch
STEP: getting 02/24/23 11:30:58.657
STEP: listing in namespace 02/24/23 11:30:58.662
STEP: listing across namespaces 02/24/23 11:30:58.666
STEP: patching 02/24/23 11:30:58.67
STEP: updating 02/24/23 11:30:58.676
Feb 24 11:30:58.682: INFO: waiting for watch events with expected annotations in namespace
Feb 24 11:30:58.683: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/24/23 11:30:58.683
STEP: deleting a collection 02/24/23 11:30:58.698
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Feb 24 11:30:58.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-9477" for this suite. 02/24/23 11:30:58.725
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":148,"skipped":2376,"failed":0}
------------------------------
• [0.160 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:58.573
    Feb 24 11:30:58.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename csistoragecapacity 02/24/23 11:30:58.574
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:58.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:58.607
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/24/23 11:30:58.615
    STEP: getting /apis/storage.k8s.io 02/24/23 11:30:58.62
    STEP: getting /apis/storage.k8s.io/v1 02/24/23 11:30:58.622
    STEP: creating 02/24/23 11:30:58.624
    STEP: watching 02/24/23 11:30:58.647
    Feb 24 11:30:58.648: INFO: starting watch
    STEP: getting 02/24/23 11:30:58.657
    STEP: listing in namespace 02/24/23 11:30:58.662
    STEP: listing across namespaces 02/24/23 11:30:58.666
    STEP: patching 02/24/23 11:30:58.67
    STEP: updating 02/24/23 11:30:58.676
    Feb 24 11:30:58.682: INFO: waiting for watch events with expected annotations in namespace
    Feb 24 11:30:58.683: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/24/23 11:30:58.683
    STEP: deleting a collection 02/24/23 11:30:58.698
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Feb 24 11:30:58.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-9477" for this suite. 02/24/23 11:30:58.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:30:58.734
Feb 24 11:30:58.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:30:58.735
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:58.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:58.767
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 24 11:30:58.787: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:31:58.852: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 02/24/23 11:31:58.857
Feb 24 11:31:58.884: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 24 11:31:58.901: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 24 11:31:58.925: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 24 11:31:58.939: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 24 11:31:58.971: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 24 11:31:58.982: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/24/23 11:31:58.983
Feb 24 11:31:58.983: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:31:58.990: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.673257ms
Feb 24 11:32:00.996: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013693698s
Feb 24 11:32:02.997: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.013826792s
Feb 24 11:32:02.997: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 24 11:32:02.997: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:32:03.008: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.660764ms
Feb 24 11:32:03.008: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:32:03.008: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:32:03.013: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925507ms
Feb 24 11:32:05.019: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010724309s
Feb 24 11:32:07.020: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011758937s
Feb 24 11:32:09.019: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010812272s
Feb 24 11:32:11.020: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.011866866s
Feb 24 11:32:11.020: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:32:11.020: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:32:11.026: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.810093ms
Feb 24 11:32:11.026: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:32:11.026: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:32:11.034: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.099119ms
Feb 24 11:32:11.034: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 11:32:11.035: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:32:11.039: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.79954ms
Feb 24 11:32:11.039: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/24/23 11:32:11.039
Feb 24 11:32:11.048: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3197" to be "running"
Feb 24 11:32:11.053: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7735ms
Feb 24 11:32:13.060: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012569585s
Feb 24 11:32:15.060: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012049231s
Feb 24 11:32:15.060: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:32:15.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3197" for this suite. 02/24/23 11:32:15.107
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":149,"skipped":2393,"failed":0}
------------------------------
• [SLOW TEST] [76.451 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:30:58.734
    Feb 24 11:30:58.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 11:30:58.735
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:30:58.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:30:58.767
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 24 11:30:58.787: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:31:58.852: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 02/24/23 11:31:58.857
    Feb 24 11:31:58.884: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 24 11:31:58.901: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 24 11:31:58.925: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 24 11:31:58.939: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 24 11:31:58.971: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 24 11:31:58.982: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/24/23 11:31:58.983
    Feb 24 11:31:58.983: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:31:58.990: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.673257ms
    Feb 24 11:32:00.996: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013693698s
    Feb 24 11:32:02.997: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.013826792s
    Feb 24 11:32:02.997: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 24 11:32:02.997: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:32:03.008: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.660764ms
    Feb 24 11:32:03.008: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:32:03.008: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:32:03.013: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925507ms
    Feb 24 11:32:05.019: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010724309s
    Feb 24 11:32:07.020: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011758937s
    Feb 24 11:32:09.019: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010812272s
    Feb 24 11:32:11.020: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.011866866s
    Feb 24 11:32:11.020: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:32:11.020: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:32:11.026: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.810093ms
    Feb 24 11:32:11.026: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:32:11.026: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:32:11.034: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.099119ms
    Feb 24 11:32:11.034: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 11:32:11.035: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:32:11.039: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.79954ms
    Feb 24 11:32:11.039: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/24/23 11:32:11.039
    Feb 24 11:32:11.048: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3197" to be "running"
    Feb 24 11:32:11.053: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7735ms
    Feb 24 11:32:13.060: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012569585s
    Feb 24 11:32:15.060: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.012049231s
    Feb 24 11:32:15.060: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:32:15.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3197" for this suite. 02/24/23 11:32:15.107
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:15.222
Feb 24 11:32:15.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:32:15.227
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:15.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:15.264
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 11:32:15.272
Feb 24 11:32:15.285: INFO: Waiting up to 5m0s for pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9" in namespace "emptydir-1324" to be "Succeeded or Failed"
Feb 24 11:32:15.291: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458411ms
Feb 24 11:32:17.297: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011073758s
Feb 24 11:32:19.298: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012901147s
STEP: Saw pod success 02/24/23 11:32:19.298
Feb 24 11:32:19.299: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9" satisfied condition "Succeeded or Failed"
Feb 24 11:32:19.304: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-82525a75-2f76-47a4-a707-918a7c8147e9 container test-container: <nil>
STEP: delete the pod 02/24/23 11:32:19.327
Feb 24 11:32:19.346: INFO: Waiting for pod pod-82525a75-2f76-47a4-a707-918a7c8147e9 to disappear
Feb 24 11:32:19.352: INFO: Pod pod-82525a75-2f76-47a4-a707-918a7c8147e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:32:19.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1324" for this suite. 02/24/23 11:32:19.364
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":150,"skipped":2430,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:15.222
    Feb 24 11:32:15.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:32:15.227
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:15.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:15.264
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 11:32:15.272
    Feb 24 11:32:15.285: INFO: Waiting up to 5m0s for pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9" in namespace "emptydir-1324" to be "Succeeded or Failed"
    Feb 24 11:32:15.291: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458411ms
    Feb 24 11:32:17.297: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011073758s
    Feb 24 11:32:19.298: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012901147s
    STEP: Saw pod success 02/24/23 11:32:19.298
    Feb 24 11:32:19.299: INFO: Pod "pod-82525a75-2f76-47a4-a707-918a7c8147e9" satisfied condition "Succeeded or Failed"
    Feb 24 11:32:19.304: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-82525a75-2f76-47a4-a707-918a7c8147e9 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:32:19.327
    Feb 24 11:32:19.346: INFO: Waiting for pod pod-82525a75-2f76-47a4-a707-918a7c8147e9 to disappear
    Feb 24 11:32:19.352: INFO: Pod pod-82525a75-2f76-47a4-a707-918a7c8147e9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:32:19.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1324" for this suite. 02/24/23 11:32:19.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:19.385
Feb 24 11:32:19.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:32:19.393
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:19.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:19.466
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-090e61ba-7db1-4e07-9a3b-07802312568d 02/24/23 11:32:19.47
STEP: Creating a pod to test consume secrets 02/24/23 11:32:19.476
Feb 24 11:32:19.487: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be" in namespace "projected-9225" to be "Succeeded or Failed"
Feb 24 11:32:19.497: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.896622ms
Feb 24 11:32:21.506: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015746834s
Feb 24 11:32:23.505: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014304645s
STEP: Saw pod success 02/24/23 11:32:23.505
Feb 24 11:32:23.505: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be" satisfied condition "Succeeded or Failed"
Feb 24 11:32:23.510: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:32:23.518
Feb 24 11:32:23.543: INFO: Waiting for pod pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be to disappear
Feb 24 11:32:23.548: INFO: Pod pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 11:32:23.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9225" for this suite. 02/24/23 11:32:23.556
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":151,"skipped":2460,"failed":0}
------------------------------
• [4.199 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:19.385
    Feb 24 11:32:19.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:32:19.393
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:19.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:19.466
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-090e61ba-7db1-4e07-9a3b-07802312568d 02/24/23 11:32:19.47
    STEP: Creating a pod to test consume secrets 02/24/23 11:32:19.476
    Feb 24 11:32:19.487: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be" in namespace "projected-9225" to be "Succeeded or Failed"
    Feb 24 11:32:19.497: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be": Phase="Pending", Reason="", readiness=false. Elapsed: 6.896622ms
    Feb 24 11:32:21.506: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015746834s
    Feb 24 11:32:23.505: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014304645s
    STEP: Saw pod success 02/24/23 11:32:23.505
    Feb 24 11:32:23.505: INFO: Pod "pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be" satisfied condition "Succeeded or Failed"
    Feb 24 11:32:23.510: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:32:23.518
    Feb 24 11:32:23.543: INFO: Waiting for pod pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be to disappear
    Feb 24 11:32:23.548: INFO: Pod pod-projected-secrets-6ad5ad43-b204-4e49-b422-77e80cdc45be no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 11:32:23.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9225" for this suite. 02/24/23 11:32:23.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:23.585
Feb 24 11:32:23.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 11:32:23.586
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:23.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:23.671
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 24 11:32:23.680: INFO: Creating deployment "webserver-deployment"
Feb 24 11:32:23.699: INFO: Waiting for observed generation 1
Feb 24 11:32:25.710: INFO: Waiting for all required pods to come up
Feb 24 11:32:25.716: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/24/23 11:32:25.716
Feb 24 11:32:25.716: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wd7nd" in namespace "deployment-4169" to be "running"
Feb 24 11:32:25.716: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8rnjz" in namespace "deployment-4169" to be "running"
Feb 24 11:32:25.717: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hn57c" in namespace "deployment-4169" to be "running"
Feb 24 11:32:25.717: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-szlqg" in namespace "deployment-4169" to be "running"
Feb 24 11:32:25.723: INFO: Pod "webserver-deployment-845c8977d9-wd7nd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.04074ms
Feb 24 11:32:25.723: INFO: Pod "webserver-deployment-845c8977d9-szlqg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.322088ms
Feb 24 11:32:25.723: INFO: Pod "webserver-deployment-845c8977d9-8rnjz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887089ms
Feb 24 11:32:25.728: INFO: Pod "webserver-deployment-845c8977d9-hn57c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.783409ms
Feb 24 11:32:27.729: INFO: Pod "webserver-deployment-845c8977d9-wd7nd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012651082s
Feb 24 11:32:27.729: INFO: Pod "webserver-deployment-845c8977d9-wd7nd" satisfied condition "running"
Feb 24 11:32:27.733: INFO: Pod "webserver-deployment-845c8977d9-szlqg": Phase="Running", Reason="", readiness=true. Elapsed: 2.015475116s
Feb 24 11:32:27.733: INFO: Pod "webserver-deployment-845c8977d9-szlqg" satisfied condition "running"
Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-8rnjz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017224998s
Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-8rnjz" satisfied condition "running"
Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-hn57c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017387193s
Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-hn57c" satisfied condition "running"
Feb 24 11:32:27.734: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 24 11:32:27.742: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 24 11:32:27.753: INFO: Updating deployment webserver-deployment
Feb 24 11:32:27.753: INFO: Waiting for observed generation 2
Feb 24 11:32:29.762: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 24 11:32:29.766: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 24 11:32:29.770: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 24 11:32:29.781: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 24 11:32:29.781: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 24 11:32:29.785: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 24 11:32:29.793: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 24 11:32:29.793: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 24 11:32:29.803: INFO: Updating deployment webserver-deployment
Feb 24 11:32:29.803: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 24 11:32:29.811: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 24 11:32:29.815: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:32:31.831: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4169  3a9b1714-8523-49cf-bea5-f5ae7f00e24e 23414 3 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004850278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:32:29 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-24 11:32:30 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 24 11:32:31.836: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4169  c995fe75-b970-44b4-b366-ac132264728a 23408 3 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3a9b1714-8523-49cf-bea5-f5ae7f00e24e 0xc002d202f7 0xc002d202f8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a9b1714-8523-49cf-bea5-f5ae7f00e24e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d20398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:32:31.836: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 24 11:32:31.836: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4169  548c7312-9887-4699-92d1-53e29e9e904f 23410 3 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3a9b1714-8523-49cf-bea5-f5ae7f00e24e 0xc002d203f7 0xc002d203f8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a9b1714-8523-49cf-bea5-f5ae7f00e24e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d20488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:32:31.845: INFO: Pod "webserver-deployment-69b7448995-27mfg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-27mfg webserver-deployment-69b7448995- deployment-4169  060adba9-a641-4dcb-80d4-320a1b35d672 23298 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f679e247bf3d834a6c8794da64829323941e5f451dde894626982a528c4abc27 cni.projectcalico.org/podIP:10.244.3.188/32 cni.projectcalico.org/podIPs:10.244.3.188/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20947 0xc002d20948}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pflrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pflrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-4z82b" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4z82b webserver-deployment-69b7448995- deployment-4169  eb5b2e10-8b4d-45f4-be52-3521a166b9dc 23466 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fed586d16de5fd7c76ca3ec30a0c58560c6f833dc419e0696a6b5a4a75f56a8a cni.projectcalico.org/podIP:10.244.4.138/32 cni.projectcalico.org/podIPs:10.244.4.138/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20b67 0xc002d20b68}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgxjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgxjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-68vff" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-68vff webserver-deployment-69b7448995- deployment-4169  7360f241-8f9e-4a5b-b8c0-76283f83d99a 23299 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:384cd94803d9a04f4011ed9cfa25c079b7e6892da0f95c055f16bf0c0d10e97e cni.projectcalico.org/podIP:10.244.4.134/32 cni.projectcalico.org/podIPs:10.244.4.134/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20d87 0xc002d20d88}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv2ht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv2ht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-7nsk4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7nsk4 webserver-deployment-69b7448995- deployment-4169  8220a486-a702-4863-88d6-9410c79f0379 23294 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f6448e03c919cdc4f4e1aadcbf1b1a3acf929cbd74828e973b7ae73a99d89f81 cni.projectcalico.org/podIP:10.244.3.187/32 cni.projectcalico.org/podIPs:10.244.3.187/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20fa7 0xc002d20fa8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7cqsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cqsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-9thwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9thwc webserver-deployment-69b7448995- deployment-4169  c1e61da6-53a2-469a-8441-7650899d136a 23303 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e4c7c8225ac5532f3e8cfdd595f634c71755909c3f13a94c076963e697f3e3ac cni.projectcalico.org/podIP:10.244.4.135/32 cni.projectcalico.org/podIPs:10.244.4.135/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d211c7 0xc002d211c8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22ncf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22ncf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.847: INFO: Pod "webserver-deployment-69b7448995-ddzqt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ddzqt webserver-deployment-69b7448995- deployment-4169  f404dc50-f6c2-4623-b75f-bd0fb2c19e68 23428 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1f5b44789eafa2b862736f68f6109b4f91e89a51aa4898216f9f3f878f2b642d cni.projectcalico.org/podIP:10.244.5.96/32 cni.projectcalico.org/podIPs:10.244.5.96/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d213e7 0xc002d213e8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mw84w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw84w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.847: INFO: Pod "webserver-deployment-69b7448995-dsggc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dsggc webserver-deployment-69b7448995- deployment-4169  7d2c5a67-9ea0-45ce-8611-e6928bcb50d5 23490 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e985c65e9983af4ae861fc981f18349752cc0d14f464d4c72c0bb20a36093295 cni.projectcalico.org/podIP:10.244.3.193/32 cni.projectcalico.org/podIPs:10.244.3.193/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21617 0xc002d21618}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frtzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frtzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.847: INFO: Pod "webserver-deployment-69b7448995-gmkwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-gmkwf webserver-deployment-69b7448995- deployment-4169  32566c3f-9b9f-4713-8768-a61f233db115 23295 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:56d0846c95e738bfab43c015e8434e8b1ba035911f38eb41189d9d54438924a6 cni.projectcalico.org/podIP:10.244.5.94/32 cni.projectcalico.org/podIPs:10.244.5.94/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21837 0xc002d21838}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djr4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djr4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-hw86r" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hw86r webserver-deployment-69b7448995- deployment-4169  96d0795c-662b-4eb4-8d86-260257df5fd8 23461 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a8fca6400b2bc4088e32b6b54bce099c4c254cc7b564ae37f79d7765d6bb102e cni.projectcalico.org/podIP:10.244.5.98/32 cni.projectcalico.org/podIPs:10.244.5.98/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21a57 0xc002d21a58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9l67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9l67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-k4jfp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-k4jfp webserver-deployment-69b7448995- deployment-4169  ffbac7f0-5a45-44b7-aaab-304dd69de2b7 23445 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d8ab9b720f4db06b6729ddb2eeb227e814735c5edb07f0a5473d01224a81ceed cni.projectcalico.org/podIP:10.244.5.97/32 cni.projectcalico.org/podIPs:10.244.5.97/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21c77 0xc002d21c78}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zclxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zclxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-ksxk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ksxk5 webserver-deployment-69b7448995- deployment-4169  25c447cf-70d3-458f-8c64-d9e6e4dafb1d 23464 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7a547da31fb3d166c9318fd3a433c24bd2eec423fc06e1234bcffca25199d806 cni.projectcalico.org/podIP:10.244.4.139/32 cni.projectcalico.org/podIPs:10.244.4.139/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21e97 0xc002d21e98}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fdp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fdp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-mgwt9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mgwt9 webserver-deployment-69b7448995- deployment-4169  1523b238-c8b0-4785-b5e3-e2d9f1ccb97d 23495 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a542e1dc7429cced7450406cf07db9f63074c242a0ce2b6cf535ab8531b7c8a9 cni.projectcalico.org/podIP:10.244.4.140/32 cni.projectcalico.org/podIPs:10.244.4.140/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc00401a0b7 0xc00401a0b8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85s9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85s9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-69b7448995-ng4cf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-ng4cf webserver-deployment-69b7448995- deployment-4169  796d0f0c-038e-46ae-847d-47ed995e54aa 23372 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc00401a2b7 0xc00401a2b8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bf52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bf52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-845c8977d9-5k2cv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5k2cv webserver-deployment-845c8977d9- deployment-4169  6efede53-4baa-41f1-8c13-64826c48ff7f 23208 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6705c49f5cf1a404353a34e34f4ebd4106c8ab5cc1c268b7656eb6470e95016b cni.projectcalico.org/podIP:10.244.5.92/32 cni.projectcalico.org/podIPs:10.244.5.92/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401a440 0xc00401a441}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkqjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkqjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.92,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7a4cce708d9c0765b7116bb2d21497f3f05cf391042790b72e2fad06885f6bfe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-845c8977d9-78b6g" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-78b6g webserver-deployment-845c8977d9- deployment-4169  8a3177d5-d257-4180-b84f-eaf2da0cce69 23437 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:823405ab467047a66d1e15190ef098eb7cab9cf86aae5b0105f7517c85f02821 cni.projectcalico.org/podIP:10.244.3.190/32 cni.projectcalico.org/podIPs:10.244.3.190/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401a660 0xc00401a661}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ks9p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ks9p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-845c8977d9-8hdll" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8hdll webserver-deployment-845c8977d9- deployment-4169  6ab47af7-9ea8-4e34-aba0-11150e99277c 23450 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:da4cd1fea3dfb2890c1197c9ff21faef04e5edce6b57e8d1b022526fe1d81f27 cni.projectcalico.org/podIP:10.244.4.136/32 cni.projectcalico.org/podIPs:10.244.4.136/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401a867 0xc00401a868}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9tg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9tg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-9ftgg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9ftgg webserver-deployment-845c8977d9- deployment-4169  d3f3690d-d315-4785-9077-afd8c5fe0701 23210 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ba2686f4509ede9526a61c001d8301c7686f127419951c19c18c1b89f548a835 cni.projectcalico.org/podIP:10.244.5.93/32 cni.projectcalico.org/podIPs:10.244.5.93/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401aa87 0xc00401aa88}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slz7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slz7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.93,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6c2e20f3c5d7ab3a9da0b4acc4f0118435649f3928a26a8ee89c86141003e99a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-f5wzq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f5wzq webserver-deployment-845c8977d9- deployment-4169  55673b65-fb4b-4c52-a302-60021990ae72 23471 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:374e4d41e9a2e30fa609ad098814601469d3d2d11db4c2f58ba53e9d09eb4b94 cni.projectcalico.org/podIP:10.244.3.192/32 cni.projectcalico.org/podIPs:10.244.3.192/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401aca0 0xc00401aca1}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfhvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfhvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-fldpt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fldpt webserver-deployment-845c8977d9- deployment-4169  bb4f7bcb-986d-498c-b1b5-d963c0788758 23204 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d69cb803b4b2f3a0b252e505cb7b76b7a3e05971e1b21d2765c54b205bb753c0 cni.projectcalico.org/podIP:10.244.5.91/32 cni.projectcalico.org/podIPs:10.244.5.91/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401ae40 0xc00401ae41}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tgvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tgvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.91,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f414e0c35f44b71cdea1edf61239a370cccfb15586f2718a3916e471092a51d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-fnvpw" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fnvpw webserver-deployment-845c8977d9- deployment-4169  b425285e-da78-4a27-95c7-ff4ba04fba33 23217 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:99ea4b6ce87ad972e4113429f69c055873c3192361aac3d460e8848452a2ff2c cni.projectcalico.org/podIP:10.244.3.183/32 cni.projectcalico.org/podIPs:10.244.3.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b060 0xc00401b061}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54l9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54l9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.183,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://69b2923fa282f8fd071f8a53ce803f6e112c40dba0b0fd6c7578602db379ad27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-frrsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-frrsz webserver-deployment-845c8977d9- deployment-4169  4a92b45e-fc2e-471f-9bdd-024687cb65ce 23460 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c8e0de1f8f6f769a0c9da6b611c306117a92957d1a8a9346440093e7b9be3ae cni.projectcalico.org/podIP:10.244.4.137/32 cni.projectcalico.org/podIPs:10.244.4.137/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b287 0xc00401b288}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hswlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hswlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-hp8kr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hp8kr webserver-deployment-845c8977d9- deployment-4169  cb28fb92-c358-4afc-b4bf-f90b3f9e0c2d 23478 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:457c0dd5328a8a37e8669846271b2764edabb1fc8bdc5675bf28e7bc51cb8c80 cni.projectcalico.org/podIP:10.244.5.99/32 cni.projectcalico.org/podIPs:10.244.5.99/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b497 0xc00401b498}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x7mt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x7mt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-jnpf6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jnpf6 webserver-deployment-845c8977d9- deployment-4169  63e895d1-e4f3-4526-85ca-94e0e2e93d2f 23496 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:777c55af85aaac7b9c9218a8f0f1943ae4ce460508ece005bff4cbc0fc9020d2 cni.projectcalico.org/podIP:10.244.3.194/32 cni.projectcalico.org/podIPs:10.244.3.194/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b697 0xc00401b698}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndkbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndkbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-nzrr7" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nzrr7 webserver-deployment-845c8977d9- deployment-4169  bad265c7-ca2b-49de-bb68-f938897072ba 23196 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:afa6063df2b4b282ea4da765e3ab8745e844a2fe7a5d941e791b2aff614054dd cni.projectcalico.org/podIP:10.244.3.184/32 cni.projectcalico.org/podIPs:10.244.3.184/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b830 0xc00401b831}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mv52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mv52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.184,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e66f824dc2bbfc3c7d8c71544dd0bcaf08a2225e08f11d98fbe0f91814c43b2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-r7l9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r7l9q webserver-deployment-845c8977d9- deployment-4169  0fa260cc-1725-4b02-9102-533203a3434b 23476 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:22c268ec79e9e3ba8d420e098c58e9977fb4a19822d2339491eaf448a5d91ad1 cni.projectcalico.org/podIP:10.244.5.100/32 cni.projectcalico.org/podIPs:10.244.5.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401ba57 0xc00401ba58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nznhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nznhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-s74sg" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-s74sg webserver-deployment-845c8977d9- deployment-4169  d6d7528f-2220-43eb-bf09-52b600c64934 23477 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8fb69a99dbded3ecbc4d8c7361374744a742c8d4f194b9a7c6ae4d4d0b468203 cni.projectcalico.org/podIP:10.244.4.141/32 cni.projectcalico.org/podIPs:10.244.4.141/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401bc57 0xc00401bc58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p76q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p76q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-szlqg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-szlqg webserver-deployment-845c8977d9- deployment-4169  2f34d173-f7de-4f75-b24a-a88b727943a1 23223 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:68a37830ca54cfee4ece5efd107fdbebee28e9c43ef5514ce681393e7644bd9d cni.projectcalico.org/podIP:10.244.4.132/32 cni.projectcalico.org/podIPs:10.244.4.132/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401be57 0xc00401be58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slkzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slkzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.132,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8b1eead2f114b6ffb0dddcac3b3a127ff108e5788480d0ee378af2db580d6c7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-tq2jv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-tq2jv webserver-deployment-845c8977d9- deployment-4169  b2fbd866-ad48-47d1-b235-3de4d76534de 23470 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cb719aa6c4169bde17ccc76c52e321bd067b826bccfe8a4c0ff114ec29a4d4bb cni.projectcalico.org/podIP:10.244.3.191/32 cni.projectcalico.org/podIPs:10.244.3.191/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c097 0xc004b8c098}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btjbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btjbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-vqc7v" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vqc7v webserver-deployment-845c8977d9- deployment-4169  0bb58b36-df44-40a9-8d06-391dfcc7d225 23435 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a4f021fd347d573464406ae055dc55f1633169774b7337acfb50c153698ead86 cni.projectcalico.org/podIP:10.244.3.189/32 cni.projectcalico.org/podIPs:10.244.3.189/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c297 0xc004b8c298}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8z4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8z4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-wcdk9" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wcdk9 webserver-deployment-845c8977d9- deployment-4169  22541a0e-e40a-4596-ba4b-5040ab956c95 23173 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:67a9334ffda7b318d730f45701b7a29f064e868da15b1664bb4e78cc6633f49e cni.projectcalico.org/podIP:10.244.4.131/32 cni.projectcalico.org/podIPs:10.244.4.131/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c497 0xc004b8c498}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-786wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-786wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.131,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1831e793cafe92b2701baa53547f99253e9dc7a821d8c4dba6b5a11287f384d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-wd7nd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wd7nd webserver-deployment-845c8977d9- deployment-4169  d2fb5e52-f34b-4b54-9247-b343dfaefc8d 23220 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5530714c8b549f7ca6f600eabe8809b1211b63e765a944f150e1416a3f661201 cni.projectcalico.org/podIP:10.244.4.133/32 cni.projectcalico.org/podIPs:10.244.4.133/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c6b7 0xc004b8c6b8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcb8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcb8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.133,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cfe02cfcd593e0805155b1246b54ee8710e71db2adcf76fd113c9004b786472f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-z7z29" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7z29 webserver-deployment-845c8977d9- deployment-4169  44e290e2-75f4-4684-bef2-e8f76764f208 23484 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f9f4b3c0f23f05b1c33b134af65037ef408dbc4f6515486573a5a596f8072dd3 cni.projectcalico.org/podIP:10.244.4.142/32 cni.projectcalico.org/podIPs:10.244.4.142/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c8d7 0xc004b8c8d8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77282,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77282,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 11:32:31.854: INFO: Pod "webserver-deployment-845c8977d9-zjjdj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zjjdj webserver-deployment-845c8977d9- deployment-4169  e211d647-aef7-4d9f-b3cc-81df8d18ba18 23424 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c690613ab9aae2154798d647694eceb7a7515e34df540800e2734f2dc97101ca cni.projectcalico.org/podIP:10.244.5.95/32 cni.projectcalico.org/podIPs:10.244.5.95/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8ca70 0xc004b8ca71}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wklsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wklsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 11:32:31.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4169" for this suite. 02/24/23 11:32:31.861
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":152,"skipped":2472,"failed":0}
------------------------------
• [SLOW TEST] [8.289 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:23.585
    Feb 24 11:32:23.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 11:32:23.586
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:23.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:23.671
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 24 11:32:23.680: INFO: Creating deployment "webserver-deployment"
    Feb 24 11:32:23.699: INFO: Waiting for observed generation 1
    Feb 24 11:32:25.710: INFO: Waiting for all required pods to come up
    Feb 24 11:32:25.716: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/24/23 11:32:25.716
    Feb 24 11:32:25.716: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wd7nd" in namespace "deployment-4169" to be "running"
    Feb 24 11:32:25.716: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8rnjz" in namespace "deployment-4169" to be "running"
    Feb 24 11:32:25.717: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hn57c" in namespace "deployment-4169" to be "running"
    Feb 24 11:32:25.717: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-szlqg" in namespace "deployment-4169" to be "running"
    Feb 24 11:32:25.723: INFO: Pod "webserver-deployment-845c8977d9-wd7nd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.04074ms
    Feb 24 11:32:25.723: INFO: Pod "webserver-deployment-845c8977d9-szlqg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.322088ms
    Feb 24 11:32:25.723: INFO: Pod "webserver-deployment-845c8977d9-8rnjz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887089ms
    Feb 24 11:32:25.728: INFO: Pod "webserver-deployment-845c8977d9-hn57c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.783409ms
    Feb 24 11:32:27.729: INFO: Pod "webserver-deployment-845c8977d9-wd7nd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012651082s
    Feb 24 11:32:27.729: INFO: Pod "webserver-deployment-845c8977d9-wd7nd" satisfied condition "running"
    Feb 24 11:32:27.733: INFO: Pod "webserver-deployment-845c8977d9-szlqg": Phase="Running", Reason="", readiness=true. Elapsed: 2.015475116s
    Feb 24 11:32:27.733: INFO: Pod "webserver-deployment-845c8977d9-szlqg" satisfied condition "running"
    Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-8rnjz": Phase="Running", Reason="", readiness=true. Elapsed: 2.017224998s
    Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-8rnjz" satisfied condition "running"
    Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-hn57c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017387193s
    Feb 24 11:32:27.734: INFO: Pod "webserver-deployment-845c8977d9-hn57c" satisfied condition "running"
    Feb 24 11:32:27.734: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 24 11:32:27.742: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 24 11:32:27.753: INFO: Updating deployment webserver-deployment
    Feb 24 11:32:27.753: INFO: Waiting for observed generation 2
    Feb 24 11:32:29.762: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 24 11:32:29.766: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 24 11:32:29.770: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 24 11:32:29.781: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 24 11:32:29.781: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 24 11:32:29.785: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 24 11:32:29.793: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 24 11:32:29.793: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 24 11:32:29.803: INFO: Updating deployment webserver-deployment
    Feb 24 11:32:29.803: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 24 11:32:29.811: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 24 11:32:29.815: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:32:31.831: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4169  3a9b1714-8523-49cf-bea5-f5ae7f00e24e 23414 3 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004850278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 11:32:29 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-24 11:32:30 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb 24 11:32:31.836: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4169  c995fe75-b970-44b4-b366-ac132264728a 23408 3 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3a9b1714-8523-49cf-bea5-f5ae7f00e24e 0xc002d202f7 0xc002d202f8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a9b1714-8523-49cf-bea5-f5ae7f00e24e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d20398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:32:31.836: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 24 11:32:31.836: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4169  548c7312-9887-4699-92d1-53e29e9e904f 23410 3 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3a9b1714-8523-49cf-bea5-f5ae7f00e24e 0xc002d203f7 0xc002d203f8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a9b1714-8523-49cf-bea5-f5ae7f00e24e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d20488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:32:31.845: INFO: Pod "webserver-deployment-69b7448995-27mfg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-27mfg webserver-deployment-69b7448995- deployment-4169  060adba9-a641-4dcb-80d4-320a1b35d672 23298 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f679e247bf3d834a6c8794da64829323941e5f451dde894626982a528c4abc27 cni.projectcalico.org/podIP:10.244.3.188/32 cni.projectcalico.org/podIPs:10.244.3.188/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20947 0xc002d20948}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pflrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pflrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-4z82b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4z82b webserver-deployment-69b7448995- deployment-4169  eb5b2e10-8b4d-45f4-be52-3521a166b9dc 23466 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fed586d16de5fd7c76ca3ec30a0c58560c6f833dc419e0696a6b5a4a75f56a8a cni.projectcalico.org/podIP:10.244.4.138/32 cni.projectcalico.org/podIPs:10.244.4.138/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20b67 0xc002d20b68}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgxjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgxjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-68vff" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-68vff webserver-deployment-69b7448995- deployment-4169  7360f241-8f9e-4a5b-b8c0-76283f83d99a 23299 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:384cd94803d9a04f4011ed9cfa25c079b7e6892da0f95c055f16bf0c0d10e97e cni.projectcalico.org/podIP:10.244.4.134/32 cni.projectcalico.org/podIPs:10.244.4.134/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20d87 0xc002d20d88}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv2ht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv2ht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-7nsk4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7nsk4 webserver-deployment-69b7448995- deployment-4169  8220a486-a702-4863-88d6-9410c79f0379 23294 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:f6448e03c919cdc4f4e1aadcbf1b1a3acf929cbd74828e973b7ae73a99d89f81 cni.projectcalico.org/podIP:10.244.3.187/32 cni.projectcalico.org/podIPs:10.244.3.187/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d20fa7 0xc002d20fa8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7cqsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cqsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.846: INFO: Pod "webserver-deployment-69b7448995-9thwc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9thwc webserver-deployment-69b7448995- deployment-4169  c1e61da6-53a2-469a-8441-7650899d136a 23303 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e4c7c8225ac5532f3e8cfdd595f634c71755909c3f13a94c076963e697f3e3ac cni.projectcalico.org/podIP:10.244.4.135/32 cni.projectcalico.org/podIPs:10.244.4.135/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d211c7 0xc002d211c8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22ncf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22ncf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.847: INFO: Pod "webserver-deployment-69b7448995-ddzqt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ddzqt webserver-deployment-69b7448995- deployment-4169  f404dc50-f6c2-4623-b75f-bd0fb2c19e68 23428 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1f5b44789eafa2b862736f68f6109b4f91e89a51aa4898216f9f3f878f2b642d cni.projectcalico.org/podIP:10.244.5.96/32 cni.projectcalico.org/podIPs:10.244.5.96/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d213e7 0xc002d213e8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mw84w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mw84w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.847: INFO: Pod "webserver-deployment-69b7448995-dsggc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dsggc webserver-deployment-69b7448995- deployment-4169  7d2c5a67-9ea0-45ce-8611-e6928bcb50d5 23490 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e985c65e9983af4ae861fc981f18349752cc0d14f464d4c72c0bb20a36093295 cni.projectcalico.org/podIP:10.244.3.193/32 cni.projectcalico.org/podIPs:10.244.3.193/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21617 0xc002d21618}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frtzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frtzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.847: INFO: Pod "webserver-deployment-69b7448995-gmkwf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-gmkwf webserver-deployment-69b7448995- deployment-4169  32566c3f-9b9f-4713-8768-a61f233db115 23295 0 2023-02-24 11:32:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:56d0846c95e738bfab43c015e8434e8b1ba035911f38eb41189d9d54438924a6 cni.projectcalico.org/podIP:10.244.5.94/32 cni.projectcalico.org/podIPs:10.244.5.94/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21837 0xc002d21838}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djr4t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djr4t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-hw86r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hw86r webserver-deployment-69b7448995- deployment-4169  96d0795c-662b-4eb4-8d86-260257df5fd8 23461 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a8fca6400b2bc4088e32b6b54bce099c4c254cc7b564ae37f79d7765d6bb102e cni.projectcalico.org/podIP:10.244.5.98/32 cni.projectcalico.org/podIPs:10.244.5.98/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21a57 0xc002d21a58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9l67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9l67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-k4jfp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-k4jfp webserver-deployment-69b7448995- deployment-4169  ffbac7f0-5a45-44b7-aaab-304dd69de2b7 23445 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d8ab9b720f4db06b6729ddb2eeb227e814735c5edb07f0a5473d01224a81ceed cni.projectcalico.org/podIP:10.244.5.97/32 cni.projectcalico.org/podIPs:10.244.5.97/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21c77 0xc002d21c78}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zclxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zclxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-ksxk5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ksxk5 webserver-deployment-69b7448995- deployment-4169  25c447cf-70d3-458f-8c64-d9e6e4dafb1d 23464 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7a547da31fb3d166c9318fd3a433c24bd2eec423fc06e1234bcffca25199d806 cni.projectcalico.org/podIP:10.244.4.139/32 cni.projectcalico.org/podIPs:10.244.4.139/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc002d21e97 0xc002d21e98}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fdp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fdp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.848: INFO: Pod "webserver-deployment-69b7448995-mgwt9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mgwt9 webserver-deployment-69b7448995- deployment-4169  1523b238-c8b0-4785-b5e3-e2d9f1ccb97d 23495 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a542e1dc7429cced7450406cf07db9f63074c242a0ce2b6cf535ab8531b7c8a9 cni.projectcalico.org/podIP:10.244.4.140/32 cni.projectcalico.org/podIPs:10.244.4.140/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc00401a0b7 0xc00401a0b8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85s9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85s9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-69b7448995-ng4cf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-ng4cf webserver-deployment-69b7448995- deployment-4169  796d0f0c-038e-46ae-847d-47ed995e54aa 23372 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 c995fe75-b970-44b4-b366-ac132264728a 0xc00401a2b7 0xc00401a2b8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c995fe75-b970-44b4-b366-ac132264728a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bf52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bf52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-845c8977d9-5k2cv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5k2cv webserver-deployment-845c8977d9- deployment-4169  6efede53-4baa-41f1-8c13-64826c48ff7f 23208 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6705c49f5cf1a404353a34e34f4ebd4106c8ab5cc1c268b7656eb6470e95016b cni.projectcalico.org/podIP:10.244.5.92/32 cni.projectcalico.org/podIPs:10.244.5.92/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401a440 0xc00401a441}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkqjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkqjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.92,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7a4cce708d9c0765b7116bb2d21497f3f05cf391042790b72e2fad06885f6bfe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-845c8977d9-78b6g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-78b6g webserver-deployment-845c8977d9- deployment-4169  8a3177d5-d257-4180-b84f-eaf2da0cce69 23437 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:823405ab467047a66d1e15190ef098eb7cab9cf86aae5b0105f7517c85f02821 cni.projectcalico.org/podIP:10.244.3.190/32 cni.projectcalico.org/podIPs:10.244.3.190/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401a660 0xc00401a661}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ks9p6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ks9p6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.849: INFO: Pod "webserver-deployment-845c8977d9-8hdll" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8hdll webserver-deployment-845c8977d9- deployment-4169  6ab47af7-9ea8-4e34-aba0-11150e99277c 23450 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:da4cd1fea3dfb2890c1197c9ff21faef04e5edce6b57e8d1b022526fe1d81f27 cni.projectcalico.org/podIP:10.244.4.136/32 cni.projectcalico.org/podIPs:10.244.4.136/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401a867 0xc00401a868}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l9tg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l9tg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-9ftgg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9ftgg webserver-deployment-845c8977d9- deployment-4169  d3f3690d-d315-4785-9077-afd8c5fe0701 23210 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ba2686f4509ede9526a61c001d8301c7686f127419951c19c18c1b89f548a835 cni.projectcalico.org/podIP:10.244.5.93/32 cni.projectcalico.org/podIPs:10.244.5.93/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401aa87 0xc00401aa88}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slz7w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slz7w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.93,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6c2e20f3c5d7ab3a9da0b4acc4f0118435649f3928a26a8ee89c86141003e99a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-f5wzq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f5wzq webserver-deployment-845c8977d9- deployment-4169  55673b65-fb4b-4c52-a302-60021990ae72 23471 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:374e4d41e9a2e30fa609ad098814601469d3d2d11db4c2f58ba53e9d09eb4b94 cni.projectcalico.org/podIP:10.244.3.192/32 cni.projectcalico.org/podIPs:10.244.3.192/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401aca0 0xc00401aca1}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfhvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfhvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-fldpt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fldpt webserver-deployment-845c8977d9- deployment-4169  bb4f7bcb-986d-498c-b1b5-d963c0788758 23204 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d69cb803b4b2f3a0b252e505cb7b76b7a3e05971e1b21d2765c54b205bb753c0 cni.projectcalico.org/podIP:10.244.5.91/32 cni.projectcalico.org/podIPs:10.244.5.91/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401ae40 0xc00401ae41}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tgvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tgvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.91,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f414e0c35f44b71cdea1edf61239a370cccfb15586f2718a3916e471092a51d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.850: INFO: Pod "webserver-deployment-845c8977d9-fnvpw" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fnvpw webserver-deployment-845c8977d9- deployment-4169  b425285e-da78-4a27-95c7-ff4ba04fba33 23217 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:99ea4b6ce87ad972e4113429f69c055873c3192361aac3d460e8848452a2ff2c cni.projectcalico.org/podIP:10.244.3.183/32 cni.projectcalico.org/podIPs:10.244.3.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b060 0xc00401b061}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54l9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54l9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.183,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://69b2923fa282f8fd071f8a53ce803f6e112c40dba0b0fd6c7578602db379ad27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-frrsz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-frrsz webserver-deployment-845c8977d9- deployment-4169  4a92b45e-fc2e-471f-9bdd-024687cb65ce 23460 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4c8e0de1f8f6f769a0c9da6b611c306117a92957d1a8a9346440093e7b9be3ae cni.projectcalico.org/podIP:10.244.4.137/32 cni.projectcalico.org/podIPs:10.244.4.137/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b287 0xc00401b288}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hswlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hswlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-hp8kr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hp8kr webserver-deployment-845c8977d9- deployment-4169  cb28fb92-c358-4afc-b4bf-f90b3f9e0c2d 23478 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:457c0dd5328a8a37e8669846271b2764edabb1fc8bdc5675bf28e7bc51cb8c80 cni.projectcalico.org/podIP:10.244.5.99/32 cni.projectcalico.org/podIPs:10.244.5.99/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b497 0xc00401b498}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5x7mt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5x7mt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-jnpf6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jnpf6 webserver-deployment-845c8977d9- deployment-4169  63e895d1-e4f3-4526-85ca-94e0e2e93d2f 23496 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:777c55af85aaac7b9c9218a8f0f1943ae4ce460508ece005bff4cbc0fc9020d2 cni.projectcalico.org/podIP:10.244.3.194/32 cni.projectcalico.org/podIPs:10.244.3.194/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b697 0xc00401b698}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndkbn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndkbn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.851: INFO: Pod "webserver-deployment-845c8977d9-nzrr7" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nzrr7 webserver-deployment-845c8977d9- deployment-4169  bad265c7-ca2b-49de-bb68-f938897072ba 23196 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:afa6063df2b4b282ea4da765e3ab8745e844a2fe7a5d941e791b2aff614054dd cni.projectcalico.org/podIP:10.244.3.184/32 cni.projectcalico.org/podIPs:10.244.3.184/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401b830 0xc00401b831}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mv52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mv52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.184,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e66f824dc2bbfc3c7d8c71544dd0bcaf08a2225e08f11d98fbe0f91814c43b2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-r7l9q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r7l9q webserver-deployment-845c8977d9- deployment-4169  0fa260cc-1725-4b02-9102-533203a3434b 23476 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:22c268ec79e9e3ba8d420e098c58e9977fb4a19822d2339491eaf448a5d91ad1 cni.projectcalico.org/podIP:10.244.5.100/32 cni.projectcalico.org/podIPs:10.244.5.100/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401ba57 0xc00401ba58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nznhv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nznhv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-s74sg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-s74sg webserver-deployment-845c8977d9- deployment-4169  d6d7528f-2220-43eb-bf09-52b600c64934 23477 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:8fb69a99dbded3ecbc4d8c7361374744a742c8d4f194b9a7c6ae4d4d0b468203 cni.projectcalico.org/podIP:10.244.4.141/32 cni.projectcalico.org/podIPs:10.244.4.141/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401bc57 0xc00401bc58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p76q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p76q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 11:32:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-szlqg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-szlqg webserver-deployment-845c8977d9- deployment-4169  2f34d173-f7de-4f75-b24a-a88b727943a1 23223 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:68a37830ca54cfee4ece5efd107fdbebee28e9c43ef5514ce681393e7644bd9d cni.projectcalico.org/podIP:10.244.4.132/32 cni.projectcalico.org/podIPs:10.244.4.132/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc00401be57 0xc00401be58}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slkzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slkzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.132,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8b1eead2f114b6ffb0dddcac3b3a127ff108e5788480d0ee378af2db580d6c7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.852: INFO: Pod "webserver-deployment-845c8977d9-tq2jv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-tq2jv webserver-deployment-845c8977d9- deployment-4169  b2fbd866-ad48-47d1-b235-3de4d76534de 23470 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cb719aa6c4169bde17ccc76c52e321bd067b826bccfe8a4c0ff114ec29a4d4bb cni.projectcalico.org/podIP:10.244.3.191/32 cni.projectcalico.org/podIPs:10.244.3.191/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c097 0xc004b8c098}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btjbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btjbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-vqc7v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vqc7v webserver-deployment-845c8977d9- deployment-4169  0bb58b36-df44-40a9-8d06-391dfcc7d225 23435 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:a4f021fd347d573464406ae055dc55f1633169774b7337acfb50c153698ead86 cni.projectcalico.org/podIP:10.244.3.189/32 cni.projectcalico.org/podIPs:10.244.3.189/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c297 0xc004b8c298}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8z4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8z4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-wcdk9" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wcdk9 webserver-deployment-845c8977d9- deployment-4169  22541a0e-e40a-4596-ba4b-5040ab956c95 23173 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:67a9334ffda7b318d730f45701b7a29f064e868da15b1664bb4e78cc6633f49e cni.projectcalico.org/podIP:10.244.4.131/32 cni.projectcalico.org/podIPs:10.244.4.131/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c497 0xc004b8c498}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-786wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-786wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.131,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1831e793cafe92b2701baa53547f99253e9dc7a821d8c4dba6b5a11287f384d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-wd7nd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wd7nd webserver-deployment-845c8977d9- deployment-4169  d2fb5e52-f34b-4b54-9247-b343dfaefc8d 23220 0 2023-02-24 11:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5530714c8b549f7ca6f600eabe8809b1211b63e765a944f150e1416a3f661201 cni.projectcalico.org/podIP:10.244.4.133/32 cni.projectcalico.org/podIPs:10.244.4.133/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c6b7 0xc004b8c6b8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcb8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcb8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.133,StartTime:2023-02-24 11:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:32:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cfe02cfcd593e0805155b1246b54ee8710e71db2adcf76fd113c9004b786472f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.853: INFO: Pod "webserver-deployment-845c8977d9-z7z29" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-z7z29 webserver-deployment-845c8977d9- deployment-4169  44e290e2-75f4-4684-bef2-e8f76764f208 23484 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f9f4b3c0f23f05b1c33b134af65037ef408dbc4f6515486573a5a596f8072dd3 cni.projectcalico.org/podIP:10.244.4.142/32 cni.projectcalico.org/podIPs:10.244.4.142/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8c8d7 0xc004b8c8d8}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77282,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77282,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 11:32:31.854: INFO: Pod "webserver-deployment-845c8977d9-zjjdj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zjjdj webserver-deployment-845c8977d9- deployment-4169  e211d647-aef7-4d9f-b3cc-81df8d18ba18 23424 0 2023-02-24 11:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c690613ab9aae2154798d647694eceb7a7515e34df540800e2734f2dc97101ca cni.projectcalico.org/podIP:10.244.5.95/32 cni.projectcalico.org/podIPs:10.244.5.95/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 548c7312-9887-4699-92d1-53e29e9e904f 0xc004b8ca70 0xc004b8ca71}] [] [{kube-controller-manager Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548c7312-9887-4699-92d1-53e29e9e904f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 11:32:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-02-24 11:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wklsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wklsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:,StartTime:2023-02-24 11:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 11:32:31.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4169" for this suite. 02/24/23 11:32:31.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:31.885
Feb 24 11:32:31.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:32:31.886
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:31.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:31.913
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-714dc1d4-9b37-43dc-b507-a6990ac46523 02/24/23 11:32:31.922
STEP: Creating the pod 02/24/23 11:32:31.928
Feb 24 11:32:31.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546" in namespace "configmap-1150" to be "running and ready"
Feb 24 11:32:31.946: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546": Phase="Pending", Reason="", readiness=false. Elapsed: 8.603146ms
Feb 24 11:32:31.946: INFO: The phase of Pod pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:32:33.952: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014224294s
Feb 24 11:32:33.952: INFO: The phase of Pod pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:32:35.954: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546": Phase="Running", Reason="", readiness=true. Elapsed: 4.016544495s
Feb 24 11:32:35.954: INFO: The phase of Pod pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546 is Running (Ready = true)
Feb 24 11:32:35.954: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-714dc1d4-9b37-43dc-b507-a6990ac46523 02/24/23 11:32:36.068
STEP: waiting to observe update in volume 02/24/23 11:32:36.076
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:32:38.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1150" for this suite. 02/24/23 11:32:38.109
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":153,"skipped":2494,"failed":0}
------------------------------
• [SLOW TEST] [6.233 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:31.885
    Feb 24 11:32:31.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:32:31.886
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:31.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:31.913
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-714dc1d4-9b37-43dc-b507-a6990ac46523 02/24/23 11:32:31.922
    STEP: Creating the pod 02/24/23 11:32:31.928
    Feb 24 11:32:31.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546" in namespace "configmap-1150" to be "running and ready"
    Feb 24 11:32:31.946: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546": Phase="Pending", Reason="", readiness=false. Elapsed: 8.603146ms
    Feb 24 11:32:31.946: INFO: The phase of Pod pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:32:33.952: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014224294s
    Feb 24 11:32:33.952: INFO: The phase of Pod pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:32:35.954: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546": Phase="Running", Reason="", readiness=true. Elapsed: 4.016544495s
    Feb 24 11:32:35.954: INFO: The phase of Pod pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546 is Running (Ready = true)
    Feb 24 11:32:35.954: INFO: Pod "pod-configmaps-68b782a4-ef50-4418-ba21-dcfb5b02c546" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-714dc1d4-9b37-43dc-b507-a6990ac46523 02/24/23 11:32:36.068
    STEP: waiting to observe update in volume 02/24/23 11:32:36.076
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:32:38.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1150" for this suite. 02/24/23 11:32:38.109
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:38.121
Feb 24 11:32:38.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:32:38.137
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:38.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:38.175
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 02/24/23 11:32:38.183
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/24/23 11:32:38.191
STEP: patching the secret 02/24/23 11:32:38.201
STEP: deleting the secret using a LabelSelector 02/24/23 11:32:38.213
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/24/23 11:32:38.281
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:32:38.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1641" for this suite. 02/24/23 11:32:38.312
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":154,"skipped":2496,"failed":0}
------------------------------
• [0.199 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:38.121
    Feb 24 11:32:38.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:32:38.137
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:38.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:38.175
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 02/24/23 11:32:38.183
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/24/23 11:32:38.191
    STEP: patching the secret 02/24/23 11:32:38.201
    STEP: deleting the secret using a LabelSelector 02/24/23 11:32:38.213
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/24/23 11:32:38.281
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:32:38.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1641" for this suite. 02/24/23 11:32:38.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:38.323
Feb 24 11:32:38.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename proxy 02/24/23 11:32:38.324
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:38.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:38.423
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 24 11:32:38.427: INFO: Creating pod...
Feb 24 11:32:38.437: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5665" to be "running"
Feb 24 11:32:38.441: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100187ms
Feb 24 11:32:40.447: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009809447s
Feb 24 11:32:42.447: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.01005224s
Feb 24 11:32:42.447: INFO: Pod "agnhost" satisfied condition "running"
Feb 24 11:32:42.447: INFO: Creating service...
Feb 24 11:32:42.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/DELETE
Feb 24 11:32:42.487: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 11:32:42.488: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/GET
Feb 24 11:32:42.502: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 24 11:32:42.502: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/HEAD
Feb 24 11:32:42.511: INFO: http.Client request:HEAD | StatusCode:200
Feb 24 11:32:42.512: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 24 11:32:42.520: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 11:32:42.521: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/PATCH
Feb 24 11:32:42.534: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 11:32:42.534: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/POST
Feb 24 11:32:42.541: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 11:32:42.544: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/PUT
Feb 24 11:32:42.551: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 24 11:32:42.551: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/DELETE
Feb 24 11:32:42.569: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 24 11:32:42.570: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/GET
Feb 24 11:32:42.580: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 24 11:32:42.581: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/HEAD
Feb 24 11:32:42.595: INFO: http.Client request:HEAD | StatusCode:200
Feb 24 11:32:42.597: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/OPTIONS
Feb 24 11:32:42.620: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 24 11:32:42.620: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/PATCH
Feb 24 11:32:42.630: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 24 11:32:42.632: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/POST
Feb 24 11:32:42.640: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 24 11:32:42.640: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/PUT
Feb 24 11:32:42.649: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 24 11:32:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5665" for this suite. 02/24/23 11:32:42.664
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":155,"skipped":2513,"failed":0}
------------------------------
• [4.353 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:38.323
    Feb 24 11:32:38.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename proxy 02/24/23 11:32:38.324
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:38.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:38.423
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 24 11:32:38.427: INFO: Creating pod...
    Feb 24 11:32:38.437: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5665" to be "running"
    Feb 24 11:32:38.441: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100187ms
    Feb 24 11:32:40.447: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009809447s
    Feb 24 11:32:42.447: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.01005224s
    Feb 24 11:32:42.447: INFO: Pod "agnhost" satisfied condition "running"
    Feb 24 11:32:42.447: INFO: Creating service...
    Feb 24 11:32:42.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/DELETE
    Feb 24 11:32:42.487: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 11:32:42.488: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/GET
    Feb 24 11:32:42.502: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 24 11:32:42.502: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/HEAD
    Feb 24 11:32:42.511: INFO: http.Client request:HEAD | StatusCode:200
    Feb 24 11:32:42.512: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 24 11:32:42.520: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 11:32:42.521: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/PATCH
    Feb 24 11:32:42.534: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 11:32:42.534: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/POST
    Feb 24 11:32:42.541: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 11:32:42.544: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/pods/agnhost/proxy/some/path/with/PUT
    Feb 24 11:32:42.551: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 24 11:32:42.551: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/DELETE
    Feb 24 11:32:42.569: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 24 11:32:42.570: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/GET
    Feb 24 11:32:42.580: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 24 11:32:42.581: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/HEAD
    Feb 24 11:32:42.595: INFO: http.Client request:HEAD | StatusCode:200
    Feb 24 11:32:42.597: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/OPTIONS
    Feb 24 11:32:42.620: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 24 11:32:42.620: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/PATCH
    Feb 24 11:32:42.630: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 24 11:32:42.632: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/POST
    Feb 24 11:32:42.640: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 24 11:32:42.640: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5665/services/test-service/proxy/some/path/with/PUT
    Feb 24 11:32:42.649: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 24 11:32:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5665" for this suite. 02/24/23 11:32:42.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:42.697
Feb 24 11:32:42.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:32:42.7
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:42.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:42.762
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 11:32:42.768
Feb 24 11:32:42.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 24 11:32:42.981: INFO: stderr: ""
Feb 24 11:32:42.981: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/24/23 11:32:42.981
STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 11:32:48.032
Feb 24 11:32:48.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 get pod e2e-test-httpd-pod -o json'
Feb 24 11:32:48.149: INFO: stderr: ""
Feb 24 11:32:48.149: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4146b1bece964198277fec8a5ea3112fec27a7354b3b312c6b704abd861c4e53\",\n            \"cni.projectcalico.org/podIP\": \"10.244.4.143/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.4.143/32\"\n        },\n        \"creationTimestamp\": \"2023-02-24T11:32:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6084\",\n        \"resourceVersion\": \"23892\",\n        \"uid\": \"e4903ec6-9d1b-4733-a92b-107c39febfa5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vdtvh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-215-124.eu-west-3.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vdtvh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ce90a284ca9b763013a386cc9d14047977fa7d658185456cc18f54bf101470ca\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-24T11:32:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.215.124\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.143\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.4.143\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-24T11:32:42Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/24/23 11:32:48.149
Feb 24 11:32:48.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 replace -f -'
Feb 24 11:32:48.894: INFO: stderr: ""
Feb 24 11:32:48.894: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/24/23 11:32:48.894
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Feb 24 11:32:48.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 delete pods e2e-test-httpd-pod'
Feb 24 11:32:51.105: INFO: stderr: ""
Feb 24 11:32:51.105: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:32:51.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6084" for this suite. 02/24/23 11:32:51.112
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":156,"skipped":2576,"failed":0}
------------------------------
• [SLOW TEST] [8.427 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:42.697
    Feb 24 11:32:42.697: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:32:42.7
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:42.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:42.762
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 11:32:42.768
    Feb 24 11:32:42.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 24 11:32:42.981: INFO: stderr: ""
    Feb 24 11:32:42.981: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/24/23 11:32:42.981
    STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 11:32:48.032
    Feb 24 11:32:48.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 get pod e2e-test-httpd-pod -o json'
    Feb 24 11:32:48.149: INFO: stderr: ""
    Feb 24 11:32:48.149: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4146b1bece964198277fec8a5ea3112fec27a7354b3b312c6b704abd861c4e53\",\n            \"cni.projectcalico.org/podIP\": \"10.244.4.143/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.4.143/32\"\n        },\n        \"creationTimestamp\": \"2023-02-24T11:32:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6084\",\n        \"resourceVersion\": \"23892\",\n        \"uid\": \"e4903ec6-9d1b-4733-a92b-107c39febfa5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-vdtvh\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-215-124.eu-west-3.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-vdtvh\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:45Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-24T11:32:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ce90a284ca9b763013a386cc9d14047977fa7d658185456cc18f54bf101470ca\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-24T11:32:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.215.124\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.4.143\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.4.143\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-24T11:32:42Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/24/23 11:32:48.149
    Feb 24 11:32:48.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 replace -f -'
    Feb 24 11:32:48.894: INFO: stderr: ""
    Feb 24 11:32:48.894: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/24/23 11:32:48.894
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Feb 24 11:32:48.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6084 delete pods e2e-test-httpd-pod'
    Feb 24 11:32:51.105: INFO: stderr: ""
    Feb 24 11:32:51.105: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:32:51.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6084" for this suite. 02/24/23 11:32:51.112
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:51.125
Feb 24 11:32:51.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:32:51.127
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:51.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:51.157
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 02/24/23 11:32:51.162
Feb 24 11:32:51.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-8271 create -f -'
Feb 24 11:32:51.626: INFO: stderr: ""
Feb 24 11:32:51.626: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/24/23 11:32:51.626
Feb 24 11:32:52.632: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:32:52.632: INFO: Found 0 / 1
Feb 24 11:32:53.632: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:32:53.632: INFO: Found 1 / 1
Feb 24 11:32:53.632: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/24/23 11:32:53.632
Feb 24 11:32:53.636: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:32:53.636: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 11:32:53.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-8271 patch pod agnhost-primary-xgjq2 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 24 11:32:53.770: INFO: stderr: ""
Feb 24 11:32:53.770: INFO: stdout: "pod/agnhost-primary-xgjq2 patched\n"
STEP: checking annotations 02/24/23 11:32:53.77
Feb 24 11:32:53.775: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 11:32:53.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:32:53.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8271" for this suite. 02/24/23 11:32:53.782
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":157,"skipped":2579,"failed":0}
------------------------------
• [2.666 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:51.125
    Feb 24 11:32:51.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:32:51.127
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:51.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:51.157
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 02/24/23 11:32:51.162
    Feb 24 11:32:51.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-8271 create -f -'
    Feb 24 11:32:51.626: INFO: stderr: ""
    Feb 24 11:32:51.626: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/24/23 11:32:51.626
    Feb 24 11:32:52.632: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:32:52.632: INFO: Found 0 / 1
    Feb 24 11:32:53.632: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:32:53.632: INFO: Found 1 / 1
    Feb 24 11:32:53.632: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/24/23 11:32:53.632
    Feb 24 11:32:53.636: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:32:53.636: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 24 11:32:53.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-8271 patch pod agnhost-primary-xgjq2 -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 24 11:32:53.770: INFO: stderr: ""
    Feb 24 11:32:53.770: INFO: stdout: "pod/agnhost-primary-xgjq2 patched\n"
    STEP: checking annotations 02/24/23 11:32:53.77
    Feb 24 11:32:53.775: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 11:32:53.775: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:32:53.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8271" for this suite. 02/24/23 11:32:53.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:53.792
Feb 24 11:32:53.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replicaset 02/24/23 11:32:53.793
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:53.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:53.824
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/24/23 11:32:53.828
STEP: Verify that the required pods have come up 02/24/23 11:32:53.834
Feb 24 11:32:53.838: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 24 11:32:58.846: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/24/23 11:32:58.846
Feb 24 11:32:58.851: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/24/23 11:32:58.851
STEP: DeleteCollection of the ReplicaSets 02/24/23 11:32:58.856
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/24/23 11:32:58.866
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 24 11:32:58.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-673" for this suite. 02/24/23 11:32:58.878
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":158,"skipped":2586,"failed":0}
------------------------------
• [SLOW TEST] [5.119 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:53.792
    Feb 24 11:32:53.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replicaset 02/24/23 11:32:53.793
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:53.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:53.824
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/24/23 11:32:53.828
    STEP: Verify that the required pods have come up 02/24/23 11:32:53.834
    Feb 24 11:32:53.838: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 24 11:32:58.846: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/24/23 11:32:58.846
    Feb 24 11:32:58.851: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/24/23 11:32:58.851
    STEP: DeleteCollection of the ReplicaSets 02/24/23 11:32:58.856
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/24/23 11:32:58.866
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 24 11:32:58.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-673" for this suite. 02/24/23 11:32:58.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:32:58.92
Feb 24 11:32:58.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:32:58.926
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:58.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:59.011
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-4a510ed5-5901-4b53-aadb-4f5abfa5e500 02/24/23 11:32:59.031
STEP: Creating configMap with name cm-test-opt-upd-ef113d5e-2033-4839-a201-5eee247bebf5 02/24/23 11:32:59.042
STEP: Creating the pod 02/24/23 11:32:59.056
Feb 24 11:32:59.097: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1" in namespace "configmap-4865" to be "running and ready"
Feb 24 11:32:59.122: INFO: Pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 25.129539ms
Feb 24 11:32:59.122: INFO: The phase of Pod pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:33:01.128: INFO: Pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.03057146s
Feb 24 11:33:01.128: INFO: The phase of Pod pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1 is Running (Ready = true)
Feb 24 11:33:01.128: INFO: Pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-4a510ed5-5901-4b53-aadb-4f5abfa5e500 02/24/23 11:33:01.16
STEP: Updating configmap cm-test-opt-upd-ef113d5e-2033-4839-a201-5eee247bebf5 02/24/23 11:33:01.17
STEP: Creating configMap with name cm-test-opt-create-f2200ccb-aa9c-40ea-a604-cba925736564 02/24/23 11:33:01.176
STEP: waiting to observe update in volume 02/24/23 11:33:01.181
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:33:05.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4865" for this suite. 02/24/23 11:33:05.244
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":159,"skipped":2591,"failed":0}
------------------------------
• [SLOW TEST] [6.338 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:32:58.92
    Feb 24 11:32:58.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:32:58.926
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:32:58.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:32:59.011
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-4a510ed5-5901-4b53-aadb-4f5abfa5e500 02/24/23 11:32:59.031
    STEP: Creating configMap with name cm-test-opt-upd-ef113d5e-2033-4839-a201-5eee247bebf5 02/24/23 11:32:59.042
    STEP: Creating the pod 02/24/23 11:32:59.056
    Feb 24 11:32:59.097: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1" in namespace "configmap-4865" to be "running and ready"
    Feb 24 11:32:59.122: INFO: Pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1": Phase="Pending", Reason="", readiness=false. Elapsed: 25.129539ms
    Feb 24 11:32:59.122: INFO: The phase of Pod pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:33:01.128: INFO: Pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.03057146s
    Feb 24 11:33:01.128: INFO: The phase of Pod pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1 is Running (Ready = true)
    Feb 24 11:33:01.128: INFO: Pod "pod-configmaps-dfb74dc5-eaa6-493e-9a51-980c8bc2a9e1" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-4a510ed5-5901-4b53-aadb-4f5abfa5e500 02/24/23 11:33:01.16
    STEP: Updating configmap cm-test-opt-upd-ef113d5e-2033-4839-a201-5eee247bebf5 02/24/23 11:33:01.17
    STEP: Creating configMap with name cm-test-opt-create-f2200ccb-aa9c-40ea-a604-cba925736564 02/24/23 11:33:01.176
    STEP: waiting to observe update in volume 02/24/23 11:33:01.181
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:33:05.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4865" for this suite. 02/24/23 11:33:05.244
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:05.259
Feb 24 11:33:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 11:33:05.26
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:05.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:05.309
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/24/23 11:33:05.324
STEP: waiting for Deployment to be created 02/24/23 11:33:05.333
STEP: waiting for all Replicas to be Ready 02/24/23 11:33:05.336
Feb 24 11:33:05.339: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.339: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.367: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.367: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.421: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.421: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.495: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:05.495: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 24 11:33:06.476: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 24 11:33:06.477: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 24 11:33:06.771: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/24/23 11:33:06.771
W0224 11:33:06.784949      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 24 11:33:06.787: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/24/23 11:33:06.787
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.880: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.881: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.944: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.944: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:06.987: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:06.987: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:07.009: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:07.009: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:08.523: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:08.523: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:08.631: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
STEP: listing Deployments 02/24/23 11:33:08.631
Feb 24 11:33:08.637: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/24/23 11:33:08.637
Feb 24 11:33:08.651: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/24/23 11:33:08.651
Feb 24 11:33:08.659: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:08.704: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:08.776: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:08.908: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:09.019: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:09.837: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:10.528: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:10.586: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:10.607: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 24 11:33:11.803: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/24/23 11:33:11.925
STEP: fetching the DeploymentStatus 02/24/23 11:33:12.005
Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:12.028: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
Feb 24 11:33:12.029: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:12.032: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3
Feb 24 11:33:12.036: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:12.036: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
Feb 24 11:33:12.036: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3
STEP: deleting the Deployment 02/24/23 11:33:12.036
Feb 24 11:33:12.060: INFO: observed event type MODIFIED
Feb 24 11:33:12.061: INFO: observed event type MODIFIED
Feb 24 11:33:12.061: INFO: observed event type MODIFIED
Feb 24 11:33:12.061: INFO: observed event type MODIFIED
Feb 24 11:33:12.061: INFO: observed event type MODIFIED
Feb 24 11:33:12.061: INFO: observed event type MODIFIED
Feb 24 11:33:12.062: INFO: observed event type MODIFIED
Feb 24 11:33:12.062: INFO: observed event type MODIFIED
Feb 24 11:33:12.062: INFO: observed event type MODIFIED
Feb 24 11:33:12.062: INFO: observed event type MODIFIED
Feb 24 11:33:12.062: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:33:12.068: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 24 11:33:12.073: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2472  5c83693f-99ba-4b92-8fa0-0b918a3bf4d1 24319 2 2023-02-24 11:33:08 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e917af42-cce7-4681-8f0b-bb921b0ce8c6 0xc002292927 0xc002292928}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e917af42-cce7-4681-8f0b-bb921b0ce8c6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:33:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022929b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 24 11:33:12.079: INFO: pod: "test-deployment-7c7d8d58c8-75sjk":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-75sjk test-deployment-7c7d8d58c8- deployment-2472  f38aa7fa-4f46-4c4a-81e7-bdc87e1a2a28 24318 0 2023-02-24 11:33:10 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:cd87314389bd7d714638e0b2e4975eef6f267301754605ef1a54ab348e34f36c cni.projectcalico.org/podIP:10.244.5.105/32 cni.projectcalico.org/podIPs:10.244.5.105/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5c83693f-99ba-4b92-8fa0-0b918a3bf4d1 0xc001571da7 0xc001571da8}] [] [{kube-controller-manager Update v1 2023-02-24 11:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c83693f-99ba-4b92-8fa0-0b918a3bf4d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:33:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:33:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mt4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mt4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.105,StartTime:2023-02-24 11:33:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:33:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fcb07e33bbc6cfb34e5403cf8b9d13d77fac30540065275f479948573d30054d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 24 11:33:12.079: INFO: pod: "test-deployment-7c7d8d58c8-kghsh":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-kghsh test-deployment-7c7d8d58c8- deployment-2472  d4185aad-6c08-4947-957b-7851f5fe6ac4 24277 0 2023-02-24 11:33:08 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:1384fecca333e972c25671322176ffe6480acdca6299b797ee5942c49698a3be cni.projectcalico.org/podIP:10.244.3.201/32 cni.projectcalico.org/podIPs:10.244.3.201/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5c83693f-99ba-4b92-8fa0-0b918a3bf4d1 0xc003db6137 0xc003db6138}] [] [{kube-controller-manager Update v1 2023-02-24 11:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c83693f-99ba-4b92-8fa0-0b918a3bf4d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:33:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:33:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zh5p4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zh5p4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.201,StartTime:2023-02-24 11:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:33:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d6d9d2665b02f0ff7fdf572d1bf066fbc112b94604af3a53a2f9359d246e2994,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 11:33:12.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2472" for this suite. 02/24/23 11:33:12.086
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":160,"skipped":2594,"failed":0}
------------------------------
• [SLOW TEST] [6.843 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:05.259
    Feb 24 11:33:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 11:33:05.26
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:05.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:05.309
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/24/23 11:33:05.324
    STEP: waiting for Deployment to be created 02/24/23 11:33:05.333
    STEP: waiting for all Replicas to be Ready 02/24/23 11:33:05.336
    Feb 24 11:33:05.339: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.339: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.367: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.367: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.421: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.421: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.495: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:05.495: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 24 11:33:06.476: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 24 11:33:06.477: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 24 11:33:06.771: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/24/23 11:33:06.771
    W0224 11:33:06.784949      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 24 11:33:06.787: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/24/23 11:33:06.787
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 0
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.790: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.880: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.881: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.944: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.944: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:06.987: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:06.987: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:07.009: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:07.009: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:08.523: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:08.523: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:08.631: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    STEP: listing Deployments 02/24/23 11:33:08.631
    Feb 24 11:33:08.637: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/24/23 11:33:08.637
    Feb 24 11:33:08.651: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/24/23 11:33:08.651
    Feb 24 11:33:08.659: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:08.704: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:08.776: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:08.908: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:09.019: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:09.837: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:10.528: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:10.586: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:10.607: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 24 11:33:11.803: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/24/23 11:33:11.925
    STEP: fetching the DeploymentStatus 02/24/23 11:33:12.005
    Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:12.024: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:12.028: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 1
    Feb 24 11:33:12.029: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:12.032: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3
    Feb 24 11:33:12.036: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:12.036: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 2
    Feb 24 11:33:12.036: INFO: observed Deployment test-deployment in namespace deployment-2472 with ReadyReplicas 3
    STEP: deleting the Deployment 02/24/23 11:33:12.036
    Feb 24 11:33:12.060: INFO: observed event type MODIFIED
    Feb 24 11:33:12.061: INFO: observed event type MODIFIED
    Feb 24 11:33:12.061: INFO: observed event type MODIFIED
    Feb 24 11:33:12.061: INFO: observed event type MODIFIED
    Feb 24 11:33:12.061: INFO: observed event type MODIFIED
    Feb 24 11:33:12.061: INFO: observed event type MODIFIED
    Feb 24 11:33:12.062: INFO: observed event type MODIFIED
    Feb 24 11:33:12.062: INFO: observed event type MODIFIED
    Feb 24 11:33:12.062: INFO: observed event type MODIFIED
    Feb 24 11:33:12.062: INFO: observed event type MODIFIED
    Feb 24 11:33:12.062: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:33:12.068: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb 24 11:33:12.073: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2472  5c83693f-99ba-4b92-8fa0-0b918a3bf4d1 24319 2 2023-02-24 11:33:08 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e917af42-cce7-4681-8f0b-bb921b0ce8c6 0xc002292927 0xc002292928}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e917af42-cce7-4681-8f0b-bb921b0ce8c6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:33:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0022929b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Feb 24 11:33:12.079: INFO: pod: "test-deployment-7c7d8d58c8-75sjk":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-75sjk test-deployment-7c7d8d58c8- deployment-2472  f38aa7fa-4f46-4c4a-81e7-bdc87e1a2a28 24318 0 2023-02-24 11:33:10 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:cd87314389bd7d714638e0b2e4975eef6f267301754605ef1a54ab348e34f36c cni.projectcalico.org/podIP:10.244.5.105/32 cni.projectcalico.org/podIPs:10.244.5.105/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5c83693f-99ba-4b92-8fa0-0b918a3bf4d1 0xc001571da7 0xc001571da8}] [] [{kube-controller-manager Update v1 2023-02-24 11:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c83693f-99ba-4b92-8fa0-0b918a3bf4d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:33:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:33:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mt4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mt4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-217-191.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.217.191,PodIP:10.244.5.105,StartTime:2023-02-24 11:33:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:33:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://fcb07e33bbc6cfb34e5403cf8b9d13d77fac30540065275f479948573d30054d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 24 11:33:12.079: INFO: pod: "test-deployment-7c7d8d58c8-kghsh":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-kghsh test-deployment-7c7d8d58c8- deployment-2472  d4185aad-6c08-4947-957b-7851f5fe6ac4 24277 0 2023-02-24 11:33:08 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:1384fecca333e972c25671322176ffe6480acdca6299b797ee5942c49698a3be cni.projectcalico.org/podIP:10.244.3.201/32 cni.projectcalico.org/podIPs:10.244.3.201/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 5c83693f-99ba-4b92-8fa0-0b918a3bf4d1 0xc003db6137 0xc003db6138}] [] [{kube-controller-manager Update v1 2023-02-24 11:33:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c83693f-99ba-4b92-8fa0-0b918a3bf4d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:33:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:33:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zh5p4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zh5p4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:33:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.201,StartTime:2023-02-24 11:33:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:33:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d6d9d2665b02f0ff7fdf572d1bf066fbc112b94604af3a53a2f9359d246e2994,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 11:33:12.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2472" for this suite. 02/24/23 11:33:12.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:12.12
Feb 24 11:33:12.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:33:12.121
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:12.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:12.178
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Feb 24 11:33:12.227: INFO: Create a RollingUpdate DaemonSet
Feb 24 11:33:12.244: INFO: Check that daemon pods launch on every node of the cluster
Feb 24 11:33:12.252: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:12.252: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:12.252: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:12.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:12.260: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:33:13.270: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:13.270: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:13.270: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:13.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:33:13.275: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:33:14.269: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:14.269: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:14.269: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:14.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:33:14.274: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb 24 11:33:14.274: INFO: Update the DaemonSet to trigger a rollout
Feb 24 11:33:14.285: INFO: Updating DaemonSet daemon-set
Feb 24 11:33:17.312: INFO: Roll back the DaemonSet before rollout is complete
Feb 24 11:33:17.330: INFO: Updating DaemonSet daemon-set
Feb 24 11:33:17.330: INFO: Make sure DaemonSet rollback is complete
Feb 24 11:33:17.335: INFO: Wrong image for pod: daemon-set-tw65m. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Feb 24 11:33:17.335: INFO: Pod daemon-set-tw65m is not available
Feb 24 11:33:17.342: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:17.342: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:17.342: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:18.355: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:18.355: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:18.356: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:19.357: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:19.359: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:19.359: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:20.351: INFO: Pod daemon-set-84qrx is not available
Feb 24 11:33:20.359: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:20.359: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:33:20.360: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:33:20.394
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9655, will wait for the garbage collector to delete the pods 02/24/23 11:33:20.394
Feb 24 11:33:20.458: INFO: Deleting DaemonSet.extensions daemon-set took: 8.28487ms
Feb 24 11:33:20.559: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.818066ms
Feb 24 11:33:22.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:33:22.565: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:33:22.569: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24551"},"items":null}

Feb 24 11:33:22.573: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24551"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:33:22.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9655" for this suite. 02/24/23 11:33:22.602
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":161,"skipped":2609,"failed":0}
------------------------------
• [SLOW TEST] [10.492 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:12.12
    Feb 24 11:33:12.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:33:12.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:12.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:12.178
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Feb 24 11:33:12.227: INFO: Create a RollingUpdate DaemonSet
    Feb 24 11:33:12.244: INFO: Check that daemon pods launch on every node of the cluster
    Feb 24 11:33:12.252: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:12.252: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:12.252: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:12.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:12.260: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:33:13.270: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:13.270: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:13.270: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:13.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:33:13.275: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:33:14.269: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:14.269: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:14.269: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:14.274: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:33:14.274: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb 24 11:33:14.274: INFO: Update the DaemonSet to trigger a rollout
    Feb 24 11:33:14.285: INFO: Updating DaemonSet daemon-set
    Feb 24 11:33:17.312: INFO: Roll back the DaemonSet before rollout is complete
    Feb 24 11:33:17.330: INFO: Updating DaemonSet daemon-set
    Feb 24 11:33:17.330: INFO: Make sure DaemonSet rollback is complete
    Feb 24 11:33:17.335: INFO: Wrong image for pod: daemon-set-tw65m. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Feb 24 11:33:17.335: INFO: Pod daemon-set-tw65m is not available
    Feb 24 11:33:17.342: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:17.342: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:17.342: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:18.355: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:18.355: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:18.356: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:19.357: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:19.359: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:19.359: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:20.351: INFO: Pod daemon-set-84qrx is not available
    Feb 24 11:33:20.359: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:20.359: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:33:20.360: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:33:20.394
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9655, will wait for the garbage collector to delete the pods 02/24/23 11:33:20.394
    Feb 24 11:33:20.458: INFO: Deleting DaemonSet.extensions daemon-set took: 8.28487ms
    Feb 24 11:33:20.559: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.818066ms
    Feb 24 11:33:22.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:33:22.565: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:33:22.569: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24551"},"items":null}

    Feb 24 11:33:22.573: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24551"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:33:22.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9655" for this suite. 02/24/23 11:33:22.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:22.612
Feb 24 11:33:22.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:33:22.613
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:22.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:22.649
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 02/24/23 11:33:22.654
Feb 24 11:33:22.664: INFO: Waiting up to 5m0s for pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0" in namespace "emptydir-5579" to be "Succeeded or Failed"
Feb 24 11:33:22.671: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.69138ms
Feb 24 11:33:24.677: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012538817s
Feb 24 11:33:26.676: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011550825s
STEP: Saw pod success 02/24/23 11:33:26.676
Feb 24 11:33:26.676: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0" satisfied condition "Succeeded or Failed"
Feb 24 11:33:26.680: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-5731df1a-7e66-44cb-a50a-a278dda862d0 container test-container: <nil>
STEP: delete the pod 02/24/23 11:33:26.689
Feb 24 11:33:26.708: INFO: Waiting for pod pod-5731df1a-7e66-44cb-a50a-a278dda862d0 to disappear
Feb 24 11:33:26.712: INFO: Pod pod-5731df1a-7e66-44cb-a50a-a278dda862d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:33:26.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5579" for this suite. 02/24/23 11:33:26.719
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":162,"skipped":2615,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:22.612
    Feb 24 11:33:22.612: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:33:22.613
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:22.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:22.649
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 02/24/23 11:33:22.654
    Feb 24 11:33:22.664: INFO: Waiting up to 5m0s for pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0" in namespace "emptydir-5579" to be "Succeeded or Failed"
    Feb 24 11:33:22.671: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.69138ms
    Feb 24 11:33:24.677: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012538817s
    Feb 24 11:33:26.676: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011550825s
    STEP: Saw pod success 02/24/23 11:33:26.676
    Feb 24 11:33:26.676: INFO: Pod "pod-5731df1a-7e66-44cb-a50a-a278dda862d0" satisfied condition "Succeeded or Failed"
    Feb 24 11:33:26.680: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-5731df1a-7e66-44cb-a50a-a278dda862d0 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:33:26.689
    Feb 24 11:33:26.708: INFO: Waiting for pod pod-5731df1a-7e66-44cb-a50a-a278dda862d0 to disappear
    Feb 24 11:33:26.712: INFO: Pod pod-5731df1a-7e66-44cb-a50a-a278dda862d0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:33:26.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5579" for this suite. 02/24/23 11:33:26.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:26.739
Feb 24 11:33:26.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename podtemplate 02/24/23 11:33:26.74
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:26.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:26.768
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/24/23 11:33:26.772
Feb 24 11:33:26.779: INFO: created test-podtemplate-1
Feb 24 11:33:26.785: INFO: created test-podtemplate-2
Feb 24 11:33:26.791: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/24/23 11:33:26.792
STEP: delete collection of pod templates 02/24/23 11:33:26.796
Feb 24 11:33:26.796: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/24/23 11:33:26.817
Feb 24 11:33:26.817: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 24 11:33:26.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7678" for this suite. 02/24/23 11:33:26.827
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":163,"skipped":2661,"failed":0}
------------------------------
• [0.096 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:26.739
    Feb 24 11:33:26.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename podtemplate 02/24/23 11:33:26.74
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:26.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:26.768
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/24/23 11:33:26.772
    Feb 24 11:33:26.779: INFO: created test-podtemplate-1
    Feb 24 11:33:26.785: INFO: created test-podtemplate-2
    Feb 24 11:33:26.791: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/24/23 11:33:26.792
    STEP: delete collection of pod templates 02/24/23 11:33:26.796
    Feb 24 11:33:26.796: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/24/23 11:33:26.817
    Feb 24 11:33:26.817: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 24 11:33:26.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7678" for this suite. 02/24/23 11:33:26.827
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:26.845
Feb 24 11:33:26.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:33:26.847
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:26.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:26.876
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Feb 24 11:33:26.892: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce" in namespace "security-context-test-3216" to be "Succeeded or Failed"
Feb 24 11:33:26.896: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660489ms
Feb 24 11:33:28.903: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011292158s
Feb 24 11:33:30.902: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937651s
Feb 24 11:33:30.902: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce" satisfied condition "Succeeded or Failed"
Feb 24 11:33:30.911: INFO: Got logs for pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 24 11:33:30.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3216" for this suite. 02/24/23 11:33:30.919
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":164,"skipped":2664,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:26.845
    Feb 24 11:33:26.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:33:26.847
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:26.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:26.876
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Feb 24 11:33:26.892: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce" in namespace "security-context-test-3216" to be "Succeeded or Failed"
    Feb 24 11:33:26.896: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660489ms
    Feb 24 11:33:28.903: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011292158s
    Feb 24 11:33:30.902: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937651s
    Feb 24 11:33:30.902: INFO: Pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce" satisfied condition "Succeeded or Failed"
    Feb 24 11:33:30.911: INFO: Got logs for pod "busybox-privileged-false-3291a737-b8fd-4a2b-a246-e6945584abce": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 24 11:33:30.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3216" for this suite. 02/24/23 11:33:30.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:30.941
Feb 24 11:33:30.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:33:30.944
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:30.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:30.973
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 02/24/23 11:33:30.977
Feb 24 11:33:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-7879 cluster-info'
Feb 24 11:33:31.077: INFO: stderr: ""
Feb 24 11:33:31.077: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:33:31.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7879" for this suite. 02/24/23 11:33:31.087
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":165,"skipped":2685,"failed":0}
------------------------------
• [0.155 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:30.941
    Feb 24 11:33:30.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:33:30.944
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:30.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:30.973
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 02/24/23 11:33:30.977
    Feb 24 11:33:30.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-7879 cluster-info'
    Feb 24 11:33:31.077: INFO: stderr: ""
    Feb 24 11:33:31.077: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:33:31.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7879" for this suite. 02/24/23 11:33:31.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:31.096
Feb 24 11:33:31.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:33:31.098
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:31.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:31.133
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 02/24/23 11:33:31.136
STEP: Creating a ResourceQuota 02/24/23 11:33:36.142
STEP: Ensuring resource quota status is calculated 02/24/23 11:33:36.149
STEP: Creating a Pod that fits quota 02/24/23 11:33:38.155
STEP: Ensuring ResourceQuota status captures the pod usage 02/24/23 11:33:38.172
STEP: Not allowing a pod to be created that exceeds remaining quota 02/24/23 11:33:40.178
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/24/23 11:33:40.18
STEP: Ensuring a pod cannot update its resource requirements 02/24/23 11:33:40.183
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/24/23 11:33:40.188
STEP: Deleting the pod 02/24/23 11:33:42.194
STEP: Ensuring resource quota status released the pod usage 02/24/23 11:33:42.21
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 11:33:44.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6172" for this suite. 02/24/23 11:33:44.224
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":166,"skipped":2690,"failed":0}
------------------------------
• [SLOW TEST] [13.134 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:31.096
    Feb 24 11:33:31.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:33:31.098
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:31.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:31.133
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 02/24/23 11:33:31.136
    STEP: Creating a ResourceQuota 02/24/23 11:33:36.142
    STEP: Ensuring resource quota status is calculated 02/24/23 11:33:36.149
    STEP: Creating a Pod that fits quota 02/24/23 11:33:38.155
    STEP: Ensuring ResourceQuota status captures the pod usage 02/24/23 11:33:38.172
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/24/23 11:33:40.178
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/24/23 11:33:40.18
    STEP: Ensuring a pod cannot update its resource requirements 02/24/23 11:33:40.183
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/24/23 11:33:40.188
    STEP: Deleting the pod 02/24/23 11:33:42.194
    STEP: Ensuring resource quota status released the pod usage 02/24/23 11:33:42.21
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 11:33:44.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6172" for this suite. 02/24/23 11:33:44.224
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:44.231
Feb 24 11:33:44.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:33:44.232
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:44.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:44.265
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:33:44.288
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:33:44.692
STEP: Deploying the webhook pod 02/24/23 11:33:44.701
STEP: Wait for the deployment to be ready 02/24/23 11:33:44.715
Feb 24 11:33:44.724: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:33:46.737
STEP: Verifying the service has paired with the endpoint 02/24/23 11:33:46.821
Feb 24 11:33:47.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 11:33:47.826
STEP: create a pod 02/24/23 11:33:47.849
Feb 24 11:33:47.859: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1291" to be "running"
Feb 24 11:33:47.864: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632685ms
Feb 24 11:33:49.870: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010886157s
Feb 24 11:33:49.870: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/24/23 11:33:49.87
Feb 24 11:33:49.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=webhook-1291 attach --namespace=webhook-1291 to-be-attached-pod -i -c=container1'
Feb 24 11:33:49.985: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:33:50.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1291" for this suite. 02/24/23 11:33:50.055
STEP: Destroying namespace "webhook-1291-markers" for this suite. 02/24/23 11:33:50.07
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":167,"skipped":2691,"failed":0}
------------------------------
• [SLOW TEST] [5.932 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:44.231
    Feb 24 11:33:44.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:33:44.232
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:44.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:44.265
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:33:44.288
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:33:44.692
    STEP: Deploying the webhook pod 02/24/23 11:33:44.701
    STEP: Wait for the deployment to be ready 02/24/23 11:33:44.715
    Feb 24 11:33:44.724: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:33:46.737
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:33:46.821
    Feb 24 11:33:47.821: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 11:33:47.826
    STEP: create a pod 02/24/23 11:33:47.849
    Feb 24 11:33:47.859: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1291" to be "running"
    Feb 24 11:33:47.864: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.632685ms
    Feb 24 11:33:49.870: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010886157s
    Feb 24 11:33:49.870: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/24/23 11:33:49.87
    Feb 24 11:33:49.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=webhook-1291 attach --namespace=webhook-1291 to-be-attached-pod -i -c=container1'
    Feb 24 11:33:49.985: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:33:50.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1291" for this suite. 02/24/23 11:33:50.055
    STEP: Destroying namespace "webhook-1291-markers" for this suite. 02/24/23 11:33:50.07
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:33:50.186
Feb 24 11:33:50.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 11:33:50.208
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:50.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:50.243
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-704 02/24/23 11:33:50.251
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-704 02/24/23 11:33:50.271
Feb 24 11:33:50.283: INFO: Found 0 stateful pods, waiting for 1
Feb 24 11:34:00.295: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/24/23 11:34:00.304
STEP: Getting /status 02/24/23 11:34:00.321
Feb 24 11:34:00.328: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/24/23 11:34:00.328
Feb 24 11:34:00.341: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/24/23 11:34:00.341
Feb 24 11:34:00.344: INFO: Observed &StatefulSet event: ADDED
Feb 24 11:34:00.344: INFO: Found Statefulset ss in namespace statefulset-704 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 11:34:00.344: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/24/23 11:34:00.344
Feb 24 11:34:00.345: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 24 11:34:00.352: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/24/23 11:34:00.353
Feb 24 11:34:00.355: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 11:34:00.356: INFO: Deleting all statefulset in ns statefulset-704
Feb 24 11:34:00.360: INFO: Scaling statefulset ss to 0
Feb 24 11:34:10.384: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 11:34:10.389: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 11:34:10.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-704" for this suite. 02/24/23 11:34:10.43
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":168,"skipped":2720,"failed":0}
------------------------------
• [SLOW TEST] [20.253 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:33:50.186
    Feb 24 11:33:50.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 11:33:50.208
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:33:50.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:33:50.243
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-704 02/24/23 11:33:50.251
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-704 02/24/23 11:33:50.271
    Feb 24 11:33:50.283: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 11:34:00.295: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/24/23 11:34:00.304
    STEP: Getting /status 02/24/23 11:34:00.321
    Feb 24 11:34:00.328: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/24/23 11:34:00.328
    Feb 24 11:34:00.341: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/24/23 11:34:00.341
    Feb 24 11:34:00.344: INFO: Observed &StatefulSet event: ADDED
    Feb 24 11:34:00.344: INFO: Found Statefulset ss in namespace statefulset-704 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 11:34:00.344: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/24/23 11:34:00.344
    Feb 24 11:34:00.345: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 24 11:34:00.352: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/24/23 11:34:00.353
    Feb 24 11:34:00.355: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 11:34:00.356: INFO: Deleting all statefulset in ns statefulset-704
    Feb 24 11:34:00.360: INFO: Scaling statefulset ss to 0
    Feb 24 11:34:10.384: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 11:34:10.389: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 11:34:10.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-704" for this suite. 02/24/23 11:34:10.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:34:10.446
Feb 24 11:34:10.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:34:10.45
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:10.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:10.499
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:34:10.532
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:34:10.538
Feb 24 11:34:10.544: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:10.544: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:10.544: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:10.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:34:10.549: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:34:11.557: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:11.557: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:11.557: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:11.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:34:11.566: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:34:12.557: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:12.558: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:12.558: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:34:12.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:34:12.565: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/24/23 11:34:12.57
STEP: DeleteCollection of the DaemonSets 02/24/23 11:34:12.577
STEP: Verify that ReplicaSets have been deleted 02/24/23 11:34:12.588
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Feb 24 11:34:12.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25037"},"items":null}

Feb 24 11:34:12.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25037"},"items":[{"metadata":{"name":"daemon-set-bgtcd","generateName":"daemon-set-","namespace":"daemonsets-1526","uid":"0b935df3-6e9c-47e0-8d10-523eee49ae26","resourceVersion":"25028","creationTimestamp":"2023-02-24T11:34:10Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"001734556111ff72bf1473fd2e58988a7c6e0dea5ad0536ff8f26f5641bed948","cni.projectcalico.org/podIP":"10.244.3.209/32","cni.projectcalico.org/podIPs":"10.244.3.209/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ca72643-66c5-4c44-9d09-3cc2966d7bb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca72643-66c5-4c44-9d09-3cc2966d7bb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9ddcf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9ddcf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-216-47.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-216-47.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"}],"hostIP":"172.31.216.47","podIP":"10.244.3.209","podIPs":[{"ip":"10.244.3.209"}],"startTime":"2023-02-24T11:34:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:34:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4c277f385f941648ea02ac6397ec3ff742034793953df41a96e45a6280fd8d2d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hxgnj","generateName":"daemon-set-","namespace":"daemonsets-1526","uid":"b9e5d3be-4e74-41e4-a6fa-f68f2839834d","resourceVersion":"25031","creationTimestamp":"2023-02-24T11:34:10Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d088b661e1c01b2847c7072380ca159ad51128e22333fa0f0387774637614118","cni.projectcalico.org/podIP":"10.244.5.107/32","cni.projectcalico.org/podIPs":"10.244.5.107/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ca72643-66c5-4c44-9d09-3cc2966d7bb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca72643-66c5-4c44-9d09-3cc2966d7bb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mxxng","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mxxng","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-217-191.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-217-191.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"}],"hostIP":"172.31.217.191","podIP":"10.244.5.107","podIPs":[{"ip":"10.244.5.107"}],"startTime":"2023-02-24T11:34:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:34:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c0b173500f3b55e0a918e2d05c769890d1e6646cc6bd4fa4e1925a304f61ae59","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w5qrv","generateName":"daemon-set-","namespace":"daemonsets-1526","uid":"cac91456-c045-4cdb-a634-83b59a86ba0d","resourceVersion":"25035","creationTimestamp":"2023-02-24T11:34:10Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1018bccc88255ca9883ec41a4c339b53f4e381967fe7944b141eef4e3554922b","cni.projectcalico.org/podIP":"10.244.4.148/32","cni.projectcalico.org/podIPs":"10.244.4.148/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ca72643-66c5-4c44-9d09-3cc2966d7bb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca72643-66c5-4c44-9d09-3cc2966d7bb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5mnws","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5mnws","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-215-124.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-215-124.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"}],"hostIP":"172.31.215.124","podIP":"10.244.4.148","podIPs":[{"ip":"10.244.4.148"}],"startTime":"2023-02-24T11:34:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:34:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://28f9e30f5bf5c0f1b5be54c265e3aa2f90f6a3b15e2735ff62e5975c2f0b1ca1","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:34:12.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1526" for this suite. 02/24/23 11:34:12.646
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":169,"skipped":2728,"failed":0}
------------------------------
• [2.219 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:34:10.446
    Feb 24 11:34:10.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:34:10.45
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:10.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:10.499
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:34:10.532
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:34:10.538
    Feb 24 11:34:10.544: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:10.544: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:10.544: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:10.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:34:10.549: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:34:11.557: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:11.557: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:11.557: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:11.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:34:11.566: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:34:12.557: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:12.558: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:12.558: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:34:12.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:34:12.565: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/24/23 11:34:12.57
    STEP: DeleteCollection of the DaemonSets 02/24/23 11:34:12.577
    STEP: Verify that ReplicaSets have been deleted 02/24/23 11:34:12.588
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Feb 24 11:34:12.602: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25037"},"items":null}

    Feb 24 11:34:12.612: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25037"},"items":[{"metadata":{"name":"daemon-set-bgtcd","generateName":"daemon-set-","namespace":"daemonsets-1526","uid":"0b935df3-6e9c-47e0-8d10-523eee49ae26","resourceVersion":"25028","creationTimestamp":"2023-02-24T11:34:10Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"001734556111ff72bf1473fd2e58988a7c6e0dea5ad0536ff8f26f5641bed948","cni.projectcalico.org/podIP":"10.244.3.209/32","cni.projectcalico.org/podIPs":"10.244.3.209/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ca72643-66c5-4c44-9d09-3cc2966d7bb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca72643-66c5-4c44-9d09-3cc2966d7bb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9ddcf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9ddcf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-216-47.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-216-47.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"}],"hostIP":"172.31.216.47","podIP":"10.244.3.209","podIPs":[{"ip":"10.244.3.209"}],"startTime":"2023-02-24T11:34:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:34:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://4c277f385f941648ea02ac6397ec3ff742034793953df41a96e45a6280fd8d2d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hxgnj","generateName":"daemon-set-","namespace":"daemonsets-1526","uid":"b9e5d3be-4e74-41e4-a6fa-f68f2839834d","resourceVersion":"25031","creationTimestamp":"2023-02-24T11:34:10Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d088b661e1c01b2847c7072380ca159ad51128e22333fa0f0387774637614118","cni.projectcalico.org/podIP":"10.244.5.107/32","cni.projectcalico.org/podIPs":"10.244.5.107/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ca72643-66c5-4c44-9d09-3cc2966d7bb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca72643-66c5-4c44-9d09-3cc2966d7bb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-mxxng","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-mxxng","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-217-191.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-217-191.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"}],"hostIP":"172.31.217.191","podIP":"10.244.5.107","podIPs":[{"ip":"10.244.5.107"}],"startTime":"2023-02-24T11:34:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:34:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://c0b173500f3b55e0a918e2d05c769890d1e6646cc6bd4fa4e1925a304f61ae59","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w5qrv","generateName":"daemon-set-","namespace":"daemonsets-1526","uid":"cac91456-c045-4cdb-a634-83b59a86ba0d","resourceVersion":"25035","creationTimestamp":"2023-02-24T11:34:10Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1018bccc88255ca9883ec41a4c339b53f4e381967fe7944b141eef4e3554922b","cni.projectcalico.org/podIP":"10.244.4.148/32","cni.projectcalico.org/podIPs":"10.244.4.148/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4ca72643-66c5-4c44-9d09-3cc2966d7bb2","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ca72643-66c5-4c44-9d09-3cc2966d7bb2\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-24T11:34:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5mnws","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5mnws","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-215-124.eu-west-3.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-215-124.eu-west-3.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:12Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:12Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-24T11:34:10Z"}],"hostIP":"172.31.215.124","podIP":"10.244.4.148","podIPs":[{"ip":"10.244.4.148"}],"startTime":"2023-02-24T11:34:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-24T11:34:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://28f9e30f5bf5c0f1b5be54c265e3aa2f90f6a3b15e2735ff62e5975c2f0b1ca1","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:34:12.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1526" for this suite. 02/24/23 11:34:12.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:34:12.667
Feb 24 11:34:12.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:34:12.668
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:12.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:12.696
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 02/24/23 11:34:12.7
STEP: setting up watch 02/24/23 11:34:12.7
STEP: submitting the pod to kubernetes 02/24/23 11:34:12.807
STEP: verifying the pod is in kubernetes 02/24/23 11:34:12.82
STEP: verifying pod creation was observed 02/24/23 11:34:12.825
Feb 24 11:34:12.825: INFO: Waiting up to 5m0s for pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3" in namespace "pods-8958" to be "running"
Feb 24 11:34:12.834: INFO: Pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.092062ms
Feb 24 11:34:14.840: INFO: Pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.01441512s
Feb 24 11:34:14.840: INFO: Pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3" satisfied condition "running"
STEP: deleting the pod gracefully 02/24/23 11:34:14.846
STEP: verifying pod deletion was observed 02/24/23 11:34:14.855
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 11:34:16.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8958" for this suite. 02/24/23 11:34:16.701
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":170,"skipped":2765,"failed":0}
------------------------------
• [4.042 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:34:12.667
    Feb 24 11:34:12.667: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:34:12.668
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:12.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:12.696
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 02/24/23 11:34:12.7
    STEP: setting up watch 02/24/23 11:34:12.7
    STEP: submitting the pod to kubernetes 02/24/23 11:34:12.807
    STEP: verifying the pod is in kubernetes 02/24/23 11:34:12.82
    STEP: verifying pod creation was observed 02/24/23 11:34:12.825
    Feb 24 11:34:12.825: INFO: Waiting up to 5m0s for pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3" in namespace "pods-8958" to be "running"
    Feb 24 11:34:12.834: INFO: Pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.092062ms
    Feb 24 11:34:14.840: INFO: Pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.01441512s
    Feb 24 11:34:14.840: INFO: Pod "pod-submit-remove-cb9b9899-ff29-4d7f-8048-2bd7079116a3" satisfied condition "running"
    STEP: deleting the pod gracefully 02/24/23 11:34:14.846
    STEP: verifying pod deletion was observed 02/24/23 11:34:14.855
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 11:34:16.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8958" for this suite. 02/24/23 11:34:16.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:34:16.718
Feb 24 11:34:16.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename cronjob 02/24/23 11:34:16.722
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:16.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:16.769
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/24/23 11:34:16.773
STEP: Ensuring no jobs are scheduled 02/24/23 11:34:16.786
STEP: Ensuring no job exists by listing jobs explicitly 02/24/23 11:39:16.797
STEP: Removing cronjob 02/24/23 11:39:16.801
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 24 11:39:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6494" for this suite. 02/24/23 11:39:16.817
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":171,"skipped":2784,"failed":0}
------------------------------
• [SLOW TEST] [300.110 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:34:16.718
    Feb 24 11:34:16.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename cronjob 02/24/23 11:34:16.722
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:34:16.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:34:16.769
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/24/23 11:34:16.773
    STEP: Ensuring no jobs are scheduled 02/24/23 11:34:16.786
    STEP: Ensuring no job exists by listing jobs explicitly 02/24/23 11:39:16.797
    STEP: Removing cronjob 02/24/23 11:39:16.801
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 24 11:39:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6494" for this suite. 02/24/23 11:39:16.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:39:16.832
Feb 24 11:39:16.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:39:16.833
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:16.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:16.86
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-973e73f8-27c1-4b73-8611-65aa1baa740f 02/24/23 11:39:16.863
STEP: Creating a pod to test consume configMaps 02/24/23 11:39:16.869
Feb 24 11:39:16.888: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72" in namespace "projected-6738" to be "Succeeded or Failed"
Feb 24 11:39:16.893: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190699ms
Feb 24 11:39:18.899: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011405384s
Feb 24 11:39:20.904: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016273652s
STEP: Saw pod success 02/24/23 11:39:20.904
Feb 24 11:39:20.905: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72" satisfied condition "Succeeded or Failed"
Feb 24 11:39:20.909: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:39:20.925
Feb 24 11:39:20.939: INFO: Waiting for pod pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72 to disappear
Feb 24 11:39:20.943: INFO: Pod pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 11:39:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6738" for this suite. 02/24/23 11:39:20.951
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":172,"skipped":2808,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:39:16.832
    Feb 24 11:39:16.832: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:39:16.833
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:16.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:16.86
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-973e73f8-27c1-4b73-8611-65aa1baa740f 02/24/23 11:39:16.863
    STEP: Creating a pod to test consume configMaps 02/24/23 11:39:16.869
    Feb 24 11:39:16.888: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72" in namespace "projected-6738" to be "Succeeded or Failed"
    Feb 24 11:39:16.893: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.190699ms
    Feb 24 11:39:18.899: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011405384s
    Feb 24 11:39:20.904: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016273652s
    STEP: Saw pod success 02/24/23 11:39:20.904
    Feb 24 11:39:20.905: INFO: Pod "pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72" satisfied condition "Succeeded or Failed"
    Feb 24 11:39:20.909: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:39:20.925
    Feb 24 11:39:20.939: INFO: Waiting for pod pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72 to disappear
    Feb 24 11:39:20.943: INFO: Pod pod-projected-configmaps-7097fe16-3ff7-4d67-816e-13aba76e8d72 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 11:39:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6738" for this suite. 02/24/23 11:39:20.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:39:20.962
Feb 24 11:39:20.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename namespaces 02/24/23 11:39:20.963
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:20.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:20.994
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 02/24/23 11:39:20.997
Feb 24 11:39:21.002: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/24/23 11:39:21.002
Feb 24 11:39:21.010: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/24/23 11:39:21.01
Feb 24 11:39:21.021: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:39:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6840" for this suite. 02/24/23 11:39:21.027
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":173,"skipped":2831,"failed":0}
------------------------------
• [0.075 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:39:20.962
    Feb 24 11:39:20.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename namespaces 02/24/23 11:39:20.963
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:20.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:20.994
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 02/24/23 11:39:20.997
    Feb 24 11:39:21.002: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/24/23 11:39:21.002
    Feb 24 11:39:21.010: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/24/23 11:39:21.01
    Feb 24 11:39:21.021: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:39:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6840" for this suite. 02/24/23 11:39:21.027
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:39:21.042
Feb 24 11:39:21.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:39:21.045
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:21.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:21.075
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:39:21.105
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:39:21.115
Feb 24 11:39:21.122: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:21.122: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:21.122: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:21.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:39:21.191: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:39:22.198: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:22.199: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:22.199: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:22.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:39:22.203: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:39:23.201: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:23.201: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:23.201: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:23.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:39:23.207: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/24/23 11:39:23.211
Feb 24 11:39:23.235: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:23.235: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:23.235: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:23.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:39:23.241: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:39:24.250: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:24.250: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:24.250: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:24.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:39:24.255: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:39:25.250: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:25.250: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:25.250: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:25.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:39:25.256: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:39:26.250: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:26.250: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:26.250: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:26.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 24 11:39:26.255: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:39:27.249: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:27.249: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:27.249: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 11:39:27.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 11:39:27.258: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:39:27.262
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-868, will wait for the garbage collector to delete the pods 02/24/23 11:39:27.262
Feb 24 11:39:27.329: INFO: Deleting DaemonSet.extensions daemon-set took: 11.388215ms
Feb 24 11:39:27.430: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.991302ms
Feb 24 11:39:29.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:39:29.337: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:39:29.341: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26418"},"items":null}

Feb 24 11:39:29.346: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26418"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:39:29.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-868" for this suite. 02/24/23 11:39:29.379
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":174,"skipped":2833,"failed":0}
------------------------------
• [SLOW TEST] [8.348 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:39:21.042
    Feb 24 11:39:21.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:39:21.045
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:21.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:21.075
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 02/24/23 11:39:21.105
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 11:39:21.115
    Feb 24 11:39:21.122: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:21.122: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:21.122: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:21.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:39:21.191: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:39:22.198: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:22.199: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:22.199: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:22.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:39:22.203: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:39:23.201: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:23.201: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:23.201: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:23.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:39:23.207: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/24/23 11:39:23.211
    Feb 24 11:39:23.235: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:23.235: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:23.235: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:23.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:39:23.241: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:39:24.250: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:24.250: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:24.250: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:24.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:39:24.255: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:39:25.250: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:25.250: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:25.250: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:25.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:39:25.256: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:39:26.250: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:26.250: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:26.250: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:26.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 24 11:39:26.255: INFO: Node ip-172-31-216-47.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:39:27.249: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:27.249: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:27.249: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 11:39:27.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 11:39:27.258: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:39:27.262
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-868, will wait for the garbage collector to delete the pods 02/24/23 11:39:27.262
    Feb 24 11:39:27.329: INFO: Deleting DaemonSet.extensions daemon-set took: 11.388215ms
    Feb 24 11:39:27.430: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.991302ms
    Feb 24 11:39:29.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:39:29.337: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:39:29.341: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26418"},"items":null}

    Feb 24 11:39:29.346: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26418"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:39:29.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-868" for this suite. 02/24/23 11:39:29.379
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:39:29.392
Feb 24 11:39:29.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename security-context 02/24/23 11:39:29.393
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:29.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:29.43
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 11:39:29.434
Feb 24 11:39:29.445: INFO: Waiting up to 5m0s for pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93" in namespace "security-context-7984" to be "Succeeded or Failed"
Feb 24 11:39:29.451: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474767ms
Feb 24 11:39:31.457: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012328419s
Feb 24 11:39:33.457: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012173635s
STEP: Saw pod success 02/24/23 11:39:33.457
Feb 24 11:39:33.457: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93" satisfied condition "Succeeded or Failed"
Feb 24 11:39:33.462: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93 container test-container: <nil>
STEP: delete the pod 02/24/23 11:39:33.47
Feb 24 11:39:33.484: INFO: Waiting for pod security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93 to disappear
Feb 24 11:39:33.488: INFO: Pod security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 24 11:39:33.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7984" for this suite. 02/24/23 11:39:33.496
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":175,"skipped":2836,"failed":0}
------------------------------
• [4.112 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:39:29.392
    Feb 24 11:39:29.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename security-context 02/24/23 11:39:29.393
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:29.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:29.43
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 11:39:29.434
    Feb 24 11:39:29.445: INFO: Waiting up to 5m0s for pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93" in namespace "security-context-7984" to be "Succeeded or Failed"
    Feb 24 11:39:29.451: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474767ms
    Feb 24 11:39:31.457: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012328419s
    Feb 24 11:39:33.457: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012173635s
    STEP: Saw pod success 02/24/23 11:39:33.457
    Feb 24 11:39:33.457: INFO: Pod "security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93" satisfied condition "Succeeded or Failed"
    Feb 24 11:39:33.462: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:39:33.47
    Feb 24 11:39:33.484: INFO: Waiting for pod security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93 to disappear
    Feb 24 11:39:33.488: INFO: Pod security-context-6cf038e4-d864-4abc-bf89-96fb5a0cbe93 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 24 11:39:33.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-7984" for this suite. 02/24/23 11:39:33.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:39:33.518
Feb 24 11:39:33.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 11:39:33.519
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:33.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:33.548
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 02/24/23 11:39:33.552
STEP: waiting for pod running 02/24/23 11:39:33.563
Feb 24 11:39:33.564: INFO: Waiting up to 2m0s for pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" in namespace "var-expansion-7709" to be "running"
Feb 24 11:39:33.571: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.835826ms
Feb 24 11:39:35.576: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011756799s
Feb 24 11:39:35.576: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" satisfied condition "running"
STEP: creating a file in subpath 02/24/23 11:39:35.576
Feb 24 11:39:35.581: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7709 PodName:var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:39:35.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:39:35.581: INFO: ExecWithOptions: Clientset creation
Feb 24 11:39:35.582: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7709/pods/var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/24/23 11:39:35.665
Feb 24 11:39:35.669: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7709 PodName:var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 11:39:35.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 11:39:35.670: INFO: ExecWithOptions: Clientset creation
Feb 24 11:39:35.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7709/pods/var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/24/23 11:39:35.748
Feb 24 11:39:36.264: INFO: Successfully updated pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6"
STEP: waiting for annotated pod running 02/24/23 11:39:36.264
Feb 24 11:39:36.264: INFO: Waiting up to 2m0s for pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" in namespace "var-expansion-7709" to be "running"
Feb 24 11:39:36.269: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6": Phase="Running", Reason="", readiness=true. Elapsed: 4.104303ms
Feb 24 11:39:36.269: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" satisfied condition "running"
STEP: deleting the pod gracefully 02/24/23 11:39:36.269
Feb 24 11:39:36.269: INFO: Deleting pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" in namespace "var-expansion-7709"
Feb 24 11:39:36.278: INFO: Wait up to 5m0s for pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 11:40:10.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7709" for this suite. 02/24/23 11:40:10.295
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":176,"skipped":2882,"failed":0}
------------------------------
• [SLOW TEST] [36.787 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:39:33.518
    Feb 24 11:39:33.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 11:39:33.519
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:39:33.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:39:33.548
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 02/24/23 11:39:33.552
    STEP: waiting for pod running 02/24/23 11:39:33.563
    Feb 24 11:39:33.564: INFO: Waiting up to 2m0s for pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" in namespace "var-expansion-7709" to be "running"
    Feb 24 11:39:33.571: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.835826ms
    Feb 24 11:39:35.576: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011756799s
    Feb 24 11:39:35.576: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" satisfied condition "running"
    STEP: creating a file in subpath 02/24/23 11:39:35.576
    Feb 24 11:39:35.581: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7709 PodName:var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:39:35.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:39:35.581: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:39:35.582: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7709/pods/var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/24/23 11:39:35.665
    Feb 24 11:39:35.669: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7709 PodName:var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 11:39:35.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 11:39:35.670: INFO: ExecWithOptions: Clientset creation
    Feb 24 11:39:35.670: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-7709/pods/var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/24/23 11:39:35.748
    Feb 24 11:39:36.264: INFO: Successfully updated pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6"
    STEP: waiting for annotated pod running 02/24/23 11:39:36.264
    Feb 24 11:39:36.264: INFO: Waiting up to 2m0s for pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" in namespace "var-expansion-7709" to be "running"
    Feb 24 11:39:36.269: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6": Phase="Running", Reason="", readiness=true. Elapsed: 4.104303ms
    Feb 24 11:39:36.269: INFO: Pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" satisfied condition "running"
    STEP: deleting the pod gracefully 02/24/23 11:39:36.269
    Feb 24 11:39:36.269: INFO: Deleting pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" in namespace "var-expansion-7709"
    Feb 24 11:39:36.278: INFO: Wait up to 5m0s for pod "var-expansion-0eaae400-2849-4f33-876e-1326e8a6b6d6" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 11:40:10.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7709" for this suite. 02/24/23 11:40:10.295
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:40:10.306
Feb 24 11:40:10.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename subpath 02/24/23 11:40:10.308
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:10.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:10.342
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 11:40:10.346
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-7sfv 02/24/23 11:40:10.357
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:40:10.357
Feb 24 11:40:10.369: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7sfv" in namespace "subpath-6113" to be "Succeeded or Failed"
Feb 24 11:40:10.376: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.621481ms
Feb 24 11:40:12.383: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 2.014728325s
Feb 24 11:40:14.381: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 4.012771096s
Feb 24 11:40:16.384: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 6.014956801s
Feb 24 11:40:18.381: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 8.012893045s
Feb 24 11:40:20.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 10.013722882s
Feb 24 11:40:22.383: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 12.014372896s
Feb 24 11:40:24.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 14.01312339s
Feb 24 11:40:26.381: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 16.012896198s
Feb 24 11:40:28.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 18.013246693s
Feb 24 11:40:30.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 20.013263686s
Feb 24 11:40:32.383: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=false. Elapsed: 22.01402419s
Feb 24 11:40:34.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012986671s
STEP: Saw pod success 02/24/23 11:40:34.382
Feb 24 11:40:34.382: INFO: Pod "pod-subpath-test-secret-7sfv" satisfied condition "Succeeded or Failed"
Feb 24 11:40:34.387: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-secret-7sfv container test-container-subpath-secret-7sfv: <nil>
STEP: delete the pod 02/24/23 11:40:34.397
Feb 24 11:40:34.435: INFO: Waiting for pod pod-subpath-test-secret-7sfv to disappear
Feb 24 11:40:34.449: INFO: Pod pod-subpath-test-secret-7sfv no longer exists
STEP: Deleting pod pod-subpath-test-secret-7sfv 02/24/23 11:40:34.45
Feb 24 11:40:34.450: INFO: Deleting pod "pod-subpath-test-secret-7sfv" in namespace "subpath-6113"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 24 11:40:34.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6113" for this suite. 02/24/23 11:40:34.463
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":177,"skipped":2883,"failed":0}
------------------------------
• [SLOW TEST] [24.166 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:40:10.306
    Feb 24 11:40:10.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename subpath 02/24/23 11:40:10.308
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:10.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:10.342
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 11:40:10.346
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-7sfv 02/24/23 11:40:10.357
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:40:10.357
    Feb 24 11:40:10.369: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7sfv" in namespace "subpath-6113" to be "Succeeded or Failed"
    Feb 24 11:40:10.376: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.621481ms
    Feb 24 11:40:12.383: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 2.014728325s
    Feb 24 11:40:14.381: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 4.012771096s
    Feb 24 11:40:16.384: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 6.014956801s
    Feb 24 11:40:18.381: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 8.012893045s
    Feb 24 11:40:20.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 10.013722882s
    Feb 24 11:40:22.383: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 12.014372896s
    Feb 24 11:40:24.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 14.01312339s
    Feb 24 11:40:26.381: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 16.012896198s
    Feb 24 11:40:28.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 18.013246693s
    Feb 24 11:40:30.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=true. Elapsed: 20.013263686s
    Feb 24 11:40:32.383: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Running", Reason="", readiness=false. Elapsed: 22.01402419s
    Feb 24 11:40:34.382: INFO: Pod "pod-subpath-test-secret-7sfv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012986671s
    STEP: Saw pod success 02/24/23 11:40:34.382
    Feb 24 11:40:34.382: INFO: Pod "pod-subpath-test-secret-7sfv" satisfied condition "Succeeded or Failed"
    Feb 24 11:40:34.387: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-secret-7sfv container test-container-subpath-secret-7sfv: <nil>
    STEP: delete the pod 02/24/23 11:40:34.397
    Feb 24 11:40:34.435: INFO: Waiting for pod pod-subpath-test-secret-7sfv to disappear
    Feb 24 11:40:34.449: INFO: Pod pod-subpath-test-secret-7sfv no longer exists
    STEP: Deleting pod pod-subpath-test-secret-7sfv 02/24/23 11:40:34.45
    Feb 24 11:40:34.450: INFO: Deleting pod "pod-subpath-test-secret-7sfv" in namespace "subpath-6113"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 24 11:40:34.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6113" for this suite. 02/24/23 11:40:34.463
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:40:34.474
Feb 24 11:40:34.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:40:34.476
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:34.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:34.506
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:40:34.536
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:40:35.176
STEP: Deploying the webhook pod 02/24/23 11:40:35.185
STEP: Wait for the deployment to be ready 02/24/23 11:40:35.203
Feb 24 11:40:35.212: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:40:37.228
STEP: Verifying the service has paired with the endpoint 02/24/23 11:40:37.244
Feb 24 11:40:38.245: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 02/24/23 11:40:38.25
STEP: Creating a custom resource definition that should be denied by the webhook 02/24/23 11:40:38.269
Feb 24 11:40:38.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:40:38.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1726" for this suite. 02/24/23 11:40:38.295
STEP: Destroying namespace "webhook-1726-markers" for this suite. 02/24/23 11:40:38.302
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":178,"skipped":2885,"failed":0}
------------------------------
• [3.901 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:40:34.474
    Feb 24 11:40:34.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:40:34.476
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:34.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:34.506
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:40:34.536
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:40:35.176
    STEP: Deploying the webhook pod 02/24/23 11:40:35.185
    STEP: Wait for the deployment to be ready 02/24/23 11:40:35.203
    Feb 24 11:40:35.212: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:40:37.228
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:40:37.244
    Feb 24 11:40:38.245: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/24/23 11:40:38.25
    STEP: Creating a custom resource definition that should be denied by the webhook 02/24/23 11:40:38.269
    Feb 24 11:40:38.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:40:38.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1726" for this suite. 02/24/23 11:40:38.295
    STEP: Destroying namespace "webhook-1726-markers" for this suite. 02/24/23 11:40:38.302
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:40:38.387
Feb 24 11:40:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:40:38.388
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:38.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:38.469
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-4054 02/24/23 11:40:38.473
Feb 24 11:40:38.484: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4054" to be "running and ready"
Feb 24 11:40:38.491: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.759135ms
Feb 24 11:40:38.491: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:40:40.498: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014789686s
Feb 24 11:40:40.498: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 24 11:40:40.498: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 24 11:40:40.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 24 11:40:40.655: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 24 11:40:40.655: INFO: stdout: "iptables"
Feb 24 11:40:40.655: INFO: proxyMode: iptables
Feb 24 11:40:40.668: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 24 11:40:40.672: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4054 02/24/23 11:40:40.672
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4054 02/24/23 11:40:40.689
I0224 11:40:40.702287      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4054, replica count: 3
I0224 11:40:43.753326      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:40:43.763: INFO: Creating new exec pod
Feb 24 11:40:43.774: INFO: Waiting up to 5m0s for pod "execpod-affinityzskjr" in namespace "services-4054" to be "running"
Feb 24 11:40:43.783: INFO: Pod "execpod-affinityzskjr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437325ms
Feb 24 11:40:45.788: INFO: Pod "execpod-affinityzskjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.013656805s
Feb 24 11:40:45.788: INFO: Pod "execpod-affinityzskjr" satisfied condition "running"
Feb 24 11:40:46.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 24 11:40:46.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 24 11:40:46.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:40:46.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.244.214 80'
Feb 24 11:40:47.150: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.111.244.214 80\nConnection to 10.111.244.214 80 port [tcp/http] succeeded!\n"
Feb 24 11:40:47.150: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:40:47.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.244.214:80/ ; done'
Feb 24 11:40:47.432: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n"
Feb 24 11:40:47.432: INFO: stdout: "\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2"
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
Feb 24 11:40:47.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.111.244.214:80/'
Feb 24 11:40:47.623: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n"
Feb 24 11:40:47.623: INFO: stdout: "affinity-clusterip-timeout-4tgm2"
Feb 24 11:41:07.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.111.244.214:80/'
Feb 24 11:41:07.790: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n"
Feb 24 11:41:07.790: INFO: stdout: "affinity-clusterip-timeout-8gqlz"
Feb 24 11:41:07.790: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4054, will wait for the garbage collector to delete the pods 02/24/23 11:41:07.805
Feb 24 11:41:07.870: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 8.652114ms
Feb 24 11:41:07.973: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.742179ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:41:10.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4054" for this suite. 02/24/23 11:41:10.31
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":179,"skipped":2932,"failed":0}
------------------------------
• [SLOW TEST] [31.934 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:40:38.387
    Feb 24 11:40:38.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:40:38.388
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:40:38.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:40:38.469
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-4054 02/24/23 11:40:38.473
    Feb 24 11:40:38.484: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-4054" to be "running and ready"
    Feb 24 11:40:38.491: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.759135ms
    Feb 24 11:40:38.491: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:40:40.498: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014789686s
    Feb 24 11:40:40.498: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 24 11:40:40.498: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 24 11:40:40.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 24 11:40:40.655: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb 24 11:40:40.655: INFO: stdout: "iptables"
    Feb 24 11:40:40.655: INFO: proxyMode: iptables
    Feb 24 11:40:40.668: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 24 11:40:40.672: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-4054 02/24/23 11:40:40.672
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-4054 02/24/23 11:40:40.689
    I0224 11:40:40.702287      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4054, replica count: 3
    I0224 11:40:43.753326      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:40:43.763: INFO: Creating new exec pod
    Feb 24 11:40:43.774: INFO: Waiting up to 5m0s for pod "execpod-affinityzskjr" in namespace "services-4054" to be "running"
    Feb 24 11:40:43.783: INFO: Pod "execpod-affinityzskjr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.437325ms
    Feb 24 11:40:45.788: INFO: Pod "execpod-affinityzskjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.013656805s
    Feb 24 11:40:45.788: INFO: Pod "execpod-affinityzskjr" satisfied condition "running"
    Feb 24 11:40:46.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Feb 24 11:40:46.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Feb 24 11:40:46.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:40:46.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.244.214 80'
    Feb 24 11:40:47.150: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.111.244.214 80\nConnection to 10.111.244.214 80 port [tcp/http] succeeded!\n"
    Feb 24 11:40:47.150: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:40:47.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.244.214:80/ ; done'
    Feb 24 11:40:47.432: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n"
    Feb 24 11:40:47.432: INFO: stdout: "\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2\naffinity-clusterip-timeout-4tgm2"
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Received response from host: affinity-clusterip-timeout-4tgm2
    Feb 24 11:40:47.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.111.244.214:80/'
    Feb 24 11:40:47.623: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n"
    Feb 24 11:40:47.623: INFO: stdout: "affinity-clusterip-timeout-4tgm2"
    Feb 24 11:41:07.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-4054 exec execpod-affinityzskjr -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.111.244.214:80/'
    Feb 24 11:41:07.790: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.111.244.214:80/\n"
    Feb 24 11:41:07.790: INFO: stdout: "affinity-clusterip-timeout-8gqlz"
    Feb 24 11:41:07.790: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4054, will wait for the garbage collector to delete the pods 02/24/23 11:41:07.805
    Feb 24 11:41:07.870: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 8.652114ms
    Feb 24 11:41:07.973: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.742179ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:41:10.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4054" for this suite. 02/24/23 11:41:10.31
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:41:10.325
Feb 24 11:41:10.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:41:10.326
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:10.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:10.357
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Feb 24 11:41:10.371: INFO: Waiting up to 5m0s for pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945" in namespace "security-context-test-4973" to be "Succeeded or Failed"
Feb 24 11:41:10.378: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945": Phase="Pending", Reason="", readiness=false. Elapsed: 6.431158ms
Feb 24 11:41:12.384: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012724948s
Feb 24 11:41:14.386: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015059137s
Feb 24 11:41:14.386: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 24 11:41:14.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4973" for this suite. 02/24/23 11:41:14.403
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":2959,"failed":0}
------------------------------
• [4.087 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:41:10.325
    Feb 24 11:41:10.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:41:10.326
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:10.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:10.357
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Feb 24 11:41:10.371: INFO: Waiting up to 5m0s for pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945" in namespace "security-context-test-4973" to be "Succeeded or Failed"
    Feb 24 11:41:10.378: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945": Phase="Pending", Reason="", readiness=false. Elapsed: 6.431158ms
    Feb 24 11:41:12.384: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012724948s
    Feb 24 11:41:14.386: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015059137s
    Feb 24 11:41:14.386: INFO: Pod "busybox-user-65534-16575030-6081-4d1a-a309-e39fd0b98945" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 24 11:41:14.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4973" for this suite. 02/24/23 11:41:14.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:41:14.415
Feb 24 11:41:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename podtemplate 02/24/23 11:41:14.416
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:14.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:14.446
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/24/23 11:41:14.454
STEP: Replace a pod template 02/24/23 11:41:14.46
Feb 24 11:41:14.472: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 24 11:41:14.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3545" for this suite. 02/24/23 11:41:14.48
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":181,"skipped":2980,"failed":0}
------------------------------
• [0.074 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:41:14.415
    Feb 24 11:41:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename podtemplate 02/24/23 11:41:14.416
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:14.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:14.446
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/24/23 11:41:14.454
    STEP: Replace a pod template 02/24/23 11:41:14.46
    Feb 24 11:41:14.472: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 24 11:41:14.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3545" for this suite. 02/24/23 11:41:14.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:41:14.495
Feb 24 11:41:14.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 11:41:14.496
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:14.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:14.527
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/24/23 11:41:14.53
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6825;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6825;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +notcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_tcp@PTR;sleep 1; done
 02/24/23 11:41:14.558
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6825;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6825;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +notcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_tcp@PTR;sleep 1; done
 02/24/23 11:41:14.558
STEP: creating a pod to probe DNS 02/24/23 11:41:14.559
STEP: submitting the pod to kubernetes 02/24/23 11:41:14.559
Feb 24 11:41:14.582: INFO: Waiting up to 15m0s for pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc" in namespace "dns-6825" to be "running"
Feb 24 11:41:14.602: INFO: Pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.932583ms
Feb 24 11:41:16.613: INFO: Pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.031314976s
Feb 24 11:41:16.613: INFO: Pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:41:16.613
STEP: looking for the results for each expected name from probers 02/24/23 11:41:16.619
Feb 24 11:41:16.631: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.639: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.647: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.653: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.677: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.682: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.715: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.721: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.728: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.734: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.739: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.745: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.750: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.756: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:16.783: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:21.800: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.809: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.818: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.823: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.847: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.854: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.903: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.909: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.915: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.921: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.935: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.942: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:21.983: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:26.790: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.796: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.801: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.807: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.825: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.831: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.860: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.866: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.874: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.886: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.897: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.904: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:26.932: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:31.792: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.799: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.810: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.815: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.827: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.835: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.863: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.870: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.875: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.890: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.896: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.902: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.907: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:31.928: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:36.792: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.798: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.819: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.830: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.836: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.866: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.872: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.877: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.886: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.894: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.906: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.913: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:36.939: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:41.790: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.796: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.802: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.807: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.815: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.827: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.834: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.866: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.872: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.877: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.892: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.897: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.903: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.908: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:41.936: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:46.825: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:46.831: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:46.898: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:46.904: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
Feb 24 11:41:46.929: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

Feb 24 11:41:51.950: INFO: DNS probes using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc succeeded

STEP: deleting the pod 02/24/23 11:41:51.95
STEP: deleting the test service 02/24/23 11:41:51.982
STEP: deleting the test headless service 02/24/23 11:41:52.1
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 11:41:52.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6825" for this suite. 02/24/23 11:41:52.148
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":182,"skipped":3032,"failed":0}
------------------------------
• [SLOW TEST] [37.680 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:41:14.495
    Feb 24 11:41:14.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 11:41:14.496
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:14.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:14.527
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/24/23 11:41:14.53
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6825;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6825;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +notcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_tcp@PTR;sleep 1; done
     02/24/23 11:41:14.558
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6825;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6825;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6825.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6825.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6825.svc;check="$$(dig +notcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.120.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.120.51_tcp@PTR;sleep 1; done
     02/24/23 11:41:14.558
    STEP: creating a pod to probe DNS 02/24/23 11:41:14.559
    STEP: submitting the pod to kubernetes 02/24/23 11:41:14.559
    Feb 24 11:41:14.582: INFO: Waiting up to 15m0s for pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc" in namespace "dns-6825" to be "running"
    Feb 24 11:41:14.602: INFO: Pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.932583ms
    Feb 24 11:41:16.613: INFO: Pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.031314976s
    Feb 24 11:41:16.613: INFO: Pod "dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:41:16.613
    STEP: looking for the results for each expected name from probers 02/24/23 11:41:16.619
    Feb 24 11:41:16.631: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.639: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.647: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.653: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.677: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.682: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.715: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.721: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.728: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.734: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.739: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.745: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.750: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.756: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:16.783: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:21.800: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.809: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.818: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.823: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.847: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.854: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.903: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.909: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.915: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.921: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.935: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.942: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.948: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.955: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:21.983: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:26.790: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.796: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.801: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.807: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.825: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.831: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.860: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.866: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.874: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.886: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.892: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.897: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.904: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:26.932: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:31.792: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.799: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.810: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.815: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.821: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.827: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.835: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.863: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.870: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.875: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.890: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.896: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.902: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.907: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:31.928: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:36.792: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.798: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.819: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.825: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.830: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.836: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.866: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.872: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.877: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.886: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.894: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.901: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.906: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.913: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:36.939: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:41.790: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.796: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.802: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.807: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.815: INFO: Unable to read wheezy_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.827: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.834: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.866: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.872: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.877: INFO: Unable to read jessie_udp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.883: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825 from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.892: INFO: Unable to read jessie_udp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.897: INFO: Unable to read jessie_tcp@dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.903: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.908: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:41.936: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6825 wheezy_tcp@dns-test-service.dns-6825 wheezy_udp@dns-test-service.dns-6825.svc wheezy_tcp@dns-test-service.dns-6825.svc wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6825 jessie_tcp@dns-test-service.dns-6825 jessie_udp@dns-test-service.dns-6825.svc jessie_tcp@dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:46.825: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:46.831: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:46.898: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:46.904: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc from pod dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc: the server could not find the requested resource (get pods dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc)
    Feb 24 11:41:46.929: INFO: Lookups using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6825.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6825.svc jessie_udp@_http._tcp.dns-test-service.dns-6825.svc jessie_tcp@_http._tcp.dns-test-service.dns-6825.svc]

    Feb 24 11:41:51.950: INFO: DNS probes using dns-6825/dns-test-21583c72-6b19-4bb0-a4b8-662363d328fc succeeded

    STEP: deleting the pod 02/24/23 11:41:51.95
    STEP: deleting the test service 02/24/23 11:41:51.982
    STEP: deleting the test headless service 02/24/23 11:41:52.1
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 11:41:52.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6825" for this suite. 02/24/23 11:41:52.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:41:52.184
Feb 24 11:41:52.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 11:41:52.191
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:52.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:52.251
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb 24 11:41:52.272: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 24 11:41:57.277: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 11:41:57.277
Feb 24 11:41:57.278: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 24 11:41:59.283: INFO: Creating deployment "test-rollover-deployment"
Feb 24 11:41:59.295: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 24 11:42:01.310: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 24 11:42:01.354: INFO: Ensure that both replica sets have 1 created replica
Feb 24 11:42:01.388: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 24 11:42:01.404: INFO: Updating deployment test-rollover-deployment
Feb 24 11:42:01.404: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 24 11:42:03.419: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 24 11:42:03.429: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 24 11:42:03.445: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:42:03.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:42:05.455: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:42:05.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:42:07.455: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:42:07.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:42:09.455: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:42:09.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:42:11.457: INFO: all replica sets need to contain the pod-template-hash label
Feb 24 11:42:11.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 11:42:13.456: INFO: 
Feb 24 11:42:13.456: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 11:42:13.469: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2921  01b1cd7a-133e-4f95-a1c6-0b6945d4badd 27520 2 2023-02-24 11:41:59 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c5ba38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:41:59 +0000 UTC,LastTransitionTime:2023-02-24 11:41:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-24 11:42:12 +0000 UTC,LastTransitionTime:2023-02-24 11:41:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 11:42:13.473: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2921  a43d16cc-bf64-45e0-ab21-58bae92ae6d5 27509 2 2023-02-24 11:42:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 01b1cd7a-133e-4f95-a1c6-0b6945d4badd 0xc0031dcb27 0xc0031dcb28}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01b1cd7a-133e-4f95-a1c6-0b6945d4badd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031dcbd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:42:13.473: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 24 11:42:13.480: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2921  c74f1890-4374-4b01-9dc5-7117254c0393 27519 2 2023-02-24 11:41:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 01b1cd7a-133e-4f95-a1c6-0b6945d4badd 0xc0031dc8a7 0xc0031dc8a8}] [] [{e2e.test Update apps/v1 2023-02-24 11:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01b1cd7a-133e-4f95-a1c6-0b6945d4badd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031dc978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:42:13.481: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2921  f0e46646-5e56-4b8c-a262-1e04fc0cad97 27460 2 2023-02-24 11:41:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 01b1cd7a-133e-4f95-a1c6-0b6945d4badd 0xc0031dca07 0xc0031dca08}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01b1cd7a-133e-4f95-a1c6-0b6945d4badd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031dcab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 24 11:42:13.488: INFO: Pod "test-rollover-deployment-6d45fd857b-q6z44" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-q6z44 test-rollover-deployment-6d45fd857b- deployment-2921  48821051-4ddc-4784-9758-db57c61cadcc 27469 0 2023-02-24 11:42:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:34d5192222f158f3bc9002d39684b02a9e24797c8f1678fc38e9b8dc0401867b cni.projectcalico.org/podIP:10.244.3.223/32 cni.projectcalico.org/podIPs:10.244.3.223/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a43d16cc-bf64-45e0-ab21-58bae92ae6d5 0xc0031dd167 0xc0031dd168}] [] [{kube-controller-manager Update v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a43d16cc-bf64-45e0-ab21-58bae92ae6d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:42:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2m8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2m8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.223,StartTime:2023-02-24 11:42:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:42:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://3c55329216db19a14d9e0ae0fda48bda3dc857dbfaa512eed9a2d2ad4eb8d73c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 11:42:13.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2921" for this suite. 02/24/23 11:42:13.495
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":183,"skipped":3042,"failed":0}
------------------------------
• [SLOW TEST] [21.319 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:41:52.184
    Feb 24 11:41:52.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 11:41:52.191
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:41:52.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:41:52.251
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb 24 11:41:52.272: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 24 11:41:57.277: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 11:41:57.277
    Feb 24 11:41:57.278: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 24 11:41:59.283: INFO: Creating deployment "test-rollover-deployment"
    Feb 24 11:41:59.295: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 24 11:42:01.310: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 24 11:42:01.354: INFO: Ensure that both replica sets have 1 created replica
    Feb 24 11:42:01.388: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 24 11:42:01.404: INFO: Updating deployment test-rollover-deployment
    Feb 24 11:42:01.404: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 24 11:42:03.419: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 24 11:42:03.429: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 24 11:42:03.445: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:42:03.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:42:05.455: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:42:05.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:42:07.455: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:42:07.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:42:09.455: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:42:09.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:42:11.457: INFO: all replica sets need to contain the pod-template-hash label
    Feb 24 11:42:11.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 11, 42, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 11, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 11:42:13.456: INFO: 
    Feb 24 11:42:13.456: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 11:42:13.469: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2921  01b1cd7a-133e-4f95-a1c6-0b6945d4badd 27520 2 2023-02-24 11:41:59 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c5ba38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-24 11:41:59 +0000 UTC,LastTransitionTime:2023-02-24 11:41:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-24 11:42:12 +0000 UTC,LastTransitionTime:2023-02-24 11:41:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 11:42:13.473: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-2921  a43d16cc-bf64-45e0-ab21-58bae92ae6d5 27509 2 2023-02-24 11:42:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 01b1cd7a-133e-4f95-a1c6-0b6945d4badd 0xc0031dcb27 0xc0031dcb28}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01b1cd7a-133e-4f95-a1c6-0b6945d4badd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031dcbd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:42:13.473: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 24 11:42:13.480: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2921  c74f1890-4374-4b01-9dc5-7117254c0393 27519 2 2023-02-24 11:41:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 01b1cd7a-133e-4f95-a1c6-0b6945d4badd 0xc0031dc8a7 0xc0031dc8a8}] [] [{e2e.test Update apps/v1 2023-02-24 11:41:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01b1cd7a-133e-4f95-a1c6-0b6945d4badd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0031dc978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:42:13.481: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-2921  f0e46646-5e56-4b8c-a262-1e04fc0cad97 27460 2 2023-02-24 11:41:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 01b1cd7a-133e-4f95-a1c6-0b6945d4badd 0xc0031dca07 0xc0031dca08}] [] [{kube-controller-manager Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01b1cd7a-133e-4f95-a1c6-0b6945d4badd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031dcab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 11:42:13.488: INFO: Pod "test-rollover-deployment-6d45fd857b-q6z44" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-q6z44 test-rollover-deployment-6d45fd857b- deployment-2921  48821051-4ddc-4784-9758-db57c61cadcc 27469 0 2023-02-24 11:42:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:34d5192222f158f3bc9002d39684b02a9e24797c8f1678fc38e9b8dc0401867b cni.projectcalico.org/podIP:10.244.3.223/32 cni.projectcalico.org/podIPs:10.244.3.223/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a43d16cc-bf64-45e0-ab21-58bae92ae6d5 0xc0031dd167 0xc0031dd168}] [] [{kube-controller-manager Update v1 2023-02-24 11:42:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a43d16cc-bf64-45e0-ab21-58bae92ae6d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 11:42:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 11:42:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2m8h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2m8h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 11:42:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.223,StartTime:2023-02-24 11:42:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 11:42:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://3c55329216db19a14d9e0ae0fda48bda3dc857dbfaa512eed9a2d2ad4eb8d73c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 11:42:13.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2921" for this suite. 02/24/23 11:42:13.495
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:42:13.506
Feb 24 11:42:13.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 11:42:13.508
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:13.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:13.54
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 11:42:13.547
Feb 24 11:42:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2142 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Feb 24 11:42:13.651: INFO: stderr: ""
Feb 24 11:42:13.651: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 11:42:13.651
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Feb 24 11:42:13.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2142 delete pods e2e-test-httpd-pod'
Feb 24 11:42:15.730: INFO: stderr: ""
Feb 24 11:42:15.730: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 11:42:15.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2142" for this suite. 02/24/23 11:42:15.738
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":184,"skipped":3046,"failed":0}
------------------------------
• [2.247 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:42:13.506
    Feb 24 11:42:13.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 11:42:13.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:13.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:13.54
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 11:42:13.547
    Feb 24 11:42:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2142 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Feb 24 11:42:13.651: INFO: stderr: ""
    Feb 24 11:42:13.651: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/24/23 11:42:13.651
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Feb 24 11:42:13.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2142 delete pods e2e-test-httpd-pod'
    Feb 24 11:42:15.730: INFO: stderr: ""
    Feb 24 11:42:15.730: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 11:42:15.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2142" for this suite. 02/24/23 11:42:15.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:42:15.762
Feb 24 11:42:15.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:42:15.764
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:15.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:15.843
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-5893 02/24/23 11:42:15.864
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[] 02/24/23 11:42:15.963
Feb 24 11:42:15.971: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 24 11:42:16.982: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5893 02/24/23 11:42:16.982
Feb 24 11:42:16.994: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5893" to be "running and ready"
Feb 24 11:42:17.000: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094354ms
Feb 24 11:42:17.000: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:42:19.006: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012174653s
Feb 24 11:42:19.006: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 24 11:42:19.006: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[pod1:[100]] 02/24/23 11:42:19.024
Feb 24 11:42:19.061: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5893 02/24/23 11:42:19.061
Feb 24 11:42:19.077: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5893" to be "running and ready"
Feb 24 11:42:19.097: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.32332ms
Feb 24 11:42:19.097: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:42:21.103: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.02610173s
Feb 24 11:42:21.103: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 24 11:42:21.103: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[pod1:[100] pod2:[101]] 02/24/23 11:42:21.115
Feb 24 11:42:21.136: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/24/23 11:42:21.137
Feb 24 11:42:21.137: INFO: Creating new exec pod
Feb 24 11:42:21.144: INFO: Waiting up to 5m0s for pod "execpodfr5mr" in namespace "services-5893" to be "running"
Feb 24 11:42:21.152: INFO: Pod "execpodfr5mr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364501ms
Feb 24 11:42:23.157: INFO: Pod "execpodfr5mr": Phase="Running", Reason="", readiness=true. Elapsed: 2.013241374s
Feb 24 11:42:23.157: INFO: Pod "execpodfr5mr" satisfied condition "running"
Feb 24 11:42:24.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 24 11:42:24.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 24 11:42:24.347: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:42:24.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.38.44 80'
Feb 24 11:42:24.493: INFO: stderr: "+ nc -v -t -w 2 10.98.38.44 80\n+ echo hostName\nConnection to 10.98.38.44 80 port [tcp/http] succeeded!\n"
Feb 24 11:42:24.494: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:42:24.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 24 11:42:24.665: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 24 11:42:24.665: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 24 11:42:24.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.38.44 81'
Feb 24 11:42:24.839: INFO: stderr: "+ + nc -v -t -w 2 10.98.38.44 81\necho hostName\nConnection to 10.98.38.44 81 port [tcp/*] succeeded!\n"
Feb 24 11:42:24.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5893 02/24/23 11:42:24.839
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[pod2:[101]] 02/24/23 11:42:24.857
Feb 24 11:42:25.890: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5893 02/24/23 11:42:25.89
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[] 02/24/23 11:42:25.91
Feb 24 11:42:26.930: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:42:26.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5893" for this suite. 02/24/23 11:42:26.982
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":185,"skipped":3063,"failed":0}
------------------------------
• [SLOW TEST] [11.241 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:42:15.762
    Feb 24 11:42:15.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:42:15.764
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:15.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:15.843
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-5893 02/24/23 11:42:15.864
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[] 02/24/23 11:42:15.963
    Feb 24 11:42:15.971: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Feb 24 11:42:16.982: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5893 02/24/23 11:42:16.982
    Feb 24 11:42:16.994: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5893" to be "running and ready"
    Feb 24 11:42:17.000: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.094354ms
    Feb 24 11:42:17.000: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:42:19.006: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012174653s
    Feb 24 11:42:19.006: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 24 11:42:19.006: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[pod1:[100]] 02/24/23 11:42:19.024
    Feb 24 11:42:19.061: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5893 02/24/23 11:42:19.061
    Feb 24 11:42:19.077: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5893" to be "running and ready"
    Feb 24 11:42:19.097: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.32332ms
    Feb 24 11:42:19.097: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:42:21.103: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.02610173s
    Feb 24 11:42:21.103: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 24 11:42:21.103: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[pod1:[100] pod2:[101]] 02/24/23 11:42:21.115
    Feb 24 11:42:21.136: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/24/23 11:42:21.137
    Feb 24 11:42:21.137: INFO: Creating new exec pod
    Feb 24 11:42:21.144: INFO: Waiting up to 5m0s for pod "execpodfr5mr" in namespace "services-5893" to be "running"
    Feb 24 11:42:21.152: INFO: Pod "execpodfr5mr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.364501ms
    Feb 24 11:42:23.157: INFO: Pod "execpodfr5mr": Phase="Running", Reason="", readiness=true. Elapsed: 2.013241374s
    Feb 24 11:42:23.157: INFO: Pod "execpodfr5mr" satisfied condition "running"
    Feb 24 11:42:24.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Feb 24 11:42:24.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 24 11:42:24.347: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:42:24.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.38.44 80'
    Feb 24 11:42:24.493: INFO: stderr: "+ nc -v -t -w 2 10.98.38.44 80\n+ echo hostName\nConnection to 10.98.38.44 80 port [tcp/http] succeeded!\n"
    Feb 24 11:42:24.494: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:42:24.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Feb 24 11:42:24.665: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 24 11:42:24.665: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 24 11:42:24.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-5893 exec execpodfr5mr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.38.44 81'
    Feb 24 11:42:24.839: INFO: stderr: "+ + nc -v -t -w 2 10.98.38.44 81\necho hostName\nConnection to 10.98.38.44 81 port [tcp/*] succeeded!\n"
    Feb 24 11:42:24.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5893 02/24/23 11:42:24.839
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[pod2:[101]] 02/24/23 11:42:24.857
    Feb 24 11:42:25.890: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5893 02/24/23 11:42:25.89
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5893 to expose endpoints map[] 02/24/23 11:42:25.91
    Feb 24 11:42:26.930: INFO: successfully validated that service multi-endpoint-test in namespace services-5893 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:42:26.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5893" for this suite. 02/24/23 11:42:26.982
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:42:27.013
Feb 24 11:42:27.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-pred 02/24/23 11:42:27.014
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:27.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:27.091
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 24 11:42:27.095: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 11:42:27.107: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 11:42:27.111: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
Feb 24 11:42:27.125: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.125: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:42:27.125: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:42:27.125: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
Feb 24 11:42:27.125: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:42:27.125: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:42:27.125: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:42:27.125: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.125: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:42:27.125: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.125: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:42:27.125: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.125: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 11:42:27.125: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:42:27.125: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:42:27.125: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
Feb 24 11:42:27.136: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.136: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:42:27.136: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:42:27.136: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
Feb 24 11:42:27.137: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:42:27.137: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:42:27.137: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:42:27.137: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.137: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:42:27.137: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.137: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:42:27.137: INFO: execpodfr5mr from services-5893 started at 2023-02-24 11:42:21 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.137: INFO: 	Container agnhost-container ready: true, restart count 0
Feb 24 11:42:27.137: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.137: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:42:27.137: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 11:42:27.137: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
Feb 24 11:42:27.147: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.147: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 11:42:27.147: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 11:42:27.147: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
Feb 24 11:42:27.148: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 11:42:27.148: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 11:42:27.148: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 11:42:27.148: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.148: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 11:42:27.148: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 11:42:27.148: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 11:42:27.148: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.148: INFO: 	Container e2e ready: true, restart count 0
Feb 24 11:42:27.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:42:27.148: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 11:42:27.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 11:42:27.148: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:42:27.148
Feb 24 11:42:27.162: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2576" to be "running"
Feb 24 11:42:27.168: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.980647ms
Feb 24 11:42:29.173: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010778215s
Feb 24 11:42:29.173: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:42:29.177
STEP: Trying to apply a random label on the found node. 02/24/23 11:42:29.195
STEP: verifying the node has the label kubernetes.io/e2e-0d5ff2b6-b198-4c73-8841-23f24ff721e1 95 02/24/23 11:42:29.209
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/24/23 11:42:29.214
Feb 24 11:42:29.223: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2576" to be "not pending"
Feb 24 11:42:29.228: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.951902ms
Feb 24 11:42:31.232: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009808723s
Feb 24 11:42:31.232: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.215.124 on the node which pod4 resides and expect not scheduled 02/24/23 11:42:31.232
Feb 24 11:42:31.239: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2576" to be "not pending"
Feb 24 11:42:31.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.589971ms
Feb 24 11:42:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011675159s
Feb 24 11:42:35.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019291642s
Feb 24 11:42:37.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015055966s
Feb 24 11:42:39.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011763573s
Feb 24 11:42:41.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01200426s
Feb 24 11:42:43.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013203537s
Feb 24 11:42:45.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021112067s
Feb 24 11:42:47.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015152166s
Feb 24 11:42:49.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011958134s
Feb 24 11:42:51.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015248521s
Feb 24 11:42:53.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016307658s
Feb 24 11:42:55.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015587313s
Feb 24 11:42:57.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012190475s
Feb 24 11:42:59.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012600645s
Feb 24 11:43:01.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014051406s
Feb 24 11:43:03.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.014831658s
Feb 24 11:43:05.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018487981s
Feb 24 11:43:07.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015882024s
Feb 24 11:43:09.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011879432s
Feb 24 11:43:11.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.012892845s
Feb 24 11:43:13.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01588385s
Feb 24 11:43:15.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013389326s
Feb 24 11:43:17.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.011676064s
Feb 24 11:43:19.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011566388s
Feb 24 11:43:21.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018036711s
Feb 24 11:43:23.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01210271s
Feb 24 11:43:25.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015685715s
Feb 24 11:43:27.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014185074s
Feb 24 11:43:29.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01418292s
Feb 24 11:43:31.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012367923s
Feb 24 11:43:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012170361s
Feb 24 11:43:35.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01452151s
Feb 24 11:43:37.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011361124s
Feb 24 11:43:39.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011660987s
Feb 24 11:43:41.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014661811s
Feb 24 11:43:43.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01154366s
Feb 24 11:43:45.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.015684821s
Feb 24 11:43:47.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01335551s
Feb 24 11:43:49.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011789607s
Feb 24 11:43:51.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013614357s
Feb 24 11:43:53.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01203025s
Feb 24 11:43:55.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011621806s
Feb 24 11:43:57.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.017789692s
Feb 24 11:43:59.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014073342s
Feb 24 11:44:01.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011939764s
Feb 24 11:44:03.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012868897s
Feb 24 11:44:05.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012741084s
Feb 24 11:44:07.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013164381s
Feb 24 11:44:09.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011693838s
Feb 24 11:44:11.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01221455s
Feb 24 11:44:13.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011897105s
Feb 24 11:44:15.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012541458s
Feb 24 11:44:17.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.014272216s
Feb 24 11:44:19.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012676691s
Feb 24 11:44:21.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01538642s
Feb 24 11:44:23.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015753055s
Feb 24 11:44:25.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012215189s
Feb 24 11:44:27.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011800453s
Feb 24 11:44:29.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014263268s
Feb 24 11:44:31.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018461254s
Feb 24 11:44:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.012078348s
Feb 24 11:44:35.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013476851s
Feb 24 11:44:37.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011542478s
Feb 24 11:44:39.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.016495662s
Feb 24 11:44:41.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012730535s
Feb 24 11:44:43.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013122924s
Feb 24 11:44:45.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.016562988s
Feb 24 11:44:47.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011488765s
Feb 24 11:44:49.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.013723549s
Feb 24 11:44:51.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.011846263s
Feb 24 11:44:53.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.015491706s
Feb 24 11:44:55.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.014610975s
Feb 24 11:44:57.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.014558069s
Feb 24 11:44:59.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011897477s
Feb 24 11:45:01.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.015387271s
Feb 24 11:45:03.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.014243816s
Feb 24 11:45:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015169234s
Feb 24 11:45:07.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.011793712s
Feb 24 11:45:09.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012714774s
Feb 24 11:45:11.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.01552595s
Feb 24 11:45:13.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.015442066s
Feb 24 11:45:15.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012032637s
Feb 24 11:45:17.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.016032118s
Feb 24 11:45:19.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.011643355s
Feb 24 11:45:21.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015676713s
Feb 24 11:45:23.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.011672049s
Feb 24 11:45:25.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.015638154s
Feb 24 11:45:27.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.01519525s
Feb 24 11:45:29.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.015443304s
Feb 24 11:45:31.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012510353s
Feb 24 11:45:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.011830268s
Feb 24 11:45:35.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.021691351s
Feb 24 11:45:37.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.015799373s
Feb 24 11:45:39.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.0118586s
Feb 24 11:45:41.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.011586361s
Feb 24 11:45:43.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.013837987s
Feb 24 11:45:45.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.01288242s
Feb 24 11:45:47.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.016016154s
Feb 24 11:45:49.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.016668444s
Feb 24 11:45:51.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011518227s
Feb 24 11:45:53.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012317625s
Feb 24 11:45:55.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.013845802s
Feb 24 11:45:57.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.015314734s
Feb 24 11:45:59.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.014829192s
Feb 24 11:46:01.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012646028s
Feb 24 11:46:03.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.013542671s
Feb 24 11:46:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.014985557s
Feb 24 11:46:07.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.011667007s
Feb 24 11:46:09.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.015536279s
Feb 24 11:46:11.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.011549133s
Feb 24 11:46:13.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011530524s
Feb 24 11:46:15.266: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.027065619s
Feb 24 11:46:17.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.011901456s
Feb 24 11:46:19.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.018048714s
Feb 24 11:46:21.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011807854s
Feb 24 11:46:23.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.016509544s
Feb 24 11:46:25.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.011350238s
Feb 24 11:46:27.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.014235969s
Feb 24 11:46:29.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.017295869s
Feb 24 11:46:31.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.013518836s
Feb 24 11:46:33.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015804822s
Feb 24 11:46:35.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.01504448s
Feb 24 11:46:37.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.011865857s
Feb 24 11:46:39.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.01577858s
Feb 24 11:46:41.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.01240326s
Feb 24 11:46:43.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014000894s
Feb 24 11:46:45.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012401996s
Feb 24 11:46:47.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.011815704s
Feb 24 11:46:49.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.017148067s
Feb 24 11:46:51.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012337115s
Feb 24 11:46:53.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.015805005s
Feb 24 11:46:55.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.012185755s
Feb 24 11:46:57.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.015710867s
Feb 24 11:46:59.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.011719832s
Feb 24 11:47:01.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.016681548s
Feb 24 11:47:03.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.011749134s
Feb 24 11:47:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.015108145s
Feb 24 11:47:07.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.012326169s
Feb 24 11:47:09.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.015920296s
Feb 24 11:47:11.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.012687564s
Feb 24 11:47:13.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014299765s
Feb 24 11:47:15.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012723358s
Feb 24 11:47:17.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.017956048s
Feb 24 11:47:19.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018122627s
Feb 24 11:47:21.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.015916532s
Feb 24 11:47:23.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.013005345s
Feb 24 11:47:25.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.014017566s
Feb 24 11:47:27.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.011341371s
Feb 24 11:47:29.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.011650205s
Feb 24 11:47:31.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011458102s
Feb 24 11:47:31.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017251363s
STEP: removing the label kubernetes.io/e2e-0d5ff2b6-b198-4c73-8841-23f24ff721e1 off the node ip-172-31-215-124.eu-west-3.compute.internal 02/24/23 11:47:31.256
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0d5ff2b6-b198-4c73-8841-23f24ff721e1 02/24/23 11:47:31.298
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:47:31.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2576" for this suite. 02/24/23 11:47:31.313
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":186,"skipped":3086,"failed":0}
------------------------------
• [SLOW TEST] [304.317 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:42:27.013
    Feb 24 11:42:27.013: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-pred 02/24/23 11:42:27.014
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:42:27.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:42:27.091
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 24 11:42:27.095: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 11:42:27.107: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 11:42:27.111: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
    Feb 24 11:42:27.125: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.125: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
    Feb 24 11:42:27.125: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.125: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.125: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.125: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:42:27.125: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
    Feb 24 11:42:27.136: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.136: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:42:27.136: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:42:27.136: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
    Feb 24 11:42:27.137: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.137: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.137: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: execpodfr5mr from services-5893 started at 2023-02-24 11:42:21 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.137: INFO: 	Container agnhost-container ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.137: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 11:42:27.137: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
    Feb 24 11:42:27.147: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.147: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 11:42:27.147: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 11:42:27.147: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
    Feb 24 11:42:27.148: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.148: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 11:42:27.148: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.148: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 11:42:27.148: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 11:42:27.148: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 11:42:27.148
    Feb 24 11:42:27.162: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2576" to be "running"
    Feb 24 11:42:27.168: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.980647ms
    Feb 24 11:42:29.173: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010778215s
    Feb 24 11:42:29.173: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 11:42:29.177
    STEP: Trying to apply a random label on the found node. 02/24/23 11:42:29.195
    STEP: verifying the node has the label kubernetes.io/e2e-0d5ff2b6-b198-4c73-8841-23f24ff721e1 95 02/24/23 11:42:29.209
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/24/23 11:42:29.214
    Feb 24 11:42:29.223: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2576" to be "not pending"
    Feb 24 11:42:29.228: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.951902ms
    Feb 24 11:42:31.232: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009808723s
    Feb 24 11:42:31.232: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.215.124 on the node which pod4 resides and expect not scheduled 02/24/23 11:42:31.232
    Feb 24 11:42:31.239: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2576" to be "not pending"
    Feb 24 11:42:31.246: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.589971ms
    Feb 24 11:42:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011675159s
    Feb 24 11:42:35.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019291642s
    Feb 24 11:42:37.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015055966s
    Feb 24 11:42:39.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011763573s
    Feb 24 11:42:41.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.01200426s
    Feb 24 11:42:43.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013203537s
    Feb 24 11:42:45.260: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021112067s
    Feb 24 11:42:47.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015152166s
    Feb 24 11:42:49.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011958134s
    Feb 24 11:42:51.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015248521s
    Feb 24 11:42:53.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016307658s
    Feb 24 11:42:55.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015587313s
    Feb 24 11:42:57.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012190475s
    Feb 24 11:42:59.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012600645s
    Feb 24 11:43:01.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014051406s
    Feb 24 11:43:03.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.014831658s
    Feb 24 11:43:05.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018487981s
    Feb 24 11:43:07.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015882024s
    Feb 24 11:43:09.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.011879432s
    Feb 24 11:43:11.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.012892845s
    Feb 24 11:43:13.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01588385s
    Feb 24 11:43:15.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013389326s
    Feb 24 11:43:17.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.011676064s
    Feb 24 11:43:19.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011566388s
    Feb 24 11:43:21.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018036711s
    Feb 24 11:43:23.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01210271s
    Feb 24 11:43:25.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015685715s
    Feb 24 11:43:27.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.014185074s
    Feb 24 11:43:29.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01418292s
    Feb 24 11:43:31.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012367923s
    Feb 24 11:43:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012170361s
    Feb 24 11:43:35.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01452151s
    Feb 24 11:43:37.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011361124s
    Feb 24 11:43:39.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011660987s
    Feb 24 11:43:41.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.014661811s
    Feb 24 11:43:43.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01154366s
    Feb 24 11:43:45.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.015684821s
    Feb 24 11:43:47.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01335551s
    Feb 24 11:43:49.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011789607s
    Feb 24 11:43:51.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013614357s
    Feb 24 11:43:53.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.01203025s
    Feb 24 11:43:55.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011621806s
    Feb 24 11:43:57.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.017789692s
    Feb 24 11:43:59.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014073342s
    Feb 24 11:44:01.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011939764s
    Feb 24 11:44:03.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.012868897s
    Feb 24 11:44:05.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012741084s
    Feb 24 11:44:07.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013164381s
    Feb 24 11:44:09.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011693838s
    Feb 24 11:44:11.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01221455s
    Feb 24 11:44:13.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011897105s
    Feb 24 11:44:15.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012541458s
    Feb 24 11:44:17.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.014272216s
    Feb 24 11:44:19.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012676691s
    Feb 24 11:44:21.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01538642s
    Feb 24 11:44:23.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.015753055s
    Feb 24 11:44:25.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012215189s
    Feb 24 11:44:27.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.011800453s
    Feb 24 11:44:29.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.014263268s
    Feb 24 11:44:31.258: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018461254s
    Feb 24 11:44:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.012078348s
    Feb 24 11:44:35.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.013476851s
    Feb 24 11:44:37.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011542478s
    Feb 24 11:44:39.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.016495662s
    Feb 24 11:44:41.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012730535s
    Feb 24 11:44:43.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013122924s
    Feb 24 11:44:45.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.016562988s
    Feb 24 11:44:47.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011488765s
    Feb 24 11:44:49.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.013723549s
    Feb 24 11:44:51.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.011846263s
    Feb 24 11:44:53.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.015491706s
    Feb 24 11:44:55.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.014610975s
    Feb 24 11:44:57.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.014558069s
    Feb 24 11:44:59.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011897477s
    Feb 24 11:45:01.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.015387271s
    Feb 24 11:45:03.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.014243816s
    Feb 24 11:45:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.015169234s
    Feb 24 11:45:07.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.011793712s
    Feb 24 11:45:09.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012714774s
    Feb 24 11:45:11.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.01552595s
    Feb 24 11:45:13.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.015442066s
    Feb 24 11:45:15.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012032637s
    Feb 24 11:45:17.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.016032118s
    Feb 24 11:45:19.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.011643355s
    Feb 24 11:45:21.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015676713s
    Feb 24 11:45:23.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.011672049s
    Feb 24 11:45:25.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.015638154s
    Feb 24 11:45:27.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.01519525s
    Feb 24 11:45:29.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.015443304s
    Feb 24 11:45:31.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012510353s
    Feb 24 11:45:33.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.011830268s
    Feb 24 11:45:35.261: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.021691351s
    Feb 24 11:45:37.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.015799373s
    Feb 24 11:45:39.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.0118586s
    Feb 24 11:45:41.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.011586361s
    Feb 24 11:45:43.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.013837987s
    Feb 24 11:45:45.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.01288242s
    Feb 24 11:45:47.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.016016154s
    Feb 24 11:45:49.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.016668444s
    Feb 24 11:45:51.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.011518227s
    Feb 24 11:45:53.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.012317625s
    Feb 24 11:45:55.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.013845802s
    Feb 24 11:45:57.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.015314734s
    Feb 24 11:45:59.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.014829192s
    Feb 24 11:46:01.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.012646028s
    Feb 24 11:46:03.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.013542671s
    Feb 24 11:46:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.014985557s
    Feb 24 11:46:07.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.011667007s
    Feb 24 11:46:09.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.015536279s
    Feb 24 11:46:11.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.011549133s
    Feb 24 11:46:13.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011530524s
    Feb 24 11:46:15.266: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.027065619s
    Feb 24 11:46:17.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.011901456s
    Feb 24 11:46:19.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.018048714s
    Feb 24 11:46:21.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.011807854s
    Feb 24 11:46:23.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.016509544s
    Feb 24 11:46:25.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.011350238s
    Feb 24 11:46:27.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.014235969s
    Feb 24 11:46:29.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.017295869s
    Feb 24 11:46:31.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.013518836s
    Feb 24 11:46:33.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015804822s
    Feb 24 11:46:35.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.01504448s
    Feb 24 11:46:37.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.011865857s
    Feb 24 11:46:39.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.01577858s
    Feb 24 11:46:41.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.01240326s
    Feb 24 11:46:43.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.014000894s
    Feb 24 11:46:45.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.012401996s
    Feb 24 11:46:47.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.011815704s
    Feb 24 11:46:49.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.017148067s
    Feb 24 11:46:51.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012337115s
    Feb 24 11:46:53.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.015805005s
    Feb 24 11:46:55.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.012185755s
    Feb 24 11:46:57.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.015710867s
    Feb 24 11:46:59.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.011719832s
    Feb 24 11:47:01.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.016681548s
    Feb 24 11:47:03.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.011749134s
    Feb 24 11:47:05.254: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.015108145s
    Feb 24 11:47:07.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.012326169s
    Feb 24 11:47:09.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.015920296s
    Feb 24 11:47:11.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.012687564s
    Feb 24 11:47:13.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.014299765s
    Feb 24 11:47:15.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012723358s
    Feb 24 11:47:17.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.017956048s
    Feb 24 11:47:19.257: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018122627s
    Feb 24 11:47:21.255: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.015916532s
    Feb 24 11:47:23.252: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.013005345s
    Feb 24 11:47:25.253: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.014017566s
    Feb 24 11:47:27.250: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.011341371s
    Feb 24 11:47:29.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.011650205s
    Feb 24 11:47:31.251: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011458102s
    Feb 24 11:47:31.256: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017251363s
    STEP: removing the label kubernetes.io/e2e-0d5ff2b6-b198-4c73-8841-23f24ff721e1 off the node ip-172-31-215-124.eu-west-3.compute.internal 02/24/23 11:47:31.256
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-0d5ff2b6-b198-4c73-8841-23f24ff721e1 02/24/23 11:47:31.298
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:47:31.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2576" for this suite. 02/24/23 11:47:31.313
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:31.336
Feb 24 11:47:31.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 11:47:31.337
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:31.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:31.413
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/24/23 11:47:31.423
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/24/23 11:47:31.437
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/24/23 11:47:31.44
STEP: creating a pod to probe DNS 02/24/23 11:47:31.441
STEP: submitting the pod to kubernetes 02/24/23 11:47:31.441
Feb 24 11:47:31.455: INFO: Waiting up to 15m0s for pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373" in namespace "dns-2423" to be "running"
Feb 24 11:47:31.467: INFO: Pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373": Phase="Pending", Reason="", readiness=false. Elapsed: 11.268573ms
Feb 24 11:47:33.473: INFO: Pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373": Phase="Running", Reason="", readiness=true. Elapsed: 2.017359372s
Feb 24 11:47:33.473: INFO: Pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373" satisfied condition "running"
STEP: retrieving the pod 02/24/23 11:47:33.473
STEP: looking for the results for each expected name from probers 02/24/23 11:47:33.477
Feb 24 11:47:33.508: INFO: DNS probes using dns-2423/dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373 succeeded

STEP: deleting the pod 02/24/23 11:47:33.508
STEP: deleting the test headless service 02/24/23 11:47:33.525
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 11:47:33.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2423" for this suite. 02/24/23 11:47:33.554
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":187,"skipped":3136,"failed":0}
------------------------------
• [2.228 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:31.336
    Feb 24 11:47:31.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 11:47:31.337
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:31.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:31.413
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/24/23 11:47:31.423
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/24/23 11:47:31.437
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2423.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/24/23 11:47:31.44
    STEP: creating a pod to probe DNS 02/24/23 11:47:31.441
    STEP: submitting the pod to kubernetes 02/24/23 11:47:31.441
    Feb 24 11:47:31.455: INFO: Waiting up to 15m0s for pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373" in namespace "dns-2423" to be "running"
    Feb 24 11:47:31.467: INFO: Pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373": Phase="Pending", Reason="", readiness=false. Elapsed: 11.268573ms
    Feb 24 11:47:33.473: INFO: Pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373": Phase="Running", Reason="", readiness=true. Elapsed: 2.017359372s
    Feb 24 11:47:33.473: INFO: Pod "dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 11:47:33.473
    STEP: looking for the results for each expected name from probers 02/24/23 11:47:33.477
    Feb 24 11:47:33.508: INFO: DNS probes using dns-2423/dns-test-e7102cd4-c4c5-4ddf-8ed9-a95e65a5b373 succeeded

    STEP: deleting the pod 02/24/23 11:47:33.508
    STEP: deleting the test headless service 02/24/23 11:47:33.525
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 11:47:33.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2423" for this suite. 02/24/23 11:47:33.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:33.567
Feb 24 11:47:33.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 11:47:33.569
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:33.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:33.607
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 02/24/23 11:47:33.611
Feb 24 11:47:33.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9" in namespace "downward-api-6587" to be "Succeeded or Failed"
Feb 24 11:47:33.628: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.301271ms
Feb 24 11:47:35.634: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Running", Reason="", readiness=false. Elapsed: 2.012489739s
Feb 24 11:47:37.634: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Running", Reason="", readiness=false. Elapsed: 4.011995024s
Feb 24 11:47:39.633: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011586681s
STEP: Saw pod success 02/24/23 11:47:39.633
Feb 24 11:47:39.633: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9" satisfied condition "Succeeded or Failed"
Feb 24 11:47:39.638: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9 container client-container: <nil>
STEP: delete the pod 02/24/23 11:47:39.653
Feb 24 11:47:39.668: INFO: Waiting for pod downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9 to disappear
Feb 24 11:47:39.672: INFO: Pod downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 11:47:39.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6587" for this suite. 02/24/23 11:47:39.68
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":188,"skipped":3147,"failed":0}
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:33.567
    Feb 24 11:47:33.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 11:47:33.569
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:33.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:33.607
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 02/24/23 11:47:33.611
    Feb 24 11:47:33.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9" in namespace "downward-api-6587" to be "Succeeded or Failed"
    Feb 24 11:47:33.628: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.301271ms
    Feb 24 11:47:35.634: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Running", Reason="", readiness=false. Elapsed: 2.012489739s
    Feb 24 11:47:37.634: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Running", Reason="", readiness=false. Elapsed: 4.011995024s
    Feb 24 11:47:39.633: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011586681s
    STEP: Saw pod success 02/24/23 11:47:39.633
    Feb 24 11:47:39.633: INFO: Pod "downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9" satisfied condition "Succeeded or Failed"
    Feb 24 11:47:39.638: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9 container client-container: <nil>
    STEP: delete the pod 02/24/23 11:47:39.653
    Feb 24 11:47:39.668: INFO: Waiting for pod downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9 to disappear
    Feb 24 11:47:39.672: INFO: Pod downwardapi-volume-6e147ae7-442e-4764-a667-9c316a51acb9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 11:47:39.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6587" for this suite. 02/24/23 11:47:39.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:39.69
Feb 24 11:47:39.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:47:39.692
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:39.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:39.72
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 11:47:39.724
Feb 24 11:47:39.734: INFO: Waiting up to 5m0s for pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571" in namespace "emptydir-402" to be "Succeeded or Failed"
Feb 24 11:47:39.742: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571": Phase="Pending", Reason="", readiness=false. Elapsed: 7.205585ms
Feb 24 11:47:41.747: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012747629s
Feb 24 11:47:43.747: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012769172s
STEP: Saw pod success 02/24/23 11:47:43.747
Feb 24 11:47:43.748: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571" satisfied condition "Succeeded or Failed"
Feb 24 11:47:43.752: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571 container test-container: <nil>
STEP: delete the pod 02/24/23 11:47:43.76
Feb 24 11:47:43.772: INFO: Waiting for pod pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571 to disappear
Feb 24 11:47:43.776: INFO: Pod pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:47:43.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-402" for this suite. 02/24/23 11:47:43.783
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":189,"skipped":3160,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:39.69
    Feb 24 11:47:39.691: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:47:39.692
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:39.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:39.72
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/24/23 11:47:39.724
    Feb 24 11:47:39.734: INFO: Waiting up to 5m0s for pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571" in namespace "emptydir-402" to be "Succeeded or Failed"
    Feb 24 11:47:39.742: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571": Phase="Pending", Reason="", readiness=false. Elapsed: 7.205585ms
    Feb 24 11:47:41.747: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012747629s
    Feb 24 11:47:43.747: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012769172s
    STEP: Saw pod success 02/24/23 11:47:43.747
    Feb 24 11:47:43.748: INFO: Pod "pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571" satisfied condition "Succeeded or Failed"
    Feb 24 11:47:43.752: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:47:43.76
    Feb 24 11:47:43.772: INFO: Waiting for pod pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571 to disappear
    Feb 24 11:47:43.776: INFO: Pod pod-33ecf7d0-68e6-4422-bb4b-3a98b3bac571 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:47:43.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-402" for this suite. 02/24/23 11:47:43.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:43.794
Feb 24 11:47:43.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename proxy 02/24/23 11:47:43.795
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:43.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:43.824
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/24/23 11:47:43.845
STEP: creating replication controller proxy-service-z8mqf in namespace proxy-2111 02/24/23 11:47:43.845
I0224 11:47:43.859028      21 runners.go:193] Created replication controller with name: proxy-service-z8mqf, namespace: proxy-2111, replica count: 1
I0224 11:47:44.909800      21 runners.go:193] proxy-service-z8mqf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0224 11:47:45.910603      21 runners.go:193] proxy-service-z8mqf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 11:47:45.916: INFO: setup took 2.087476785s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/24/23 11:47:45.916
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 18.666538ms)
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 18.412098ms)
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 18.878318ms)
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 18.812777ms)
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 19.068608ms)
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 18.868814ms)
Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 18.656931ms)
Feb 24 11:47:45.936: INFO: (0) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.52897ms)
Feb 24 11:47:45.937: INFO: (0) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 20.17004ms)
Feb 24 11:47:45.937: INFO: (0) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.535268ms)
Feb 24 11:47:45.937: INFO: (0) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 20.478875ms)
Feb 24 11:47:45.938: INFO: (0) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 21.775048ms)
Feb 24 11:47:45.940: INFO: (0) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 23.998766ms)
Feb 24 11:47:45.941: INFO: (0) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 24.435502ms)
Feb 24 11:47:45.941: INFO: (0) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 25.14777ms)
Feb 24 11:47:45.941: INFO: (0) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 24.829174ms)
Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 13.784002ms)
Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.506962ms)
Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.30067ms)
Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.024229ms)
Feb 24 11:47:45.957: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.550087ms)
Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.375148ms)
Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.701956ms)
Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.896361ms)
Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.330146ms)
Feb 24 11:47:45.959: INFO: (1) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.614651ms)
Feb 24 11:47:45.959: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 16.803766ms)
Feb 24 11:47:45.961: INFO: (1) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 18.94203ms)
Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 19.811166ms)
Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 20.276607ms)
Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 19.762213ms)
Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.081664ms)
Feb 24 11:47:45.976: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.365832ms)
Feb 24 11:47:45.976: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 13.658964ms)
Feb 24 11:47:45.977: INFO: (2) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 14.422224ms)
Feb 24 11:47:45.977: INFO: (2) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 13.619526ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.213247ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.534333ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.130517ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.335549ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.846636ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 15.553579ms)
Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.101024ms)
Feb 24 11:47:45.979: INFO: (2) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.214282ms)
Feb 24 11:47:45.979: INFO: (2) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 16.555354ms)
Feb 24 11:47:45.979: INFO: (2) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 16.388235ms)
Feb 24 11:47:45.982: INFO: (2) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.057354ms)
Feb 24 11:47:45.986: INFO: (2) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 23.967262ms)
Feb 24 11:47:46.015: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 28.737416ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 28.803373ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 29.02243ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 28.971378ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 29.304349ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 29.511154ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 30.014555ms)
Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 29.828232ms)
Feb 24 11:47:46.017: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 29.881335ms)
Feb 24 11:47:46.017: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 29.925332ms)
Feb 24 11:47:46.022: INFO: (3) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 35.750385ms)
Feb 24 11:47:46.023: INFO: (3) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 35.779524ms)
Feb 24 11:47:46.023: INFO: (3) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 36.051508ms)
Feb 24 11:47:46.023: INFO: (3) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 36.456256ms)
Feb 24 11:47:46.024: INFO: (3) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 36.703505ms)
Feb 24 11:47:46.024: INFO: (3) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 36.652831ms)
Feb 24 11:47:46.038: INFO: (4) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.938207ms)
Feb 24 11:47:46.038: INFO: (4) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.013897ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 14.799772ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.433848ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.341981ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.058719ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.910279ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.959959ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.904611ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.015118ms)
Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.45199ms)
Feb 24 11:47:46.045: INFO: (4) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.272811ms)
Feb 24 11:47:46.045: INFO: (4) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.806769ms)
Feb 24 11:47:46.045: INFO: (4) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 21.035789ms)
Feb 24 11:47:46.046: INFO: (4) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 21.353332ms)
Feb 24 11:47:46.046: INFO: (4) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 21.329765ms)
Feb 24 11:47:46.059: INFO: (5) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.109696ms)
Feb 24 11:47:46.059: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.52403ms)
Feb 24 11:47:46.060: INFO: (5) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.869195ms)
Feb 24 11:47:46.060: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.210949ms)
Feb 24 11:47:46.060: INFO: (5) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.512186ms)
Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.327416ms)
Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.240369ms)
Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.285938ms)
Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.434036ms)
Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.487312ms)
Feb 24 11:47:46.062: INFO: (5) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 15.993897ms)
Feb 24 11:47:46.065: INFO: (5) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 18.760228ms)
Feb 24 11:47:46.066: INFO: (5) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.181713ms)
Feb 24 11:47:46.066: INFO: (5) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.132207ms)
Feb 24 11:47:46.067: INFO: (5) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.553085ms)
Feb 24 11:47:46.067: INFO: (5) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 20.513558ms)
Feb 24 11:47:46.081: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.220884ms)
Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 16.074153ms)
Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.260994ms)
Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 16.273605ms)
Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 16.173959ms)
Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.44143ms)
Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 16.368595ms)
Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.632188ms)
Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 16.529117ms)
Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 16.756357ms)
Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 17.116867ms)
Feb 24 11:47:46.085: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 17.719312ms)
Feb 24 11:47:46.086: INFO: (6) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.145978ms)
Feb 24 11:47:46.090: INFO: (6) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 22.508612ms)
Feb 24 11:47:46.090: INFO: (6) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 23.107111ms)
Feb 24 11:47:46.090: INFO: (6) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 22.879237ms)
Feb 24 11:47:46.104: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.248295ms)
Feb 24 11:47:46.105: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.529543ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.734465ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.760807ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.35215ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.360769ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.483179ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 15.610762ms)
Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.870267ms)
Feb 24 11:47:46.107: INFO: (7) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 16.033879ms)
Feb 24 11:47:46.111: INFO: (7) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 20.191455ms)
Feb 24 11:47:46.112: INFO: (7) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 21.154721ms)
Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 22.135477ms)
Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 21.82937ms)
Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 21.809288ms)
Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 22.589804ms)
Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.584897ms)
Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 14.930739ms)
Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.692424ms)
Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.388331ms)
Feb 24 11:47:46.129: INFO: (8) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.614832ms)
Feb 24 11:47:46.129: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.66156ms)
Feb 24 11:47:46.129: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.96596ms)
Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.870274ms)
Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.353473ms)
Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.930339ms)
Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 16.314624ms)
Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.8885ms)
Feb 24 11:47:46.132: INFO: (8) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 18.424055ms)
Feb 24 11:47:46.132: INFO: (8) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 18.238444ms)
Feb 24 11:47:46.137: INFO: (8) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 23.756078ms)
Feb 24 11:47:46.138: INFO: (8) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 23.802561ms)
Feb 24 11:47:46.152: INFO: (9) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.999193ms)
Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.500794ms)
Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.105285ms)
Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 15.366396ms)
Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.449811ms)
Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 15.81316ms)
Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.65539ms)
Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 16.290245ms)
Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 16.301242ms)
Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 16.544513ms)
Feb 24 11:47:46.155: INFO: (9) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 17.679755ms)
Feb 24 11:47:46.157: INFO: (9) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 18.767004ms)
Feb 24 11:47:46.157: INFO: (9) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 19.190738ms)
Feb 24 11:47:46.158: INFO: (9) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.508178ms)
Feb 24 11:47:46.159: INFO: (9) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 20.698823ms)
Feb 24 11:47:46.159: INFO: (9) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 20.761458ms)
Feb 24 11:47:46.174: INFO: (10) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.787171ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.30184ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.806594ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.461428ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.423028ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 15.235197ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.096614ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.065628ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 15.646512ms)
Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.285649ms)
Feb 24 11:47:46.176: INFO: (10) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 16.829264ms)
Feb 24 11:47:46.178: INFO: (10) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 17.956636ms)
Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 20.385379ms)
Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.223908ms)
Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.663232ms)
Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.461378ms)
Feb 24 11:47:46.191: INFO: (11) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 11.024196ms)
Feb 24 11:47:46.194: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.725236ms)
Feb 24 11:47:46.194: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.712179ms)
Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 13.928015ms)
Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.23778ms)
Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.785093ms)
Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.162567ms)
Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.679908ms)
Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.169464ms)
Feb 24 11:47:46.196: INFO: (11) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.885494ms)
Feb 24 11:47:46.197: INFO: (11) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 16.112614ms)
Feb 24 11:47:46.199: INFO: (11) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 18.065623ms)
Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.026313ms)
Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 19.15957ms)
Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.113531ms)
Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 19.507488ms)
Feb 24 11:47:46.212: INFO: (12) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 11.477608ms)
Feb 24 11:47:46.212: INFO: (12) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 11.682038ms)
Feb 24 11:47:46.212: INFO: (12) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 11.775173ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 12.218062ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 11.917117ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 12.19407ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.536538ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 12.339543ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 12.243931ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.201184ms)
Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 13.180909ms)
Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 16.543303ms)
Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.678244ms)
Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 16.758117ms)
Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 16.920924ms)
Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 17.144374ms)
Feb 24 11:47:46.229: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 11.828369ms)
Feb 24 11:47:46.229: INFO: (13) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 11.882705ms)
Feb 24 11:47:46.229: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 11.937096ms)
Feb 24 11:47:46.230: INFO: (13) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 12.986311ms)
Feb 24 11:47:46.231: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.073248ms)
Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.453007ms)
Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.13552ms)
Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.937036ms)
Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.220636ms)
Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.019915ms)
Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 14.552134ms)
Feb 24 11:47:46.236: INFO: (13) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 18.20806ms)
Feb 24 11:47:46.236: INFO: (13) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 18.431248ms)
Feb 24 11:47:46.237: INFO: (13) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 18.721913ms)
Feb 24 11:47:46.237: INFO: (13) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 18.693509ms)
Feb 24 11:47:46.237: INFO: (13) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 18.597906ms)
Feb 24 11:47:46.243: INFO: (14) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 6.428871ms)
Feb 24 11:47:46.248: INFO: (14) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 10.534056ms)
Feb 24 11:47:46.248: INFO: (14) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 10.892614ms)
Feb 24 11:47:46.249: INFO: (14) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 11.716355ms)
Feb 24 11:47:46.249: INFO: (14) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 11.787397ms)
Feb 24 11:47:46.250: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.897246ms)
Feb 24 11:47:46.250: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 12.762177ms)
Feb 24 11:47:46.251: INFO: (14) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 13.447509ms)
Feb 24 11:47:46.251: INFO: (14) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.813052ms)
Feb 24 11:47:46.251: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.475829ms)
Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 15.683954ms)
Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 15.356038ms)
Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.274172ms)
Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 15.537849ms)
Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.216619ms)
Feb 24 11:47:46.254: INFO: (14) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 16.088639ms)
Feb 24 11:47:46.259: INFO: (15) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 5.353444ms)
Feb 24 11:47:46.263: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 9.251175ms)
Feb 24 11:47:46.266: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 12.107161ms)
Feb 24 11:47:46.267: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.790591ms)
Feb 24 11:47:46.267: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 12.75068ms)
Feb 24 11:47:46.267: INFO: (15) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 13.099228ms)
Feb 24 11:47:46.268: INFO: (15) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 13.032325ms)
Feb 24 11:47:46.268: INFO: (15) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 13.262888ms)
Feb 24 11:47:46.268: INFO: (15) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.936387ms)
Feb 24 11:47:46.269: INFO: (15) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.215516ms)
Feb 24 11:47:46.273: INFO: (15) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 18.813671ms)
Feb 24 11:47:46.274: INFO: (15) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.597168ms)
Feb 24 11:47:46.274: INFO: (15) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 19.737702ms)
Feb 24 11:47:46.274: INFO: (15) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.766453ms)
Feb 24 11:47:46.275: INFO: (15) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 20.23848ms)
Feb 24 11:47:46.275: INFO: (15) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.096709ms)
Feb 24 11:47:46.296: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 21.428097ms)
Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 20.74921ms)
Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 21.744567ms)
Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 22.12422ms)
Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 22.040944ms)
Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 22.454575ms)
Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 22.176807ms)
Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 21.072726ms)
Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 21.119745ms)
Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 21.749989ms)
Feb 24 11:47:46.299: INFO: (16) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 23.047308ms)
Feb 24 11:47:46.300: INFO: (16) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 24.808298ms)
Feb 24 11:47:46.301: INFO: (16) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 24.537953ms)
Feb 24 11:47:46.301: INFO: (16) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 24.595728ms)
Feb 24 11:47:46.301: INFO: (16) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 24.61069ms)
Feb 24 11:47:46.302: INFO: (16) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 25.040314ms)
Feb 24 11:47:46.319: INFO: (17) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.995413ms)
Feb 24 11:47:46.321: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 19.642753ms)
Feb 24 11:47:46.321: INFO: (17) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 18.836124ms)
Feb 24 11:47:46.322: INFO: (17) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 19.665661ms)
Feb 24 11:47:46.322: INFO: (17) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 19.84576ms)
Feb 24 11:47:46.322: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 19.787808ms)
Feb 24 11:47:46.323: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 20.964241ms)
Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 21.038109ms)
Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 21.755747ms)
Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 21.42142ms)
Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 21.299106ms)
Feb 24 11:47:46.326: INFO: (17) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 24.077632ms)
Feb 24 11:47:46.328: INFO: (17) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 25.40175ms)
Feb 24 11:47:46.328: INFO: (17) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 26.412344ms)
Feb 24 11:47:46.329: INFO: (17) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 27.379144ms)
Feb 24 11:47:46.330: INFO: (17) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 27.736973ms)
Feb 24 11:47:46.345: INFO: (18) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.884004ms)
Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 15.718783ms)
Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.112642ms)
Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.203081ms)
Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.528704ms)
Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.218999ms)
Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.896553ms)
Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.913852ms)
Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.552239ms)
Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.616097ms)
Feb 24 11:47:46.348: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 17.404229ms)
Feb 24 11:47:46.349: INFO: (18) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 18.419001ms)
Feb 24 11:47:46.351: INFO: (18) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 20.052598ms)
Feb 24 11:47:46.351: INFO: (18) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.996556ms)
Feb 24 11:47:46.352: INFO: (18) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 21.221164ms)
Feb 24 11:47:46.352: INFO: (18) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 21.549958ms)
Feb 24 11:47:46.366: INFO: (19) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.212382ms)
Feb 24 11:47:46.366: INFO: (19) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.799304ms)
Feb 24 11:47:46.366: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.963034ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.031554ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.754766ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.016285ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.562341ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.313009ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.665817ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 15.188718ms)
Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.335305ms)
Feb 24 11:47:46.373: INFO: (19) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.955298ms)
Feb 24 11:47:46.373: INFO: (19) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 20.637276ms)
Feb 24 11:47:46.374: INFO: (19) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 21.314817ms)
Feb 24 11:47:46.374: INFO: (19) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.913777ms)
Feb 24 11:47:46.374: INFO: (19) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 21.435972ms)
STEP: deleting ReplicationController proxy-service-z8mqf in namespace proxy-2111, will wait for the garbage collector to delete the pods 02/24/23 11:47:46.374
Feb 24 11:47:46.437: INFO: Deleting ReplicationController proxy-service-z8mqf took: 8.497079ms
Feb 24 11:47:46.538: INFO: Terminating ReplicationController proxy-service-z8mqf pods took: 100.672076ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 24 11:47:49.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2111" for this suite. 02/24/23 11:47:49.454
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":190,"skipped":3167,"failed":0}
------------------------------
• [SLOW TEST] [5.673 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:43.794
    Feb 24 11:47:43.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename proxy 02/24/23 11:47:43.795
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:43.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:43.824
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/24/23 11:47:43.845
    STEP: creating replication controller proxy-service-z8mqf in namespace proxy-2111 02/24/23 11:47:43.845
    I0224 11:47:43.859028      21 runners.go:193] Created replication controller with name: proxy-service-z8mqf, namespace: proxy-2111, replica count: 1
    I0224 11:47:44.909800      21 runners.go:193] proxy-service-z8mqf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0224 11:47:45.910603      21 runners.go:193] proxy-service-z8mqf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 11:47:45.916: INFO: setup took 2.087476785s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/24/23 11:47:45.916
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 18.666538ms)
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 18.412098ms)
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 18.878318ms)
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 18.812777ms)
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 19.068608ms)
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 18.868814ms)
    Feb 24 11:47:45.935: INFO: (0) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 18.656931ms)
    Feb 24 11:47:45.936: INFO: (0) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.52897ms)
    Feb 24 11:47:45.937: INFO: (0) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 20.17004ms)
    Feb 24 11:47:45.937: INFO: (0) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.535268ms)
    Feb 24 11:47:45.937: INFO: (0) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 20.478875ms)
    Feb 24 11:47:45.938: INFO: (0) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 21.775048ms)
    Feb 24 11:47:45.940: INFO: (0) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 23.998766ms)
    Feb 24 11:47:45.941: INFO: (0) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 24.435502ms)
    Feb 24 11:47:45.941: INFO: (0) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 25.14777ms)
    Feb 24 11:47:45.941: INFO: (0) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 24.829174ms)
    Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 13.784002ms)
    Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.506962ms)
    Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.30067ms)
    Feb 24 11:47:45.955: INFO: (1) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.024229ms)
    Feb 24 11:47:45.957: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.550087ms)
    Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.375148ms)
    Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.701956ms)
    Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.896361ms)
    Feb 24 11:47:45.958: INFO: (1) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.330146ms)
    Feb 24 11:47:45.959: INFO: (1) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.614651ms)
    Feb 24 11:47:45.959: INFO: (1) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 16.803766ms)
    Feb 24 11:47:45.961: INFO: (1) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 18.94203ms)
    Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 19.811166ms)
    Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 20.276607ms)
    Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 19.762213ms)
    Feb 24 11:47:45.962: INFO: (1) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.081664ms)
    Feb 24 11:47:45.976: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.365832ms)
    Feb 24 11:47:45.976: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 13.658964ms)
    Feb 24 11:47:45.977: INFO: (2) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 14.422224ms)
    Feb 24 11:47:45.977: INFO: (2) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 13.619526ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.213247ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.534333ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.130517ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.335549ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.846636ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 15.553579ms)
    Feb 24 11:47:45.978: INFO: (2) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.101024ms)
    Feb 24 11:47:45.979: INFO: (2) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.214282ms)
    Feb 24 11:47:45.979: INFO: (2) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 16.555354ms)
    Feb 24 11:47:45.979: INFO: (2) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 16.388235ms)
    Feb 24 11:47:45.982: INFO: (2) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.057354ms)
    Feb 24 11:47:45.986: INFO: (2) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 23.967262ms)
    Feb 24 11:47:46.015: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 28.737416ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 28.803373ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 29.02243ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 28.971378ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 29.304349ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 29.511154ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 30.014555ms)
    Feb 24 11:47:46.016: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 29.828232ms)
    Feb 24 11:47:46.017: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 29.881335ms)
    Feb 24 11:47:46.017: INFO: (3) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 29.925332ms)
    Feb 24 11:47:46.022: INFO: (3) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 35.750385ms)
    Feb 24 11:47:46.023: INFO: (3) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 35.779524ms)
    Feb 24 11:47:46.023: INFO: (3) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 36.051508ms)
    Feb 24 11:47:46.023: INFO: (3) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 36.456256ms)
    Feb 24 11:47:46.024: INFO: (3) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 36.703505ms)
    Feb 24 11:47:46.024: INFO: (3) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 36.652831ms)
    Feb 24 11:47:46.038: INFO: (4) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.938207ms)
    Feb 24 11:47:46.038: INFO: (4) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.013897ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 14.799772ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.433848ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.341981ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.058719ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.910279ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.959959ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.904611ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.015118ms)
    Feb 24 11:47:46.039: INFO: (4) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.45199ms)
    Feb 24 11:47:46.045: INFO: (4) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.272811ms)
    Feb 24 11:47:46.045: INFO: (4) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.806769ms)
    Feb 24 11:47:46.045: INFO: (4) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 21.035789ms)
    Feb 24 11:47:46.046: INFO: (4) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 21.353332ms)
    Feb 24 11:47:46.046: INFO: (4) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 21.329765ms)
    Feb 24 11:47:46.059: INFO: (5) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.109696ms)
    Feb 24 11:47:46.059: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.52403ms)
    Feb 24 11:47:46.060: INFO: (5) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.869195ms)
    Feb 24 11:47:46.060: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.210949ms)
    Feb 24 11:47:46.060: INFO: (5) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.512186ms)
    Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.327416ms)
    Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.240369ms)
    Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.285938ms)
    Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.434036ms)
    Feb 24 11:47:46.061: INFO: (5) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.487312ms)
    Feb 24 11:47:46.062: INFO: (5) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 15.993897ms)
    Feb 24 11:47:46.065: INFO: (5) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 18.760228ms)
    Feb 24 11:47:46.066: INFO: (5) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.181713ms)
    Feb 24 11:47:46.066: INFO: (5) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.132207ms)
    Feb 24 11:47:46.067: INFO: (5) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.553085ms)
    Feb 24 11:47:46.067: INFO: (5) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 20.513558ms)
    Feb 24 11:47:46.081: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.220884ms)
    Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 16.074153ms)
    Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.260994ms)
    Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 16.273605ms)
    Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 16.173959ms)
    Feb 24 11:47:46.083: INFO: (6) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.44143ms)
    Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 16.368595ms)
    Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.632188ms)
    Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 16.529117ms)
    Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 16.756357ms)
    Feb 24 11:47:46.084: INFO: (6) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 17.116867ms)
    Feb 24 11:47:46.085: INFO: (6) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 17.719312ms)
    Feb 24 11:47:46.086: INFO: (6) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.145978ms)
    Feb 24 11:47:46.090: INFO: (6) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 22.508612ms)
    Feb 24 11:47:46.090: INFO: (6) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 23.107111ms)
    Feb 24 11:47:46.090: INFO: (6) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 22.879237ms)
    Feb 24 11:47:46.104: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.248295ms)
    Feb 24 11:47:46.105: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.529543ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.734465ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.760807ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.35215ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.360769ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.483179ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 15.610762ms)
    Feb 24 11:47:46.106: INFO: (7) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.870267ms)
    Feb 24 11:47:46.107: INFO: (7) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 16.033879ms)
    Feb 24 11:47:46.111: INFO: (7) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 20.191455ms)
    Feb 24 11:47:46.112: INFO: (7) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 21.154721ms)
    Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 22.135477ms)
    Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 21.82937ms)
    Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 21.809288ms)
    Feb 24 11:47:46.113: INFO: (7) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 22.589804ms)
    Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 14.584897ms)
    Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 14.930739ms)
    Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.692424ms)
    Feb 24 11:47:46.128: INFO: (8) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.388331ms)
    Feb 24 11:47:46.129: INFO: (8) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.614832ms)
    Feb 24 11:47:46.129: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.66156ms)
    Feb 24 11:47:46.129: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.96596ms)
    Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.870274ms)
    Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.353473ms)
    Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.930339ms)
    Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 16.314624ms)
    Feb 24 11:47:46.130: INFO: (8) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.8885ms)
    Feb 24 11:47:46.132: INFO: (8) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 18.424055ms)
    Feb 24 11:47:46.132: INFO: (8) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 18.238444ms)
    Feb 24 11:47:46.137: INFO: (8) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 23.756078ms)
    Feb 24 11:47:46.138: INFO: (8) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 23.802561ms)
    Feb 24 11:47:46.152: INFO: (9) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.999193ms)
    Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.500794ms)
    Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.105285ms)
    Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 15.366396ms)
    Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.449811ms)
    Feb 24 11:47:46.153: INFO: (9) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 15.81316ms)
    Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.65539ms)
    Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 16.290245ms)
    Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 16.301242ms)
    Feb 24 11:47:46.154: INFO: (9) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 16.544513ms)
    Feb 24 11:47:46.155: INFO: (9) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 17.679755ms)
    Feb 24 11:47:46.157: INFO: (9) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 18.767004ms)
    Feb 24 11:47:46.157: INFO: (9) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 19.190738ms)
    Feb 24 11:47:46.158: INFO: (9) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.508178ms)
    Feb 24 11:47:46.159: INFO: (9) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 20.698823ms)
    Feb 24 11:47:46.159: INFO: (9) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 20.761458ms)
    Feb 24 11:47:46.174: INFO: (10) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.787171ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.30184ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.806594ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.461428ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.423028ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 15.235197ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.096614ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.065628ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 15.646512ms)
    Feb 24 11:47:46.175: INFO: (10) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.285649ms)
    Feb 24 11:47:46.176: INFO: (10) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 16.829264ms)
    Feb 24 11:47:46.178: INFO: (10) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 17.956636ms)
    Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 20.385379ms)
    Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.223908ms)
    Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.663232ms)
    Feb 24 11:47:46.180: INFO: (10) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.461378ms)
    Feb 24 11:47:46.191: INFO: (11) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 11.024196ms)
    Feb 24 11:47:46.194: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.725236ms)
    Feb 24 11:47:46.194: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.712179ms)
    Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 13.928015ms)
    Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.23778ms)
    Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.785093ms)
    Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.162567ms)
    Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.679908ms)
    Feb 24 11:47:46.195: INFO: (11) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.169464ms)
    Feb 24 11:47:46.196: INFO: (11) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.885494ms)
    Feb 24 11:47:46.197: INFO: (11) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 16.112614ms)
    Feb 24 11:47:46.199: INFO: (11) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 18.065623ms)
    Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.026313ms)
    Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 19.15957ms)
    Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.113531ms)
    Feb 24 11:47:46.200: INFO: (11) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 19.507488ms)
    Feb 24 11:47:46.212: INFO: (12) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 11.477608ms)
    Feb 24 11:47:46.212: INFO: (12) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 11.682038ms)
    Feb 24 11:47:46.212: INFO: (12) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 11.775173ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 12.218062ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 11.917117ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 12.19407ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.536538ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 12.339543ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 12.243931ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.201184ms)
    Feb 24 11:47:46.213: INFO: (12) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 13.180909ms)
    Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 16.543303ms)
    Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.678244ms)
    Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 16.758117ms)
    Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 16.920924ms)
    Feb 24 11:47:46.217: INFO: (12) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 17.144374ms)
    Feb 24 11:47:46.229: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 11.828369ms)
    Feb 24 11:47:46.229: INFO: (13) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 11.882705ms)
    Feb 24 11:47:46.229: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 11.937096ms)
    Feb 24 11:47:46.230: INFO: (13) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 12.986311ms)
    Feb 24 11:47:46.231: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.073248ms)
    Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.453007ms)
    Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.13552ms)
    Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.937036ms)
    Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.220636ms)
    Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.019915ms)
    Feb 24 11:47:46.232: INFO: (13) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 14.552134ms)
    Feb 24 11:47:46.236: INFO: (13) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 18.20806ms)
    Feb 24 11:47:46.236: INFO: (13) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 18.431248ms)
    Feb 24 11:47:46.237: INFO: (13) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 18.721913ms)
    Feb 24 11:47:46.237: INFO: (13) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 18.693509ms)
    Feb 24 11:47:46.237: INFO: (13) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 18.597906ms)
    Feb 24 11:47:46.243: INFO: (14) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 6.428871ms)
    Feb 24 11:47:46.248: INFO: (14) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 10.534056ms)
    Feb 24 11:47:46.248: INFO: (14) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 10.892614ms)
    Feb 24 11:47:46.249: INFO: (14) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 11.716355ms)
    Feb 24 11:47:46.249: INFO: (14) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 11.787397ms)
    Feb 24 11:47:46.250: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.897246ms)
    Feb 24 11:47:46.250: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 12.762177ms)
    Feb 24 11:47:46.251: INFO: (14) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 13.447509ms)
    Feb 24 11:47:46.251: INFO: (14) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.813052ms)
    Feb 24 11:47:46.251: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.475829ms)
    Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 15.683954ms)
    Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 15.356038ms)
    Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.274172ms)
    Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 15.537849ms)
    Feb 24 11:47:46.253: INFO: (14) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.216619ms)
    Feb 24 11:47:46.254: INFO: (14) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 16.088639ms)
    Feb 24 11:47:46.259: INFO: (15) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 5.353444ms)
    Feb 24 11:47:46.263: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 9.251175ms)
    Feb 24 11:47:46.266: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 12.107161ms)
    Feb 24 11:47:46.267: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.790591ms)
    Feb 24 11:47:46.267: INFO: (15) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 12.75068ms)
    Feb 24 11:47:46.267: INFO: (15) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 13.099228ms)
    Feb 24 11:47:46.268: INFO: (15) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 13.032325ms)
    Feb 24 11:47:46.268: INFO: (15) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 13.262888ms)
    Feb 24 11:47:46.268: INFO: (15) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.936387ms)
    Feb 24 11:47:46.269: INFO: (15) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 14.215516ms)
    Feb 24 11:47:46.273: INFO: (15) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 18.813671ms)
    Feb 24 11:47:46.274: INFO: (15) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.597168ms)
    Feb 24 11:47:46.274: INFO: (15) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 19.737702ms)
    Feb 24 11:47:46.274: INFO: (15) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 19.766453ms)
    Feb 24 11:47:46.275: INFO: (15) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 20.23848ms)
    Feb 24 11:47:46.275: INFO: (15) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 20.096709ms)
    Feb 24 11:47:46.296: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 21.428097ms)
    Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 20.74921ms)
    Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 21.744567ms)
    Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 22.12422ms)
    Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 22.040944ms)
    Feb 24 11:47:46.297: INFO: (16) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 22.454575ms)
    Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 22.176807ms)
    Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 21.072726ms)
    Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 21.119745ms)
    Feb 24 11:47:46.298: INFO: (16) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 21.749989ms)
    Feb 24 11:47:46.299: INFO: (16) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 23.047308ms)
    Feb 24 11:47:46.300: INFO: (16) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 24.808298ms)
    Feb 24 11:47:46.301: INFO: (16) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 24.537953ms)
    Feb 24 11:47:46.301: INFO: (16) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 24.595728ms)
    Feb 24 11:47:46.301: INFO: (16) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 24.61069ms)
    Feb 24 11:47:46.302: INFO: (16) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 25.040314ms)
    Feb 24 11:47:46.319: INFO: (17) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.995413ms)
    Feb 24 11:47:46.321: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 19.642753ms)
    Feb 24 11:47:46.321: INFO: (17) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 18.836124ms)
    Feb 24 11:47:46.322: INFO: (17) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 19.665661ms)
    Feb 24 11:47:46.322: INFO: (17) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 19.84576ms)
    Feb 24 11:47:46.322: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 19.787808ms)
    Feb 24 11:47:46.323: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 20.964241ms)
    Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 21.038109ms)
    Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 21.755747ms)
    Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 21.42142ms)
    Feb 24 11:47:46.324: INFO: (17) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 21.299106ms)
    Feb 24 11:47:46.326: INFO: (17) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 24.077632ms)
    Feb 24 11:47:46.328: INFO: (17) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 25.40175ms)
    Feb 24 11:47:46.328: INFO: (17) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 26.412344ms)
    Feb 24 11:47:46.329: INFO: (17) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 27.379144ms)
    Feb 24 11:47:46.330: INFO: (17) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 27.736973ms)
    Feb 24 11:47:46.345: INFO: (18) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 14.884004ms)
    Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 15.718783ms)
    Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 15.112642ms)
    Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 15.203081ms)
    Feb 24 11:47:46.346: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 16.528704ms)
    Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 16.218999ms)
    Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 16.896553ms)
    Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 15.913852ms)
    Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 15.552239ms)
    Feb 24 11:47:46.347: INFO: (18) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 15.616097ms)
    Feb 24 11:47:46.348: INFO: (18) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 17.404229ms)
    Feb 24 11:47:46.349: INFO: (18) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 18.419001ms)
    Feb 24 11:47:46.351: INFO: (18) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 20.052598ms)
    Feb 24 11:47:46.351: INFO: (18) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 20.996556ms)
    Feb 24 11:47:46.352: INFO: (18) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 21.221164ms)
    Feb 24 11:47:46.352: INFO: (18) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 21.549958ms)
    Feb 24 11:47:46.366: INFO: (19) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 13.212382ms)
    Feb 24 11:47:46.366: INFO: (19) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 13.799304ms)
    Feb 24 11:47:46.366: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:160/proxy/: foo (200; 12.963034ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:443/proxy/tlsrewritem... (200; 14.031554ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq/proxy/rewriteme">test</a> (200; 13.754766ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">test<... (200; 14.016285ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/: <a href="/api/v1/namespaces/proxy-2111/pods/http:proxy-service-z8mqf-j7kqq:1080/proxy/rewriteme">... (200; 14.562341ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/proxy-service-z8mqf-j7kqq:162/proxy/: bar (200; 14.313009ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:460/proxy/: tls baz (200; 14.665817ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname1/proxy/: foo (200; 15.188718ms)
    Feb 24 11:47:46.367: INFO: (19) /api/v1/namespaces/proxy-2111/pods/https:proxy-service-z8mqf-j7kqq:462/proxy/: tls qux (200; 15.335305ms)
    Feb 24 11:47:46.373: INFO: (19) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname2/proxy/: bar (200; 19.955298ms)
    Feb 24 11:47:46.373: INFO: (19) /api/v1/namespaces/proxy-2111/services/proxy-service-z8mqf:portname2/proxy/: bar (200; 20.637276ms)
    Feb 24 11:47:46.374: INFO: (19) /api/v1/namespaces/proxy-2111/services/http:proxy-service-z8mqf:portname1/proxy/: foo (200; 21.314817ms)
    Feb 24 11:47:46.374: INFO: (19) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname2/proxy/: tls qux (200; 20.913777ms)
    Feb 24 11:47:46.374: INFO: (19) /api/v1/namespaces/proxy-2111/services/https:proxy-service-z8mqf:tlsportname1/proxy/: tls baz (200; 21.435972ms)
    STEP: deleting ReplicationController proxy-service-z8mqf in namespace proxy-2111, will wait for the garbage collector to delete the pods 02/24/23 11:47:46.374
    Feb 24 11:47:46.437: INFO: Deleting ReplicationController proxy-service-z8mqf took: 8.497079ms
    Feb 24 11:47:46.538: INFO: Terminating ReplicationController proxy-service-z8mqf pods took: 100.672076ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 24 11:47:49.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2111" for this suite. 02/24/23 11:47:49.454
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:49.471
Feb 24 11:47:49.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 11:47:49.472
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:49.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:49.507
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 02/24/23 11:47:49.518
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 11:47:49.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3603" for this suite. 02/24/23 11:47:49.531
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":191,"skipped":3169,"failed":0}
------------------------------
• [0.071 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:49.471
    Feb 24 11:47:49.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 11:47:49.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:49.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:49.507
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 02/24/23 11:47:49.518
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 11:47:49.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3603" for this suite. 02/24/23 11:47:49.531
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:49.543
Feb 24 11:47:49.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 11:47:49.55
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:49.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:49.593
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/24/23 11:47:49.597
STEP: delete the rc 02/24/23 11:47:54.608
STEP: wait for all pods to be garbage collected 02/24/23 11:47:54.618
STEP: Gathering metrics 02/24/23 11:47:59.628
Feb 24 11:47:59.664: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 11:47:59.669: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.704372ms
Feb 24 11:47:59.669: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 11:47:59.669: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 11:47:59.756: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 11:47:59.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4365" for this suite. 02/24/23 11:47:59.766
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":192,"skipped":3175,"failed":0}
------------------------------
• [SLOW TEST] [10.232 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:49.543
    Feb 24 11:47:49.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 11:47:49.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:49.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:49.593
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/24/23 11:47:49.597
    STEP: delete the rc 02/24/23 11:47:54.608
    STEP: wait for all pods to be garbage collected 02/24/23 11:47:54.618
    STEP: Gathering metrics 02/24/23 11:47:59.628
    Feb 24 11:47:59.664: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 11:47:59.669: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.704372ms
    Feb 24 11:47:59.669: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 11:47:59.669: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 11:47:59.756: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 11:47:59.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4365" for this suite. 02/24/23 11:47:59.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:47:59.776
Feb 24 11:47:59.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename security-context-test 02/24/23 11:47:59.781
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:59.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:59.81
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Feb 24 11:47:59.824: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286" in namespace "security-context-test-9578" to be "Succeeded or Failed"
Feb 24 11:47:59.829: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286": Phase="Pending", Reason="", readiness=false. Elapsed: 4.602527ms
Feb 24 11:48:01.835: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01102247s
Feb 24 11:48:03.835: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010306071s
Feb 24 11:48:03.835: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 24 11:48:03.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9578" for this suite. 02/24/23 11:48:03.848
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":193,"skipped":3180,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:47:59.776
    Feb 24 11:47:59.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename security-context-test 02/24/23 11:47:59.781
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:47:59.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:47:59.81
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Feb 24 11:47:59.824: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286" in namespace "security-context-test-9578" to be "Succeeded or Failed"
    Feb 24 11:47:59.829: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286": Phase="Pending", Reason="", readiness=false. Elapsed: 4.602527ms
    Feb 24 11:48:01.835: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01102247s
    Feb 24 11:48:03.835: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010306071s
    Feb 24 11:48:03.835: INFO: Pod "busybox-readonly-false-204fbc08-5374-431c-9093-5a179c1c0286" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 24 11:48:03.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9578" for this suite. 02/24/23 11:48:03.848
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:48:03.865
Feb 24 11:48:03.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 11:48:03.867
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:48:04.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:48:04.137
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-ba847c71-f998-4ca4-a854-a621c3fb32d7 02/24/23 11:48:04.141
STEP: Creating secret with name secret-projected-all-test-volume-6cca4042-68ff-4710-bc58-f38b18c685f9 02/24/23 11:48:04.151
STEP: Creating a pod to test Check all projections for projected volume plugin 02/24/23 11:48:04.158
Feb 24 11:48:04.174: INFO: Waiting up to 5m0s for pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0" in namespace "projected-5192" to be "Succeeded or Failed"
Feb 24 11:48:04.181: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.63269ms
Feb 24 11:48:06.188: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013232449s
Feb 24 11:48:08.187: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01269821s
STEP: Saw pod success 02/24/23 11:48:08.187
Feb 24 11:48:08.188: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0" satisfied condition "Succeeded or Failed"
Feb 24 11:48:08.192: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0 container projected-all-volume-test: <nil>
STEP: delete the pod 02/24/23 11:48:08.201
Feb 24 11:48:08.218: INFO: Waiting for pod projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0 to disappear
Feb 24 11:48:08.222: INFO: Pod projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Feb 24 11:48:08.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5192" for this suite. 02/24/23 11:48:08.229
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":194,"skipped":3183,"failed":0}
------------------------------
• [4.377 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:48:03.865
    Feb 24 11:48:03.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 11:48:03.867
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:48:04.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:48:04.137
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-ba847c71-f998-4ca4-a854-a621c3fb32d7 02/24/23 11:48:04.141
    STEP: Creating secret with name secret-projected-all-test-volume-6cca4042-68ff-4710-bc58-f38b18c685f9 02/24/23 11:48:04.151
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/24/23 11:48:04.158
    Feb 24 11:48:04.174: INFO: Waiting up to 5m0s for pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0" in namespace "projected-5192" to be "Succeeded or Failed"
    Feb 24 11:48:04.181: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.63269ms
    Feb 24 11:48:06.188: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013232449s
    Feb 24 11:48:08.187: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01269821s
    STEP: Saw pod success 02/24/23 11:48:08.187
    Feb 24 11:48:08.188: INFO: Pod "projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0" satisfied condition "Succeeded or Failed"
    Feb 24 11:48:08.192: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0 container projected-all-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:48:08.201
    Feb 24 11:48:08.218: INFO: Waiting for pod projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0 to disappear
    Feb 24 11:48:08.222: INFO: Pod projected-volume-593c010f-abf7-4864-9286-0ead1ae9b3a0 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Feb 24 11:48:08.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5192" for this suite. 02/24/23 11:48:08.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:48:08.248
Feb 24 11:48:08.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 11:48:08.249
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:48:08.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:48:08.277
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3 in namespace container-probe-573 02/24/23 11:48:08.282
Feb 24 11:48:08.296: INFO: Waiting up to 5m0s for pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3" in namespace "container-probe-573" to be "not pending"
Feb 24 11:48:08.301: INFO: Pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392801ms
Feb 24 11:48:10.307: INFO: Pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011020151s
Feb 24 11:48:10.307: INFO: Pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3" satisfied condition "not pending"
Feb 24 11:48:10.307: INFO: Started pod liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3 in namespace container-probe-573
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:48:10.307
Feb 24 11:48:10.317: INFO: Initial restart count of pod liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3 is 0
STEP: deleting the pod 02/24/23 11:52:11.128
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 11:52:11.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-573" for this suite. 02/24/23 11:52:11.156
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":195,"skipped":3223,"failed":0}
------------------------------
• [SLOW TEST] [242.917 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:48:08.248
    Feb 24 11:48:08.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:48:08.249
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:48:08.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:48:08.277
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3 in namespace container-probe-573 02/24/23 11:48:08.282
    Feb 24 11:48:08.296: INFO: Waiting up to 5m0s for pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3" in namespace "container-probe-573" to be "not pending"
    Feb 24 11:48:08.301: INFO: Pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392801ms
    Feb 24 11:48:10.307: INFO: Pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3": Phase="Running", Reason="", readiness=true. Elapsed: 2.011020151s
    Feb 24 11:48:10.307: INFO: Pod "liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3" satisfied condition "not pending"
    Feb 24 11:48:10.307: INFO: Started pod liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3 in namespace container-probe-573
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:48:10.307
    Feb 24 11:48:10.317: INFO: Initial restart count of pod liveness-b6ff0de9-9959-4a7b-973e-1bfd1b53fac3 is 0
    STEP: deleting the pod 02/24/23 11:52:11.128
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 11:52:11.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-573" for this suite. 02/24/23 11:52:11.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:52:11.167
Feb 24 11:52:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 11:52:11.168
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:11.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:11.2
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 02/24/23 11:52:11.205
Feb 24 11:52:11.215: INFO: Waiting up to 5m0s for pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9" in namespace "emptydir-1011" to be "Succeeded or Failed"
Feb 24 11:52:11.221: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969732ms
Feb 24 11:52:13.228: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9": Phase="Running", Reason="", readiness=false. Elapsed: 2.013211793s
Feb 24 11:52:15.227: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012145593s
STEP: Saw pod success 02/24/23 11:52:15.228
Feb 24 11:52:15.228: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9" satisfied condition "Succeeded or Failed"
Feb 24 11:52:15.232: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9 container test-container: <nil>
STEP: delete the pod 02/24/23 11:52:15.246
Feb 24 11:52:15.263: INFO: Waiting for pod pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9 to disappear
Feb 24 11:52:15.270: INFO: Pod pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 11:52:15.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1011" for this suite. 02/24/23 11:52:15.278
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":196,"skipped":3253,"failed":0}
------------------------------
• [4.120 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:52:11.167
    Feb 24 11:52:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 11:52:11.168
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:11.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:11.2
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/24/23 11:52:11.205
    Feb 24 11:52:11.215: INFO: Waiting up to 5m0s for pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9" in namespace "emptydir-1011" to be "Succeeded or Failed"
    Feb 24 11:52:11.221: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.969732ms
    Feb 24 11:52:13.228: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9": Phase="Running", Reason="", readiness=false. Elapsed: 2.013211793s
    Feb 24 11:52:15.227: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012145593s
    STEP: Saw pod success 02/24/23 11:52:15.228
    Feb 24 11:52:15.228: INFO: Pod "pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9" satisfied condition "Succeeded or Failed"
    Feb 24 11:52:15.232: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9 container test-container: <nil>
    STEP: delete the pod 02/24/23 11:52:15.246
    Feb 24 11:52:15.263: INFO: Waiting for pod pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9 to disappear
    Feb 24 11:52:15.270: INFO: Pod pod-e6728d28-7305-4aed-a8f5-92a07eb7b9a9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 11:52:15.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1011" for this suite. 02/24/23 11:52:15.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:52:15.292
Feb 24 11:52:15.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:52:15.293
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:15.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:15.331
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:52:15.354
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:52:15.865
STEP: Deploying the webhook pod 02/24/23 11:52:15.876
STEP: Wait for the deployment to be ready 02/24/23 11:52:15.893
Feb 24 11:52:15.902: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:52:17.919
STEP: Verifying the service has paired with the endpoint 02/24/23 11:52:17.938
Feb 24 11:52:18.939: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Feb 24 11:52:18.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7783-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:52:19.474
STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:52:19.515
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:52:22.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5328" for this suite. 02/24/23 11:52:22.224
STEP: Destroying namespace "webhook-5328-markers" for this suite. 02/24/23 11:52:22.232
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":197,"skipped":3259,"failed":0}
------------------------------
• [SLOW TEST] [7.017 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:52:15.292
    Feb 24 11:52:15.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:52:15.293
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:15.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:15.331
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:52:15.354
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:52:15.865
    STEP: Deploying the webhook pod 02/24/23 11:52:15.876
    STEP: Wait for the deployment to be ready 02/24/23 11:52:15.893
    Feb 24 11:52:15.902: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:52:17.919
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:52:17.938
    Feb 24 11:52:18.939: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Feb 24 11:52:18.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7783-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:52:19.474
    STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:52:19.515
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:52:22.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5328" for this suite. 02/24/23 11:52:22.224
    STEP: Destroying namespace "webhook-5328-markers" for this suite. 02/24/23 11:52:22.232
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:52:22.322
Feb 24 11:52:22.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 11:52:22.323
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:22.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:22.359
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-e4390d6a-f547-410b-b6b4-ff6ea14ce55e 02/24/23 11:52:22.364
STEP: Creating a pod to test consume secrets 02/24/23 11:52:22.373
Feb 24 11:52:22.386: INFO: Waiting up to 5m0s for pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1" in namespace "secrets-8226" to be "Succeeded or Failed"
Feb 24 11:52:22.396: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.685875ms
Feb 24 11:52:24.401: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014826861s
Feb 24 11:52:26.402: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015581872s
STEP: Saw pod success 02/24/23 11:52:26.402
Feb 24 11:52:26.402: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1" satisfied condition "Succeeded or Failed"
Feb 24 11:52:26.406: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-ab162350-e761-405e-892e-22090c03a7a1 container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 11:52:26.415
Feb 24 11:52:26.432: INFO: Waiting for pod pod-secrets-ab162350-e761-405e-892e-22090c03a7a1 to disappear
Feb 24 11:52:26.436: INFO: Pod pod-secrets-ab162350-e761-405e-892e-22090c03a7a1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 11:52:26.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8226" for this suite. 02/24/23 11:52:26.444
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":198,"skipped":3294,"failed":0}
------------------------------
• [4.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:52:22.322
    Feb 24 11:52:22.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 11:52:22.323
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:22.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:22.359
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-e4390d6a-f547-410b-b6b4-ff6ea14ce55e 02/24/23 11:52:22.364
    STEP: Creating a pod to test consume secrets 02/24/23 11:52:22.373
    Feb 24 11:52:22.386: INFO: Waiting up to 5m0s for pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1" in namespace "secrets-8226" to be "Succeeded or Failed"
    Feb 24 11:52:22.396: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.685875ms
    Feb 24 11:52:24.401: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014826861s
    Feb 24 11:52:26.402: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015581872s
    STEP: Saw pod success 02/24/23 11:52:26.402
    Feb 24 11:52:26.402: INFO: Pod "pod-secrets-ab162350-e761-405e-892e-22090c03a7a1" satisfied condition "Succeeded or Failed"
    Feb 24 11:52:26.406: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-ab162350-e761-405e-892e-22090c03a7a1 container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 11:52:26.415
    Feb 24 11:52:26.432: INFO: Waiting for pod pod-secrets-ab162350-e761-405e-892e-22090c03a7a1 to disappear
    Feb 24 11:52:26.436: INFO: Pod pod-secrets-ab162350-e761-405e-892e-22090c03a7a1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 11:52:26.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8226" for this suite. 02/24/23 11:52:26.444
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:52:26.452
Feb 24 11:52:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename taint-single-pod 02/24/23 11:52:26.453
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:26.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:26.49
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Feb 24 11:52:26.494: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 11:53:26.546: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Feb 24 11:53:26.558: INFO: Starting informer...
STEP: Starting pod... 02/24/23 11:53:26.558
Feb 24 11:53:26.783: INFO: Pod is running on ip-172-31-216-47.eu-west-3.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 02/24/23 11:53:26.783
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 11:53:26.809
STEP: Waiting short time to make sure Pod is queued for deletion 02/24/23 11:53:26.814
Feb 24 11:53:26.815: INFO: Pod wasn't evicted. Proceeding
Feb 24 11:53:26.815: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 11:53:26.83
STEP: Waiting some time to make sure that toleration time passed. 02/24/23 11:53:26.835
Feb 24 11:54:41.835: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:54:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7551" for this suite. 02/24/23 11:54:41.844
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":199,"skipped":3297,"failed":0}
------------------------------
• [SLOW TEST] [135.399 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:52:26.452
    Feb 24 11:52:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename taint-single-pod 02/24/23 11:52:26.453
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:52:26.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:52:26.49
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Feb 24 11:52:26.494: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 11:53:26.546: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Feb 24 11:53:26.558: INFO: Starting informer...
    STEP: Starting pod... 02/24/23 11:53:26.558
    Feb 24 11:53:26.783: INFO: Pod is running on ip-172-31-216-47.eu-west-3.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 02/24/23 11:53:26.783
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 11:53:26.809
    STEP: Waiting short time to make sure Pod is queued for deletion 02/24/23 11:53:26.814
    Feb 24 11:53:26.815: INFO: Pod wasn't evicted. Proceeding
    Feb 24 11:53:26.815: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 11:53:26.83
    STEP: Waiting some time to make sure that toleration time passed. 02/24/23 11:53:26.835
    Feb 24 11:54:41.835: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:54:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-7551" for this suite. 02/24/23 11:54:41.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:54:41.861
Feb 24 11:54:41.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 11:54:41.862
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:54:41.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:54:41.892
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-2589/configmap-test-d782a8e3-2546-4a54-be2f-26dc051e8970 02/24/23 11:54:41.896
STEP: Creating a pod to test consume configMaps 02/24/23 11:54:41.903
Feb 24 11:54:41.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1" in namespace "configmap-2589" to be "Succeeded or Failed"
Feb 24 11:54:41.917: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163251ms
Feb 24 11:54:43.923: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009648679s
Feb 24 11:54:45.923: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009487198s
STEP: Saw pod success 02/24/23 11:54:45.923
Feb 24 11:54:45.923: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1" satisfied condition "Succeeded or Failed"
Feb 24 11:54:45.928: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1 container env-test: <nil>
STEP: delete the pod 02/24/23 11:54:45.951
Feb 24 11:54:45.971: INFO: Waiting for pod pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1 to disappear
Feb 24 11:54:45.979: INFO: Pod pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 11:54:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2589" for this suite. 02/24/23 11:54:45.991
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":200,"skipped":3320,"failed":0}
------------------------------
• [4.139 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:54:41.861
    Feb 24 11:54:41.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 11:54:41.862
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:54:41.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:54:41.892
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-2589/configmap-test-d782a8e3-2546-4a54-be2f-26dc051e8970 02/24/23 11:54:41.896
    STEP: Creating a pod to test consume configMaps 02/24/23 11:54:41.903
    Feb 24 11:54:41.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1" in namespace "configmap-2589" to be "Succeeded or Failed"
    Feb 24 11:54:41.917: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163251ms
    Feb 24 11:54:43.923: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009648679s
    Feb 24 11:54:45.923: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009487198s
    STEP: Saw pod success 02/24/23 11:54:45.923
    Feb 24 11:54:45.923: INFO: Pod "pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1" satisfied condition "Succeeded or Failed"
    Feb 24 11:54:45.928: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1 container env-test: <nil>
    STEP: delete the pod 02/24/23 11:54:45.951
    Feb 24 11:54:45.971: INFO: Waiting for pod pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1 to disappear
    Feb 24 11:54:45.979: INFO: Pod pod-configmaps-f48da57e-1878-4329-87ce-0114940669a1 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 11:54:45.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2589" for this suite. 02/24/23 11:54:45.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:54:46.016
Feb 24 11:54:46.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename containers 02/24/23 11:54:46.017
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:54:46.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:54:46.074
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 02/24/23 11:54:46.078
Feb 24 11:54:46.098: INFO: Waiting up to 5m0s for pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde" in namespace "containers-81" to be "Succeeded or Failed"
Feb 24 11:54:46.109: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde": Phase="Pending", Reason="", readiness=false. Elapsed: 11.131564ms
Feb 24 11:54:48.114: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016349539s
Feb 24 11:54:50.116: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018567473s
STEP: Saw pod success 02/24/23 11:54:50.116
Feb 24 11:54:50.117: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde" satisfied condition "Succeeded or Failed"
Feb 24 11:54:50.122: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde container agnhost-container: <nil>
STEP: delete the pod 02/24/23 11:54:50.139
Feb 24 11:54:50.159: INFO: Waiting for pod client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde to disappear
Feb 24 11:54:50.165: INFO: Pod client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 24 11:54:50.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-81" for this suite. 02/24/23 11:54:50.173
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":201,"skipped":3394,"failed":0}
------------------------------
• [4.167 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:54:46.016
    Feb 24 11:54:46.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename containers 02/24/23 11:54:46.017
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:54:46.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:54:46.074
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 02/24/23 11:54:46.078
    Feb 24 11:54:46.098: INFO: Waiting up to 5m0s for pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde" in namespace "containers-81" to be "Succeeded or Failed"
    Feb 24 11:54:46.109: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde": Phase="Pending", Reason="", readiness=false. Elapsed: 11.131564ms
    Feb 24 11:54:48.114: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016349539s
    Feb 24 11:54:50.116: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018567473s
    STEP: Saw pod success 02/24/23 11:54:50.116
    Feb 24 11:54:50.117: INFO: Pod "client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde" satisfied condition "Succeeded or Failed"
    Feb 24 11:54:50.122: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 11:54:50.139
    Feb 24 11:54:50.159: INFO: Waiting for pod client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde to disappear
    Feb 24 11:54:50.165: INFO: Pod client-containers-692fc7d0-3fc0-46f9-ab01-59f0fda61bde no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 24 11:54:50.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-81" for this suite. 02/24/23 11:54:50.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:54:50.185
Feb 24 11:54:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename subpath 02/24/23 11:54:50.186
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:54:50.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:54:50.223
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/24/23 11:54:50.226
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-q2dx 02/24/23 11:54:50.242
STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:54:50.243
Feb 24 11:54:50.255: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-q2dx" in namespace "subpath-1096" to be "Succeeded or Failed"
Feb 24 11:54:50.261: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848083ms
Feb 24 11:54:52.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 2.012029156s
Feb 24 11:54:54.266: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 4.010502186s
Feb 24 11:54:56.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 6.011872795s
Feb 24 11:54:58.268: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 8.012492633s
Feb 24 11:55:00.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012369634s
Feb 24 11:55:02.268: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 12.012735962s
Feb 24 11:55:04.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 14.012100692s
Feb 24 11:55:06.266: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 16.01120306s
Feb 24 11:55:08.266: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 18.010746847s
Feb 24 11:55:10.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 20.011460841s
Feb 24 11:55:12.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=false. Elapsed: 22.01185716s
Feb 24 11:55:14.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011865543s
STEP: Saw pod success 02/24/23 11:55:14.267
Feb 24 11:55:14.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx" satisfied condition "Succeeded or Failed"
Feb 24 11:55:14.272: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-downwardapi-q2dx container test-container-subpath-downwardapi-q2dx: <nil>
STEP: delete the pod 02/24/23 11:55:14.281
Feb 24 11:55:14.299: INFO: Waiting for pod pod-subpath-test-downwardapi-q2dx to disappear
Feb 24 11:55:14.303: INFO: Pod pod-subpath-test-downwardapi-q2dx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-q2dx 02/24/23 11:55:14.303
Feb 24 11:55:14.303: INFO: Deleting pod "pod-subpath-test-downwardapi-q2dx" in namespace "subpath-1096"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 24 11:55:14.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1096" for this suite. 02/24/23 11:55:14.317
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":202,"skipped":3416,"failed":0}
------------------------------
• [SLOW TEST] [24.140 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:54:50.185
    Feb 24 11:54:50.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename subpath 02/24/23 11:54:50.186
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:54:50.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:54:50.223
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/24/23 11:54:50.226
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-q2dx 02/24/23 11:54:50.242
    STEP: Creating a pod to test atomic-volume-subpath 02/24/23 11:54:50.243
    Feb 24 11:54:50.255: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-q2dx" in namespace "subpath-1096" to be "Succeeded or Failed"
    Feb 24 11:54:50.261: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.848083ms
    Feb 24 11:54:52.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 2.012029156s
    Feb 24 11:54:54.266: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 4.010502186s
    Feb 24 11:54:56.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 6.011872795s
    Feb 24 11:54:58.268: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 8.012492633s
    Feb 24 11:55:00.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012369634s
    Feb 24 11:55:02.268: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 12.012735962s
    Feb 24 11:55:04.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 14.012100692s
    Feb 24 11:55:06.266: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 16.01120306s
    Feb 24 11:55:08.266: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 18.010746847s
    Feb 24 11:55:10.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=true. Elapsed: 20.011460841s
    Feb 24 11:55:12.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Running", Reason="", readiness=false. Elapsed: 22.01185716s
    Feb 24 11:55:14.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011865543s
    STEP: Saw pod success 02/24/23 11:55:14.267
    Feb 24 11:55:14.267: INFO: Pod "pod-subpath-test-downwardapi-q2dx" satisfied condition "Succeeded or Failed"
    Feb 24 11:55:14.272: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-subpath-test-downwardapi-q2dx container test-container-subpath-downwardapi-q2dx: <nil>
    STEP: delete the pod 02/24/23 11:55:14.281
    Feb 24 11:55:14.299: INFO: Waiting for pod pod-subpath-test-downwardapi-q2dx to disappear
    Feb 24 11:55:14.303: INFO: Pod pod-subpath-test-downwardapi-q2dx no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-q2dx 02/24/23 11:55:14.303
    Feb 24 11:55:14.303: INFO: Deleting pod "pod-subpath-test-downwardapi-q2dx" in namespace "subpath-1096"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 24 11:55:14.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1096" for this suite. 02/24/23 11:55:14.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:55:14.328
Feb 24 11:55:14.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 11:55:14.329
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:14.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:14.356
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 02/24/23 11:55:14.361
STEP: Creating a ResourceQuota 02/24/23 11:55:19.38
STEP: Ensuring resource quota status is calculated 02/24/23 11:55:19.391
STEP: Creating a ReplicationController 02/24/23 11:55:21.396
STEP: Ensuring resource quota status captures replication controller creation 02/24/23 11:55:21.409
STEP: Deleting a ReplicationController 02/24/23 11:55:23.418
STEP: Ensuring resource quota status released usage 02/24/23 11:55:23.435
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 11:55:25.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4533" for this suite. 02/24/23 11:55:25.45
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":203,"skipped":3424,"failed":0}
------------------------------
• [SLOW TEST] [11.130 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:55:14.328
    Feb 24 11:55:14.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 11:55:14.329
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:14.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:14.356
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 02/24/23 11:55:14.361
    STEP: Creating a ResourceQuota 02/24/23 11:55:19.38
    STEP: Ensuring resource quota status is calculated 02/24/23 11:55:19.391
    STEP: Creating a ReplicationController 02/24/23 11:55:21.396
    STEP: Ensuring resource quota status captures replication controller creation 02/24/23 11:55:21.409
    STEP: Deleting a ReplicationController 02/24/23 11:55:23.418
    STEP: Ensuring resource quota status released usage 02/24/23 11:55:23.435
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 11:55:25.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4533" for this suite. 02/24/23 11:55:25.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:55:25.468
Feb 24 11:55:25.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 11:55:25.47
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:25.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:25.504
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e in namespace container-probe-8769 02/24/23 11:55:25.509
Feb 24 11:55:25.522: INFO: Waiting up to 5m0s for pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e" in namespace "container-probe-8769" to be "not pending"
Feb 24 11:55:25.533: INFO: Pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722535ms
Feb 24 11:55:27.538: INFO: Pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016463609s
Feb 24 11:55:27.538: INFO: Pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e" satisfied condition "not pending"
Feb 24 11:55:27.538: INFO: Started pod busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e in namespace container-probe-8769
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:55:27.538
Feb 24 11:55:27.543: INFO: Initial restart count of pod busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e is 0
STEP: deleting the pod 02/24/23 11:59:28.254
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 11:59:28.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8769" for this suite. 02/24/23 11:59:28.278
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":204,"skipped":3433,"failed":0}
------------------------------
• [SLOW TEST] [242.818 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:55:25.468
    Feb 24 11:55:25.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 11:55:25.47
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:55:25.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:55:25.504
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e in namespace container-probe-8769 02/24/23 11:55:25.509
    Feb 24 11:55:25.522: INFO: Waiting up to 5m0s for pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e" in namespace "container-probe-8769" to be "not pending"
    Feb 24 11:55:25.533: INFO: Pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.722535ms
    Feb 24 11:55:27.538: INFO: Pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e": Phase="Running", Reason="", readiness=true. Elapsed: 2.016463609s
    Feb 24 11:55:27.538: INFO: Pod "busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e" satisfied condition "not pending"
    Feb 24 11:55:27.538: INFO: Started pod busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e in namespace container-probe-8769
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 11:55:27.538
    Feb 24 11:55:27.543: INFO: Initial restart count of pod busybox-f34330a1-06c1-4d57-8cb6-24b32f95e82e is 0
    STEP: deleting the pod 02/24/23 11:59:28.254
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 11:59:28.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8769" for this suite. 02/24/23 11:59:28.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:59:28.29
Feb 24 11:59:28.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 11:59:28.291
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:28.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:28.321
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 11:59:28.342
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:59:28.751
STEP: Deploying the webhook pod 02/24/23 11:59:28.761
STEP: Wait for the deployment to be ready 02/24/23 11:59:28.775
Feb 24 11:59:28.784: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 11:59:30.798
STEP: Verifying the service has paired with the endpoint 02/24/23 11:59:30.816
Feb 24 11:59:31.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Feb 24 11:59:31.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5019-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:59:32.333
STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:59:32.353
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:59:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1544" for this suite. 02/24/23 11:59:34.975
STEP: Destroying namespace "webhook-1544-markers" for this suite. 02/24/23 11:59:34.985
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":205,"skipped":3463,"failed":0}
------------------------------
• [SLOW TEST] [6.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:59:28.29
    Feb 24 11:59:28.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 11:59:28.291
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:28.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:28.321
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 11:59:28.342
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 11:59:28.751
    STEP: Deploying the webhook pod 02/24/23 11:59:28.761
    STEP: Wait for the deployment to be ready 02/24/23 11:59:28.775
    Feb 24 11:59:28.784: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 11:59:30.798
    STEP: Verifying the service has paired with the endpoint 02/24/23 11:59:30.816
    Feb 24 11:59:31.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Feb 24 11:59:31.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5019-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 11:59:32.333
    STEP: Creating a custom resource that should be mutated by the webhook 02/24/23 11:59:32.353
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:59:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1544" for this suite. 02/24/23 11:59:34.975
    STEP: Destroying namespace "webhook-1544-markers" for this suite. 02/24/23 11:59:34.985
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:59:35.145
Feb 24 11:59:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 11:59:35.148
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:35.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:35.229
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Feb 24 11:59:35.275: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/24/23 11:59:35.287
Feb 24 11:59:35.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:35.299: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/24/23 11:59:35.299
Feb 24 11:59:35.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:35.340: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:59:36.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:36.346: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:59:37.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:59:37.347: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/24/23 11:59:37.354
Feb 24 11:59:37.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:59:37.377: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 24 11:59:38.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:38.383: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/24/23 11:59:38.383
Feb 24 11:59:38.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:38.394: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:59:39.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:39.399: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:59:40.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:40.400: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:59:41.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:41.399: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 11:59:42.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 24 11:59:42.400: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:59:42.409
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6270, will wait for the garbage collector to delete the pods 02/24/23 11:59:42.409
Feb 24 11:59:42.473: INFO: Deleting DaemonSet.extensions daemon-set took: 8.324712ms
Feb 24 11:59:42.574: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.92739ms
Feb 24 11:59:44.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 11:59:44.780: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 11:59:44.784: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32343"},"items":null}

Feb 24 11:59:44.788: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32343"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 11:59:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6270" for this suite. 02/24/23 11:59:44.831
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":206,"skipped":3463,"failed":0}
------------------------------
• [SLOW TEST] [9.694 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:59:35.145
    Feb 24 11:59:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 11:59:35.148
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:35.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:35.229
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Feb 24 11:59:35.275: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/24/23 11:59:35.287
    Feb 24 11:59:35.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:35.299: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/24/23 11:59:35.299
    Feb 24 11:59:35.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:35.340: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:59:36.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:36.346: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:59:37.347: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:59:37.347: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/24/23 11:59:37.354
    Feb 24 11:59:37.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:59:37.377: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb 24 11:59:38.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:38.383: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/24/23 11:59:38.383
    Feb 24 11:59:38.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:38.394: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:59:39.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:39.399: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:59:40.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:40.400: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:59:41.399: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:41.399: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 11:59:42.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 24 11:59:42.400: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 11:59:42.409
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6270, will wait for the garbage collector to delete the pods 02/24/23 11:59:42.409
    Feb 24 11:59:42.473: INFO: Deleting DaemonSet.extensions daemon-set took: 8.324712ms
    Feb 24 11:59:42.574: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.92739ms
    Feb 24 11:59:44.780: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 11:59:44.780: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 11:59:44.784: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"32343"},"items":null}

    Feb 24 11:59:44.788: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"32343"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 11:59:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6270" for this suite. 02/24/23 11:59:44.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:59:44.856
Feb 24 11:59:44.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:59:44.859
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:44.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:44.9
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Feb 24 11:59:44.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:59:48.145
Feb 24 11:59:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 create -f -'
Feb 24 11:59:49.204: INFO: stderr: ""
Feb 24 11:59:49.204: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 24 11:59:49.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-8162-crds test-cr'
Feb 24 11:59:49.305: INFO: stderr: ""
Feb 24 11:59:49.305: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 24 11:59:49.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 apply -f -'
Feb 24 11:59:49.567: INFO: stderr: ""
Feb 24 11:59:49.567: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 24 11:59:49.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-8162-crds test-cr'
Feb 24 11:59:49.644: INFO: stderr: ""
Feb 24 11:59:49.644: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/24/23 11:59:49.644
Feb 24 11:59:49.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 explain e2e-test-crd-publish-openapi-8162-crds'
Feb 24 11:59:50.830: INFO: stderr: ""
Feb 24 11:59:50.830: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8162-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:59:54.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2355" for this suite. 02/24/23 11:59:54.055
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":207,"skipped":3497,"failed":0}
------------------------------
• [SLOW TEST] [9.224 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:59:44.856
    Feb 24 11:59:44.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 11:59:44.859
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:44.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:44.9
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Feb 24 11:59:44.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 11:59:48.145
    Feb 24 11:59:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 create -f -'
    Feb 24 11:59:49.204: INFO: stderr: ""
    Feb 24 11:59:49.204: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 24 11:59:49.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-8162-crds test-cr'
    Feb 24 11:59:49.305: INFO: stderr: ""
    Feb 24 11:59:49.305: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 24 11:59:49.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 apply -f -'
    Feb 24 11:59:49.567: INFO: stderr: ""
    Feb 24 11:59:49.567: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 24 11:59:49.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 --namespace=crd-publish-openapi-2355 delete e2e-test-crd-publish-openapi-8162-crds test-cr'
    Feb 24 11:59:49.644: INFO: stderr: ""
    Feb 24 11:59:49.644: INFO: stdout: "e2e-test-crd-publish-openapi-8162-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/24/23 11:59:49.644
    Feb 24 11:59:49.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-2355 explain e2e-test-crd-publish-openapi-8162-crds'
    Feb 24 11:59:50.830: INFO: stderr: ""
    Feb 24 11:59:50.830: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8162-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:59:54.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2355" for this suite. 02/24/23 11:59:54.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:59:54.081
Feb 24 11:59:54.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:59:54.082
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:54.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:54.141
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 24 11:59:54.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 11:59:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6814" for this suite. 02/24/23 11:59:57.385
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":208,"skipped":3512,"failed":0}
------------------------------
• [3.317 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:59:54.081
    Feb 24 11:59:54.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 11:59:54.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:54.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:54.141
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 24 11:59:54.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 11:59:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6814" for this suite. 02/24/23 11:59:57.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 11:59:57.4
Feb 24 11:59:57.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 11:59:57.401
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:57.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:57.445
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 02/24/23 11:59:57.45
STEP: submitting the pod to kubernetes 02/24/23 11:59:57.45
Feb 24 11:59:57.465: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" in namespace "pods-3212" to be "running and ready"
Feb 24 11:59:57.471: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.508614ms
Feb 24 11:59:57.472: INFO: The phase of Pod pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d is Pending, waiting for it to be Running (with Ready = true)
Feb 24 11:59:59.482: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016724938s
Feb 24 11:59:59.482: INFO: The phase of Pod pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d is Running (Ready = true)
Feb 24 11:59:59.482: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/24/23 11:59:59.488
STEP: updating the pod 02/24/23 11:59:59.495
Feb 24 12:00:00.110: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d"
Feb 24 12:00:00.110: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" in namespace "pods-3212" to be "terminated with reason DeadlineExceeded"
Feb 24 12:00:00.128: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Running", Reason="", readiness=true. Elapsed: 18.47128ms
Feb 24 12:00:02.141: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Running", Reason="", readiness=true. Elapsed: 2.031294251s
Feb 24 12:00:04.147: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.037634583s
Feb 24 12:00:04.147: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 12:00:04.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3212" for this suite. 02/24/23 12:00:04.174
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":209,"skipped":3523,"failed":0}
------------------------------
• [SLOW TEST] [6.861 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 11:59:57.4
    Feb 24 11:59:57.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 11:59:57.401
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 11:59:57.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 11:59:57.445
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 02/24/23 11:59:57.45
    STEP: submitting the pod to kubernetes 02/24/23 11:59:57.45
    Feb 24 11:59:57.465: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" in namespace "pods-3212" to be "running and ready"
    Feb 24 11:59:57.471: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.508614ms
    Feb 24 11:59:57.472: INFO: The phase of Pod pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 11:59:59.482: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016724938s
    Feb 24 11:59:59.482: INFO: The phase of Pod pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d is Running (Ready = true)
    Feb 24 11:59:59.482: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/24/23 11:59:59.488
    STEP: updating the pod 02/24/23 11:59:59.495
    Feb 24 12:00:00.110: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d"
    Feb 24 12:00:00.110: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" in namespace "pods-3212" to be "terminated with reason DeadlineExceeded"
    Feb 24 12:00:00.128: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Running", Reason="", readiness=true. Elapsed: 18.47128ms
    Feb 24 12:00:02.141: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Running", Reason="", readiness=true. Elapsed: 2.031294251s
    Feb 24 12:00:04.147: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.037634583s
    Feb 24 12:00:04.147: INFO: Pod "pod-update-activedeadlineseconds-b7f3638e-a446-4474-a04c-2fb27d1a158d" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 12:00:04.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3212" for this suite. 02/24/23 12:00:04.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:00:04.262
Feb 24 12:00:04.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:00:04.263
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:04.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:04.389
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:00:04.482
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:00:05.734
STEP: Deploying the webhook pod 02/24/23 12:00:05.766
STEP: Wait for the deployment to be ready 02/24/23 12:00:05.823
Feb 24 12:00:05.858: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 24 12:00:07.878: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/24/23 12:00:09.887
STEP: Verifying the service has paired with the endpoint 02/24/23 12:00:09.97
Feb 24 12:00:10.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/24/23 12:00:10.977
STEP: create a configmap that should be updated by the webhook 02/24/23 12:00:11.002
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:00:11.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8255" for this suite. 02/24/23 12:00:11.043
STEP: Destroying namespace "webhook-8255-markers" for this suite. 02/24/23 12:00:11.056
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":210,"skipped":3531,"failed":0}
------------------------------
• [SLOW TEST] [6.923 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:00:04.262
    Feb 24 12:00:04.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:00:04.263
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:04.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:04.389
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:00:04.482
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:00:05.734
    STEP: Deploying the webhook pod 02/24/23 12:00:05.766
    STEP: Wait for the deployment to be ready 02/24/23 12:00:05.823
    Feb 24 12:00:05.858: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 24 12:00:07.878: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 0, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/24/23 12:00:09.887
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:00:09.97
    Feb 24 12:00:10.970: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/24/23 12:00:10.977
    STEP: create a configmap that should be updated by the webhook 02/24/23 12:00:11.002
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:00:11.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8255" for this suite. 02/24/23 12:00:11.043
    STEP: Destroying namespace "webhook-8255-markers" for this suite. 02/24/23 12:00:11.056
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:00:11.189
Feb 24 12:00:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 12:00:11.19
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:11.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:11.255
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/24/23 12:00:11.261
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local;sleep 1; done
 02/24/23 12:00:11.27
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local;sleep 1; done
 02/24/23 12:00:11.27
STEP: creating a pod to probe DNS 02/24/23 12:00:11.27
STEP: submitting the pod to kubernetes 02/24/23 12:00:11.27
Feb 24 12:00:11.287: INFO: Waiting up to 15m0s for pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed" in namespace "dns-210" to be "running"
Feb 24 12:00:11.296: INFO: Pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610825ms
Feb 24 12:00:13.303: INFO: Pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed": Phase="Running", Reason="", readiness=true. Elapsed: 2.016412404s
Feb 24 12:00:13.304: INFO: Pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:00:13.304
STEP: looking for the results for each expected name from probers 02/24/23 12:00:13.311
Feb 24 12:00:13.321: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.330: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.337: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.346: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.354: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.365: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.373: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.382: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:13.382: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:18.390: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.399: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.406: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.417: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.434: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.443: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.456: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:18.456: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:23.390: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.410: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.418: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.446: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.459: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.468: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.476: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:23.476: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:28.390: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.410: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.418: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.433: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.442: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.449: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:28.449: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:33.392: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.412: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.419: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.434: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.441: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.449: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:33.449: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:38.391: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.418: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.443: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.466: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.487: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.499: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.510: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.523: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:38.523: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:43.394: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.406: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.416: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.424: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.435: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.443: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.451: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.458: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
Feb 24 12:00:43.458: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

Feb 24 12:00:48.466: INFO: DNS probes using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed succeeded

STEP: deleting the pod 02/24/23 12:00:48.466
STEP: deleting the test headless service 02/24/23 12:00:48.517
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 12:00:48.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-210" for this suite. 02/24/23 12:00:48.559
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":211,"skipped":3548,"failed":0}
------------------------------
• [SLOW TEST] [37.386 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:00:11.189
    Feb 24 12:00:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 12:00:11.19
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:11.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:11.255
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/24/23 12:00:11.261
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local;sleep 1; done
     02/24/23 12:00:11.27
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-210.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-210.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local;sleep 1; done
     02/24/23 12:00:11.27
    STEP: creating a pod to probe DNS 02/24/23 12:00:11.27
    STEP: submitting the pod to kubernetes 02/24/23 12:00:11.27
    Feb 24 12:00:11.287: INFO: Waiting up to 15m0s for pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed" in namespace "dns-210" to be "running"
    Feb 24 12:00:11.296: INFO: Pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610825ms
    Feb 24 12:00:13.303: INFO: Pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed": Phase="Running", Reason="", readiness=true. Elapsed: 2.016412404s
    Feb 24 12:00:13.304: INFO: Pod "dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:00:13.304
    STEP: looking for the results for each expected name from probers 02/24/23 12:00:13.311
    Feb 24 12:00:13.321: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.330: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.337: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.346: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.354: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.365: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.373: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.382: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:13.382: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:18.390: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.399: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.406: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.417: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.434: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.443: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.456: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:18.456: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:23.390: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.410: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.418: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.446: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.459: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.468: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.476: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:23.476: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:28.390: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.410: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.418: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.433: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.442: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.449: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:28.449: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:33.392: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.412: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.419: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.434: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.441: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.449: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:33.449: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:38.391: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.418: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.443: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.466: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.487: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.499: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.510: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.523: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:38.523: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:43.394: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.406: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.416: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.424: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.435: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.443: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.451: INFO: Unable to read jessie_udp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.458: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local from pod dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed: the server could not find the requested resource (get pods dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed)
    Feb 24 12:00:43.458: INFO: Lookups using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local wheezy_udp@dns-test-service-2.dns-210.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-210.svc.cluster.local jessie_udp@dns-test-service-2.dns-210.svc.cluster.local jessie_tcp@dns-test-service-2.dns-210.svc.cluster.local]

    Feb 24 12:00:48.466: INFO: DNS probes using dns-210/dns-test-f3fead87-4e10-4eeb-abac-13f6821e3aed succeeded

    STEP: deleting the pod 02/24/23 12:00:48.466
    STEP: deleting the test headless service 02/24/23 12:00:48.517
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 12:00:48.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-210" for this suite. 02/24/23 12:00:48.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:00:48.594
Feb 24 12:00:48.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 12:00:48.595
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:48.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:48.627
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 02/24/23 12:00:48.645
STEP: watching for the Service to be added 02/24/23 12:00:48.689
Feb 24 12:00:48.696: INFO: Found Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 24 12:00:48.696: INFO: Service test-service-fs8lh created
STEP: Getting /status 02/24/23 12:00:48.696
Feb 24 12:00:48.709: INFO: Service test-service-fs8lh has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/24/23 12:00:48.709
STEP: watching for the Service to be patched 02/24/23 12:00:48.722
Feb 24 12:00:48.729: INFO: observed Service test-service-fs8lh in namespace services-7324 with annotations: map[] & LoadBalancer: {[]}
Feb 24 12:00:48.730: INFO: Found Service test-service-fs8lh in namespace services-7324 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 24 12:00:48.730: INFO: Service test-service-fs8lh has service status patched
STEP: updating the ServiceStatus 02/24/23 12:00:48.73
Feb 24 12:00:48.769: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/24/23 12:00:48.77
Feb 24 12:00:48.779: INFO: Observed Service test-service-fs8lh in namespace services-7324 with annotations: map[] & Conditions: {[]}
Feb 24 12:00:48.780: INFO: Observed event: &Service{ObjectMeta:{test-service-fs8lh  services-7324  0c3a8ad8-447b-4cec-a39a-619913684858 32785 0 2023-02-24 12:00:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-24 12:00:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-24 12:00:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.104.54,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.104.54],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 24 12:00:48.781: INFO: Found Service test-service-fs8lh in namespace services-7324 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 12:00:48.782: INFO: Service test-service-fs8lh has service status updated
STEP: patching the service 02/24/23 12:00:48.782
STEP: watching for the Service to be patched 02/24/23 12:00:48.824
Feb 24 12:00:48.832: INFO: observed Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true]
Feb 24 12:00:48.832: INFO: observed Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true]
Feb 24 12:00:48.832: INFO: observed Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true]
Feb 24 12:00:48.832: INFO: Found Service test-service-fs8lh in namespace services-7324 with labels: map[test-service:patched test-service-static:true]
Feb 24 12:00:48.832: INFO: Service test-service-fs8lh patched
STEP: deleting the service 02/24/23 12:00:48.832
STEP: watching for the Service to be deleted 02/24/23 12:00:48.882
Feb 24 12:00:48.896: INFO: Observed event: ADDED
Feb 24 12:00:48.896: INFO: Observed event: MODIFIED
Feb 24 12:00:48.897: INFO: Observed event: MODIFIED
Feb 24 12:00:48.897: INFO: Observed event: MODIFIED
Feb 24 12:00:48.898: INFO: Found Service test-service-fs8lh in namespace services-7324 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 24 12:00:48.898: INFO: Service test-service-fs8lh deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 12:00:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7324" for this suite. 02/24/23 12:00:48.91
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":212,"skipped":3629,"failed":0}
------------------------------
• [0.354 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:00:48.594
    Feb 24 12:00:48.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 12:00:48.595
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:48.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:48.627
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 02/24/23 12:00:48.645
    STEP: watching for the Service to be added 02/24/23 12:00:48.689
    Feb 24 12:00:48.696: INFO: Found Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 24 12:00:48.696: INFO: Service test-service-fs8lh created
    STEP: Getting /status 02/24/23 12:00:48.696
    Feb 24 12:00:48.709: INFO: Service test-service-fs8lh has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/24/23 12:00:48.709
    STEP: watching for the Service to be patched 02/24/23 12:00:48.722
    Feb 24 12:00:48.729: INFO: observed Service test-service-fs8lh in namespace services-7324 with annotations: map[] & LoadBalancer: {[]}
    Feb 24 12:00:48.730: INFO: Found Service test-service-fs8lh in namespace services-7324 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 24 12:00:48.730: INFO: Service test-service-fs8lh has service status patched
    STEP: updating the ServiceStatus 02/24/23 12:00:48.73
    Feb 24 12:00:48.769: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/24/23 12:00:48.77
    Feb 24 12:00:48.779: INFO: Observed Service test-service-fs8lh in namespace services-7324 with annotations: map[] & Conditions: {[]}
    Feb 24 12:00:48.780: INFO: Observed event: &Service{ObjectMeta:{test-service-fs8lh  services-7324  0c3a8ad8-447b-4cec-a39a-619913684858 32785 0 2023-02-24 12:00:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-24 12:00:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-24 12:00:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.98.104.54,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.98.104.54],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 24 12:00:48.781: INFO: Found Service test-service-fs8lh in namespace services-7324 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 12:00:48.782: INFO: Service test-service-fs8lh has service status updated
    STEP: patching the service 02/24/23 12:00:48.782
    STEP: watching for the Service to be patched 02/24/23 12:00:48.824
    Feb 24 12:00:48.832: INFO: observed Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true]
    Feb 24 12:00:48.832: INFO: observed Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true]
    Feb 24 12:00:48.832: INFO: observed Service test-service-fs8lh in namespace services-7324 with labels: map[test-service-static:true]
    Feb 24 12:00:48.832: INFO: Found Service test-service-fs8lh in namespace services-7324 with labels: map[test-service:patched test-service-static:true]
    Feb 24 12:00:48.832: INFO: Service test-service-fs8lh patched
    STEP: deleting the service 02/24/23 12:00:48.832
    STEP: watching for the Service to be deleted 02/24/23 12:00:48.882
    Feb 24 12:00:48.896: INFO: Observed event: ADDED
    Feb 24 12:00:48.896: INFO: Observed event: MODIFIED
    Feb 24 12:00:48.897: INFO: Observed event: MODIFIED
    Feb 24 12:00:48.897: INFO: Observed event: MODIFIED
    Feb 24 12:00:48.898: INFO: Found Service test-service-fs8lh in namespace services-7324 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 24 12:00:48.898: INFO: Service test-service-fs8lh deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 12:00:48.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7324" for this suite. 02/24/23 12:00:48.91
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:00:48.951
Feb 24 12:00:48.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename watch 02/24/23 12:00:48.952
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:49.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:49.011
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/24/23 12:00:49.016
STEP: creating a new configmap 02/24/23 12:00:49.018
STEP: modifying the configmap once 02/24/23 12:00:49.032
STEP: changing the label value of the configmap 02/24/23 12:00:49.057
STEP: Expecting to observe a delete notification for the watched object 02/24/23 12:00:49.076
Feb 24 12:00:49.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32794 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:00:49.077: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32795 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:00:49.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32796 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/24/23 12:00:49.077
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/24/23 12:00:49.093
STEP: changing the label value of the configmap back 02/24/23 12:00:59.095
STEP: modifying the configmap a third time 02/24/23 12:00:59.113
STEP: deleting the configmap 02/24/23 12:00:59.128
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/24/23 12:00:59.139
Feb 24 12:00:59.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32859 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:00:59.140: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32860 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:00:59.140: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32861 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 24 12:00:59.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2540" for this suite. 02/24/23 12:00:59.154
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":213,"skipped":3648,"failed":0}
------------------------------
• [SLOW TEST] [10.217 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:00:48.951
    Feb 24 12:00:48.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename watch 02/24/23 12:00:48.952
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:49.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:49.011
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/24/23 12:00:49.016
    STEP: creating a new configmap 02/24/23 12:00:49.018
    STEP: modifying the configmap once 02/24/23 12:00:49.032
    STEP: changing the label value of the configmap 02/24/23 12:00:49.057
    STEP: Expecting to observe a delete notification for the watched object 02/24/23 12:00:49.076
    Feb 24 12:00:49.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32794 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:00:49.077: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32795 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:00:49.077: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32796 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/24/23 12:00:49.077
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/24/23 12:00:49.093
    STEP: changing the label value of the configmap back 02/24/23 12:00:59.095
    STEP: modifying the configmap a third time 02/24/23 12:00:59.113
    STEP: deleting the configmap 02/24/23 12:00:59.128
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/24/23 12:00:59.139
    Feb 24 12:00:59.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32859 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:00:59.140: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32860 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:00:59.140: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2540  56da53ed-f4e5-4299-9742-cc94d85ba00b 32861 0 2023-02-24 12:00:49 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-24 12:00:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 24 12:00:59.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2540" for this suite. 02/24/23 12:00:59.154
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:00:59.169
Feb 24 12:00:59.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 12:00:59.171
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:59.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:59.203
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 02/24/23 12:00:59.209
Feb 24 12:00:59.224: INFO: Waiting up to 5m0s for pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c" in namespace "downward-api-5351" to be "Succeeded or Failed"
Feb 24 12:00:59.231: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.994074ms
Feb 24 12:01:01.240: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015345137s
Feb 24 12:01:03.243: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018117453s
STEP: Saw pod success 02/24/23 12:01:03.243
Feb 24 12:01:03.243: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c" satisfied condition "Succeeded or Failed"
Feb 24 12:01:03.249: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:01:03.269
Feb 24 12:01:03.292: INFO: Waiting for pod downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c to disappear
Feb 24 12:01:03.298: INFO: Pod downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 24 12:01:03.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5351" for this suite. 02/24/23 12:01:03.311
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":214,"skipped":3652,"failed":0}
------------------------------
• [4.159 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:00:59.169
    Feb 24 12:00:59.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:00:59.171
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:00:59.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:00:59.203
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 02/24/23 12:00:59.209
    Feb 24 12:00:59.224: INFO: Waiting up to 5m0s for pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c" in namespace "downward-api-5351" to be "Succeeded or Failed"
    Feb 24 12:00:59.231: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.994074ms
    Feb 24 12:01:01.240: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015345137s
    Feb 24 12:01:03.243: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018117453s
    STEP: Saw pod success 02/24/23 12:01:03.243
    Feb 24 12:01:03.243: INFO: Pod "downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c" satisfied condition "Succeeded or Failed"
    Feb 24 12:01:03.249: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:01:03.269
    Feb 24 12:01:03.292: INFO: Waiting for pod downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c to disappear
    Feb 24 12:01:03.298: INFO: Pod downward-api-82cf77f1-dbff-40be-8f18-ce28059bf06c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 24 12:01:03.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5351" for this suite. 02/24/23 12:01:03.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:03.332
Feb 24 12:01:03.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:01:03.333
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:03.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:03.37
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/24/23 12:01:03.397
Feb 24 12:01:03.397: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a" in namespace "kubelet-test-6462" to be "completed"
Feb 24 12:01:03.416: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.263735ms
Feb 24 12:01:05.423: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026241243s
Feb 24 12:01:07.426: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029187769s
Feb 24 12:01:07.426: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 24 12:01:07.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6462" for this suite. 02/24/23 12:01:07.447
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":215,"skipped":3670,"failed":0}
------------------------------
• [4.127 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:03.332
    Feb 24 12:01:03.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:01:03.333
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:03.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:03.37
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/24/23 12:01:03.397
    Feb 24 12:01:03.397: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a" in namespace "kubelet-test-6462" to be "completed"
    Feb 24 12:01:03.416: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.263735ms
    Feb 24 12:01:05.423: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026241243s
    Feb 24 12:01:07.426: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029187769s
    Feb 24 12:01:07.426: INFO: Pod "agnhost-host-aliasescee33349-d14a-4910-b66a-bb5461b0fb6a" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 24 12:01:07.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6462" for this suite. 02/24/23 12:01:07.447
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:07.46
Feb 24 12:01:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replication-controller 02/24/23 12:01:07.461
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:07.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:07.493
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 02/24/23 12:01:07.504
STEP: waiting for RC to be added 02/24/23 12:01:07.513
STEP: waiting for available Replicas 02/24/23 12:01:07.514
STEP: patching ReplicationController 02/24/23 12:01:09.123
STEP: waiting for RC to be modified 02/24/23 12:01:09.137
STEP: patching ReplicationController status 02/24/23 12:01:09.137
STEP: waiting for RC to be modified 02/24/23 12:01:09.147
STEP: waiting for available Replicas 02/24/23 12:01:09.147
STEP: fetching ReplicationController status 02/24/23 12:01:09.166
STEP: patching ReplicationController scale 02/24/23 12:01:09.173
STEP: waiting for RC to be modified 02/24/23 12:01:09.183
STEP: waiting for ReplicationController's scale to be the max amount 02/24/23 12:01:09.184
STEP: fetching ReplicationController; ensuring that it's patched 02/24/23 12:01:10.925
STEP: updating ReplicationController status 02/24/23 12:01:10.94
STEP: waiting for RC to be modified 02/24/23 12:01:10.951
STEP: listing all ReplicationControllers 02/24/23 12:01:10.952
STEP: checking that ReplicationController has expected values 02/24/23 12:01:10.961
STEP: deleting ReplicationControllers by collection 02/24/23 12:01:10.961
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/24/23 12:01:10.985
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 24 12:01:11.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8428" for this suite. 02/24/23 12:01:11.104
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":216,"skipped":3670,"failed":0}
------------------------------
• [3.659 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:07.46
    Feb 24 12:01:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replication-controller 02/24/23 12:01:07.461
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:07.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:07.493
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 02/24/23 12:01:07.504
    STEP: waiting for RC to be added 02/24/23 12:01:07.513
    STEP: waiting for available Replicas 02/24/23 12:01:07.514
    STEP: patching ReplicationController 02/24/23 12:01:09.123
    STEP: waiting for RC to be modified 02/24/23 12:01:09.137
    STEP: patching ReplicationController status 02/24/23 12:01:09.137
    STEP: waiting for RC to be modified 02/24/23 12:01:09.147
    STEP: waiting for available Replicas 02/24/23 12:01:09.147
    STEP: fetching ReplicationController status 02/24/23 12:01:09.166
    STEP: patching ReplicationController scale 02/24/23 12:01:09.173
    STEP: waiting for RC to be modified 02/24/23 12:01:09.183
    STEP: waiting for ReplicationController's scale to be the max amount 02/24/23 12:01:09.184
    STEP: fetching ReplicationController; ensuring that it's patched 02/24/23 12:01:10.925
    STEP: updating ReplicationController status 02/24/23 12:01:10.94
    STEP: waiting for RC to be modified 02/24/23 12:01:10.951
    STEP: listing all ReplicationControllers 02/24/23 12:01:10.952
    STEP: checking that ReplicationController has expected values 02/24/23 12:01:10.961
    STEP: deleting ReplicationControllers by collection 02/24/23 12:01:10.961
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/24/23 12:01:10.985
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 24 12:01:11.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8428" for this suite. 02/24/23 12:01:11.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:11.12
Feb 24 12:01:11.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename watch 02/24/23 12:01:11.121
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:11.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:11.167
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/24/23 12:01:11.172
STEP: starting a background goroutine to produce watch events 02/24/23 12:01:11.179
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/24/23 12:01:11.179
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 24 12:01:13.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5158" for this suite. 02/24/23 12:01:13.982
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":217,"skipped":3676,"failed":0}
------------------------------
• [2.916 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:11.12
    Feb 24 12:01:11.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename watch 02/24/23 12:01:11.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:11.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:11.167
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/24/23 12:01:11.172
    STEP: starting a background goroutine to produce watch events 02/24/23 12:01:11.179
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/24/23 12:01:11.179
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 24 12:01:13.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5158" for this suite. 02/24/23 12:01:13.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:14.036
Feb 24 12:01:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:01:14.038
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:14.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:14.076
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 24 12:01:14.111: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2831 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 24 12:01:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2831" for this suite. 02/24/23 12:01:14.146
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":218,"skipped":3681,"failed":0}
------------------------------
• [0.136 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:14.036
    Feb 24 12:01:14.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:01:14.038
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:14.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:14.076
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 24 12:01:14.111: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2831 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 24 12:01:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2831" for this suite. 02/24/23 12:01:14.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:14.175
Feb 24 12:01:14.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:01:14.179
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:14.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:14.211
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 24 12:01:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:01:14.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6730" for this suite. 02/24/23 12:01:14.851
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":219,"skipped":3701,"failed":0}
------------------------------
• [0.709 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:14.175
    Feb 24 12:01:14.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:01:14.179
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:14.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:14.211
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 24 12:01:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:01:14.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6730" for this suite. 02/24/23 12:01:14.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:14.888
Feb 24 12:01:14.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:01:14.89
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:14.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:14.942
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:01:14.958
Feb 24 12:01:14.974: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2841" to be "running and ready"
Feb 24 12:01:14.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.671152ms
Feb 24 12:01:14.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:01:16.997: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.022681595s
Feb 24 12:01:16.997: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 12:01:16.997: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 02/24/23 12:01:17.004
Feb 24 12:01:17.018: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2841" to be "running and ready"
Feb 24 12:01:17.029: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.940954ms
Feb 24 12:01:17.029: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:01:19.037: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018690058s
Feb 24 12:01:19.037: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 24 12:01:19.037: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/24/23 12:01:19.048
Feb 24 12:01:19.066: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 12:01:19.093: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 12:01:21.093: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 12:01:21.101: INFO: Pod pod-with-prestop-http-hook still exists
Feb 24 12:01:23.093: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 24 12:01:23.100: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/24/23 12:01:23.1
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 24 12:01:23.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2841" for this suite. 02/24/23 12:01:23.141
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":220,"skipped":3708,"failed":0}
------------------------------
• [SLOW TEST] [8.266 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:14.888
    Feb 24 12:01:14.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:01:14.89
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:14.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:14.942
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:01:14.958
    Feb 24 12:01:14.974: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2841" to be "running and ready"
    Feb 24 12:01:14.990: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.671152ms
    Feb 24 12:01:14.990: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:01:16.997: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.022681595s
    Feb 24 12:01:16.997: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 12:01:16.997: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 02/24/23 12:01:17.004
    Feb 24 12:01:17.018: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-2841" to be "running and ready"
    Feb 24 12:01:17.029: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.940954ms
    Feb 24 12:01:17.029: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:01:19.037: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018690058s
    Feb 24 12:01:19.037: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 24 12:01:19.037: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/24/23 12:01:19.048
    Feb 24 12:01:19.066: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 24 12:01:19.093: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 24 12:01:21.093: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 24 12:01:21.101: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 24 12:01:23.093: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 24 12:01:23.100: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/24/23 12:01:23.1
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 24 12:01:23.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-2841" for this suite. 02/24/23 12:01:23.141
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:23.155
Feb 24 12:01:23.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename prestop 02/24/23 12:01:23.156
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:23.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:23.189
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-8722 02/24/23 12:01:23.194
STEP: Waiting for pods to come up. 02/24/23 12:01:23.209
Feb 24 12:01:23.210: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8722" to be "running"
Feb 24 12:01:23.217: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992396ms
Feb 24 12:01:25.230: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020846994s
Feb 24 12:01:25.230: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-8722 02/24/23 12:01:25.244
Feb 24 12:01:25.257: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8722" to be "running"
Feb 24 12:01:25.264: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.084134ms
Feb 24 12:01:27.272: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.015366567s
Feb 24 12:01:27.272: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/24/23 12:01:27.272
Feb 24 12:01:32.309: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/24/23 12:01:32.309
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Feb 24 12:01:32.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8722" for this suite. 02/24/23 12:01:32.383
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":221,"skipped":3710,"failed":0}
------------------------------
• [SLOW TEST] [9.260 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:23.155
    Feb 24 12:01:23.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename prestop 02/24/23 12:01:23.156
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:23.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:23.189
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-8722 02/24/23 12:01:23.194
    STEP: Waiting for pods to come up. 02/24/23 12:01:23.209
    Feb 24 12:01:23.210: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8722" to be "running"
    Feb 24 12:01:23.217: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.992396ms
    Feb 24 12:01:25.230: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.020846994s
    Feb 24 12:01:25.230: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-8722 02/24/23 12:01:25.244
    Feb 24 12:01:25.257: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8722" to be "running"
    Feb 24 12:01:25.264: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.084134ms
    Feb 24 12:01:27.272: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.015366567s
    Feb 24 12:01:27.272: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/24/23 12:01:27.272
    Feb 24 12:01:32.309: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/24/23 12:01:32.309
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Feb 24 12:01:32.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-8722" for this suite. 02/24/23 12:01:32.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:32.447
Feb 24 12:01:32.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 12:01:32.466
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:32.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:32.502
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9540 02/24/23 12:01:32.506
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 02/24/23 12:01:32.518
STEP: Creating pod with conflicting port in namespace statefulset-9540 02/24/23 12:01:32.53
STEP: Waiting until pod test-pod will start running in namespace statefulset-9540 02/24/23 12:01:32.543
Feb 24 12:01:32.543: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9540" to be "running"
Feb 24 12:01:32.550: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.7408ms
Feb 24 12:01:34.560: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01654277s
Feb 24 12:01:34.560: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9540 02/24/23 12:01:34.56
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9540 02/24/23 12:01:34.582
Feb 24 12:01:34.621: INFO: Observed stateful pod in namespace: statefulset-9540, name: ss-0, uid: 8418defb-fd2c-43a3-a25f-bddb04e8999f, status phase: Pending. Waiting for statefulset controller to delete.
Feb 24 12:01:34.646: INFO: Observed stateful pod in namespace: statefulset-9540, name: ss-0, uid: 8418defb-fd2c-43a3-a25f-bddb04e8999f, status phase: Failed. Waiting for statefulset controller to delete.
Feb 24 12:01:34.678: INFO: Observed stateful pod in namespace: statefulset-9540, name: ss-0, uid: 8418defb-fd2c-43a3-a25f-bddb04e8999f, status phase: Failed. Waiting for statefulset controller to delete.
Feb 24 12:01:34.678: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9540
STEP: Removing pod with conflicting port in namespace statefulset-9540 02/24/23 12:01:34.678
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9540 and will be in running state 02/24/23 12:01:34.72
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 12:01:36.746: INFO: Deleting all statefulset in ns statefulset-9540
Feb 24 12:01:36.752: INFO: Scaling statefulset ss to 0
Feb 24 12:01:46.788: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 12:01:46.794: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 12:01:46.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9540" for this suite. 02/24/23 12:01:46.829
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":222,"skipped":3778,"failed":0}
------------------------------
• [SLOW TEST] [14.400 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:32.447
    Feb 24 12:01:32.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 12:01:32.466
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:32.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:32.502
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9540 02/24/23 12:01:32.506
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 02/24/23 12:01:32.518
    STEP: Creating pod with conflicting port in namespace statefulset-9540 02/24/23 12:01:32.53
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9540 02/24/23 12:01:32.543
    Feb 24 12:01:32.543: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9540" to be "running"
    Feb 24 12:01:32.550: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.7408ms
    Feb 24 12:01:34.560: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01654277s
    Feb 24 12:01:34.560: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9540 02/24/23 12:01:34.56
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9540 02/24/23 12:01:34.582
    Feb 24 12:01:34.621: INFO: Observed stateful pod in namespace: statefulset-9540, name: ss-0, uid: 8418defb-fd2c-43a3-a25f-bddb04e8999f, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 24 12:01:34.646: INFO: Observed stateful pod in namespace: statefulset-9540, name: ss-0, uid: 8418defb-fd2c-43a3-a25f-bddb04e8999f, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 24 12:01:34.678: INFO: Observed stateful pod in namespace: statefulset-9540, name: ss-0, uid: 8418defb-fd2c-43a3-a25f-bddb04e8999f, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 24 12:01:34.678: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9540
    STEP: Removing pod with conflicting port in namespace statefulset-9540 02/24/23 12:01:34.678
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9540 and will be in running state 02/24/23 12:01:34.72
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 12:01:36.746: INFO: Deleting all statefulset in ns statefulset-9540
    Feb 24 12:01:36.752: INFO: Scaling statefulset ss to 0
    Feb 24 12:01:46.788: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 12:01:46.794: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 12:01:46.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9540" for this suite. 02/24/23 12:01:46.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:46.849
Feb 24 12:01:46.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 12:01:46.851
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:46.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:46.895
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 02/24/23 12:01:46.9
Feb 24 12:01:46.919: INFO: created test-pod-1
Feb 24 12:01:46.931: INFO: created test-pod-2
Feb 24 12:01:46.942: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/24/23 12:01:46.942
Feb 24 12:01:46.942: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-30' to be running and ready
Feb 24 12:01:46.972: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 12:01:46.972: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 12:01:46.972: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 24 12:01:46.972: INFO: 0 / 3 pods in namespace 'pods-30' are running and ready (0 seconds elapsed)
Feb 24 12:01:46.972: INFO: expected 0 pod replicas in namespace 'pods-30', 0 are Running and Ready.
Feb 24 12:01:46.972: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
Feb 24 12:01:46.972: INFO: test-pod-1  ip-172-31-215-124.eu-west-3.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  }]
Feb 24 12:01:46.972: INFO: test-pod-2  ip-172-31-216-47.eu-west-3.compute.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  }]
Feb 24 12:01:46.973: INFO: test-pod-3  ip-172-31-216-47.eu-west-3.compute.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  }]
Feb 24 12:01:46.973: INFO: 
Feb 24 12:01:48.992: INFO: 3 / 3 pods in namespace 'pods-30' are running and ready (2 seconds elapsed)
Feb 24 12:01:48.992: INFO: expected 0 pod replicas in namespace 'pods-30', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/24/23 12:01:49.032
Feb 24 12:01:49.039: INFO: Pod quantity 3 is different from expected quantity 0
Feb 24 12:01:50.047: INFO: Pod quantity 3 is different from expected quantity 0
Feb 24 12:01:51.046: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 12:01:52.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-30" for this suite. 02/24/23 12:01:52.055
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":223,"skipped":3784,"failed":0}
------------------------------
• [SLOW TEST] [5.222 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:46.849
    Feb 24 12:01:46.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 12:01:46.851
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:46.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:46.895
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 02/24/23 12:01:46.9
    Feb 24 12:01:46.919: INFO: created test-pod-1
    Feb 24 12:01:46.931: INFO: created test-pod-2
    Feb 24 12:01:46.942: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/24/23 12:01:46.942
    Feb 24 12:01:46.942: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-30' to be running and ready
    Feb 24 12:01:46.972: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 24 12:01:46.972: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 24 12:01:46.972: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 24 12:01:46.972: INFO: 0 / 3 pods in namespace 'pods-30' are running and ready (0 seconds elapsed)
    Feb 24 12:01:46.972: INFO: expected 0 pod replicas in namespace 'pods-30', 0 are Running and Ready.
    Feb 24 12:01:46.972: INFO: POD         NODE                                          PHASE    GRACE  CONDITIONS
    Feb 24 12:01:46.972: INFO: test-pod-1  ip-172-31-215-124.eu-west-3.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  }]
    Feb 24 12:01:46.972: INFO: test-pod-2  ip-172-31-216-47.eu-west-3.compute.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  }]
    Feb 24 12:01:46.973: INFO: test-pod-3  ip-172-31-216-47.eu-west-3.compute.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:01:46 +0000 UTC  }]
    Feb 24 12:01:46.973: INFO: 
    Feb 24 12:01:48.992: INFO: 3 / 3 pods in namespace 'pods-30' are running and ready (2 seconds elapsed)
    Feb 24 12:01:48.992: INFO: expected 0 pod replicas in namespace 'pods-30', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/24/23 12:01:49.032
    Feb 24 12:01:49.039: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 24 12:01:50.047: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 24 12:01:51.046: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 12:01:52.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-30" for this suite. 02/24/23 12:01:52.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:52.073
Feb 24 12:01:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:01:52.074
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:52.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:52.111
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:01:52.118
Feb 24 12:01:52.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838" in namespace "projected-1091" to be "Succeeded or Failed"
Feb 24 12:01:52.142: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838": Phase="Pending", Reason="", readiness=false. Elapsed: 9.347673ms
Feb 24 12:01:54.149: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016735675s
Feb 24 12:01:56.150: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017225231s
STEP: Saw pod success 02/24/23 12:01:56.15
Feb 24 12:01:56.150: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838" satisfied condition "Succeeded or Failed"
Feb 24 12:01:56.157: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838 container client-container: <nil>
STEP: delete the pod 02/24/23 12:01:56.169
Feb 24 12:01:56.194: INFO: Waiting for pod downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838 to disappear
Feb 24 12:01:56.201: INFO: Pod downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:01:56.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1091" for this suite. 02/24/23 12:01:56.212
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":224,"skipped":3795,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:52.073
    Feb 24 12:01:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:01:52.074
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:52.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:52.111
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:01:52.118
    Feb 24 12:01:52.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838" in namespace "projected-1091" to be "Succeeded or Failed"
    Feb 24 12:01:52.142: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838": Phase="Pending", Reason="", readiness=false. Elapsed: 9.347673ms
    Feb 24 12:01:54.149: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016735675s
    Feb 24 12:01:56.150: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017225231s
    STEP: Saw pod success 02/24/23 12:01:56.15
    Feb 24 12:01:56.150: INFO: Pod "downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838" satisfied condition "Succeeded or Failed"
    Feb 24 12:01:56.157: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:01:56.169
    Feb 24 12:01:56.194: INFO: Waiting for pod downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838 to disappear
    Feb 24 12:01:56.201: INFO: Pod downwardapi-volume-bd51b6e7-e69e-47ef-b958-355aa2a49838 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:01:56.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1091" for this suite. 02/24/23 12:01:56.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:01:56.23
Feb 24 12:01:56.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 12:01:56.232
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:56.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:56.274
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 02/24/23 12:01:56.28
Feb 24 12:01:56.294: INFO: Waiting up to 5m0s for pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724" in namespace "downward-api-779" to be "Succeeded or Failed"
Feb 24 12:01:56.302: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724": Phase="Pending", Reason="", readiness=false. Elapsed: 7.728089ms
Feb 24 12:01:58.310: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015916299s
Feb 24 12:02:00.309: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015480679s
STEP: Saw pod success 02/24/23 12:02:00.309
Feb 24 12:02:00.310: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724" satisfied condition "Succeeded or Failed"
Feb 24 12:02:00.317: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724 container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:02:00.331
Feb 24 12:02:00.354: INFO: Waiting for pod downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724 to disappear
Feb 24 12:02:00.361: INFO: Pod downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 24 12:02:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-779" for this suite. 02/24/23 12:02:00.372
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":225,"skipped":3804,"failed":0}
------------------------------
• [4.163 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:01:56.23
    Feb 24 12:01:56.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:01:56.232
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:01:56.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:01:56.274
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 02/24/23 12:01:56.28
    Feb 24 12:01:56.294: INFO: Waiting up to 5m0s for pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724" in namespace "downward-api-779" to be "Succeeded or Failed"
    Feb 24 12:01:56.302: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724": Phase="Pending", Reason="", readiness=false. Elapsed: 7.728089ms
    Feb 24 12:01:58.310: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015916299s
    Feb 24 12:02:00.309: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015480679s
    STEP: Saw pod success 02/24/23 12:02:00.309
    Feb 24 12:02:00.310: INFO: Pod "downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724" satisfied condition "Succeeded or Failed"
    Feb 24 12:02:00.317: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724 container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:02:00.331
    Feb 24 12:02:00.354: INFO: Waiting for pod downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724 to disappear
    Feb 24 12:02:00.361: INFO: Pod downward-api-5d7dc12f-0f3c-40c1-b15c-353741a49724 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 24 12:02:00.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-779" for this suite. 02/24/23 12:02:00.372
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:00.394
Feb 24 12:02:00.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:02:00.395
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:00.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:00.445
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:02:00.514
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:02:01.06
STEP: Deploying the webhook pod 02/24/23 12:02:01.09
STEP: Wait for the deployment to be ready 02/24/23 12:02:01.11
Feb 24 12:02:01.124: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:02:03.147
STEP: Verifying the service has paired with the endpoint 02/24/23 12:02:03.171
Feb 24 12:02:04.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/24/23 12:02:04.178
STEP: create a pod that should be updated by the webhook 02/24/23 12:02:04.2
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:02:04.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1501" for this suite. 02/24/23 12:02:04.269
STEP: Destroying namespace "webhook-1501-markers" for this suite. 02/24/23 12:02:04.289
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":226,"skipped":3808,"failed":0}
------------------------------
• [4.058 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:00.394
    Feb 24 12:02:00.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:02:00.395
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:00.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:00.445
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:02:00.514
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:02:01.06
    STEP: Deploying the webhook pod 02/24/23 12:02:01.09
    STEP: Wait for the deployment to be ready 02/24/23 12:02:01.11
    Feb 24 12:02:01.124: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:02:03.147
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:02:03.171
    Feb 24 12:02:04.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/24/23 12:02:04.178
    STEP: create a pod that should be updated by the webhook 02/24/23 12:02:04.2
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:02:04.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1501" for this suite. 02/24/23 12:02:04.269
    STEP: Destroying namespace "webhook-1501-markers" for this suite. 02/24/23 12:02:04.289
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:04.454
Feb 24 12:02:04.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:02:04.455
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:04.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:04.702
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Feb 24 12:02:04.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 12:02:09.385
Feb 24 12:02:09.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 create -f -'
Feb 24 12:02:10.512: INFO: stderr: ""
Feb 24 12:02:10.512: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 24 12:02:10.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 delete e2e-test-crd-publish-openapi-419-crds test-cr'
Feb 24 12:02:10.615: INFO: stderr: ""
Feb 24 12:02:10.615: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 24 12:02:10.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 apply -f -'
Feb 24 12:02:11.707: INFO: stderr: ""
Feb 24 12:02:11.707: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 24 12:02:11.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 delete e2e-test-crd-publish-openapi-419-crds test-cr'
Feb 24 12:02:11.801: INFO: stderr: ""
Feb 24 12:02:11.801: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/24/23 12:02:11.801
Feb 24 12:02:11.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 explain e2e-test-crd-publish-openapi-419-crds'
Feb 24 12:02:12.534: INFO: stderr: ""
Feb 24 12:02:12.534: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-419-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:02:16.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8029" for this suite. 02/24/23 12:02:16.219
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":227,"skipped":3815,"failed":0}
------------------------------
• [SLOW TEST] [11.775 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:04.454
    Feb 24 12:02:04.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:02:04.455
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:04.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:04.702
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Feb 24 12:02:04.708: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 12:02:09.385
    Feb 24 12:02:09.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 create -f -'
    Feb 24 12:02:10.512: INFO: stderr: ""
    Feb 24 12:02:10.512: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 24 12:02:10.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 delete e2e-test-crd-publish-openapi-419-crds test-cr'
    Feb 24 12:02:10.615: INFO: stderr: ""
    Feb 24 12:02:10.615: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 24 12:02:10.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 apply -f -'
    Feb 24 12:02:11.707: INFO: stderr: ""
    Feb 24 12:02:11.707: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 24 12:02:11.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 --namespace=crd-publish-openapi-8029 delete e2e-test-crd-publish-openapi-419-crds test-cr'
    Feb 24 12:02:11.801: INFO: stderr: ""
    Feb 24 12:02:11.801: INFO: stdout: "e2e-test-crd-publish-openapi-419-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/24/23 12:02:11.801
    Feb 24 12:02:11.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-8029 explain e2e-test-crd-publish-openapi-419-crds'
    Feb 24 12:02:12.534: INFO: stderr: ""
    Feb 24 12:02:12.534: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-419-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:02:16.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8029" for this suite. 02/24/23 12:02:16.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:16.236
Feb 24 12:02:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:02:16.237
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:16.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:16.294
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-98c99e69-4e35-4e1e-8c60-c44adebeec6b 02/24/23 12:02:16.32
STEP: Creating a pod to test consume configMaps 02/24/23 12:02:16.337
Feb 24 12:02:16.356: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a" in namespace "projected-7982" to be "Succeeded or Failed"
Feb 24 12:02:16.369: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.11621ms
Feb 24 12:02:18.378: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022566112s
Feb 24 12:02:20.375: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019758456s
STEP: Saw pod success 02/24/23 12:02:20.376
Feb 24 12:02:20.376: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a" satisfied condition "Succeeded or Failed"
Feb 24 12:02:20.385: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:02:20.401
Feb 24 12:02:20.418: INFO: Waiting for pod pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a to disappear
Feb 24 12:02:20.430: INFO: Pod pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 12:02:20.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7982" for this suite. 02/24/23 12:02:20.437
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":228,"skipped":3843,"failed":0}
------------------------------
• [4.212 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:16.236
    Feb 24 12:02:16.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:02:16.237
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:16.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:16.294
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-98c99e69-4e35-4e1e-8c60-c44adebeec6b 02/24/23 12:02:16.32
    STEP: Creating a pod to test consume configMaps 02/24/23 12:02:16.337
    Feb 24 12:02:16.356: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a" in namespace "projected-7982" to be "Succeeded or Failed"
    Feb 24 12:02:16.369: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.11621ms
    Feb 24 12:02:18.378: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022566112s
    Feb 24 12:02:20.375: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019758456s
    STEP: Saw pod success 02/24/23 12:02:20.376
    Feb 24 12:02:20.376: INFO: Pod "pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a" satisfied condition "Succeeded or Failed"
    Feb 24 12:02:20.385: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:02:20.401
    Feb 24 12:02:20.418: INFO: Waiting for pod pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a to disappear
    Feb 24 12:02:20.430: INFO: Pod pod-projected-configmaps-ac40c8cb-2946-4893-a67e-50346e00838a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 12:02:20.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7982" for this suite. 02/24/23 12:02:20.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:20.45
Feb 24 12:02:20.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 12:02:20.451
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:20.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:20.479
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 24 12:02:20.482: INFO: Creating simple deployment test-new-deployment
Feb 24 12:02:20.510: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 02/24/23 12:02:22.53
STEP: updating a scale subresource 02/24/23 12:02:22.538
STEP: verifying the deployment Spec.Replicas was modified 02/24/23 12:02:22.551
STEP: Patch a scale subresource 02/24/23 12:02:22.56
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 12:02:22.622: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1929  583399b1-7d2d-4e76-89db-a05c6522a9d0 33881 3 2023-02-24 12:02:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-24 12:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ead98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-24 12:02:21 +0000 UTC,LastTransitionTime:2023-02-24 12:02:20 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 12:02:22 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 12:02:22.628: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1929  cb5c4f19-a56a-47ea-8df3-05522b466ac7 33882 2 2023-02-24 12:02:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 583399b1-7d2d-4e76-89db-a05c6522a9d0 0xc0041c26f7 0xc0041c26f8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"583399b1-7d2d-4e76-89db-a05c6522a9d0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c27c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 12:02:22.636: INFO: Pod "test-new-deployment-845c8977d9-n2xd7" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-n2xd7 test-new-deployment-845c8977d9- deployment-1929  240350a2-f66a-4c34-b574-54e524d57200 33860 0 2023-02-24 12:02:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6cdd15c12c93a88daccd82f17e9fc9890ff97c5c239cdabe5f54190598815cef cni.projectcalico.org/podIP:10.244.3.5/32 cni.projectcalico.org/podIPs:10.244.3.5/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 cb5c4f19-a56a-47ea-8df3-05522b466ac7 0xc0041eb3b7 0xc0041eb3b8}] [] [{kube-controller-manager Update v1 2023-02-24 12:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb5c4f19-a56a-47ea-8df3-05522b466ac7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 12:02:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fr927,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fr927,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.5,StartTime:2023-02-24 12:02:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 12:02:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79075b9eebe3f6f14db641363a078703465657495379b92ab2487bff388f532e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 12:02:22.636: INFO: Pod "test-new-deployment-845c8977d9-rnsr8" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-rnsr8 test-new-deployment-845c8977d9- deployment-1929  6324b9a2-4827-4ac7-afab-d359b3a4b213 33883 0 2023-02-24 12:02:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 cb5c4f19-a56a-47ea-8df3-05522b466ac7 0xc0041eb620 0xc0041eb621}] [] [{kube-controller-manager Update v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb5c4f19-a56a-47ea-8df3-05522b466ac7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvzkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvzkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 12:02:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 12:02:22.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1929" for this suite. 02/24/23 12:02:22.644
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":229,"skipped":3864,"failed":0}
------------------------------
• [2.221 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:20.45
    Feb 24 12:02:20.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 12:02:20.451
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:20.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:20.479
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 24 12:02:20.482: INFO: Creating simple deployment test-new-deployment
    Feb 24 12:02:20.510: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 02/24/23 12:02:22.53
    STEP: updating a scale subresource 02/24/23 12:02:22.538
    STEP: verifying the deployment Spec.Replicas was modified 02/24/23 12:02:22.551
    STEP: Patch a scale subresource 02/24/23 12:02:22.56
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 12:02:22.622: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1929  583399b1-7d2d-4e76-89db-a05c6522a9d0 33881 3 2023-02-24 12:02:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-24 12:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ead98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-24 12:02:21 +0000 UTC,LastTransitionTime:2023-02-24 12:02:20 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-24 12:02:22 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 12:02:22.628: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1929  cb5c4f19-a56a-47ea-8df3-05522b466ac7 33882 2 2023-02-24 12:02:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 583399b1-7d2d-4e76-89db-a05c6522a9d0 0xc0041c26f7 0xc0041c26f8}] [] [{kube-controller-manager Update apps/v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"583399b1-7d2d-4e76-89db-a05c6522a9d0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c27c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 12:02:22.636: INFO: Pod "test-new-deployment-845c8977d9-n2xd7" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-n2xd7 test-new-deployment-845c8977d9- deployment-1929  240350a2-f66a-4c34-b574-54e524d57200 33860 0 2023-02-24 12:02:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6cdd15c12c93a88daccd82f17e9fc9890ff97c5c239cdabe5f54190598815cef cni.projectcalico.org/podIP:10.244.3.5/32 cni.projectcalico.org/podIPs:10.244.3.5/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 cb5c4f19-a56a-47ea-8df3-05522b466ac7 0xc0041eb3b7 0xc0041eb3b8}] [] [{kube-controller-manager Update v1 2023-02-24 12:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb5c4f19-a56a-47ea-8df3-05522b466ac7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-02-24 12:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-24 12:02:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fr927,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fr927,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-216-47.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.216.47,PodIP:10.244.3.5,StartTime:2023-02-24 12:02:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 12:02:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79075b9eebe3f6f14db641363a078703465657495379b92ab2487bff388f532e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 12:02:22.636: INFO: Pod "test-new-deployment-845c8977d9-rnsr8" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-rnsr8 test-new-deployment-845c8977d9- deployment-1929  6324b9a2-4827-4ac7-afab-d359b3a4b213 33883 0 2023-02-24 12:02:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 cb5c4f19-a56a-47ea-8df3-05522b466ac7 0xc0041eb620 0xc0041eb621}] [] [{kube-controller-manager Update v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb5c4f19-a56a-47ea-8df3-05522b466ac7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 12:02:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hvzkg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hvzkg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:02:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:,StartTime:2023-02-24 12:02:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 12:02:22.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1929" for this suite. 02/24/23 12:02:22.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:22.682
Feb 24 12:02:22.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:02:22.684
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:22.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:22.717
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 02/24/23 12:02:22.72
STEP: Counting existing ResourceQuota 02/24/23 12:02:27.726
STEP: Creating a ResourceQuota 02/24/23 12:02:32.732
STEP: Ensuring resource quota status is calculated 02/24/23 12:02:32.74
STEP: Creating a Secret 02/24/23 12:02:34.747
STEP: Ensuring resource quota status captures secret creation 02/24/23 12:02:34.764
STEP: Deleting a secret 02/24/23 12:02:36.77
STEP: Ensuring resource quota status released usage 02/24/23 12:02:36.78
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 12:02:38.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7604" for this suite. 02/24/23 12:02:38.793
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":230,"skipped":3911,"failed":0}
------------------------------
• [SLOW TEST] [16.122 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:22.682
    Feb 24 12:02:22.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:02:22.684
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:22.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:22.717
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 02/24/23 12:02:22.72
    STEP: Counting existing ResourceQuota 02/24/23 12:02:27.726
    STEP: Creating a ResourceQuota 02/24/23 12:02:32.732
    STEP: Ensuring resource quota status is calculated 02/24/23 12:02:32.74
    STEP: Creating a Secret 02/24/23 12:02:34.747
    STEP: Ensuring resource quota status captures secret creation 02/24/23 12:02:34.764
    STEP: Deleting a secret 02/24/23 12:02:36.77
    STEP: Ensuring resource quota status released usage 02/24/23 12:02:36.78
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 12:02:38.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7604" for this suite. 02/24/23 12:02:38.793
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:38.804
Feb 24 12:02:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename disruption 02/24/23 12:02:38.807
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:38.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:38.836
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:38.839
Feb 24 12:02:38.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename disruption-2 02/24/23 12:02:38.84
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:38.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:38.866
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 02/24/23 12:02:38.875
STEP: Waiting for the pdb to be processed 02/24/23 12:02:40.895
STEP: Waiting for the pdb to be processed 02/24/23 12:02:42.915
STEP: listing a collection of PDBs across all namespaces 02/24/23 12:02:44.925
STEP: listing a collection of PDBs in namespace disruption-1239 02/24/23 12:02:44.931
STEP: deleting a collection of PDBs 02/24/23 12:02:44.935
STEP: Waiting for the PDB collection to be deleted 02/24/23 12:02:44.958
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Feb 24 12:02:44.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8024" for this suite. 02/24/23 12:02:44.97
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 24 12:02:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1239" for this suite. 02/24/23 12:02:44.987
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":231,"skipped":3912,"failed":0}
------------------------------
• [SLOW TEST] [6.192 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:38.804
    Feb 24 12:02:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename disruption 02/24/23 12:02:38.807
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:38.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:38.836
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:38.839
    Feb 24 12:02:38.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename disruption-2 02/24/23 12:02:38.84
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:38.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:38.866
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 02/24/23 12:02:38.875
    STEP: Waiting for the pdb to be processed 02/24/23 12:02:40.895
    STEP: Waiting for the pdb to be processed 02/24/23 12:02:42.915
    STEP: listing a collection of PDBs across all namespaces 02/24/23 12:02:44.925
    STEP: listing a collection of PDBs in namespace disruption-1239 02/24/23 12:02:44.931
    STEP: deleting a collection of PDBs 02/24/23 12:02:44.935
    STEP: Waiting for the PDB collection to be deleted 02/24/23 12:02:44.958
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Feb 24 12:02:44.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-8024" for this suite. 02/24/23 12:02:44.97
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 24 12:02:44.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1239" for this suite. 02/24/23 12:02:44.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:44.998
Feb 24 12:02:44.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename disruption 02/24/23 12:02:44.999
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:45.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:45.116
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 02/24/23 12:02:45.119
STEP: Waiting for the pdb to be processed 02/24/23 12:02:45.128
STEP: updating the pdb 02/24/23 12:02:45.136
STEP: Waiting for the pdb to be processed 02/24/23 12:02:45.151
STEP: patching the pdb 02/24/23 12:02:45.159
STEP: Waiting for the pdb to be processed 02/24/23 12:02:45.173
STEP: Waiting for the pdb to be deleted 02/24/23 12:02:45.196
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 24 12:02:45.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5367" for this suite. 02/24/23 12:02:45.218
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":232,"skipped":3924,"failed":0}
------------------------------
• [0.232 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:44.998
    Feb 24 12:02:44.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename disruption 02/24/23 12:02:44.999
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:45.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:45.116
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 02/24/23 12:02:45.119
    STEP: Waiting for the pdb to be processed 02/24/23 12:02:45.128
    STEP: updating the pdb 02/24/23 12:02:45.136
    STEP: Waiting for the pdb to be processed 02/24/23 12:02:45.151
    STEP: patching the pdb 02/24/23 12:02:45.159
    STEP: Waiting for the pdb to be processed 02/24/23 12:02:45.173
    STEP: Waiting for the pdb to be deleted 02/24/23 12:02:45.196
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 24 12:02:45.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5367" for this suite. 02/24/23 12:02:45.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:45.231
Feb 24 12:02:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replicaset 02/24/23 12:02:45.234
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:45.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:45.277
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/24/23 12:02:45.281
Feb 24 12:02:45.299: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6979" to be "running and ready"
Feb 24 12:02:45.311: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 11.532829ms
Feb 24 12:02:45.311: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:02:47.317: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.017422624s
Feb 24 12:02:47.317: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 24 12:02:47.317: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/24/23 12:02:47.322
STEP: Then the orphan pod is adopted 02/24/23 12:02:47.329
STEP: When the matched label of one of its pods change 02/24/23 12:02:48.341
Feb 24 12:02:48.357: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/24/23 12:02:48.412
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 24 12:02:49.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6979" for this suite. 02/24/23 12:02:49.451
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":233,"skipped":3931,"failed":0}
------------------------------
• [4.236 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:45.231
    Feb 24 12:02:45.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replicaset 02/24/23 12:02:45.234
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:45.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:45.277
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/24/23 12:02:45.281
    Feb 24 12:02:45.299: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6979" to be "running and ready"
    Feb 24 12:02:45.311: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 11.532829ms
    Feb 24 12:02:45.311: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:02:47.317: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.017422624s
    Feb 24 12:02:47.317: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 24 12:02:47.317: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/24/23 12:02:47.322
    STEP: Then the orphan pod is adopted 02/24/23 12:02:47.329
    STEP: When the matched label of one of its pods change 02/24/23 12:02:48.341
    Feb 24 12:02:48.357: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/24/23 12:02:48.412
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 24 12:02:49.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6979" for this suite. 02/24/23 12:02:49.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:49.472
Feb 24 12:02:49.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename certificates 02/24/23 12:02:49.473
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:49.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:49.516
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/24/23 12:02:50.158
STEP: getting /apis/certificates.k8s.io 02/24/23 12:02:50.162
STEP: getting /apis/certificates.k8s.io/v1 02/24/23 12:02:50.162
STEP: creating 02/24/23 12:02:50.163
STEP: getting 02/24/23 12:02:50.187
STEP: listing 02/24/23 12:02:50.198
STEP: watching 02/24/23 12:02:50.206
Feb 24 12:02:50.206: INFO: starting watch
STEP: patching 02/24/23 12:02:50.207
STEP: updating 02/24/23 12:02:50.217
Feb 24 12:02:50.228: INFO: waiting for watch events with expected annotations
Feb 24 12:02:50.228: INFO: saw patched and updated annotations
STEP: getting /approval 02/24/23 12:02:50.228
STEP: patching /approval 02/24/23 12:02:50.236
STEP: updating /approval 02/24/23 12:02:50.254
STEP: getting /status 02/24/23 12:02:50.265
STEP: patching /status 02/24/23 12:02:50.272
STEP: updating /status 02/24/23 12:02:50.282
STEP: deleting 02/24/23 12:02:50.299
STEP: deleting a collection 02/24/23 12:02:50.339
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:02:50.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7982" for this suite. 02/24/23 12:02:50.389
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":234,"skipped":3961,"failed":0}
------------------------------
• [0.935 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:49.472
    Feb 24 12:02:49.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename certificates 02/24/23 12:02:49.473
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:49.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:49.516
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/24/23 12:02:50.158
    STEP: getting /apis/certificates.k8s.io 02/24/23 12:02:50.162
    STEP: getting /apis/certificates.k8s.io/v1 02/24/23 12:02:50.162
    STEP: creating 02/24/23 12:02:50.163
    STEP: getting 02/24/23 12:02:50.187
    STEP: listing 02/24/23 12:02:50.198
    STEP: watching 02/24/23 12:02:50.206
    Feb 24 12:02:50.206: INFO: starting watch
    STEP: patching 02/24/23 12:02:50.207
    STEP: updating 02/24/23 12:02:50.217
    Feb 24 12:02:50.228: INFO: waiting for watch events with expected annotations
    Feb 24 12:02:50.228: INFO: saw patched and updated annotations
    STEP: getting /approval 02/24/23 12:02:50.228
    STEP: patching /approval 02/24/23 12:02:50.236
    STEP: updating /approval 02/24/23 12:02:50.254
    STEP: getting /status 02/24/23 12:02:50.265
    STEP: patching /status 02/24/23 12:02:50.272
    STEP: updating /status 02/24/23 12:02:50.282
    STEP: deleting 02/24/23 12:02:50.299
    STEP: deleting a collection 02/24/23 12:02:50.339
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:02:50.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-7982" for this suite. 02/24/23 12:02:50.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:02:50.412
Feb 24 12:02:50.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-preemption 02/24/23 12:02:50.413
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:50.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:50.455
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 24 12:02:50.486: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 12:03:50.570: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 02/24/23 12:03:50.577
Feb 24 12:03:50.612: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 24 12:03:50.623: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 24 12:03:50.660: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 24 12:03:50.672: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 24 12:03:50.711: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 24 12:03:50.725: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/24/23 12:03:50.725
Feb 24 12:03:50.725: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4346" to be "running"
Feb 24 12:03:50.734: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.162318ms
Feb 24 12:03:52.739: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013864994s
Feb 24 12:03:52.739: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 24 12:03:52.739: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
Feb 24 12:03:52.745: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.197353ms
Feb 24 12:03:52.745: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:03:52.745: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
Feb 24 12:03:52.749: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.720089ms
Feb 24 12:03:52.750: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:03:52.750: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
Feb 24 12:03:52.755: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.318835ms
Feb 24 12:03:52.755: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:03:52.755: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
Feb 24 12:03:52.760: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.569333ms
Feb 24 12:03:52.760: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 24 12:03:52.760: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
Feb 24 12:03:52.765: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.582423ms
Feb 24 12:03:52.765: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/24/23 12:03:52.765
Feb 24 12:03:52.782: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 24 12:03:52.791: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.920522ms
Feb 24 12:03:54.798: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015692623s
Feb 24 12:03:56.797: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014181958s
Feb 24 12:03:58.802: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.019235715s
Feb 24 12:03:58.802: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 24 12:03:58.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4346" for this suite. 02/24/23 12:03:58.893
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":235,"skipped":3986,"failed":0}
------------------------------
• [SLOW TEST] [68.660 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:02:50.412
    Feb 24 12:02:50.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-preemption 02/24/23 12:02:50.413
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:02:50.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:02:50.455
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 24 12:02:50.486: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 12:03:50.570: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 02/24/23 12:03:50.577
    Feb 24 12:03:50.612: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 24 12:03:50.623: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 24 12:03:50.660: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 24 12:03:50.672: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 24 12:03:50.711: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 24 12:03:50.725: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/24/23 12:03:50.725
    Feb 24 12:03:50.725: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4346" to be "running"
    Feb 24 12:03:50.734: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.162318ms
    Feb 24 12:03:52.739: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013864994s
    Feb 24 12:03:52.739: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 24 12:03:52.739: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
    Feb 24 12:03:52.745: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.197353ms
    Feb 24 12:03:52.745: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:03:52.745: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
    Feb 24 12:03:52.749: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.720089ms
    Feb 24 12:03:52.750: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:03:52.750: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
    Feb 24 12:03:52.755: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.318835ms
    Feb 24 12:03:52.755: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:03:52.755: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
    Feb 24 12:03:52.760: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.569333ms
    Feb 24 12:03:52.760: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 24 12:03:52.760: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4346" to be "running"
    Feb 24 12:03:52.765: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.582423ms
    Feb 24 12:03:52.765: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/24/23 12:03:52.765
    Feb 24 12:03:52.782: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 24 12:03:52.791: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.920522ms
    Feb 24 12:03:54.798: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015692623s
    Feb 24 12:03:56.797: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014181958s
    Feb 24 12:03:58.802: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.019235715s
    Feb 24 12:03:58.802: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 12:03:58.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4346" for this suite. 02/24/23 12:03:58.893
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:03:59.075
Feb 24 12:03:59.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 12:03:59.077
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:03:59.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:03:59.111
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Feb 24 12:03:59.129: INFO: Waiting up to 5m0s for pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099" in namespace "container-probe-1120" to be "running and ready"
Feb 24 12:03:59.135: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Pending", Reason="", readiness=false. Elapsed: 6.262445ms
Feb 24 12:03:59.135: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:04:01.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 2.012589751s
Feb 24 12:04:01.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:03.144: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 4.014843028s
Feb 24 12:04:03.144: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:05.145: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 6.015966979s
Feb 24 12:04:05.145: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:07.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 8.012459741s
Feb 24 12:04:07.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:09.145: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 10.016267099s
Feb 24 12:04:09.145: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:11.142: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 12.013408784s
Feb 24 12:04:11.143: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:13.140: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 14.011708993s
Feb 24 12:04:13.140: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:15.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 16.011947658s
Feb 24 12:04:15.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:17.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 18.012055743s
Feb 24 12:04:17.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:19.142: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 20.013312718s
Feb 24 12:04:19.142: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
Feb 24 12:04:21.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=true. Elapsed: 22.012686323s
Feb 24 12:04:21.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = true)
Feb 24 12:04:21.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099" satisfied condition "running and ready"
Feb 24 12:04:21.147: INFO: Container started at 2023-02-24 12:03:59 +0000 UTC, pod became ready at 2023-02-24 12:04:19 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 12:04:21.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1120" for this suite. 02/24/23 12:04:21.155
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":236,"skipped":4000,"failed":0}
------------------------------
• [SLOW TEST] [22.095 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:03:59.075
    Feb 24 12:03:59.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:03:59.077
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:03:59.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:03:59.111
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Feb 24 12:03:59.129: INFO: Waiting up to 5m0s for pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099" in namespace "container-probe-1120" to be "running and ready"
    Feb 24 12:03:59.135: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Pending", Reason="", readiness=false. Elapsed: 6.262445ms
    Feb 24 12:03:59.135: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:04:01.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 2.012589751s
    Feb 24 12:04:01.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:03.144: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 4.014843028s
    Feb 24 12:04:03.144: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:05.145: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 6.015966979s
    Feb 24 12:04:05.145: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:07.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 8.012459741s
    Feb 24 12:04:07.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:09.145: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 10.016267099s
    Feb 24 12:04:09.145: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:11.142: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 12.013408784s
    Feb 24 12:04:11.143: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:13.140: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 14.011708993s
    Feb 24 12:04:13.140: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:15.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 16.011947658s
    Feb 24 12:04:15.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:17.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 18.012055743s
    Feb 24 12:04:17.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:19.142: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=false. Elapsed: 20.013312718s
    Feb 24 12:04:19.142: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = false)
    Feb 24 12:04:21.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099": Phase="Running", Reason="", readiness=true. Elapsed: 22.012686323s
    Feb 24 12:04:21.141: INFO: The phase of Pod test-webserver-cf8bc907-410c-432a-a77f-40d074da4099 is Running (Ready = true)
    Feb 24 12:04:21.141: INFO: Pod "test-webserver-cf8bc907-410c-432a-a77f-40d074da4099" satisfied condition "running and ready"
    Feb 24 12:04:21.147: INFO: Container started at 2023-02-24 12:03:59 +0000 UTC, pod became ready at 2023-02-24 12:04:19 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 12:04:21.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1120" for this suite. 02/24/23 12:04:21.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:21.177
Feb 24 12:04:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 12:04:21.178
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:21.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:21.206
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Feb 24 12:04:21.221: INFO: Waiting up to 2m0s for pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" in namespace "var-expansion-2081" to be "container 0 failed with reason CreateContainerConfigError"
Feb 24 12:04:21.229: INFO: Pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68485ms
Feb 24 12:04:23.235: INFO: Pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01410957s
Feb 24 12:04:23.235: INFO: Pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 24 12:04:23.235: INFO: Deleting pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" in namespace "var-expansion-2081"
Feb 24 12:04:23.248: INFO: Wait up to 5m0s for pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 12:04:27.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2081" for this suite. 02/24/23 12:04:27.266
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":237,"skipped":4056,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:21.177
    Feb 24 12:04:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 12:04:21.178
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:21.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:21.206
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Feb 24 12:04:21.221: INFO: Waiting up to 2m0s for pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" in namespace "var-expansion-2081" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 24 12:04:21.229: INFO: Pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff": Phase="Pending", Reason="", readiness=false. Elapsed: 7.68485ms
    Feb 24 12:04:23.235: INFO: Pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01410957s
    Feb 24 12:04:23.235: INFO: Pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 24 12:04:23.235: INFO: Deleting pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" in namespace "var-expansion-2081"
    Feb 24 12:04:23.248: INFO: Wait up to 5m0s for pod "var-expansion-fde8d50f-ab58-4799-8fb1-3b3d823d28ff" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 12:04:27.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2081" for this suite. 02/24/23 12:04:27.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:27.287
Feb 24 12:04:27.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:04:27.289
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:27.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:27.314
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-891a8d0b-9321-47b6-884d-660559c6165b 02/24/23 12:04:27.317
STEP: Creating a pod to test consume secrets 02/24/23 12:04:27.325
Feb 24 12:04:27.337: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a" in namespace "projected-6743" to be "Succeeded or Failed"
Feb 24 12:04:27.344: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.600017ms
Feb 24 12:04:29.350: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012138398s
Feb 24 12:04:31.355: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017356393s
STEP: Saw pod success 02/24/23 12:04:31.355
Feb 24 12:04:31.357: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a" satisfied condition "Succeeded or Failed"
Feb 24 12:04:31.364: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:04:31.381
Feb 24 12:04:31.402: INFO: Waiting for pod pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a to disappear
Feb 24 12:04:31.408: INFO: Pod pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 12:04:31.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6743" for this suite. 02/24/23 12:04:31.415
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":238,"skipped":4076,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:27.287
    Feb 24 12:04:27.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:04:27.289
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:27.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:27.314
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-891a8d0b-9321-47b6-884d-660559c6165b 02/24/23 12:04:27.317
    STEP: Creating a pod to test consume secrets 02/24/23 12:04:27.325
    Feb 24 12:04:27.337: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a" in namespace "projected-6743" to be "Succeeded or Failed"
    Feb 24 12:04:27.344: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.600017ms
    Feb 24 12:04:29.350: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012138398s
    Feb 24 12:04:31.355: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017356393s
    STEP: Saw pod success 02/24/23 12:04:31.355
    Feb 24 12:04:31.357: INFO: Pod "pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a" satisfied condition "Succeeded or Failed"
    Feb 24 12:04:31.364: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:04:31.381
    Feb 24 12:04:31.402: INFO: Waiting for pod pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a to disappear
    Feb 24 12:04:31.408: INFO: Pod pod-projected-secrets-e0135403-909b-45da-9378-fa559fa9fa2a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 12:04:31.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6743" for this suite. 02/24/23 12:04:31.415
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:31.437
Feb 24 12:04:31.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename ephemeral-containers-test 02/24/23 12:04:31.438
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:31.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:31.467
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/24/23 12:04:31.47
Feb 24 12:04:31.483: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5779" to be "running and ready"
Feb 24 12:04:31.489: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.720187ms
Feb 24 12:04:31.490: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:04:33.496: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012902376s
Feb 24 12:04:33.496: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 24 12:04:33.496: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/24/23 12:04:33.503
Feb 24 12:04:33.518: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5779" to be "container debugger running"
Feb 24 12:04:33.524: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.981319ms
Feb 24 12:04:35.529: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011325991s
Feb 24 12:04:37.532: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013776099s
Feb 24 12:04:37.532: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/24/23 12:04:37.532
Feb 24 12:04:37.532: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5779 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:04:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:04:37.533: INFO: ExecWithOptions: Clientset creation
Feb 24 12:04:37.534: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-5779/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 24 12:04:37.646: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 12:04:37.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-5779" for this suite. 02/24/23 12:04:37.663
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":239,"skipped":4079,"failed":0}
------------------------------
• [SLOW TEST] [6.238 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:31.437
    Feb 24 12:04:31.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/24/23 12:04:31.438
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:31.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:31.467
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/24/23 12:04:31.47
    Feb 24 12:04:31.483: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5779" to be "running and ready"
    Feb 24 12:04:31.489: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.720187ms
    Feb 24 12:04:31.490: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:04:33.496: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012902376s
    Feb 24 12:04:33.496: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 24 12:04:33.496: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/24/23 12:04:33.503
    Feb 24 12:04:33.518: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5779" to be "container debugger running"
    Feb 24 12:04:33.524: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.981319ms
    Feb 24 12:04:35.529: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011325991s
    Feb 24 12:04:37.532: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.013776099s
    Feb 24 12:04:37.532: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/24/23 12:04:37.532
    Feb 24 12:04:37.532: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5779 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:04:37.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:04:37.533: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:04:37.534: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-5779/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 24 12:04:37.646: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 12:04:37.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-5779" for this suite. 02/24/23 12:04:37.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:37.675
Feb 24 12:04:37.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 12:04:37.676
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:37.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:37.706
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/24/23 12:04:37.709
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/24/23 12:04:37.709
STEP: creating a pod to probe DNS 02/24/23 12:04:37.709
STEP: submitting the pod to kubernetes 02/24/23 12:04:37.709
Feb 24 12:04:37.721: INFO: Waiting up to 15m0s for pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b" in namespace "dns-717" to be "running"
Feb 24 12:04:37.729: INFO: Pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.106363ms
Feb 24 12:04:39.745: INFO: Pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b": Phase="Running", Reason="", readiness=true. Elapsed: 2.023511229s
Feb 24 12:04:39.745: INFO: Pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:04:39.745
STEP: looking for the results for each expected name from probers 02/24/23 12:04:39.751
Feb 24 12:04:39.778: INFO: DNS probes using dns-717/dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b succeeded

STEP: deleting the pod 02/24/23 12:04:39.778
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 12:04:39.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-717" for this suite. 02/24/23 12:04:39.803
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":240,"skipped":4084,"failed":0}
------------------------------
• [2.138 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:37.675
    Feb 24 12:04:37.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 12:04:37.676
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:37.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:37.706
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/24/23 12:04:37.709
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/24/23 12:04:37.709
    STEP: creating a pod to probe DNS 02/24/23 12:04:37.709
    STEP: submitting the pod to kubernetes 02/24/23 12:04:37.709
    Feb 24 12:04:37.721: INFO: Waiting up to 15m0s for pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b" in namespace "dns-717" to be "running"
    Feb 24 12:04:37.729: INFO: Pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.106363ms
    Feb 24 12:04:39.745: INFO: Pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b": Phase="Running", Reason="", readiness=true. Elapsed: 2.023511229s
    Feb 24 12:04:39.745: INFO: Pod "dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:04:39.745
    STEP: looking for the results for each expected name from probers 02/24/23 12:04:39.751
    Feb 24 12:04:39.778: INFO: DNS probes using dns-717/dns-test-1dd8faa7-0934-4f1e-9f7a-b2fa4af7295b succeeded

    STEP: deleting the pod 02/24/23 12:04:39.778
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 12:04:39.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-717" for this suite. 02/24/23 12:04:39.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:39.817
Feb 24 12:04:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename deployment 02/24/23 12:04:39.818
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:39.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:39.87
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/24/23 12:04:39.883
Feb 24 12:04:39.883: INFO: Creating simple deployment test-deployment-gkr78
Feb 24 12:04:40.032: INFO: deployment "test-deployment-gkr78" doesn't have the required revision set
STEP: Getting /status 02/24/23 12:04:42.055
Feb 24 12:04:42.061: INFO: Deployment test-deployment-gkr78 has Conditions: [{Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 02/24/23 12:04:42.061
Feb 24 12:04:42.078: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 4, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 4, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 4, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 4, 40, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gkr78-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/24/23 12:04:42.078
Feb 24 12:04:42.083: INFO: Observed &Deployment event: ADDED
Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
Feb 24 12:04:42.083: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 12:04:42.083: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gkr78-777898ffcc" is progressing.}
Feb 24 12:04:42.084: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
Feb 24 12:04:42.084: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
Feb 24 12:04:42.084: INFO: Found Deployment test-deployment-gkr78 in namespace deployment-1216 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 12:04:42.084: INFO: Deployment test-deployment-gkr78 has an updated status
STEP: patching the Statefulset Status 02/24/23 12:04:42.084
Feb 24 12:04:42.084: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 24 12:04:42.096: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/24/23 12:04:42.096
Feb 24 12:04:42.099: INFO: Observed &Deployment event: ADDED
Feb 24 12:04:42.100: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
Feb 24 12:04:42.100: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.100: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
Feb 24 12:04:42.100: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 12:04:42.101: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gkr78-777898ffcc" is progressing.}
Feb 24 12:04:42.101: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
Feb 24 12:04:42.102: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.102: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 24 12:04:42.102: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
Feb 24 12:04:42.102: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 12:04:42.102: INFO: Observed &Deployment event: MODIFIED
Feb 24 12:04:42.102: INFO: Found deployment test-deployment-gkr78 in namespace deployment-1216 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 24 12:04:42.102: INFO: Deployment test-deployment-gkr78 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 24 12:04:42.109: INFO: Deployment "test-deployment-gkr78":
&Deployment{ObjectMeta:{test-deployment-gkr78  deployment-1216  c45a243e-ca87-49a3-9219-1071bb493d7a 34909 1 2023-02-24 12:04:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-24 12:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-24 12:04:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-24 12:04:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00449afa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gkr78-777898ffcc",LastUpdateTime:2023-02-24 12:04:42 +0000 UTC,LastTransitionTime:2023-02-24 12:04:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 24 12:04:42.119: INFO: New ReplicaSet "test-deployment-gkr78-777898ffcc" of Deployment "test-deployment-gkr78":
&ReplicaSet{ObjectMeta:{test-deployment-gkr78-777898ffcc  deployment-1216  88fdfd7d-3029-4d0b-97f3-772ef6e62de3 34903 1 2023-02-24 12:04:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gkr78 c45a243e-ca87-49a3-9219-1071bb493d7a 0xc004944040 0xc004944041}] [] [{kube-controller-manager Update apps/v1 2023-02-24 12:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c45a243e-ca87-49a3-9219-1071bb493d7a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:04:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049440e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 24 12:04:42.126: INFO: Pod "test-deployment-gkr78-777898ffcc-qz6m4" is available:
&Pod{ObjectMeta:{test-deployment-gkr78-777898ffcc-qz6m4 test-deployment-gkr78-777898ffcc- deployment-1216  dc950196-3ebf-4e47-a83c-d55e4d01e262 34902 0 2023-02-24 12:04:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:0b9ec1d4d23a2324628ef5179c73939efc24a0141a5ef46ad8a2fd064ff770a8 cni.projectcalico.org/podIP:10.244.4.167/32 cni.projectcalico.org/podIPs:10.244.4.167/32] [{apps/v1 ReplicaSet test-deployment-gkr78-777898ffcc 88fdfd7d-3029-4d0b-97f3-772ef6e62de3 0xc0049444e0 0xc0049444e1}] [] [{Go-http-client Update v1 2023-02-24 12:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 12:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fdfd7d-3029-4d0b-97f3-772ef6e62de3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 12:04:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4k6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4k6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.167,StartTime:2023-02-24 12:04:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 12:04:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f69921eeb69c1ae4a48891a23767b05e447f9704ceabaf52a99fbf6738a5c37d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 24 12:04:42.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1216" for this suite. 02/24/23 12:04:42.134
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":241,"skipped":4129,"failed":0}
------------------------------
• [2.326 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:39.817
    Feb 24 12:04:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename deployment 02/24/23 12:04:39.818
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:39.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:39.87
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/24/23 12:04:39.883
    Feb 24 12:04:39.883: INFO: Creating simple deployment test-deployment-gkr78
    Feb 24 12:04:40.032: INFO: deployment "test-deployment-gkr78" doesn't have the required revision set
    STEP: Getting /status 02/24/23 12:04:42.055
    Feb 24 12:04:42.061: INFO: Deployment test-deployment-gkr78 has Conditions: [{Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 02/24/23 12:04:42.061
    Feb 24 12:04:42.078: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 4, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 4, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 4, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 4, 40, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gkr78-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/24/23 12:04:42.078
    Feb 24 12:04:42.083: INFO: Observed &Deployment event: ADDED
    Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
    Feb 24 12:04:42.083: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
    Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 12:04:42.083: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.083: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gkr78-777898ffcc" is progressing.}
    Feb 24 12:04:42.084: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
    Feb 24 12:04:42.084: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 12:04:42.084: INFO: Observed Deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
    Feb 24 12:04:42.084: INFO: Found Deployment test-deployment-gkr78 in namespace deployment-1216 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 12:04:42.084: INFO: Deployment test-deployment-gkr78 has an updated status
    STEP: patching the Statefulset Status 02/24/23 12:04:42.084
    Feb 24 12:04:42.084: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 24 12:04:42.096: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/24/23 12:04:42.096
    Feb 24 12:04:42.099: INFO: Observed &Deployment event: ADDED
    Feb 24 12:04:42.100: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
    Feb 24 12:04:42.100: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.100: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gkr78-777898ffcc"}
    Feb 24 12:04:42.100: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 12:04:42.101: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:40 +0000 UTC 2023-02-24 12:04:40 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gkr78-777898ffcc" is progressing.}
    Feb 24 12:04:42.101: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 12:04:42.101: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
    Feb 24 12:04:42.102: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.102: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:41 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 24 12:04:42.102: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-24 12:04:41 +0000 UTC 2023-02-24 12:04:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gkr78-777898ffcc" has successfully progressed.}
    Feb 24 12:04:42.102: INFO: Observed deployment test-deployment-gkr78 in namespace deployment-1216 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 12:04:42.102: INFO: Observed &Deployment event: MODIFIED
    Feb 24 12:04:42.102: INFO: Found deployment test-deployment-gkr78 in namespace deployment-1216 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 24 12:04:42.102: INFO: Deployment test-deployment-gkr78 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 24 12:04:42.109: INFO: Deployment "test-deployment-gkr78":
    &Deployment{ObjectMeta:{test-deployment-gkr78  deployment-1216  c45a243e-ca87-49a3-9219-1071bb493d7a 34909 1 2023-02-24 12:04:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-24 12:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-24 12:04:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-24 12:04:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00449afa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-gkr78-777898ffcc",LastUpdateTime:2023-02-24 12:04:42 +0000 UTC,LastTransitionTime:2023-02-24 12:04:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 24 12:04:42.119: INFO: New ReplicaSet "test-deployment-gkr78-777898ffcc" of Deployment "test-deployment-gkr78":
    &ReplicaSet{ObjectMeta:{test-deployment-gkr78-777898ffcc  deployment-1216  88fdfd7d-3029-4d0b-97f3-772ef6e62de3 34903 1 2023-02-24 12:04:39 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gkr78 c45a243e-ca87-49a3-9219-1071bb493d7a 0xc004944040 0xc004944041}] [] [{kube-controller-manager Update apps/v1 2023-02-24 12:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c45a243e-ca87-49a3-9219-1071bb493d7a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-24 12:04:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049440e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 24 12:04:42.126: INFO: Pod "test-deployment-gkr78-777898ffcc-qz6m4" is available:
    &Pod{ObjectMeta:{test-deployment-gkr78-777898ffcc-qz6m4 test-deployment-gkr78-777898ffcc- deployment-1216  dc950196-3ebf-4e47-a83c-d55e4d01e262 34902 0 2023-02-24 12:04:40 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:0b9ec1d4d23a2324628ef5179c73939efc24a0141a5ef46ad8a2fd064ff770a8 cni.projectcalico.org/podIP:10.244.4.167/32 cni.projectcalico.org/podIPs:10.244.4.167/32] [{apps/v1 ReplicaSet test-deployment-gkr78-777898ffcc 88fdfd7d-3029-4d0b-97f3-772ef6e62de3 0xc0049444e0 0xc0049444e1}] [] [{Go-http-client Update v1 2023-02-24 12:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-24 12:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fdfd7d-3029-4d0b-97f3-772ef6e62de3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-24 12:04:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4k6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4k6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-215-124.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-24 12:04:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.215.124,PodIP:10.244.4.167,StartTime:2023-02-24 12:04:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-24 12:04:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f69921eeb69c1ae4a48891a23767b05e447f9704ceabaf52a99fbf6738a5c37d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 24 12:04:42.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1216" for this suite. 02/24/23 12:04:42.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:42.145
Feb 24 12:04:42.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename daemonsets 02/24/23 12:04:42.146
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:42.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:42.173
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 02/24/23 12:04:42.214
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 12:04:42.222
Feb 24 12:04:42.228: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:42.228: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:42.229: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:42.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 12:04:42.234: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:04:43.241: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:43.241: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:43.241: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:43.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 12:04:43.247: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:04:44.242: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:44.243: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:44.243: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:04:44.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 24 12:04:44.249: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/24/23 12:04:44.254
Feb 24 12:04:44.258: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/24/23 12:04:44.258
Feb 24 12:04:44.272: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/24/23 12:04:44.272
Feb 24 12:04:44.275: INFO: Observed &DaemonSet event: ADDED
Feb 24 12:04:44.276: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.276: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.276: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.277: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.277: INFO: Found daemon set daemon-set in namespace daemonsets-3919 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 12:04:44.277: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/24/23 12:04:44.277
STEP: watching for the daemon set status to be patched 02/24/23 12:04:44.288
Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: ADDED
Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.292: INFO: Observed daemon set daemon-set in namespace daemonsets-3919 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 12:04:44.293: INFO: Observed &DaemonSet event: MODIFIED
Feb 24 12:04:44.293: INFO: Found daemon set daemon-set in namespace daemonsets-3919 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 24 12:04:44.293: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/24/23 12:04:44.303
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3919, will wait for the garbage collector to delete the pods 02/24/23 12:04:44.304
Feb 24 12:04:44.378: INFO: Deleting DaemonSet.extensions daemon-set took: 10.713856ms
Feb 24 12:04:44.479: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820621ms
Feb 24 12:04:47.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 24 12:04:47.085: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 24 12:04:47.089: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35017"},"items":null}

Feb 24 12:04:47.094: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35017"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 24 12:04:47.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3919" for this suite. 02/24/23 12:04:47.131
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":242,"skipped":4134,"failed":0}
------------------------------
• [4.996 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:42.145
    Feb 24 12:04:42.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename daemonsets 02/24/23 12:04:42.146
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:42.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:42.173
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 02/24/23 12:04:42.214
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 12:04:42.222
    Feb 24 12:04:42.228: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:42.228: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:42.229: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:42.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 12:04:42.234: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:04:43.241: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:43.241: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:43.241: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:43.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 12:04:43.247: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:04:44.242: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:44.243: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:44.243: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:04:44.248: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 24 12:04:44.249: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/24/23 12:04:44.254
    Feb 24 12:04:44.258: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/24/23 12:04:44.258
    Feb 24 12:04:44.272: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/24/23 12:04:44.272
    Feb 24 12:04:44.275: INFO: Observed &DaemonSet event: ADDED
    Feb 24 12:04:44.276: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.276: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.276: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.277: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.277: INFO: Found daemon set daemon-set in namespace daemonsets-3919 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 12:04:44.277: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/24/23 12:04:44.277
    STEP: watching for the daemon set status to be patched 02/24/23 12:04:44.288
    Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: ADDED
    Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.292: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.292: INFO: Observed daemon set daemon-set in namespace daemonsets-3919 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 12:04:44.293: INFO: Observed &DaemonSet event: MODIFIED
    Feb 24 12:04:44.293: INFO: Found daemon set daemon-set in namespace daemonsets-3919 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 24 12:04:44.293: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/24/23 12:04:44.303
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3919, will wait for the garbage collector to delete the pods 02/24/23 12:04:44.304
    Feb 24 12:04:44.378: INFO: Deleting DaemonSet.extensions daemon-set took: 10.713856ms
    Feb 24 12:04:44.479: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820621ms
    Feb 24 12:04:47.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 24 12:04:47.085: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 24 12:04:47.089: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35017"},"items":null}

    Feb 24 12:04:47.094: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35017"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 12:04:47.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3919" for this suite. 02/24/23 12:04:47.131
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:47.142
Feb 24 12:04:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:04:47.143
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:47.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:47.203
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:04:47.233
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:04:47.783
STEP: Deploying the webhook pod 02/24/23 12:04:47.795
STEP: Wait for the deployment to be ready 02/24/23 12:04:47.811
Feb 24 12:04:47.822: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:04:49.836
STEP: Verifying the service has paired with the endpoint 02/24/23 12:04:49.859
Feb 24 12:04:50.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 12:04:50.866
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 12:04:50.889
STEP: Creating a dummy validating-webhook-configuration object 02/24/23 12:04:50.907
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/24/23 12:04:50.92
STEP: Creating a dummy mutating-webhook-configuration object 02/24/23 12:04:50.929
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/24/23 12:04:50.94
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:04:50.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4262" for this suite. 02/24/23 12:04:50.991
STEP: Destroying namespace "webhook-4262-markers" for this suite. 02/24/23 12:04:51.03
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":243,"skipped":4137,"failed":0}
------------------------------
• [3.988 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:47.142
    Feb 24 12:04:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:04:47.143
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:47.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:47.203
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:04:47.233
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:04:47.783
    STEP: Deploying the webhook pod 02/24/23 12:04:47.795
    STEP: Wait for the deployment to be ready 02/24/23 12:04:47.811
    Feb 24 12:04:47.822: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:04:49.836
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:04:49.859
    Feb 24 12:04:50.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 12:04:50.866
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/24/23 12:04:50.889
    STEP: Creating a dummy validating-webhook-configuration object 02/24/23 12:04:50.907
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/24/23 12:04:50.92
    STEP: Creating a dummy mutating-webhook-configuration object 02/24/23 12:04:50.929
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/24/23 12:04:50.94
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:04:50.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4262" for this suite. 02/24/23 12:04:50.991
    STEP: Destroying namespace "webhook-4262-markers" for this suite. 02/24/23 12:04:51.03
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:51.132
Feb 24 12:04:51.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 12:04:51.133
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:51.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:51.178
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-395c63d9-a78f-4b75-8ca5-d3d870903154 02/24/23 12:04:51.182
STEP: Creating a pod to test consume configMaps 02/24/23 12:04:51.189
Feb 24 12:04:51.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525" in namespace "configmap-3432" to be "Succeeded or Failed"
Feb 24 12:04:51.207: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233824ms
Feb 24 12:04:53.212: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01299523s
Feb 24 12:04:55.212: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013049596s
STEP: Saw pod success 02/24/23 12:04:55.212
Feb 24 12:04:55.213: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525" satisfied condition "Succeeded or Failed"
Feb 24 12:04:55.217: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:04:55.229
Feb 24 12:04:55.253: INFO: Waiting for pod pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525 to disappear
Feb 24 12:04:55.258: INFO: Pod pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 12:04:55.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3432" for this suite. 02/24/23 12:04:55.265
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":244,"skipped":4155,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:51.132
    Feb 24 12:04:51.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 12:04:51.133
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:51.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:51.178
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-395c63d9-a78f-4b75-8ca5-d3d870903154 02/24/23 12:04:51.182
    STEP: Creating a pod to test consume configMaps 02/24/23 12:04:51.189
    Feb 24 12:04:51.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525" in namespace "configmap-3432" to be "Succeeded or Failed"
    Feb 24 12:04:51.207: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233824ms
    Feb 24 12:04:53.212: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01299523s
    Feb 24 12:04:55.212: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013049596s
    STEP: Saw pod success 02/24/23 12:04:55.212
    Feb 24 12:04:55.213: INFO: Pod "pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525" satisfied condition "Succeeded or Failed"
    Feb 24 12:04:55.217: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:04:55.229
    Feb 24 12:04:55.253: INFO: Waiting for pod pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525 to disappear
    Feb 24 12:04:55.258: INFO: Pod pod-configmaps-fb6d5950-648e-483f-ab48-5e3ad0b00525 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 12:04:55.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3432" for this suite. 02/24/23 12:04:55.265
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:04:55.278
Feb 24 12:04:55.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:04:55.279
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:55.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:55.308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:04:55.331
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:04:56.103
STEP: Deploying the webhook pod 02/24/23 12:04:56.112
STEP: Wait for the deployment to be ready 02/24/23 12:04:56.136
Feb 24 12:04:56.170: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:04:58.185
STEP: Verifying the service has paired with the endpoint 02/24/23 12:04:58.272
Feb 24 12:04:59.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Feb 24 12:04:59.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5811-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 12:04:59.793
STEP: Creating a custom resource while v1 is storage version 02/24/23 12:04:59.819
STEP: Patching Custom Resource Definition to set v2 as storage 02/24/23 12:05:01.895
STEP: Patching the custom resource while v2 is storage version 02/24/23 12:05:01.937
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:05:02.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2887" for this suite. 02/24/23 12:05:02.573
STEP: Destroying namespace "webhook-2887-markers" for this suite. 02/24/23 12:05:02.59
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":245,"skipped":4159,"failed":0}
------------------------------
• [SLOW TEST] [7.457 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:04:55.278
    Feb 24 12:04:55.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:04:55.279
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:04:55.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:04:55.308
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:04:55.331
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:04:56.103
    STEP: Deploying the webhook pod 02/24/23 12:04:56.112
    STEP: Wait for the deployment to be ready 02/24/23 12:04:56.136
    Feb 24 12:04:56.170: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:04:58.185
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:04:58.272
    Feb 24 12:04:59.272: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Feb 24 12:04:59.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5811-crds.webhook.example.com via the AdmissionRegistration API 02/24/23 12:04:59.793
    STEP: Creating a custom resource while v1 is storage version 02/24/23 12:04:59.819
    STEP: Patching Custom Resource Definition to set v2 as storage 02/24/23 12:05:01.895
    STEP: Patching the custom resource while v2 is storage version 02/24/23 12:05:01.937
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:05:02.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2887" for this suite. 02/24/23 12:05:02.573
    STEP: Destroying namespace "webhook-2887-markers" for this suite. 02/24/23 12:05:02.59
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:05:02.738
Feb 24 12:05:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 12:05:02.739
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:02.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:02.785
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7975 02/24/23 12:05:02.798
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7975 02/24/23 12:05:02.808
Feb 24 12:05:02.833: INFO: Found 0 stateful pods, waiting for 1
Feb 24 12:05:12.841: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/24/23 12:05:12.854
STEP: updating a scale subresource 02/24/23 12:05:12.862
STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 12:05:12.872
STEP: Patch a scale subresource 02/24/23 12:05:12.885
STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 12:05:12.901
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 12:05:12.913: INFO: Deleting all statefulset in ns statefulset-7975
Feb 24 12:05:12.923: INFO: Scaling statefulset ss to 0
Feb 24 12:05:22.974: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 12:05:22.978: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 12:05:23.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7975" for this suite. 02/24/23 12:05:23.011
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":246,"skipped":4160,"failed":0}
------------------------------
• [SLOW TEST] [20.285 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:05:02.738
    Feb 24 12:05:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 12:05:02.739
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:02.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:02.785
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7975 02/24/23 12:05:02.798
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7975 02/24/23 12:05:02.808
    Feb 24 12:05:02.833: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 12:05:12.841: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/24/23 12:05:12.854
    STEP: updating a scale subresource 02/24/23 12:05:12.862
    STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 12:05:12.872
    STEP: Patch a scale subresource 02/24/23 12:05:12.885
    STEP: verifying the statefulset Spec.Replicas was modified 02/24/23 12:05:12.901
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 12:05:12.913: INFO: Deleting all statefulset in ns statefulset-7975
    Feb 24 12:05:12.923: INFO: Scaling statefulset ss to 0
    Feb 24 12:05:22.974: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 12:05:22.978: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 12:05:23.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7975" for this suite. 02/24/23 12:05:23.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:05:23.024
Feb 24 12:05:23.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-webhook 02/24/23 12:05:23.025
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:23.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:23.05
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/24/23 12:05:23.052
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 12:05:23.605
STEP: Deploying the custom resource conversion webhook pod 02/24/23 12:05:23.617
STEP: Wait for the deployment to be ready 02/24/23 12:05:23.632
Feb 24 12:05:23.661: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:05:25.677
STEP: Verifying the service has paired with the endpoint 02/24/23 12:05:25.704
Feb 24 12:05:26.704: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 24 12:05:26.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Creating a v1 custom resource 02/24/23 12:05:29.314
STEP: v2 custom resource should be converted 02/24/23 12:05:29.324
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:05:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5518" for this suite. 02/24/23 12:05:29.861
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":247,"skipped":4173,"failed":0}
------------------------------
• [SLOW TEST] [6.992 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:05:23.024
    Feb 24 12:05:23.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-webhook 02/24/23 12:05:23.025
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:23.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:23.05
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/24/23 12:05:23.052
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 12:05:23.605
    STEP: Deploying the custom resource conversion webhook pod 02/24/23 12:05:23.617
    STEP: Wait for the deployment to be ready 02/24/23 12:05:23.632
    Feb 24 12:05:23.661: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:05:25.677
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:05:25.704
    Feb 24 12:05:26.704: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 24 12:05:26.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Creating a v1 custom resource 02/24/23 12:05:29.314
    STEP: v2 custom resource should be converted 02/24/23 12:05:29.324
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:05:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5518" for this suite. 02/24/23 12:05:29.861
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:05:30.016
Feb 24 12:05:30.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/24/23 12:05:30.018
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:30.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:30.127
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/24/23 12:05:30.132
STEP: Creating hostNetwork=false pod 02/24/23 12:05:30.135
Feb 24 12:05:30.155: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3185" to be "running and ready"
Feb 24 12:05:30.169: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.237923ms
Feb 24 12:05:30.169: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:05:32.176: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020503405s
Feb 24 12:05:32.176: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 24 12:05:32.176: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/24/23 12:05:32.182
Feb 24 12:05:32.190: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3185" to be "running and ready"
Feb 24 12:05:32.197: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.663403ms
Feb 24 12:05:32.197: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:05:34.204: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013706856s
Feb 24 12:05:34.204: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 24 12:05:34.204: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/24/23 12:05:34.218
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/24/23 12:05:34.218
Feb 24 12:05:34.218: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.218: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.219: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 12:05:34.324: INFO: Exec stderr: ""
Feb 24 12:05:34.325: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.325: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.325: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 12:05:34.400: INFO: Exec stderr: ""
Feb 24 12:05:34.400: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.401: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.401: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 12:05:34.475: INFO: Exec stderr: ""
Feb 24 12:05:34.475: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.475: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.476: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 12:05:34.565: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/24/23 12:05:34.565
Feb 24 12:05:34.565: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.566: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.566: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 24 12:05:34.637: INFO: Exec stderr: ""
Feb 24 12:05:34.637: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.638: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.638: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 24 12:05:34.730: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/24/23 12:05:34.73
Feb 24 12:05:34.730: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.731: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.731: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 12:05:34.807: INFO: Exec stderr: ""
Feb 24 12:05:34.808: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.809: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.809: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 24 12:05:34.898: INFO: Exec stderr: ""
Feb 24 12:05:34.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.898: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 12:05:34.967: INFO: Exec stderr: ""
Feb 24 12:05:34.967: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:05:34.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:05:34.968: INFO: ExecWithOptions: Clientset creation
Feb 24 12:05:34.968: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 24 12:05:35.050: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Feb 24 12:05:35.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3185" for this suite. 02/24/23 12:05:35.058
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":248,"skipped":4173,"failed":0}
------------------------------
• [SLOW TEST] [5.054 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:05:30.016
    Feb 24 12:05:30.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/24/23 12:05:30.018
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:30.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:30.127
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/24/23 12:05:30.132
    STEP: Creating hostNetwork=false pod 02/24/23 12:05:30.135
    Feb 24 12:05:30.155: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3185" to be "running and ready"
    Feb 24 12:05:30.169: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.237923ms
    Feb 24 12:05:30.169: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:05:32.176: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020503405s
    Feb 24 12:05:32.176: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 24 12:05:32.176: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/24/23 12:05:32.182
    Feb 24 12:05:32.190: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3185" to be "running and ready"
    Feb 24 12:05:32.197: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.663403ms
    Feb 24 12:05:32.197: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:05:34.204: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013706856s
    Feb 24 12:05:34.204: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 24 12:05:34.204: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/24/23 12:05:34.218
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/24/23 12:05:34.218
    Feb 24 12:05:34.218: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.218: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.219: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 12:05:34.324: INFO: Exec stderr: ""
    Feb 24 12:05:34.325: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.325: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.325: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 12:05:34.400: INFO: Exec stderr: ""
    Feb 24 12:05:34.400: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.401: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.401: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 12:05:34.475: INFO: Exec stderr: ""
    Feb 24 12:05:34.475: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.475: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.476: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 12:05:34.565: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/24/23 12:05:34.565
    Feb 24 12:05:34.565: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.566: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.566: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 24 12:05:34.637: INFO: Exec stderr: ""
    Feb 24 12:05:34.637: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.638: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.638: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 24 12:05:34.730: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/24/23 12:05:34.73
    Feb 24 12:05:34.730: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.731: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.731: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 12:05:34.807: INFO: Exec stderr: ""
    Feb 24 12:05:34.808: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.809: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.809: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 24 12:05:34.898: INFO: Exec stderr: ""
    Feb 24 12:05:34.898: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.898: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.899: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 12:05:34.967: INFO: Exec stderr: ""
    Feb 24 12:05:34.967: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3185 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:05:34.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:05:34.968: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:05:34.968: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3185/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 24 12:05:35.050: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Feb 24 12:05:35.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3185" for this suite. 02/24/23 12:05:35.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:05:35.072
Feb 24 12:05:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 12:05:35.073
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:35.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:35.099
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-63ecb64c-cd09-499f-a499-22299ddef468 in namespace container-probe-9328 02/24/23 12:05:35.109
Feb 24 12:05:35.140: INFO: Waiting up to 5m0s for pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468" in namespace "container-probe-9328" to be "not pending"
Feb 24 12:05:35.158: INFO: Pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468": Phase="Pending", Reason="", readiness=false. Elapsed: 17.820107ms
Feb 24 12:05:37.164: INFO: Pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468": Phase="Running", Reason="", readiness=true. Elapsed: 2.024189526s
Feb 24 12:05:37.164: INFO: Pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468" satisfied condition "not pending"
Feb 24 12:05:37.164: INFO: Started pod liveness-63ecb64c-cd09-499f-a499-22299ddef468 in namespace container-probe-9328
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:05:37.164
Feb 24 12:05:37.169: INFO: Initial restart count of pod liveness-63ecb64c-cd09-499f-a499-22299ddef468 is 0
Feb 24 12:05:57.254: INFO: Restart count of pod container-probe-9328/liveness-63ecb64c-cd09-499f-a499-22299ddef468 is now 1 (20.08473832s elapsed)
STEP: deleting the pod 02/24/23 12:05:57.254
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 12:05:57.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9328" for this suite. 02/24/23 12:05:57.463
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":249,"skipped":4178,"failed":0}
------------------------------
• [SLOW TEST] [22.454 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:05:35.072
    Feb 24 12:05:35.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:05:35.073
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:35.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:35.099
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-63ecb64c-cd09-499f-a499-22299ddef468 in namespace container-probe-9328 02/24/23 12:05:35.109
    Feb 24 12:05:35.140: INFO: Waiting up to 5m0s for pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468" in namespace "container-probe-9328" to be "not pending"
    Feb 24 12:05:35.158: INFO: Pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468": Phase="Pending", Reason="", readiness=false. Elapsed: 17.820107ms
    Feb 24 12:05:37.164: INFO: Pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468": Phase="Running", Reason="", readiness=true. Elapsed: 2.024189526s
    Feb 24 12:05:37.164: INFO: Pod "liveness-63ecb64c-cd09-499f-a499-22299ddef468" satisfied condition "not pending"
    Feb 24 12:05:37.164: INFO: Started pod liveness-63ecb64c-cd09-499f-a499-22299ddef468 in namespace container-probe-9328
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:05:37.164
    Feb 24 12:05:37.169: INFO: Initial restart count of pod liveness-63ecb64c-cd09-499f-a499-22299ddef468 is 0
    Feb 24 12:05:57.254: INFO: Restart count of pod container-probe-9328/liveness-63ecb64c-cd09-499f-a499-22299ddef468 is now 1 (20.08473832s elapsed)
    STEP: deleting the pod 02/24/23 12:05:57.254
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 12:05:57.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9328" for this suite. 02/24/23 12:05:57.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:05:57.527
Feb 24 12:05:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:05:57.528
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:57.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:57.591
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:05:57.632
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:05:58.381
STEP: Deploying the webhook pod 02/24/23 12:05:58.393
STEP: Wait for the deployment to be ready 02/24/23 12:05:58.415
Feb 24 12:05:58.437: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:06:00.454
STEP: Verifying the service has paired with the endpoint 02/24/23 12:06:00.491
Feb 24 12:06:01.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 02/24/23 12:06:01.498
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/24/23 12:06:01.499
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 12:06:01.499
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:06:01.5
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:06:01.513
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:06:01.514
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:06:01.52
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:06:01.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4492" for this suite. 02/24/23 12:06:01.547
STEP: Destroying namespace "webhook-4492-markers" for this suite. 02/24/23 12:06:01.592
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":250,"skipped":4200,"failed":0}
------------------------------
• [4.542 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:05:57.527
    Feb 24 12:05:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:05:57.528
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:05:57.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:05:57.591
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:05:57.632
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:05:58.381
    STEP: Deploying the webhook pod 02/24/23 12:05:58.393
    STEP: Wait for the deployment to be ready 02/24/23 12:05:58.415
    Feb 24 12:05:58.437: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:06:00.454
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:06:00.491
    Feb 24 12:06:01.492: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 02/24/23 12:06:01.498
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/24/23 12:06:01.499
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 12:06:01.499
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:06:01.5
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/24/23 12:06:01.513
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:06:01.514
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/24/23 12:06:01.52
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:06:01.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4492" for this suite. 02/24/23 12:06:01.547
    STEP: Destroying namespace "webhook-4492-markers" for this suite. 02/24/23 12:06:01.592
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:02.07
Feb 24 12:06:02.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:06:02.073
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:02.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:02.196
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 24 12:06:06.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3401" for this suite. 02/24/23 12:06:06.287
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":251,"skipped":4208,"failed":0}
------------------------------
• [4.247 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:02.07
    Feb 24 12:06:02.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:06:02.073
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:02.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:02.196
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 24 12:06:06.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3401" for this suite. 02/24/23 12:06:06.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:06.32
Feb 24 12:06:06.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:06:06.322
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:06.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:06.375
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 02/24/23 12:06:06.378
STEP: wait for the container to reach Succeeded 02/24/23 12:06:06.394
STEP: get the container status 02/24/23 12:06:10.425
STEP: the container should be terminated 02/24/23 12:06:10.431
STEP: the termination message should be set 02/24/23 12:06:10.431
Feb 24 12:06:10.431: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/24/23 12:06:10.431
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 24 12:06:10.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3183" for this suite. 02/24/23 12:06:10.463
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":252,"skipped":4214,"failed":0}
------------------------------
• [4.165 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:06.32
    Feb 24 12:06:06.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:06:06.322
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:06.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:06.375
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 02/24/23 12:06:06.378
    STEP: wait for the container to reach Succeeded 02/24/23 12:06:06.394
    STEP: get the container status 02/24/23 12:06:10.425
    STEP: the container should be terminated 02/24/23 12:06:10.431
    STEP: the termination message should be set 02/24/23 12:06:10.431
    Feb 24 12:06:10.431: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/24/23 12:06:10.431
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 24 12:06:10.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3183" for this suite. 02/24/23 12:06:10.463
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:10.486
Feb 24 12:06:10.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:06:10.489
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:10.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:10.518
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-ac01998c-706e-4223-818b-90da8e556f21 02/24/23 12:06:10.528
STEP: Creating configMap with name cm-test-opt-upd-97886c38-f15b-45e8-9be0-106d33a09a3e 02/24/23 12:06:10.551
STEP: Creating the pod 02/24/23 12:06:10.557
Feb 24 12:06:10.572: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6" in namespace "projected-2482" to be "running and ready"
Feb 24 12:06:10.583: INFO: Pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.157822ms
Feb 24 12:06:10.583: INFO: The phase of Pod pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:06:12.590: INFO: Pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017926986s
Feb 24 12:06:12.590: INFO: The phase of Pod pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6 is Running (Ready = true)
Feb 24 12:06:12.590: INFO: Pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ac01998c-706e-4223-818b-90da8e556f21 02/24/23 12:06:12.626
STEP: Updating configmap cm-test-opt-upd-97886c38-f15b-45e8-9be0-106d33a09a3e 02/24/23 12:06:12.635
STEP: Creating configMap with name cm-test-opt-create-4e4a5537-e75a-4bc9-ab1c-9b53585223e4 02/24/23 12:06:12.641
STEP: waiting to observe update in volume 02/24/23 12:06:12.647
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 12:06:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2482" for this suite. 02/24/23 12:06:14.696
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":253,"skipped":4214,"failed":0}
------------------------------
• [4.222 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:10.486
    Feb 24 12:06:10.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:06:10.489
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:10.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:10.518
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-ac01998c-706e-4223-818b-90da8e556f21 02/24/23 12:06:10.528
    STEP: Creating configMap with name cm-test-opt-upd-97886c38-f15b-45e8-9be0-106d33a09a3e 02/24/23 12:06:10.551
    STEP: Creating the pod 02/24/23 12:06:10.557
    Feb 24 12:06:10.572: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6" in namespace "projected-2482" to be "running and ready"
    Feb 24 12:06:10.583: INFO: Pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.157822ms
    Feb 24 12:06:10.583: INFO: The phase of Pod pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:06:12.590: INFO: Pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017926986s
    Feb 24 12:06:12.590: INFO: The phase of Pod pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6 is Running (Ready = true)
    Feb 24 12:06:12.590: INFO: Pod "pod-projected-configmaps-1c7e6e6f-0f4d-477b-8237-2952b73d0dc6" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ac01998c-706e-4223-818b-90da8e556f21 02/24/23 12:06:12.626
    STEP: Updating configmap cm-test-opt-upd-97886c38-f15b-45e8-9be0-106d33a09a3e 02/24/23 12:06:12.635
    STEP: Creating configMap with name cm-test-opt-create-4e4a5537-e75a-4bc9-ab1c-9b53585223e4 02/24/23 12:06:12.641
    STEP: waiting to observe update in volume 02/24/23 12:06:12.647
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 12:06:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2482" for this suite. 02/24/23 12:06:14.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:14.726
Feb 24 12:06:14.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:06:14.727
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:14.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:14.753
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:06:14.756
Feb 24 12:06:14.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a" in namespace "projected-6165" to be "Succeeded or Failed"
Feb 24 12:06:14.775: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.495539ms
Feb 24 12:06:16.781: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014958851s
Feb 24 12:06:18.781: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014149893s
STEP: Saw pod success 02/24/23 12:06:18.781
Feb 24 12:06:18.781: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a" satisfied condition "Succeeded or Failed"
Feb 24 12:06:18.785: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a container client-container: <nil>
STEP: delete the pod 02/24/23 12:06:18.805
Feb 24 12:06:18.829: INFO: Waiting for pod downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a to disappear
Feb 24 12:06:18.834: INFO: Pod downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:06:18.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6165" for this suite. 02/24/23 12:06:18.842
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4413,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:14.726
    Feb 24 12:06:14.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:06:14.727
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:14.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:14.753
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:06:14.756
    Feb 24 12:06:14.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a" in namespace "projected-6165" to be "Succeeded or Failed"
    Feb 24 12:06:14.775: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.495539ms
    Feb 24 12:06:16.781: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014958851s
    Feb 24 12:06:18.781: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014149893s
    STEP: Saw pod success 02/24/23 12:06:18.781
    Feb 24 12:06:18.781: INFO: Pod "downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a" satisfied condition "Succeeded or Failed"
    Feb 24 12:06:18.785: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a container client-container: <nil>
    STEP: delete the pod 02/24/23 12:06:18.805
    Feb 24 12:06:18.829: INFO: Waiting for pod downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a to disappear
    Feb 24 12:06:18.834: INFO: Pod downwardapi-volume-e08b92ca-a98b-4e5a-864c-39b4a428bf2a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:06:18.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6165" for this suite. 02/24/23 12:06:18.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:18.858
Feb 24 12:06:18.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:06:18.859
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:18.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:18.888
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3743-delete-me 02/24/23 12:06:18.907
STEP: Waiting for the RuntimeClass to disappear 02/24/23 12:06:18.918
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 24 12:06:18.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3743" for this suite. 02/24/23 12:06:18.94
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":255,"skipped":4429,"failed":0}
------------------------------
• [0.093 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:18.858
    Feb 24 12:06:18.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:06:18.859
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:18.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:18.888
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3743-delete-me 02/24/23 12:06:18.907
    STEP: Waiting for the RuntimeClass to disappear 02/24/23 12:06:18.918
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 24 12:06:18.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3743" for this suite. 02/24/23 12:06:18.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:18.952
Feb 24 12:06:18.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 12:06:18.953
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:18.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:18.988
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:06:18.991
Feb 24 12:06:19.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835" in namespace "downward-api-5244" to be "Succeeded or Failed"
Feb 24 12:06:19.021: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835": Phase="Pending", Reason="", readiness=false. Elapsed: 9.684143ms
Feb 24 12:06:21.028: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016673846s
Feb 24 12:06:23.030: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018892958s
STEP: Saw pod success 02/24/23 12:06:23.03
Feb 24 12:06:23.030: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835" satisfied condition "Succeeded or Failed"
Feb 24 12:06:23.035: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835 container client-container: <nil>
STEP: delete the pod 02/24/23 12:06:23.044
Feb 24 12:06:23.063: INFO: Waiting for pod downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835 to disappear
Feb 24 12:06:23.070: INFO: Pod downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 12:06:23.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5244" for this suite. 02/24/23 12:06:23.078
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":256,"skipped":4445,"failed":0}
------------------------------
• [4.135 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:18.952
    Feb 24 12:06:18.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:06:18.953
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:18.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:18.988
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:06:18.991
    Feb 24 12:06:19.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835" in namespace "downward-api-5244" to be "Succeeded or Failed"
    Feb 24 12:06:19.021: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835": Phase="Pending", Reason="", readiness=false. Elapsed: 9.684143ms
    Feb 24 12:06:21.028: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016673846s
    Feb 24 12:06:23.030: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018892958s
    STEP: Saw pod success 02/24/23 12:06:23.03
    Feb 24 12:06:23.030: INFO: Pod "downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835" satisfied condition "Succeeded or Failed"
    Feb 24 12:06:23.035: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:06:23.044
    Feb 24 12:06:23.063: INFO: Waiting for pod downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835 to disappear
    Feb 24 12:06:23.070: INFO: Pod downwardapi-volume-f10f8337-dd26-448f-a9f9-ca93d7ea3835 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 12:06:23.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5244" for this suite. 02/24/23 12:06:23.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:23.089
Feb 24 12:06:23.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:06:23.09
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:23.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:23.117
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-56699e1d-3993-4426-9f30-def29f25b46a 02/24/23 12:06:23.12
STEP: Creating a pod to test consume secrets 02/24/23 12:06:23.127
Feb 24 12:06:23.139: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f" in namespace "projected-5669" to be "Succeeded or Failed"
Feb 24 12:06:23.145: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953576ms
Feb 24 12:06:25.151: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f": Phase="Running", Reason="", readiness=false. Elapsed: 2.011633774s
Feb 24 12:06:27.151: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01185619s
STEP: Saw pod success 02/24/23 12:06:27.152
Feb 24 12:06:27.152: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f" satisfied condition "Succeeded or Failed"
Feb 24 12:06:27.158: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f container projected-secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:06:27.17
Feb 24 12:06:27.200: INFO: Waiting for pod pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f to disappear
Feb 24 12:06:27.212: INFO: Pod pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 12:06:27.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5669" for this suite. 02/24/23 12:06:27.227
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":257,"skipped":4459,"failed":0}
------------------------------
• [4.163 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:23.089
    Feb 24 12:06:23.089: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:06:23.09
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:23.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:23.117
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-56699e1d-3993-4426-9f30-def29f25b46a 02/24/23 12:06:23.12
    STEP: Creating a pod to test consume secrets 02/24/23 12:06:23.127
    Feb 24 12:06:23.139: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f" in namespace "projected-5669" to be "Succeeded or Failed"
    Feb 24 12:06:23.145: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953576ms
    Feb 24 12:06:25.151: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f": Phase="Running", Reason="", readiness=false. Elapsed: 2.011633774s
    Feb 24 12:06:27.151: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01185619s
    STEP: Saw pod success 02/24/23 12:06:27.152
    Feb 24 12:06:27.152: INFO: Pod "pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f" satisfied condition "Succeeded or Failed"
    Feb 24 12:06:27.158: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:06:27.17
    Feb 24 12:06:27.200: INFO: Waiting for pod pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f to disappear
    Feb 24 12:06:27.212: INFO: Pod pod-projected-secrets-71e7b2ac-4a15-4cb3-aa41-b7682d88cf7f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 12:06:27.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5669" for this suite. 02/24/23 12:06:27.227
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:27.256
Feb 24 12:06:27.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename lease-test 02/24/23 12:06:27.257
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:27.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:27.289
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Feb 24 12:06:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5524" for this suite. 02/24/23 12:06:27.389
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":258,"skipped":4461,"failed":0}
------------------------------
• [0.144 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:27.256
    Feb 24 12:06:27.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename lease-test 02/24/23 12:06:27.257
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:27.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:27.289
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Feb 24 12:06:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-5524" for this suite. 02/24/23 12:06:27.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:06:27.404
Feb 24 12:06:27.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 12:06:27.406
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:27.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:27.462
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d in namespace container-probe-5726 02/24/23 12:06:27.471
Feb 24 12:06:27.487: INFO: Waiting up to 5m0s for pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d" in namespace "container-probe-5726" to be "not pending"
Feb 24 12:06:27.495: INFO: Pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.21525ms
Feb 24 12:06:29.501: INFO: Pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013153866s
Feb 24 12:06:29.501: INFO: Pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d" satisfied condition "not pending"
Feb 24 12:06:29.502: INFO: Started pod test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d in namespace container-probe-5726
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:06:29.502
Feb 24 12:06:29.509: INFO: Initial restart count of pod test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d is 0
STEP: deleting the pod 02/24/23 12:10:30.444
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 12:10:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5726" for this suite. 02/24/23 12:10:30.475
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":259,"skipped":4496,"failed":0}
------------------------------
• [SLOW TEST] [243.082 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:06:27.404
    Feb 24 12:06:27.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:06:27.406
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:06:27.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:06:27.462
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d in namespace container-probe-5726 02/24/23 12:06:27.471
    Feb 24 12:06:27.487: INFO: Waiting up to 5m0s for pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d" in namespace "container-probe-5726" to be "not pending"
    Feb 24 12:06:27.495: INFO: Pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.21525ms
    Feb 24 12:06:29.501: INFO: Pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013153866s
    Feb 24 12:06:29.501: INFO: Pod "test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d" satisfied condition "not pending"
    Feb 24 12:06:29.502: INFO: Started pod test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d in namespace container-probe-5726
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:06:29.502
    Feb 24 12:06:29.509: INFO: Initial restart count of pod test-webserver-1e8e801e-31f9-4ca1-8843-9d1ca712f38d is 0
    STEP: deleting the pod 02/24/23 12:10:30.444
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 12:10:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5726" for this suite. 02/24/23 12:10:30.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:10:30.491
Feb 24 12:10:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:10:30.508
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:10:30.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:10:30.538
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Feb 24 12:10:30.568: INFO: created pod
Feb 24 12:10:30.568: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4417" to be "Succeeded or Failed"
Feb 24 12:10:30.575: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.257197ms
Feb 24 12:10:32.584: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015172199s
Feb 24 12:10:34.583: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014914633s
STEP: Saw pod success 02/24/23 12:10:34.583
Feb 24 12:10:34.583: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 24 12:11:04.584: INFO: polling logs
Feb 24 12:11:04.608: INFO: Pod logs: 
I0224 12:10:31.254084       1 log.go:195] OK: Got token
I0224 12:10:31.255002       1 log.go:195] validating with in-cluster discovery
I0224 12:10:31.255398       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0224 12:10:31.255476       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4417:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241230, NotBefore:1677240630, IssuedAt:1677240630, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4417", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b0f55fc2-9781-4aa2-8090-e3142b6a4ad6"}}}
I0224 12:10:31.289475       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0224 12:10:31.306597       1 log.go:195] OK: Validated signature on JWT
I0224 12:10:31.306699       1 log.go:195] OK: Got valid claims from token!
I0224 12:10:31.306729       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4417:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241230, NotBefore:1677240630, IssuedAt:1677240630, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4417", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b0f55fc2-9781-4aa2-8090-e3142b6a4ad6"}}}

Feb 24 12:11:04.608: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 24 12:11:04.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4417" for this suite. 02/24/23 12:11:04.628
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":260,"skipped":4576,"failed":0}
------------------------------
• [SLOW TEST] [34.155 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:10:30.491
    Feb 24 12:10:30.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:10:30.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:10:30.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:10:30.538
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Feb 24 12:10:30.568: INFO: created pod
    Feb 24 12:10:30.568: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4417" to be "Succeeded or Failed"
    Feb 24 12:10:30.575: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.257197ms
    Feb 24 12:10:32.584: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015172199s
    Feb 24 12:10:34.583: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014914633s
    STEP: Saw pod success 02/24/23 12:10:34.583
    Feb 24 12:10:34.583: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:04.584: INFO: polling logs
    Feb 24 12:11:04.608: INFO: Pod logs: 
    I0224 12:10:31.254084       1 log.go:195] OK: Got token
    I0224 12:10:31.255002       1 log.go:195] validating with in-cluster discovery
    I0224 12:10:31.255398       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0224 12:10:31.255476       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4417:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241230, NotBefore:1677240630, IssuedAt:1677240630, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4417", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b0f55fc2-9781-4aa2-8090-e3142b6a4ad6"}}}
    I0224 12:10:31.289475       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0224 12:10:31.306597       1 log.go:195] OK: Validated signature on JWT
    I0224 12:10:31.306699       1 log.go:195] OK: Got valid claims from token!
    I0224 12:10:31.306729       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4417:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677241230, NotBefore:1677240630, IssuedAt:1677240630, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4417", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b0f55fc2-9781-4aa2-8090-e3142b6a4ad6"}}}

    Feb 24 12:11:04.608: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 24 12:11:04.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4417" for this suite. 02/24/23 12:11:04.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:04.647
Feb 24 12:11:04.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 12:11:04.648
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:04.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:04.704
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 02/24/23 12:11:04.707
Feb 24 12:11:04.721: INFO: Waiting up to 5m0s for pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f" in namespace "pods-9740" to be "running and ready"
Feb 24 12:11:04.758: INFO: Pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.241071ms
Feb 24 12:11:04.758: INFO: The phase of Pod pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:11:06.764: INFO: Pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f": Phase="Running", Reason="", readiness=true. Elapsed: 2.042279631s
Feb 24 12:11:06.764: INFO: The phase of Pod pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f is Running (Ready = true)
Feb 24 12:11:06.764: INFO: Pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f" satisfied condition "running and ready"
Feb 24 12:11:06.775: INFO: Pod pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f has hostIP: 172.31.216.47
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 12:11:06.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9740" for this suite. 02/24/23 12:11:06.783
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":261,"skipped":4586,"failed":0}
------------------------------
• [2.150 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:04.647
    Feb 24 12:11:04.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 12:11:04.648
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:04.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:04.704
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 02/24/23 12:11:04.707
    Feb 24 12:11:04.721: INFO: Waiting up to 5m0s for pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f" in namespace "pods-9740" to be "running and ready"
    Feb 24 12:11:04.758: INFO: Pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.241071ms
    Feb 24 12:11:04.758: INFO: The phase of Pod pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:11:06.764: INFO: Pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f": Phase="Running", Reason="", readiness=true. Elapsed: 2.042279631s
    Feb 24 12:11:06.764: INFO: The phase of Pod pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f is Running (Ready = true)
    Feb 24 12:11:06.764: INFO: Pod "pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f" satisfied condition "running and ready"
    Feb 24 12:11:06.775: INFO: Pod pod-hostip-74cf90c1-b65c-441b-85f4-fb943c15937f has hostIP: 172.31.216.47
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 12:11:06.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9740" for this suite. 02/24/23 12:11:06.783
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:06.797
Feb 24 12:11:06.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:11:06.8
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:06.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:06.828
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 02/24/23 12:11:06.831
STEP: wait for the container to reach Succeeded 02/24/23 12:11:06.852
STEP: get the container status 02/24/23 12:11:10.886
STEP: the container should be terminated 02/24/23 12:11:10.891
STEP: the termination message should be set 02/24/23 12:11:10.891
Feb 24 12:11:10.891: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/24/23 12:11:10.891
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 24 12:11:10.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8676" for this suite. 02/24/23 12:11:10.92
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":262,"skipped":4586,"failed":0}
------------------------------
• [4.133 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:06.797
    Feb 24 12:11:06.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:11:06.8
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:06.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:06.828
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 02/24/23 12:11:06.831
    STEP: wait for the container to reach Succeeded 02/24/23 12:11:06.852
    STEP: get the container status 02/24/23 12:11:10.886
    STEP: the container should be terminated 02/24/23 12:11:10.891
    STEP: the termination message should be set 02/24/23 12:11:10.891
    Feb 24 12:11:10.891: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/24/23 12:11:10.891
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 24 12:11:10.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8676" for this suite. 02/24/23 12:11:10.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:10.932
Feb 24 12:11:10.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename discovery 02/24/23 12:11:10.933
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:10.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:10.96
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/24/23 12:11:10.963
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 24 12:11:11.342: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 24 12:11:11.342: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 24 12:11:11.342: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 24 12:11:11.342: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 24 12:11:11.343: INFO: Checking APIGroup: apps
Feb 24 12:11:11.344: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 24 12:11:11.344: INFO: Versions found [{apps/v1 v1}]
Feb 24 12:11:11.344: INFO: apps/v1 matches apps/v1
Feb 24 12:11:11.344: INFO: Checking APIGroup: events.k8s.io
Feb 24 12:11:11.345: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 24 12:11:11.345: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 24 12:11:11.345: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 24 12:11:11.345: INFO: Checking APIGroup: authentication.k8s.io
Feb 24 12:11:11.346: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 24 12:11:11.346: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 24 12:11:11.346: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 24 12:11:11.346: INFO: Checking APIGroup: authorization.k8s.io
Feb 24 12:11:11.346: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 24 12:11:11.346: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 24 12:11:11.346: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 24 12:11:11.346: INFO: Checking APIGroup: autoscaling
Feb 24 12:11:11.347: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 24 12:11:11.347: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Feb 24 12:11:11.347: INFO: autoscaling/v2 matches autoscaling/v2
Feb 24 12:11:11.347: INFO: Checking APIGroup: batch
Feb 24 12:11:11.348: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 24 12:11:11.348: INFO: Versions found [{batch/v1 v1}]
Feb 24 12:11:11.348: INFO: batch/v1 matches batch/v1
Feb 24 12:11:11.348: INFO: Checking APIGroup: certificates.k8s.io
Feb 24 12:11:11.349: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 24 12:11:11.349: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 24 12:11:11.349: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 24 12:11:11.349: INFO: Checking APIGroup: networking.k8s.io
Feb 24 12:11:11.350: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 24 12:11:11.350: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 24 12:11:11.350: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 24 12:11:11.350: INFO: Checking APIGroup: policy
Feb 24 12:11:11.351: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 24 12:11:11.351: INFO: Versions found [{policy/v1 v1}]
Feb 24 12:11:11.351: INFO: policy/v1 matches policy/v1
Feb 24 12:11:11.351: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 24 12:11:11.351: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 24 12:11:11.351: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 24 12:11:11.351: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 24 12:11:11.351: INFO: Checking APIGroup: storage.k8s.io
Feb 24 12:11:11.352: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 24 12:11:11.352: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 24 12:11:11.352: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 24 12:11:11.352: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 24 12:11:11.353: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 24 12:11:11.353: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 24 12:11:11.353: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 24 12:11:11.353: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 24 12:11:11.354: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 24 12:11:11.354: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 24 12:11:11.354: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 24 12:11:11.354: INFO: Checking APIGroup: scheduling.k8s.io
Feb 24 12:11:11.354: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 24 12:11:11.354: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 24 12:11:11.354: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 24 12:11:11.354: INFO: Checking APIGroup: coordination.k8s.io
Feb 24 12:11:11.355: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 24 12:11:11.355: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 24 12:11:11.355: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 24 12:11:11.355: INFO: Checking APIGroup: node.k8s.io
Feb 24 12:11:11.356: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 24 12:11:11.356: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 24 12:11:11.356: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 24 12:11:11.356: INFO: Checking APIGroup: discovery.k8s.io
Feb 24 12:11:11.357: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 24 12:11:11.357: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 24 12:11:11.357: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 24 12:11:11.357: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 24 12:11:11.359: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Feb 24 12:11:11.359: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 24 12:11:11.359: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Feb 24 12:11:11.359: INFO: Checking APIGroup: crd.projectcalico.org
Feb 24 12:11:11.360: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 24 12:11:11.360: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 24 12:11:11.360: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 24 12:11:11.360: INFO: Checking APIGroup: snapshot.storage.k8s.io
Feb 24 12:11:11.361: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Feb 24 12:11:11.361: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Feb 24 12:11:11.361: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Feb 24 12:11:11.361: INFO: Checking APIGroup: cluster.k8s.io
Feb 24 12:11:11.362: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Feb 24 12:11:11.362: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Feb 24 12:11:11.362: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Feb 24 12:11:11.362: INFO: Checking APIGroup: operatingsystemmanager.k8c.io
Feb 24 12:11:11.362: INFO: PreferredVersion.GroupVersion: operatingsystemmanager.k8c.io/v1alpha1
Feb 24 12:11:11.362: INFO: Versions found [{operatingsystemmanager.k8c.io/v1alpha1 v1alpha1}]
Feb 24 12:11:11.362: INFO: operatingsystemmanager.k8c.io/v1alpha1 matches operatingsystemmanager.k8c.io/v1alpha1
Feb 24 12:11:11.362: INFO: Checking APIGroup: metrics.k8s.io
Feb 24 12:11:11.363: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 24 12:11:11.363: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 24 12:11:11.363: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Feb 24 12:11:11.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2995" for this suite. 02/24/23 12:11:11.369
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":263,"skipped":4591,"failed":0}
------------------------------
• [0.448 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:10.932
    Feb 24 12:11:10.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename discovery 02/24/23 12:11:10.933
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:10.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:10.96
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/24/23 12:11:10.963
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 24 12:11:11.342: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 24 12:11:11.342: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 24 12:11:11.342: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 24 12:11:11.342: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 24 12:11:11.343: INFO: Checking APIGroup: apps
    Feb 24 12:11:11.344: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 24 12:11:11.344: INFO: Versions found [{apps/v1 v1}]
    Feb 24 12:11:11.344: INFO: apps/v1 matches apps/v1
    Feb 24 12:11:11.344: INFO: Checking APIGroup: events.k8s.io
    Feb 24 12:11:11.345: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 24 12:11:11.345: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 24 12:11:11.345: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 24 12:11:11.345: INFO: Checking APIGroup: authentication.k8s.io
    Feb 24 12:11:11.346: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 24 12:11:11.346: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 24 12:11:11.346: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 24 12:11:11.346: INFO: Checking APIGroup: authorization.k8s.io
    Feb 24 12:11:11.346: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 24 12:11:11.346: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 24 12:11:11.346: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 24 12:11:11.346: INFO: Checking APIGroup: autoscaling
    Feb 24 12:11:11.347: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 24 12:11:11.347: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Feb 24 12:11:11.347: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 24 12:11:11.347: INFO: Checking APIGroup: batch
    Feb 24 12:11:11.348: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 24 12:11:11.348: INFO: Versions found [{batch/v1 v1}]
    Feb 24 12:11:11.348: INFO: batch/v1 matches batch/v1
    Feb 24 12:11:11.348: INFO: Checking APIGroup: certificates.k8s.io
    Feb 24 12:11:11.349: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 24 12:11:11.349: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 24 12:11:11.349: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 24 12:11:11.349: INFO: Checking APIGroup: networking.k8s.io
    Feb 24 12:11:11.350: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 24 12:11:11.350: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 24 12:11:11.350: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 24 12:11:11.350: INFO: Checking APIGroup: policy
    Feb 24 12:11:11.351: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 24 12:11:11.351: INFO: Versions found [{policy/v1 v1}]
    Feb 24 12:11:11.351: INFO: policy/v1 matches policy/v1
    Feb 24 12:11:11.351: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 24 12:11:11.351: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 24 12:11:11.351: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 24 12:11:11.351: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 24 12:11:11.351: INFO: Checking APIGroup: storage.k8s.io
    Feb 24 12:11:11.352: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 24 12:11:11.352: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 24 12:11:11.352: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 24 12:11:11.352: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 24 12:11:11.353: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 24 12:11:11.353: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 24 12:11:11.353: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 24 12:11:11.353: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 24 12:11:11.354: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 24 12:11:11.354: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 24 12:11:11.354: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 24 12:11:11.354: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 24 12:11:11.354: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 24 12:11:11.354: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 24 12:11:11.354: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 24 12:11:11.354: INFO: Checking APIGroup: coordination.k8s.io
    Feb 24 12:11:11.355: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 24 12:11:11.355: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 24 12:11:11.355: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 24 12:11:11.355: INFO: Checking APIGroup: node.k8s.io
    Feb 24 12:11:11.356: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 24 12:11:11.356: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 24 12:11:11.356: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 24 12:11:11.356: INFO: Checking APIGroup: discovery.k8s.io
    Feb 24 12:11:11.357: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 24 12:11:11.357: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 24 12:11:11.357: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 24 12:11:11.357: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 24 12:11:11.359: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Feb 24 12:11:11.359: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Feb 24 12:11:11.359: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Feb 24 12:11:11.359: INFO: Checking APIGroup: crd.projectcalico.org
    Feb 24 12:11:11.360: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Feb 24 12:11:11.360: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Feb 24 12:11:11.360: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Feb 24 12:11:11.360: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Feb 24 12:11:11.361: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Feb 24 12:11:11.361: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Feb 24 12:11:11.361: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Feb 24 12:11:11.361: INFO: Checking APIGroup: cluster.k8s.io
    Feb 24 12:11:11.362: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
    Feb 24 12:11:11.362: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
    Feb 24 12:11:11.362: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
    Feb 24 12:11:11.362: INFO: Checking APIGroup: operatingsystemmanager.k8c.io
    Feb 24 12:11:11.362: INFO: PreferredVersion.GroupVersion: operatingsystemmanager.k8c.io/v1alpha1
    Feb 24 12:11:11.362: INFO: Versions found [{operatingsystemmanager.k8c.io/v1alpha1 v1alpha1}]
    Feb 24 12:11:11.362: INFO: operatingsystemmanager.k8c.io/v1alpha1 matches operatingsystemmanager.k8c.io/v1alpha1
    Feb 24 12:11:11.362: INFO: Checking APIGroup: metrics.k8s.io
    Feb 24 12:11:11.363: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Feb 24 12:11:11.363: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Feb 24 12:11:11.363: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Feb 24 12:11:11.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-2995" for this suite. 02/24/23 12:11:11.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:11.385
Feb 24 12:11:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:11:11.386
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:11.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:11.421
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Feb 24 12:11:11.458: INFO: created pod pod-service-account-defaultsa
Feb 24 12:11:11.458: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 24 12:11:11.469: INFO: created pod pod-service-account-mountsa
Feb 24 12:11:11.469: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 24 12:11:11.479: INFO: created pod pod-service-account-nomountsa
Feb 24 12:11:11.479: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 24 12:11:11.490: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 24 12:11:11.490: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 24 12:11:11.505: INFO: created pod pod-service-account-mountsa-mountspec
Feb 24 12:11:11.505: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 24 12:11:11.523: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 24 12:11:11.524: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 24 12:11:11.548: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 24 12:11:11.548: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 24 12:11:11.556: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 24 12:11:11.556: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 24 12:11:11.573: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 24 12:11:11.573: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 24 12:11:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6122" for this suite. 02/24/23 12:11:11.588
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":264,"skipped":4607,"failed":0}
------------------------------
• [0.233 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:11.385
    Feb 24 12:11:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:11:11.386
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:11.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:11.421
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Feb 24 12:11:11.458: INFO: created pod pod-service-account-defaultsa
    Feb 24 12:11:11.458: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 24 12:11:11.469: INFO: created pod pod-service-account-mountsa
    Feb 24 12:11:11.469: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 24 12:11:11.479: INFO: created pod pod-service-account-nomountsa
    Feb 24 12:11:11.479: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 24 12:11:11.490: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 24 12:11:11.490: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 24 12:11:11.505: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 24 12:11:11.505: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 24 12:11:11.523: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 24 12:11:11.524: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 24 12:11:11.548: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 24 12:11:11.548: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 24 12:11:11.556: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 24 12:11:11.556: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 24 12:11:11.573: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 24 12:11:11.573: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 24 12:11:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6122" for this suite. 02/24/23 12:11:11.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:11.646
Feb 24 12:11:11.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 12:11:11.649
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:11.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:11.709
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Feb 24 12:11:11.737: INFO: Waiting up to 5m0s for pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7" in namespace "pods-5839" to be "running and ready"
Feb 24 12:11:11.751: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.530385ms
Feb 24 12:11:11.751: INFO: The phase of Pod server-envvars-91371105-54f0-4939-92ae-540c320744a7 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:11:13.757: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019701281s
Feb 24 12:11:13.757: INFO: The phase of Pod server-envvars-91371105-54f0-4939-92ae-540c320744a7 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:11:15.758: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7": Phase="Running", Reason="", readiness=true. Elapsed: 4.020920948s
Feb 24 12:11:15.758: INFO: The phase of Pod server-envvars-91371105-54f0-4939-92ae-540c320744a7 is Running (Ready = true)
Feb 24 12:11:15.758: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7" satisfied condition "running and ready"
Feb 24 12:11:15.815: INFO: Waiting up to 5m0s for pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100" in namespace "pods-5839" to be "Succeeded or Failed"
Feb 24 12:11:15.829: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100": Phase="Pending", Reason="", readiness=false. Elapsed: 12.496199ms
Feb 24 12:11:17.836: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019288938s
Feb 24 12:11:19.836: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019285324s
STEP: Saw pod success 02/24/23 12:11:19.836
Feb 24 12:11:19.836: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100" satisfied condition "Succeeded or Failed"
Feb 24 12:11:19.841: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod client-envvars-cbd7f130-8654-4426-9a25-d068d6098100 container env3cont: <nil>
STEP: delete the pod 02/24/23 12:11:19.866
Feb 24 12:11:19.885: INFO: Waiting for pod client-envvars-cbd7f130-8654-4426-9a25-d068d6098100 to disappear
Feb 24 12:11:19.890: INFO: Pod client-envvars-cbd7f130-8654-4426-9a25-d068d6098100 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 12:11:19.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5839" for this suite. 02/24/23 12:11:19.898
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":265,"skipped":4638,"failed":0}
------------------------------
• [SLOW TEST] [8.261 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:11.646
    Feb 24 12:11:11.647: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 12:11:11.649
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:11.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:11.709
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Feb 24 12:11:11.737: INFO: Waiting up to 5m0s for pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7" in namespace "pods-5839" to be "running and ready"
    Feb 24 12:11:11.751: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.530385ms
    Feb 24 12:11:11.751: INFO: The phase of Pod server-envvars-91371105-54f0-4939-92ae-540c320744a7 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:11:13.757: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019701281s
    Feb 24 12:11:13.757: INFO: The phase of Pod server-envvars-91371105-54f0-4939-92ae-540c320744a7 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:11:15.758: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7": Phase="Running", Reason="", readiness=true. Elapsed: 4.020920948s
    Feb 24 12:11:15.758: INFO: The phase of Pod server-envvars-91371105-54f0-4939-92ae-540c320744a7 is Running (Ready = true)
    Feb 24 12:11:15.758: INFO: Pod "server-envvars-91371105-54f0-4939-92ae-540c320744a7" satisfied condition "running and ready"
    Feb 24 12:11:15.815: INFO: Waiting up to 5m0s for pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100" in namespace "pods-5839" to be "Succeeded or Failed"
    Feb 24 12:11:15.829: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100": Phase="Pending", Reason="", readiness=false. Elapsed: 12.496199ms
    Feb 24 12:11:17.836: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019288938s
    Feb 24 12:11:19.836: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019285324s
    STEP: Saw pod success 02/24/23 12:11:19.836
    Feb 24 12:11:19.836: INFO: Pod "client-envvars-cbd7f130-8654-4426-9a25-d068d6098100" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:19.841: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod client-envvars-cbd7f130-8654-4426-9a25-d068d6098100 container env3cont: <nil>
    STEP: delete the pod 02/24/23 12:11:19.866
    Feb 24 12:11:19.885: INFO: Waiting for pod client-envvars-cbd7f130-8654-4426-9a25-d068d6098100 to disappear
    Feb 24 12:11:19.890: INFO: Pod client-envvars-cbd7f130-8654-4426-9a25-d068d6098100 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 12:11:19.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5839" for this suite. 02/24/23 12:11:19.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:19.915
Feb 24 12:11:19.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 12:11:19.916
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:19.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:19.939
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-29e88067-a893-4225-9b9e-3dec44e81347 02/24/23 12:11:19.969
STEP: Creating a pod to test consume secrets 02/24/23 12:11:19.975
Feb 24 12:11:19.987: INFO: Waiting up to 5m0s for pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b" in namespace "secrets-8305" to be "Succeeded or Failed"
Feb 24 12:11:19.992: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481301ms
Feb 24 12:11:21.998: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01123353s
Feb 24 12:11:23.998: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01144311s
STEP: Saw pod success 02/24/23 12:11:23.998
Feb 24 12:11:23.998: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b" satisfied condition "Succeeded or Failed"
Feb 24 12:11:24.007: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:11:24.019
Feb 24 12:11:24.045: INFO: Waiting for pod pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b to disappear
Feb 24 12:11:24.059: INFO: Pod pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 12:11:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8305" for this suite. 02/24/23 12:11:24.066
STEP: Destroying namespace "secret-namespace-8580" for this suite. 02/24/23 12:11:24.077
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":266,"skipped":4666,"failed":0}
------------------------------
• [4.175 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:19.915
    Feb 24 12:11:19.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 12:11:19.916
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:19.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:19.939
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-29e88067-a893-4225-9b9e-3dec44e81347 02/24/23 12:11:19.969
    STEP: Creating a pod to test consume secrets 02/24/23 12:11:19.975
    Feb 24 12:11:19.987: INFO: Waiting up to 5m0s for pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b" in namespace "secrets-8305" to be "Succeeded or Failed"
    Feb 24 12:11:19.992: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481301ms
    Feb 24 12:11:21.998: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01123353s
    Feb 24 12:11:23.998: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01144311s
    STEP: Saw pod success 02/24/23 12:11:23.998
    Feb 24 12:11:23.998: INFO: Pod "pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:24.007: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:11:24.019
    Feb 24 12:11:24.045: INFO: Waiting for pod pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b to disappear
    Feb 24 12:11:24.059: INFO: Pod pod-secrets-7c45724e-795b-4984-b08a-6a6be6b24c4b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 12:11:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8305" for this suite. 02/24/23 12:11:24.066
    STEP: Destroying namespace "secret-namespace-8580" for this suite. 02/24/23 12:11:24.077
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:24.091
Feb 24 12:11:24.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:11:24.093
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:24.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:24.127
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/24/23 12:11:24.136
STEP: getting /apis/node.k8s.io 02/24/23 12:11:24.138
STEP: getting /apis/node.k8s.io/v1 02/24/23 12:11:24.138
STEP: creating 02/24/23 12:11:24.139
STEP: watching 02/24/23 12:11:24.171
Feb 24 12:11:24.171: INFO: starting watch
STEP: getting 02/24/23 12:11:24.181
STEP: listing 02/24/23 12:11:24.188
STEP: patching 02/24/23 12:11:24.193
STEP: updating 02/24/23 12:11:24.202
Feb 24 12:11:24.209: INFO: waiting for watch events with expected annotations
STEP: deleting 02/24/23 12:11:24.209
STEP: deleting a collection 02/24/23 12:11:24.234
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 24 12:11:24.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6376" for this suite. 02/24/23 12:11:24.274
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":267,"skipped":4668,"failed":0}
------------------------------
• [0.192 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:24.091
    Feb 24 12:11:24.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:11:24.093
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:24.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:24.127
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/24/23 12:11:24.136
    STEP: getting /apis/node.k8s.io 02/24/23 12:11:24.138
    STEP: getting /apis/node.k8s.io/v1 02/24/23 12:11:24.138
    STEP: creating 02/24/23 12:11:24.139
    STEP: watching 02/24/23 12:11:24.171
    Feb 24 12:11:24.171: INFO: starting watch
    STEP: getting 02/24/23 12:11:24.181
    STEP: listing 02/24/23 12:11:24.188
    STEP: patching 02/24/23 12:11:24.193
    STEP: updating 02/24/23 12:11:24.202
    Feb 24 12:11:24.209: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/24/23 12:11:24.209
    STEP: deleting a collection 02/24/23 12:11:24.234
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 24 12:11:24.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6376" for this suite. 02/24/23 12:11:24.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:24.291
Feb 24 12:11:24.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:11:24.292
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:24.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:24.337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 02/24/23 12:11:24.342
Feb 24 12:11:24.342: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-4982 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/24/23 12:11:24.403
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:11:24.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4982" for this suite. 02/24/23 12:11:24.42
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":268,"skipped":4720,"failed":0}
------------------------------
• [0.141 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:24.291
    Feb 24 12:11:24.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:11:24.292
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:24.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:24.337
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 02/24/23 12:11:24.342
    Feb 24 12:11:24.342: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-4982 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/24/23 12:11:24.403
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:11:24.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4982" for this suite. 02/24/23 12:11:24.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:24.44
Feb 24 12:11:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:11:24.443
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:24.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:24.474
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:11:24.477
Feb 24 12:11:24.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99" in namespace "projected-1986" to be "Succeeded or Failed"
Feb 24 12:11:24.498: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99": Phase="Pending", Reason="", readiness=false. Elapsed: 6.347379ms
Feb 24 12:11:26.511: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019022008s
Feb 24 12:11:28.504: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012217317s
STEP: Saw pod success 02/24/23 12:11:28.504
Feb 24 12:11:28.504: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99" satisfied condition "Succeeded or Failed"
Feb 24 12:11:28.515: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99 container client-container: <nil>
STEP: delete the pod 02/24/23 12:11:28.526
Feb 24 12:11:28.565: INFO: Waiting for pod downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99 to disappear
Feb 24 12:11:28.574: INFO: Pod downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:11:28.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1986" for this suite. 02/24/23 12:11:28.583
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":269,"skipped":4741,"failed":0}
------------------------------
• [4.168 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:24.44
    Feb 24 12:11:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:11:24.443
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:24.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:24.474
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:11:24.477
    Feb 24 12:11:24.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99" in namespace "projected-1986" to be "Succeeded or Failed"
    Feb 24 12:11:24.498: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99": Phase="Pending", Reason="", readiness=false. Elapsed: 6.347379ms
    Feb 24 12:11:26.511: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019022008s
    Feb 24 12:11:28.504: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012217317s
    STEP: Saw pod success 02/24/23 12:11:28.504
    Feb 24 12:11:28.504: INFO: Pod "downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:28.515: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:11:28.526
    Feb 24 12:11:28.565: INFO: Waiting for pod downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99 to disappear
    Feb 24 12:11:28.574: INFO: Pod downwardapi-volume-fce32894-6413-4df0-88e0-b40eb2771c99 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:11:28.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1986" for this suite. 02/24/23 12:11:28.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:28.61
Feb 24 12:11:28.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pods 02/24/23 12:11:28.611
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:28.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:28.666
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 02/24/23 12:11:28.693
STEP: watching for Pod to be ready 02/24/23 12:11:28.723
Feb 24 12:11:28.726: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 24 12:11:28.726: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
Feb 24 12:11:28.751: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
Feb 24 12:11:29.189: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
Feb 24 12:11:30.509: INFO: Found Pod pod-test in namespace pods-4431 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/24/23 12:11:30.516
STEP: getting the Pod and ensuring that it's patched 02/24/23 12:11:30.541
STEP: replacing the Pod's status Ready condition to False 02/24/23 12:11:30.546
STEP: check the Pod again to ensure its Ready conditions are False 02/24/23 12:11:30.56
STEP: deleting the Pod via a Collection with a LabelSelector 02/24/23 12:11:30.561
STEP: watching for the Pod to be deleted 02/24/23 12:11:30.583
Feb 24 12:11:30.586: INFO: observed event type MODIFIED
Feb 24 12:11:32.513: INFO: observed event type MODIFIED
Feb 24 12:11:32.906: INFO: observed event type MODIFIED
Feb 24 12:11:33.511: INFO: observed event type MODIFIED
Feb 24 12:11:33.534: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 24 12:11:33.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4431" for this suite. 02/24/23 12:11:33.556
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":270,"skipped":4748,"failed":0}
------------------------------
• [4.959 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:28.61
    Feb 24 12:11:28.610: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pods 02/24/23 12:11:28.611
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:28.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:28.666
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 02/24/23 12:11:28.693
    STEP: watching for Pod to be ready 02/24/23 12:11:28.723
    Feb 24 12:11:28.726: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 24 12:11:28.726: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
    Feb 24 12:11:28.751: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
    Feb 24 12:11:29.189: INFO: observed Pod pod-test in namespace pods-4431 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
    Feb 24 12:11:30.509: INFO: Found Pod pod-test in namespace pods-4431 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-24 12:11:28 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/24/23 12:11:30.516
    STEP: getting the Pod and ensuring that it's patched 02/24/23 12:11:30.541
    STEP: replacing the Pod's status Ready condition to False 02/24/23 12:11:30.546
    STEP: check the Pod again to ensure its Ready conditions are False 02/24/23 12:11:30.56
    STEP: deleting the Pod via a Collection with a LabelSelector 02/24/23 12:11:30.561
    STEP: watching for the Pod to be deleted 02/24/23 12:11:30.583
    Feb 24 12:11:30.586: INFO: observed event type MODIFIED
    Feb 24 12:11:32.513: INFO: observed event type MODIFIED
    Feb 24 12:11:32.906: INFO: observed event type MODIFIED
    Feb 24 12:11:33.511: INFO: observed event type MODIFIED
    Feb 24 12:11:33.534: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 24 12:11:33.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4431" for this suite. 02/24/23 12:11:33.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:33.585
Feb 24 12:11:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename security-context-test 02/24/23 12:11:33.586
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:33.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:33.618
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Feb 24 12:11:33.680: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd" in namespace "security-context-test-4983" to be "Succeeded or Failed"
Feb 24 12:11:33.688: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.565818ms
Feb 24 12:11:35.693: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618905s
Feb 24 12:11:37.697: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016704807s
Feb 24 12:11:37.697: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 24 12:11:37.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4983" for this suite. 02/24/23 12:11:37.717
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":4837,"failed":0}
------------------------------
• [4.143 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:33.585
    Feb 24 12:11:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename security-context-test 02/24/23 12:11:33.586
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:33.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:33.618
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Feb 24 12:11:33.680: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd" in namespace "security-context-test-4983" to be "Succeeded or Failed"
    Feb 24 12:11:33.688: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.565818ms
    Feb 24 12:11:35.693: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012618905s
    Feb 24 12:11:37.697: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016704807s
    Feb 24 12:11:37.697: INFO: Pod "alpine-nnp-false-736b00ca-0b4c-4f03-bacb-f42d0756a7dd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 24 12:11:37.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4983" for this suite. 02/24/23 12:11:37.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:37.733
Feb 24 12:11:37.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename podtemplate 02/24/23 12:11:37.734
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:37.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:37.758
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 24 12:11:37.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2227" for this suite. 02/24/23 12:11:37.817
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":272,"skipped":4877,"failed":0}
------------------------------
• [0.095 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:37.733
    Feb 24 12:11:37.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename podtemplate 02/24/23 12:11:37.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:37.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:37.758
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 24 12:11:37.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2227" for this suite. 02/24/23 12:11:37.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:37.831
Feb 24 12:11:37.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 12:11:37.833
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:37.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:37.877
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-2522/secret-test-8029f30e-c4f3-4c2b-9192-69b645f64cab 02/24/23 12:11:37.886
STEP: Creating a pod to test consume secrets 02/24/23 12:11:37.898
Feb 24 12:11:37.914: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f" in namespace "secrets-2522" to be "Succeeded or Failed"
Feb 24 12:11:37.936: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.83353ms
Feb 24 12:11:39.942: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f": Phase="Running", Reason="", readiness=false. Elapsed: 2.027630125s
Feb 24 12:11:41.941: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027513228s
STEP: Saw pod success 02/24/23 12:11:41.942
Feb 24 12:11:41.942: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f" satisfied condition "Succeeded or Failed"
Feb 24 12:11:41.947: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f container env-test: <nil>
STEP: delete the pod 02/24/23 12:11:41.959
Feb 24 12:11:41.977: INFO: Waiting for pod pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f to disappear
Feb 24 12:11:41.984: INFO: Pod pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 24 12:11:41.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2522" for this suite. 02/24/23 12:11:41.991
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":273,"skipped":4907,"failed":0}
------------------------------
• [4.172 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:37.831
    Feb 24 12:11:37.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 12:11:37.833
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:37.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:37.877
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-2522/secret-test-8029f30e-c4f3-4c2b-9192-69b645f64cab 02/24/23 12:11:37.886
    STEP: Creating a pod to test consume secrets 02/24/23 12:11:37.898
    Feb 24 12:11:37.914: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f" in namespace "secrets-2522" to be "Succeeded or Failed"
    Feb 24 12:11:37.936: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.83353ms
    Feb 24 12:11:39.942: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f": Phase="Running", Reason="", readiness=false. Elapsed: 2.027630125s
    Feb 24 12:11:41.941: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027513228s
    STEP: Saw pod success 02/24/23 12:11:41.942
    Feb 24 12:11:41.942: INFO: Pod "pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:41.947: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f container env-test: <nil>
    STEP: delete the pod 02/24/23 12:11:41.959
    Feb 24 12:11:41.977: INFO: Waiting for pod pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f to disappear
    Feb 24 12:11:41.984: INFO: Pod pod-configmaps-b4eaa8dd-46f6-443f-9621-4a616597fd5f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 12:11:41.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2522" for this suite. 02/24/23 12:11:41.991
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:42.006
Feb 24 12:11:42.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 12:11:42.007
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:42.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:42.05
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:11:42.062
Feb 24 12:11:42.079: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99" in namespace "downward-api-4193" to be "Succeeded or Failed"
Feb 24 12:11:42.089: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99": Phase="Pending", Reason="", readiness=false. Elapsed: 9.457052ms
Feb 24 12:11:44.099: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019351319s
Feb 24 12:11:46.097: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01769536s
STEP: Saw pod success 02/24/23 12:11:46.097
Feb 24 12:11:46.097: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99" satisfied condition "Succeeded or Failed"
Feb 24 12:11:46.107: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99 container client-container: <nil>
STEP: delete the pod 02/24/23 12:11:46.123
Feb 24 12:11:46.147: INFO: Waiting for pod downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99 to disappear
Feb 24 12:11:46.157: INFO: Pod downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 12:11:46.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4193" for this suite. 02/24/23 12:11:46.165
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":274,"skipped":4908,"failed":0}
------------------------------
• [4.171 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:42.006
    Feb 24 12:11:42.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:11:42.007
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:42.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:42.05
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:11:42.062
    Feb 24 12:11:42.079: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99" in namespace "downward-api-4193" to be "Succeeded or Failed"
    Feb 24 12:11:42.089: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99": Phase="Pending", Reason="", readiness=false. Elapsed: 9.457052ms
    Feb 24 12:11:44.099: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019351319s
    Feb 24 12:11:46.097: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01769536s
    STEP: Saw pod success 02/24/23 12:11:46.097
    Feb 24 12:11:46.097: INFO: Pod "downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99" satisfied condition "Succeeded or Failed"
    Feb 24 12:11:46.107: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:11:46.123
    Feb 24 12:11:46.147: INFO: Waiting for pod downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99 to disappear
    Feb 24 12:11:46.157: INFO: Pod downwardapi-volume-78a2598c-a76a-460d-a0f6-09520029cb99 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 12:11:46.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4193" for this suite. 02/24/23 12:11:46.165
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:11:46.182
Feb 24 12:11:46.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pod-network-test 02/24/23 12:11:46.183
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:46.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:46.213
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6084 02/24/23 12:11:46.217
STEP: creating a selector 02/24/23 12:11:46.217
STEP: Creating the service pods in kubernetes 02/24/23 12:11:46.217
Feb 24 12:11:46.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 12:11:46.272: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6084" to be "running and ready"
Feb 24 12:11:46.301: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.346973ms
Feb 24 12:11:46.301: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:11:48.312: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.038362002s
Feb 24 12:11:48.312: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:50.306: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032530905s
Feb 24 12:11:50.306: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:52.307: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.033388464s
Feb 24 12:11:52.307: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:54.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.044621041s
Feb 24 12:11:54.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:56.307: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.034093591s
Feb 24 12:11:56.307: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:11:58.309: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.035977967s
Feb 24 12:11:58.309: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 12:11:58.309: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 12:11:58.314: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6084" to be "running and ready"
Feb 24 12:11:58.319: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.136882ms
Feb 24 12:11:58.319: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 12:11:58.320: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 12:11:58.325: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6084" to be "running and ready"
Feb 24 12:11:58.330: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 5.529173ms
Feb 24 12:11:58.331: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Feb 24 12:12:00.337: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011824688s
Feb 24 12:12:00.337: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Feb 24 12:12:02.341: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.015810218s
Feb 24 12:12:02.341: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Feb 24 12:12:04.337: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.012143179s
Feb 24 12:12:04.337: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Feb 24 12:12:06.337: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.012555675s
Feb 24 12:12:06.337: INFO: The phase of Pod netserver-2 is Running (Ready = false)
Feb 24 12:12:08.338: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.012822523s
Feb 24 12:12:08.338: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 12:12:08.338: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 12:12:08.35
Feb 24 12:12:08.366: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6084" to be "running"
Feb 24 12:12:08.373: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432336ms
Feb 24 12:12:10.381: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014716635s
Feb 24 12:12:10.381: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 12:12:10.392: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 12:12:10.392: INFO: Breadth first check of 10.244.4.182 on host 172.31.215.124...
Feb 24 12:12:10.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.32:9080/dial?request=hostname&protocol=udp&host=10.244.4.182&port=8081&tries=1'] Namespace:pod-network-test-6084 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:12:10.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:12:10.399: INFO: ExecWithOptions: Clientset creation
Feb 24 12:12:10.399: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6084/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.4.182%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 12:12:10.474: INFO: Waiting for responses: map[]
Feb 24 12:12:10.474: INFO: reached 10.244.4.182 after 0/1 tries
Feb 24 12:12:10.474: INFO: Breadth first check of 10.244.3.31 on host 172.31.216.47...
Feb 24 12:12:10.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.32:9080/dial?request=hostname&protocol=udp&host=10.244.3.31&port=8081&tries=1'] Namespace:pod-network-test-6084 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:12:10.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:12:10.480: INFO: ExecWithOptions: Clientset creation
Feb 24 12:12:10.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6084/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.31%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 12:12:10.588: INFO: Waiting for responses: map[]
Feb 24 12:12:10.588: INFO: reached 10.244.3.31 after 0/1 tries
Feb 24 12:12:10.588: INFO: Breadth first check of 10.244.5.117 on host 172.31.217.191...
Feb 24 12:12:10.595: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.32:9080/dial?request=hostname&protocol=udp&host=10.244.5.117&port=8081&tries=1'] Namespace:pod-network-test-6084 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:12:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:12:10.597: INFO: ExecWithOptions: Clientset creation
Feb 24 12:12:10.597: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6084/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.5.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 24 12:12:10.685: INFO: Waiting for responses: map[]
Feb 24 12:12:10.685: INFO: reached 10.244.5.117 after 0/1 tries
Feb 24 12:12:10.685: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 24 12:12:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6084" for this suite. 02/24/23 12:12:10.694
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":275,"skipped":4910,"failed":0}
------------------------------
• [SLOW TEST] [24.525 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:11:46.182
    Feb 24 12:11:46.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 12:11:46.183
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:11:46.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:11:46.213
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6084 02/24/23 12:11:46.217
    STEP: creating a selector 02/24/23 12:11:46.217
    STEP: Creating the service pods in kubernetes 02/24/23 12:11:46.217
    Feb 24 12:11:46.217: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 12:11:46.272: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6084" to be "running and ready"
    Feb 24 12:11:46.301: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.346973ms
    Feb 24 12:11:46.301: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:11:48.312: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.038362002s
    Feb 24 12:11:48.312: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:50.306: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032530905s
    Feb 24 12:11:50.306: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:52.307: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.033388464s
    Feb 24 12:11:52.307: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:54.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.044621041s
    Feb 24 12:11:54.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:56.307: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.034093591s
    Feb 24 12:11:56.307: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:11:58.309: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.035977967s
    Feb 24 12:11:58.309: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 12:11:58.309: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 12:11:58.314: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6084" to be "running and ready"
    Feb 24 12:11:58.319: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.136882ms
    Feb 24 12:11:58.319: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 12:11:58.320: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 12:11:58.325: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6084" to be "running and ready"
    Feb 24 12:11:58.330: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 5.529173ms
    Feb 24 12:11:58.331: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Feb 24 12:12:00.337: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 2.011824688s
    Feb 24 12:12:00.337: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Feb 24 12:12:02.341: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 4.015810218s
    Feb 24 12:12:02.341: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Feb 24 12:12:04.337: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 6.012143179s
    Feb 24 12:12:04.337: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Feb 24 12:12:06.337: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=false. Elapsed: 8.012555675s
    Feb 24 12:12:06.337: INFO: The phase of Pod netserver-2 is Running (Ready = false)
    Feb 24 12:12:08.338: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.012822523s
    Feb 24 12:12:08.338: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 12:12:08.338: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 12:12:08.35
    Feb 24 12:12:08.366: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6084" to be "running"
    Feb 24 12:12:08.373: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.432336ms
    Feb 24 12:12:10.381: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014716635s
    Feb 24 12:12:10.381: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 12:12:10.392: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 12:12:10.392: INFO: Breadth first check of 10.244.4.182 on host 172.31.215.124...
    Feb 24 12:12:10.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.32:9080/dial?request=hostname&protocol=udp&host=10.244.4.182&port=8081&tries=1'] Namespace:pod-network-test-6084 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:12:10.398: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:12:10.399: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:12:10.399: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6084/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.4.182%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 12:12:10.474: INFO: Waiting for responses: map[]
    Feb 24 12:12:10.474: INFO: reached 10.244.4.182 after 0/1 tries
    Feb 24 12:12:10.474: INFO: Breadth first check of 10.244.3.31 on host 172.31.216.47...
    Feb 24 12:12:10.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.32:9080/dial?request=hostname&protocol=udp&host=10.244.3.31&port=8081&tries=1'] Namespace:pod-network-test-6084 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:12:10.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:12:10.480: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:12:10.481: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6084/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.3.31%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 12:12:10.588: INFO: Waiting for responses: map[]
    Feb 24 12:12:10.588: INFO: reached 10.244.3.31 after 0/1 tries
    Feb 24 12:12:10.588: INFO: Breadth first check of 10.244.5.117 on host 172.31.217.191...
    Feb 24 12:12:10.595: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.32:9080/dial?request=hostname&protocol=udp&host=10.244.5.117&port=8081&tries=1'] Namespace:pod-network-test-6084 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:12:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:12:10.597: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:12:10.597: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6084/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.3.32%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.5.117%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 24 12:12:10.685: INFO: Waiting for responses: map[]
    Feb 24 12:12:10.685: INFO: reached 10.244.5.117 after 0/1 tries
    Feb 24 12:12:10.685: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 24 12:12:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6084" for this suite. 02/24/23 12:12:10.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:10.713
Feb 24 12:12:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:12:10.714
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:10.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:10.749
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 02/24/23 12:12:10.751
Feb 24 12:12:10.751: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-7724 proxy --unix-socket=/tmp/kubectl-proxy-unix390739516/test'
STEP: retrieving proxy /api/ output 02/24/23 12:12:10.799
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:12:10.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7724" for this suite. 02/24/23 12:12:10.813
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":276,"skipped":4957,"failed":0}
------------------------------
• [0.122 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:10.713
    Feb 24 12:12:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:12:10.714
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:10.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:10.749
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 02/24/23 12:12:10.751
    Feb 24 12:12:10.751: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-7724 proxy --unix-socket=/tmp/kubectl-proxy-unix390739516/test'
    STEP: retrieving proxy /api/ output 02/24/23 12:12:10.799
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:12:10.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7724" for this suite. 02/24/23 12:12:10.813
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:10.836
Feb 24 12:12:10.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:12:10.837
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:10.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:10.898
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Feb 24 12:12:10.948: INFO: Waiting up to 5m0s for pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef" in namespace "svcaccounts-542" to be "running"
Feb 24 12:12:10.958: INFO: Pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef": Phase="Pending", Reason="", readiness=false. Elapsed: 9.89645ms
Feb 24 12:12:12.965: INFO: Pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.016466946s
Feb 24 12:12:12.965: INFO: Pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef" satisfied condition "running"
STEP: reading a file in the container 02/24/23 12:12:12.965
Feb 24 12:12:12.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-542 pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/24/23 12:12:13.178
Feb 24 12:12:13.178: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-542 pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/24/23 12:12:13.411
Feb 24 12:12:13.411: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-542 pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 24 12:12:13.657: INFO: Got root ca configmap in namespace "svcaccounts-542"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 24 12:12:13.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-542" for this suite. 02/24/23 12:12:13.666
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":277,"skipped":4961,"failed":0}
------------------------------
• [2.842 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:10.836
    Feb 24 12:12:10.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename svcaccounts 02/24/23 12:12:10.837
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:10.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:10.898
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Feb 24 12:12:10.948: INFO: Waiting up to 5m0s for pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef" in namespace "svcaccounts-542" to be "running"
    Feb 24 12:12:10.958: INFO: Pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef": Phase="Pending", Reason="", readiness=false. Elapsed: 9.89645ms
    Feb 24 12:12:12.965: INFO: Pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.016466946s
    Feb 24 12:12:12.965: INFO: Pod "pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef" satisfied condition "running"
    STEP: reading a file in the container 02/24/23 12:12:12.965
    Feb 24 12:12:12.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-542 pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/24/23 12:12:13.178
    Feb 24 12:12:13.178: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-542 pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/24/23 12:12:13.411
    Feb 24 12:12:13.411: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-542 pod-service-account-24de9e45-abbc-46ff-be89-9db8b4c395ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 24 12:12:13.657: INFO: Got root ca configmap in namespace "svcaccounts-542"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 24 12:12:13.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-542" for this suite. 02/24/23 12:12:13.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:13.68
Feb 24 12:12:13.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:12:13.681
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:13.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:13.718
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Feb 24 12:12:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/24/23 12:12:19.457
Feb 24 12:12:19.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
Feb 24 12:12:20.180: INFO: stderr: ""
Feb 24 12:12:20.180: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 24 12:12:20.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 delete e2e-test-crd-publish-openapi-1841-crds test-foo'
Feb 24 12:12:20.317: INFO: stderr: ""
Feb 24 12:12:20.317: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 24 12:12:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 apply -f -'
Feb 24 12:12:21.458: INFO: stderr: ""
Feb 24 12:12:21.458: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 24 12:12:21.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 delete e2e-test-crd-publish-openapi-1841-crds test-foo'
Feb 24 12:12:21.553: INFO: stderr: ""
Feb 24 12:12:21.553: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/24/23 12:12:21.553
Feb 24 12:12:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
Feb 24 12:12:21.816: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/24/23 12:12:21.817
Feb 24 12:12:21.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
Feb 24 12:12:22.071: INFO: rc: 1
Feb 24 12:12:22.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 apply -f -'
Feb 24 12:12:23.693: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/24/23 12:12:23.693
Feb 24 12:12:23.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
Feb 24 12:12:24.033: INFO: rc: 1
Feb 24 12:12:24.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 apply -f -'
Feb 24 12:12:24.371: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/24/23 12:12:24.371
Feb 24 12:12:24.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds'
Feb 24 12:12:24.640: INFO: stderr: ""
Feb 24 12:12:24.640: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/24/23 12:12:24.641
Feb 24 12:12:24.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.metadata'
Feb 24 12:12:24.864: INFO: stderr: ""
Feb 24 12:12:24.864: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 24 12:12:24.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.spec'
Feb 24 12:12:25.111: INFO: stderr: ""
Feb 24 12:12:25.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 24 12:12:25.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.spec.bars'
Feb 24 12:12:25.327: INFO: stderr: ""
Feb 24 12:12:25.327: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/24/23 12:12:25.327
Feb 24 12:12:25.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.spec.bars2'
Feb 24 12:12:25.577: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:12:30.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4055" for this suite. 02/24/23 12:12:30.433
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":278,"skipped":4998,"failed":0}
------------------------------
• [SLOW TEST] [16.761 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:13.68
    Feb 24 12:12:13.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:12:13.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:13.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:13.718
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Feb 24 12:12:13.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/24/23 12:12:19.457
    Feb 24 12:12:19.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
    Feb 24 12:12:20.180: INFO: stderr: ""
    Feb 24 12:12:20.180: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 24 12:12:20.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 delete e2e-test-crd-publish-openapi-1841-crds test-foo'
    Feb 24 12:12:20.317: INFO: stderr: ""
    Feb 24 12:12:20.317: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 24 12:12:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 apply -f -'
    Feb 24 12:12:21.458: INFO: stderr: ""
    Feb 24 12:12:21.458: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 24 12:12:21.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 delete e2e-test-crd-publish-openapi-1841-crds test-foo'
    Feb 24 12:12:21.553: INFO: stderr: ""
    Feb 24 12:12:21.553: INFO: stdout: "e2e-test-crd-publish-openapi-1841-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/24/23 12:12:21.553
    Feb 24 12:12:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
    Feb 24 12:12:21.816: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/24/23 12:12:21.817
    Feb 24 12:12:21.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
    Feb 24 12:12:22.071: INFO: rc: 1
    Feb 24 12:12:22.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 apply -f -'
    Feb 24 12:12:23.693: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/24/23 12:12:23.693
    Feb 24 12:12:23.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 create -f -'
    Feb 24 12:12:24.033: INFO: rc: 1
    Feb 24 12:12:24.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 --namespace=crd-publish-openapi-4055 apply -f -'
    Feb 24 12:12:24.371: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/24/23 12:12:24.371
    Feb 24 12:12:24.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds'
    Feb 24 12:12:24.640: INFO: stderr: ""
    Feb 24 12:12:24.640: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/24/23 12:12:24.641
    Feb 24 12:12:24.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.metadata'
    Feb 24 12:12:24.864: INFO: stderr: ""
    Feb 24 12:12:24.864: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 24 12:12:24.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.spec'
    Feb 24 12:12:25.111: INFO: stderr: ""
    Feb 24 12:12:25.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 24 12:12:25.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.spec.bars'
    Feb 24 12:12:25.327: INFO: stderr: ""
    Feb 24 12:12:25.327: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1841-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/24/23 12:12:25.327
    Feb 24 12:12:25.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-4055 explain e2e-test-crd-publish-openapi-1841-crds.spec.bars2'
    Feb 24 12:12:25.577: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:12:30.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4055" for this suite. 02/24/23 12:12:30.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:30.445
Feb 24 12:12:30.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 12:12:30.445
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:30.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:30.476
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-295e1e8a-3759-4252-b1c7-7a9582e3e77c 02/24/23 12:12:30.48
STEP: Creating a pod to test consume secrets 02/24/23 12:12:30.487
Feb 24 12:12:30.497: INFO: Waiting up to 5m0s for pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662" in namespace "secrets-3934" to be "Succeeded or Failed"
Feb 24 12:12:30.504: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662": Phase="Pending", Reason="", readiness=false. Elapsed: 7.274793ms
Feb 24 12:12:32.512: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014930919s
Feb 24 12:12:34.511: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014234853s
STEP: Saw pod success 02/24/23 12:12:34.511
Feb 24 12:12:34.511: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662" satisfied condition "Succeeded or Failed"
Feb 24 12:12:34.516: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662 container secret-env-test: <nil>
STEP: delete the pod 02/24/23 12:12:34.533
Feb 24 12:12:34.551: INFO: Waiting for pod pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662 to disappear
Feb 24 12:12:34.556: INFO: Pod pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 24 12:12:34.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3934" for this suite. 02/24/23 12:12:34.566
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":279,"skipped":5044,"failed":0}
------------------------------
• [4.133 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:30.445
    Feb 24 12:12:30.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 12:12:30.445
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:30.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:30.476
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-295e1e8a-3759-4252-b1c7-7a9582e3e77c 02/24/23 12:12:30.48
    STEP: Creating a pod to test consume secrets 02/24/23 12:12:30.487
    Feb 24 12:12:30.497: INFO: Waiting up to 5m0s for pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662" in namespace "secrets-3934" to be "Succeeded or Failed"
    Feb 24 12:12:30.504: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662": Phase="Pending", Reason="", readiness=false. Elapsed: 7.274793ms
    Feb 24 12:12:32.512: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014930919s
    Feb 24 12:12:34.511: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014234853s
    STEP: Saw pod success 02/24/23 12:12:34.511
    Feb 24 12:12:34.511: INFO: Pod "pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662" satisfied condition "Succeeded or Failed"
    Feb 24 12:12:34.516: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662 container secret-env-test: <nil>
    STEP: delete the pod 02/24/23 12:12:34.533
    Feb 24 12:12:34.551: INFO: Waiting for pod pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662 to disappear
    Feb 24 12:12:34.556: INFO: Pod pod-secrets-ae9a88e6-a7bd-46c4-b10c-4101e9f45662 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 12:12:34.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3934" for this suite. 02/24/23 12:12:34.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:34.586
Feb 24 12:12:34.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 12:12:34.588
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:34.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:34.624
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 12:12:34.628
Feb 24 12:12:34.638: INFO: Waiting up to 5m0s for pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4" in namespace "emptydir-1351" to be "Succeeded or Failed"
Feb 24 12:12:34.646: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.001776ms
Feb 24 12:12:36.654: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01561241s
Feb 24 12:12:38.652: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01422986s
STEP: Saw pod success 02/24/23 12:12:38.652
Feb 24 12:12:38.652: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4" satisfied condition "Succeeded or Failed"
Feb 24 12:12:38.657: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-e4929f62-0501-425a-907f-6c9f5b79ced4 container test-container: <nil>
STEP: delete the pod 02/24/23 12:12:38.664
Feb 24 12:12:38.677: INFO: Waiting for pod pod-e4929f62-0501-425a-907f-6c9f5b79ced4 to disappear
Feb 24 12:12:38.683: INFO: Pod pod-e4929f62-0501-425a-907f-6c9f5b79ced4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 12:12:38.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1351" for this suite. 02/24/23 12:12:38.69
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":280,"skipped":5059,"failed":0}
------------------------------
• [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:34.586
    Feb 24 12:12:34.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:12:34.588
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:34.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:34.624
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 12:12:34.628
    Feb 24 12:12:34.638: INFO: Waiting up to 5m0s for pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4" in namespace "emptydir-1351" to be "Succeeded or Failed"
    Feb 24 12:12:34.646: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.001776ms
    Feb 24 12:12:36.654: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01561241s
    Feb 24 12:12:38.652: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01422986s
    STEP: Saw pod success 02/24/23 12:12:38.652
    Feb 24 12:12:38.652: INFO: Pod "pod-e4929f62-0501-425a-907f-6c9f5b79ced4" satisfied condition "Succeeded or Failed"
    Feb 24 12:12:38.657: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-e4929f62-0501-425a-907f-6c9f5b79ced4 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:12:38.664
    Feb 24 12:12:38.677: INFO: Waiting for pod pod-e4929f62-0501-425a-907f-6c9f5b79ced4 to disappear
    Feb 24 12:12:38.683: INFO: Pod pod-e4929f62-0501-425a-907f-6c9f5b79ced4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:12:38.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1351" for this suite. 02/24/23 12:12:38.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:38.701
Feb 24 12:12:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:12:38.702
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:38.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:38.73
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:12:38.735
Feb 24 12:12:38.752: INFO: Waiting up to 5m0s for pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4" in namespace "projected-6989" to be "Succeeded or Failed"
Feb 24 12:12:38.765: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.554322ms
Feb 24 12:12:40.770: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017882354s
Feb 24 12:12:42.771: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01850507s
STEP: Saw pod success 02/24/23 12:12:42.771
Feb 24 12:12:42.771: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4" satisfied condition "Succeeded or Failed"
Feb 24 12:12:42.777: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4 container client-container: <nil>
STEP: delete the pod 02/24/23 12:12:42.785
Feb 24 12:12:42.803: INFO: Waiting for pod downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4 to disappear
Feb 24 12:12:42.807: INFO: Pod downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:12:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6989" for this suite. 02/24/23 12:12:42.817
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":281,"skipped":5074,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:38.701
    Feb 24 12:12:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:12:38.702
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:38.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:38.73
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:12:38.735
    Feb 24 12:12:38.752: INFO: Waiting up to 5m0s for pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4" in namespace "projected-6989" to be "Succeeded or Failed"
    Feb 24 12:12:38.765: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.554322ms
    Feb 24 12:12:40.770: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017882354s
    Feb 24 12:12:42.771: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01850507s
    STEP: Saw pod success 02/24/23 12:12:42.771
    Feb 24 12:12:42.771: INFO: Pod "downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4" satisfied condition "Succeeded or Failed"
    Feb 24 12:12:42.777: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:12:42.785
    Feb 24 12:12:42.803: INFO: Waiting for pod downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4 to disappear
    Feb 24 12:12:42.807: INFO: Pod downwardapi-volume-766fd741-f5dd-4377-b3b4-c099916a61d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:12:42.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6989" for this suite. 02/24/23 12:12:42.817
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:42.829
Feb 24 12:12:42.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:12:42.831
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:42.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:42.86
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:12:42.871
Feb 24 12:12:42.883: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1456" to be "running and ready"
Feb 24 12:12:42.889: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.552861ms
Feb 24 12:12:42.889: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:12:44.895: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012078612s
Feb 24 12:12:44.895: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 12:12:44.895: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 02/24/23 12:12:44.9
Feb 24 12:12:44.908: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1456" to be "running and ready"
Feb 24 12:12:44.913: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.178111ms
Feb 24 12:12:44.913: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:12:46.919: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011367019s
Feb 24 12:12:46.919: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 24 12:12:46.919: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/24/23 12:12:46.924
STEP: delete the pod with lifecycle hook 02/24/23 12:12:46.942
Feb 24 12:12:46.950: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 12:12:46.954: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 12:12:48.955: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 12:12:48.960: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 24 12:12:50.955: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 24 12:12:50.959: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 24 12:12:50.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1456" for this suite. 02/24/23 12:12:50.968
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":282,"skipped":5074,"failed":0}
------------------------------
• [SLOW TEST] [8.148 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:42.829
    Feb 24 12:12:42.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:12:42.831
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:42.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:42.86
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:12:42.871
    Feb 24 12:12:42.883: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1456" to be "running and ready"
    Feb 24 12:12:42.889: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.552861ms
    Feb 24 12:12:42.889: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:12:44.895: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012078612s
    Feb 24 12:12:44.895: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 12:12:44.895: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 02/24/23 12:12:44.9
    Feb 24 12:12:44.908: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1456" to be "running and ready"
    Feb 24 12:12:44.913: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.178111ms
    Feb 24 12:12:44.913: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:12:46.919: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011367019s
    Feb 24 12:12:46.919: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 24 12:12:46.919: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/24/23 12:12:46.924
    STEP: delete the pod with lifecycle hook 02/24/23 12:12:46.942
    Feb 24 12:12:46.950: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 24 12:12:46.954: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 24 12:12:48.955: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 24 12:12:48.960: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 24 12:12:50.955: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 24 12:12:50.959: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 24 12:12:50.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1456" for this suite. 02/24/23 12:12:50.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:50.98
Feb 24 12:12:50.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 12:12:50.981
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:51.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:51.013
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 02/24/23 12:12:51.018
Feb 24 12:12:51.028: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba" in namespace "emptydir-4547" to be "running"
Feb 24 12:12:51.034: INFO: Pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.311642ms
Feb 24 12:12:53.040: INFO: Pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba": Phase="Running", Reason="", readiness=false. Elapsed: 2.012632688s
Feb 24 12:12:53.041: INFO: Pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/24/23 12:12:53.041
Feb 24 12:12:53.041: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4547 PodName:pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:12:53.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:12:53.042: INFO: ExecWithOptions: Clientset creation
Feb 24 12:12:53.042: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4547/pods/pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 24 12:12:53.112: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 12:12:53.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4547" for this suite. 02/24/23 12:12:53.12
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":283,"skipped":5088,"failed":0}
------------------------------
• [2.146 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:50.98
    Feb 24 12:12:50.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:12:50.981
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:51.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:51.013
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 02/24/23 12:12:51.018
    Feb 24 12:12:51.028: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba" in namespace "emptydir-4547" to be "running"
    Feb 24 12:12:51.034: INFO: Pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.311642ms
    Feb 24 12:12:53.040: INFO: Pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba": Phase="Running", Reason="", readiness=false. Elapsed: 2.012632688s
    Feb 24 12:12:53.041: INFO: Pod "pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/24/23 12:12:53.041
    Feb 24 12:12:53.041: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4547 PodName:pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:12:53.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:12:53.042: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:12:53.042: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-4547/pods/pod-sharedvolume-e6734ecc-fc36-49ca-9c56-dbdc618edfba/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 24 12:12:53.112: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:12:53.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4547" for this suite. 02/24/23 12:12:53.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:53.131
Feb 24 12:12:53.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:12:53.132
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:53.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:53.159
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:12:53.179
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:12:53.6
STEP: Deploying the webhook pod 02/24/23 12:12:53.61
STEP: Wait for the deployment to be ready 02/24/23 12:12:53.629
Feb 24 12:12:53.638: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/24/23 12:12:55.651
STEP: Verifying the service has paired with the endpoint 02/24/23 12:12:55.67
Feb 24 12:12:56.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 02/24/23 12:12:56.676
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:12:56.694
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/24/23 12:12:56.705
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:12:56.718
STEP: Patching a validating webhook configuration's rules to include the create operation 02/24/23 12:12:56.731
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:12:56.741
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:12:56.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-740" for this suite. 02/24/23 12:12:56.764
STEP: Destroying namespace "webhook-740-markers" for this suite. 02/24/23 12:12:56.772
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":284,"skipped":5103,"failed":0}
------------------------------
• [3.730 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:53.131
    Feb 24 12:12:53.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:12:53.132
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:53.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:53.159
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:12:53.179
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:12:53.6
    STEP: Deploying the webhook pod 02/24/23 12:12:53.61
    STEP: Wait for the deployment to be ready 02/24/23 12:12:53.629
    Feb 24 12:12:53.638: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/24/23 12:12:55.651
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:12:55.67
    Feb 24 12:12:56.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 02/24/23 12:12:56.676
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:12:56.694
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/24/23 12:12:56.705
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:12:56.718
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/24/23 12:12:56.731
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:12:56.741
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:12:56.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-740" for this suite. 02/24/23 12:12:56.764
    STEP: Destroying namespace "webhook-740-markers" for this suite. 02/24/23 12:12:56.772
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:56.867
Feb 24 12:12:56.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename watch 02/24/23 12:12:56.87
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:56.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:56.921
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/24/23 12:12:56.925
STEP: modifying the configmap once 02/24/23 12:12:56.931
STEP: modifying the configmap a second time 02/24/23 12:12:56.94
STEP: deleting the configmap 02/24/23 12:12:56.95
STEP: creating a watch on configmaps from the resource version returned by the first update 02/24/23 12:12:56.957
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/24/23 12:12:56.96
Feb 24 12:12:56.960: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5534  6eaacc59-a3e2-466c-b74e-7f01e05c50cc 38365 0 2023-02-24 12:12:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 12:12:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 24 12:12:56.960: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5534  6eaacc59-a3e2-466c-b74e-7f01e05c50cc 38366 0 2023-02-24 12:12:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 12:12:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 24 12:12:56.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5534" for this suite. 02/24/23 12:12:56.966
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":285,"skipped":5115,"failed":0}
------------------------------
• [0.107 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:56.867
    Feb 24 12:12:56.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename watch 02/24/23 12:12:56.87
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:56.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:56.921
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/24/23 12:12:56.925
    STEP: modifying the configmap once 02/24/23 12:12:56.931
    STEP: modifying the configmap a second time 02/24/23 12:12:56.94
    STEP: deleting the configmap 02/24/23 12:12:56.95
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/24/23 12:12:56.957
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/24/23 12:12:56.96
    Feb 24 12:12:56.960: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5534  6eaacc59-a3e2-466c-b74e-7f01e05c50cc 38365 0 2023-02-24 12:12:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 12:12:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 24 12:12:56.960: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5534  6eaacc59-a3e2-466c-b74e-7f01e05c50cc 38366 0 2023-02-24 12:12:56 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-24 12:12:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 24 12:12:56.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5534" for this suite. 02/24/23 12:12:56.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:12:56.988
Feb 24 12:12:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 12:12:56.992
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:57.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:57.051
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 02/24/23 12:12:57.056
Feb 24 12:12:57.068: INFO: Waiting up to 5m0s for pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b" in namespace "downward-api-5049" to be "Succeeded or Failed"
Feb 24 12:12:57.075: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.489748ms
Feb 24 12:12:59.081: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012992796s
Feb 24 12:13:01.080: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011865015s
STEP: Saw pod success 02/24/23 12:13:01.08
Feb 24 12:13:01.080: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b" satisfied condition "Succeeded or Failed"
Feb 24 12:13:01.085: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:13:01.093
Feb 24 12:13:01.107: INFO: Waiting for pod downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b to disappear
Feb 24 12:13:01.112: INFO: Pod downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 24 12:13:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5049" for this suite. 02/24/23 12:13:01.12
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":286,"skipped":5169,"failed":0}
------------------------------
• [4.140 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:12:56.988
    Feb 24 12:12:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:12:56.992
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:12:57.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:12:57.051
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 02/24/23 12:12:57.056
    Feb 24 12:12:57.068: INFO: Waiting up to 5m0s for pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b" in namespace "downward-api-5049" to be "Succeeded or Failed"
    Feb 24 12:12:57.075: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.489748ms
    Feb 24 12:12:59.081: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012992796s
    Feb 24 12:13:01.080: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011865015s
    STEP: Saw pod success 02/24/23 12:13:01.08
    Feb 24 12:13:01.080: INFO: Pod "downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b" satisfied condition "Succeeded or Failed"
    Feb 24 12:13:01.085: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:13:01.093
    Feb 24 12:13:01.107: INFO: Waiting for pod downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b to disappear
    Feb 24 12:13:01.112: INFO: Pod downward-api-a07f57e4-f3b3-4099-830f-16497c555f1b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 24 12:13:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5049" for this suite. 02/24/23 12:13:01.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:01.129
Feb 24 12:13:01.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename controllerrevisions 02/24/23 12:13:01.13
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:01.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:01.158
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-w7bxw-daemon-set" 02/24/23 12:13:01.194
STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 12:13:01.201
Feb 24 12:13:01.208: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:01.208: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:01.208: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:01.214: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 0
Feb 24 12:13:01.214: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:13:02.223: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:02.223: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:02.223: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:02.231: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 0
Feb 24 12:13:02.231: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:13:03.224: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:03.224: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:03.224: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:03.230: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 2
Feb 24 12:13:03.230: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
Feb 24 12:13:04.224: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:04.224: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:04.225: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 24 12:13:04.230: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 3
Feb 24 12:13:04.231: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-w7bxw-daemon-set
STEP: Confirm DaemonSet "e2e-w7bxw-daemon-set" successfully created with "daemonset-name=e2e-w7bxw-daemon-set" label 02/24/23 12:13:04.235
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-w7bxw-daemon-set" 02/24/23 12:13:04.251
Feb 24 12:13:04.256: INFO: Located ControllerRevision: "e2e-w7bxw-daemon-set-6698ddd8c4"
STEP: Patching ControllerRevision "e2e-w7bxw-daemon-set-6698ddd8c4" 02/24/23 12:13:04.261
Feb 24 12:13:04.271: INFO: e2e-w7bxw-daemon-set-6698ddd8c4 has been patched
STEP: Create a new ControllerRevision 02/24/23 12:13:04.271
Feb 24 12:13:04.278: INFO: Created ControllerRevision: e2e-w7bxw-daemon-set-6c6597bd96
STEP: Confirm that there are two ControllerRevisions 02/24/23 12:13:04.279
Feb 24 12:13:04.279: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 12:13:04.285: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-w7bxw-daemon-set-6698ddd8c4" 02/24/23 12:13:04.285
STEP: Confirm that there is only one ControllerRevision 02/24/23 12:13:04.294
Feb 24 12:13:04.295: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 12:13:04.299: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-w7bxw-daemon-set-6c6597bd96" 02/24/23 12:13:04.304
Feb 24 12:13:04.319: INFO: e2e-w7bxw-daemon-set-6c6597bd96 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/24/23 12:13:04.319
W0224 12:13:04.331266      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/24/23 12:13:04.331
Feb 24 12:13:04.331: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 12:13:05.336: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 12:13:05.341: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-w7bxw-daemon-set-6c6597bd96=updated" 02/24/23 12:13:05.342
STEP: Confirm that there is only one ControllerRevision 02/24/23 12:13:05.359
Feb 24 12:13:05.359: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 24 12:13:05.367: INFO: Found 1 ControllerRevisions
Feb 24 12:13:05.374: INFO: ControllerRevision "e2e-w7bxw-daemon-set-84748d8c69" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-w7bxw-daemon-set" 02/24/23 12:13:05.385
STEP: deleting DaemonSet.extensions e2e-w7bxw-daemon-set in namespace controllerrevisions-4407, will wait for the garbage collector to delete the pods 02/24/23 12:13:05.386
Feb 24 12:13:05.451: INFO: Deleting DaemonSet.extensions e2e-w7bxw-daemon-set took: 9.448622ms
Feb 24 12:13:05.551: INFO: Terminating DaemonSet.extensions e2e-w7bxw-daemon-set pods took: 100.648841ms
Feb 24 12:13:06.859: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 0
Feb 24 12:13:06.859: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-w7bxw-daemon-set
Feb 24 12:13:06.865: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38529"},"items":null}

Feb 24 12:13:06.869: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38529"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Feb 24 12:13:06.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-4407" for this suite. 02/24/23 12:13:06.902
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":287,"skipped":5174,"failed":0}
------------------------------
• [SLOW TEST] [5.785 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:01.129
    Feb 24 12:13:01.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename controllerrevisions 02/24/23 12:13:01.13
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:01.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:01.158
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-w7bxw-daemon-set" 02/24/23 12:13:01.194
    STEP: Check that daemon pods launch on every node of the cluster. 02/24/23 12:13:01.201
    Feb 24 12:13:01.208: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:01.208: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:01.208: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:01.214: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 0
    Feb 24 12:13:01.214: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:13:02.223: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:02.223: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:02.223: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:02.231: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 0
    Feb 24 12:13:02.231: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:13:03.224: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:03.224: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:03.224: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:03.230: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 2
    Feb 24 12:13:03.230: INFO: Node ip-172-31-215-124.eu-west-3.compute.internal is running 0 daemon pod, expected 1
    Feb 24 12:13:04.224: INFO: DaemonSet pods can't tolerate node ip-172-31-215-128.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:04.224: INFO: DaemonSet pods can't tolerate node ip-172-31-216-150.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:04.225: INFO: DaemonSet pods can't tolerate node ip-172-31-217-245.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 24 12:13:04.230: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 3
    Feb 24 12:13:04.231: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-w7bxw-daemon-set
    STEP: Confirm DaemonSet "e2e-w7bxw-daemon-set" successfully created with "daemonset-name=e2e-w7bxw-daemon-set" label 02/24/23 12:13:04.235
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-w7bxw-daemon-set" 02/24/23 12:13:04.251
    Feb 24 12:13:04.256: INFO: Located ControllerRevision: "e2e-w7bxw-daemon-set-6698ddd8c4"
    STEP: Patching ControllerRevision "e2e-w7bxw-daemon-set-6698ddd8c4" 02/24/23 12:13:04.261
    Feb 24 12:13:04.271: INFO: e2e-w7bxw-daemon-set-6698ddd8c4 has been patched
    STEP: Create a new ControllerRevision 02/24/23 12:13:04.271
    Feb 24 12:13:04.278: INFO: Created ControllerRevision: e2e-w7bxw-daemon-set-6c6597bd96
    STEP: Confirm that there are two ControllerRevisions 02/24/23 12:13:04.279
    Feb 24 12:13:04.279: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 12:13:04.285: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-w7bxw-daemon-set-6698ddd8c4" 02/24/23 12:13:04.285
    STEP: Confirm that there is only one ControllerRevision 02/24/23 12:13:04.294
    Feb 24 12:13:04.295: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 12:13:04.299: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-w7bxw-daemon-set-6c6597bd96" 02/24/23 12:13:04.304
    Feb 24 12:13:04.319: INFO: e2e-w7bxw-daemon-set-6c6597bd96 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/24/23 12:13:04.319
    W0224 12:13:04.331266      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/24/23 12:13:04.331
    Feb 24 12:13:04.331: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 12:13:05.336: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 12:13:05.341: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-w7bxw-daemon-set-6c6597bd96=updated" 02/24/23 12:13:05.342
    STEP: Confirm that there is only one ControllerRevision 02/24/23 12:13:05.359
    Feb 24 12:13:05.359: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 24 12:13:05.367: INFO: Found 1 ControllerRevisions
    Feb 24 12:13:05.374: INFO: ControllerRevision "e2e-w7bxw-daemon-set-84748d8c69" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-w7bxw-daemon-set" 02/24/23 12:13:05.385
    STEP: deleting DaemonSet.extensions e2e-w7bxw-daemon-set in namespace controllerrevisions-4407, will wait for the garbage collector to delete the pods 02/24/23 12:13:05.386
    Feb 24 12:13:05.451: INFO: Deleting DaemonSet.extensions e2e-w7bxw-daemon-set took: 9.448622ms
    Feb 24 12:13:05.551: INFO: Terminating DaemonSet.extensions e2e-w7bxw-daemon-set pods took: 100.648841ms
    Feb 24 12:13:06.859: INFO: Number of nodes with available pods controlled by daemonset e2e-w7bxw-daemon-set: 0
    Feb 24 12:13:06.859: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-w7bxw-daemon-set
    Feb 24 12:13:06.865: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38529"},"items":null}

    Feb 24 12:13:06.869: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38529"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 12:13:06.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-4407" for this suite. 02/24/23 12:13:06.902
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:06.915
Feb 24 12:13:06.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 12:13:06.917
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:06.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:06.981
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-90f8f068-e721-4fbf-8b47-2c961b970bfe 02/24/23 12:13:06.991
STEP: Creating the pod 02/24/23 12:13:06.998
Feb 24 12:13:07.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e" in namespace "configmap-1301" to be "running"
Feb 24 12:13:07.019: INFO: Pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.254175ms
Feb 24 12:13:09.025: INFO: Pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e": Phase="Running", Reason="", readiness=false. Elapsed: 2.014106895s
Feb 24 12:13:09.025: INFO: Pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e" satisfied condition "running"
STEP: Waiting for pod with text data 02/24/23 12:13:09.025
STEP: Waiting for pod with binary data 02/24/23 12:13:09.053
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 12:13:09.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1301" for this suite. 02/24/23 12:13:09.069
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":288,"skipped":5174,"failed":0}
------------------------------
• [2.162 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:06.915
    Feb 24 12:13:06.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 12:13:06.917
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:06.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:06.981
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-90f8f068-e721-4fbf-8b47-2c961b970bfe 02/24/23 12:13:06.991
    STEP: Creating the pod 02/24/23 12:13:06.998
    Feb 24 12:13:07.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e" in namespace "configmap-1301" to be "running"
    Feb 24 12:13:07.019: INFO: Pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.254175ms
    Feb 24 12:13:09.025: INFO: Pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e": Phase="Running", Reason="", readiness=false. Elapsed: 2.014106895s
    Feb 24 12:13:09.025: INFO: Pod "pod-configmaps-99424a4b-6eda-4c4a-9c63-26ab2eb04a5e" satisfied condition "running"
    STEP: Waiting for pod with text data 02/24/23 12:13:09.025
    STEP: Waiting for pod with binary data 02/24/23 12:13:09.053
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 12:13:09.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1301" for this suite. 02/24/23 12:13:09.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:09.085
Feb 24 12:13:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:13:09.086
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:09.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:09.165
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-de57f8cf-2f36-409d-b06c-7c9a300b971c 02/24/23 12:13:09.169
STEP: Creating a pod to test consume configMaps 02/24/23 12:13:09.175
Feb 24 12:13:09.185: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661" in namespace "projected-906" to be "Succeeded or Failed"
Feb 24 12:13:09.192: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661": Phase="Pending", Reason="", readiness=false. Elapsed: 6.870248ms
Feb 24 12:13:11.199: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013992919s
Feb 24 12:13:13.202: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017038621s
STEP: Saw pod success 02/24/23 12:13:13.202
Feb 24 12:13:13.202: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661" satisfied condition "Succeeded or Failed"
Feb 24 12:13:13.208: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:13:13.215
Feb 24 12:13:13.232: INFO: Waiting for pod pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661 to disappear
Feb 24 12:13:13.237: INFO: Pod pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 12:13:13.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-906" for this suite. 02/24/23 12:13:13.246
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":289,"skipped":5226,"failed":0}
------------------------------
• [4.170 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:09.085
    Feb 24 12:13:09.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:13:09.086
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:09.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:09.165
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-de57f8cf-2f36-409d-b06c-7c9a300b971c 02/24/23 12:13:09.169
    STEP: Creating a pod to test consume configMaps 02/24/23 12:13:09.175
    Feb 24 12:13:09.185: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661" in namespace "projected-906" to be "Succeeded or Failed"
    Feb 24 12:13:09.192: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661": Phase="Pending", Reason="", readiness=false. Elapsed: 6.870248ms
    Feb 24 12:13:11.199: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013992919s
    Feb 24 12:13:13.202: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017038621s
    STEP: Saw pod success 02/24/23 12:13:13.202
    Feb 24 12:13:13.202: INFO: Pod "pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661" satisfied condition "Succeeded or Failed"
    Feb 24 12:13:13.208: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:13:13.215
    Feb 24 12:13:13.232: INFO: Waiting for pod pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661 to disappear
    Feb 24 12:13:13.237: INFO: Pod pod-projected-configmaps-847342d8-6ea5-48b5-911f-7c7c6f971661 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 12:13:13.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-906" for this suite. 02/24/23 12:13:13.246
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:13.262
Feb 24 12:13:13.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename endpointslice 02/24/23 12:13:13.263
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:13.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:13.342
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 24 12:13:15.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1099" for this suite. 02/24/23 12:13:15.455
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":290,"skipped":5230,"failed":0}
------------------------------
• [2.203 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:13.262
    Feb 24 12:13:13.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename endpointslice 02/24/23 12:13:13.263
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:13.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:13.342
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 24 12:13:15.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1099" for this suite. 02/24/23 12:13:15.455
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:15.471
Feb 24 12:13:15.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:13:15.472
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:15.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:15.511
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:13:15.517
Feb 24 12:13:15.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095" in namespace "projected-3623" to be "Succeeded or Failed"
Feb 24 12:13:15.540: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095": Phase="Pending", Reason="", readiness=false. Elapsed: 8.157532ms
Feb 24 12:13:17.547: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014769601s
Feb 24 12:13:19.547: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014929784s
STEP: Saw pod success 02/24/23 12:13:19.547
Feb 24 12:13:19.548: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095" satisfied condition "Succeeded or Failed"
Feb 24 12:13:19.552: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095 container client-container: <nil>
STEP: delete the pod 02/24/23 12:13:19.56
Feb 24 12:13:19.575: INFO: Waiting for pod downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095 to disappear
Feb 24 12:13:19.580: INFO: Pod downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:13:19.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3623" for this suite. 02/24/23 12:13:19.587
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":291,"skipped":5230,"failed":0}
------------------------------
• [4.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:15.471
    Feb 24 12:13:15.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:13:15.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:15.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:15.511
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:13:15.517
    Feb 24 12:13:15.532: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095" in namespace "projected-3623" to be "Succeeded or Failed"
    Feb 24 12:13:15.540: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095": Phase="Pending", Reason="", readiness=false. Elapsed: 8.157532ms
    Feb 24 12:13:17.547: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014769601s
    Feb 24 12:13:19.547: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014929784s
    STEP: Saw pod success 02/24/23 12:13:19.547
    Feb 24 12:13:19.548: INFO: Pod "downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095" satisfied condition "Succeeded or Failed"
    Feb 24 12:13:19.552: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:13:19.56
    Feb 24 12:13:19.575: INFO: Waiting for pod downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095 to disappear
    Feb 24 12:13:19.580: INFO: Pod downwardapi-volume-10dd17cf-014b-424f-8b50-6394ba24e095 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:13:19.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3623" for this suite. 02/24/23 12:13:19.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:19.603
Feb 24 12:13:19.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 12:13:19.604
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:19.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:19.648
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 12:13:19.654
Feb 24 12:13:19.663: INFO: Waiting up to 5m0s for pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6" in namespace "emptydir-6644" to be "Succeeded or Failed"
Feb 24 12:13:19.670: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.216849ms
Feb 24 12:13:21.676: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010864936s
Feb 24 12:13:23.676: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010955172s
STEP: Saw pod success 02/24/23 12:13:23.676
Feb 24 12:13:23.676: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6" satisfied condition "Succeeded or Failed"
Feb 24 12:13:23.681: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6 container test-container: <nil>
STEP: delete the pod 02/24/23 12:13:23.689
Feb 24 12:13:23.702: INFO: Waiting for pod pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6 to disappear
Feb 24 12:13:23.707: INFO: Pod pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 12:13:23.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6644" for this suite. 02/24/23 12:13:23.719
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":292,"skipped":5237,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:19.603
    Feb 24 12:13:19.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:13:19.604
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:19.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:19.648
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/24/23 12:13:19.654
    Feb 24 12:13:19.663: INFO: Waiting up to 5m0s for pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6" in namespace "emptydir-6644" to be "Succeeded or Failed"
    Feb 24 12:13:19.670: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.216849ms
    Feb 24 12:13:21.676: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010864936s
    Feb 24 12:13:23.676: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010955172s
    STEP: Saw pod success 02/24/23 12:13:23.676
    Feb 24 12:13:23.676: INFO: Pod "pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6" satisfied condition "Succeeded or Failed"
    Feb 24 12:13:23.681: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:13:23.689
    Feb 24 12:13:23.702: INFO: Waiting for pod pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6 to disappear
    Feb 24 12:13:23.707: INFO: Pod pod-2b2708c8-5020-4cfc-a0dd-72abf6a6a8d6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:13:23.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6644" for this suite. 02/24/23 12:13:23.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:23.732
Feb 24 12:13:23.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename aggregator 02/24/23 12:13:23.733
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:23.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:23.758
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 24 12:13:23.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/24/23 12:13:23.764
Feb 24 12:13:24.298: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Feb 24 12:13:26.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:28.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:30.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:32.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:34.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:36.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:38.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:40.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:42.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:44.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:46.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 24 12:13:48.606: INFO: Waited 124.013956ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/24/23 12:13:48.68
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/24/23 12:13:48.685
STEP: List APIServices 02/24/23 12:13:48.693
Feb 24 12:13:48.701: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Feb 24 12:13:48.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-716" for this suite. 02/24/23 12:13:49.048
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":293,"skipped":5268,"failed":0}
------------------------------
• [SLOW TEST] [25.365 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:23.732
    Feb 24 12:13:23.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename aggregator 02/24/23 12:13:23.733
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:23.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:23.758
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 24 12:13:23.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/24/23 12:13:23.764
    Feb 24 12:13:24.298: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Feb 24 12:13:26.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:28.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:30.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:32.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:34.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:36.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:38.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:40.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:42.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:44.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:46.473: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 24, 12, 13, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 24 12:13:48.606: INFO: Waited 124.013956ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/24/23 12:13:48.68
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/24/23 12:13:48.685
    STEP: List APIServices 02/24/23 12:13:48.693
    Feb 24 12:13:48.701: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Feb 24 12:13:48.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-716" for this suite. 02/24/23 12:13:49.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:49.1
Feb 24 12:13:49.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename init-container 02/24/23 12:13:49.101
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:49.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:49.127
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 02/24/23 12:13:49.131
Feb 24 12:13:49.131: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 12:13:52.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4673" for this suite. 02/24/23 12:13:52.019
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":294,"skipped":5290,"failed":0}
------------------------------
• [2.928 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:49.1
    Feb 24 12:13:49.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename init-container 02/24/23 12:13:49.101
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:49.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:49.127
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 02/24/23 12:13:49.131
    Feb 24 12:13:49.131: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 12:13:52.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4673" for this suite. 02/24/23 12:13:52.019
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:52.029
Feb 24 12:13:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename events 02/24/23 12:13:52.03
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:52.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:52.059
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/24/23 12:13:52.064
STEP: get a list of Events with a label in the current namespace 02/24/23 12:13:52.086
STEP: delete a list of events 02/24/23 12:13:52.091
Feb 24 12:13:52.092: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/24/23 12:13:52.117
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 24 12:13:52.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1064" for this suite. 02/24/23 12:13:52.129
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":295,"skipped":5291,"failed":0}
------------------------------
• [0.109 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:52.029
    Feb 24 12:13:52.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename events 02/24/23 12:13:52.03
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:52.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:52.059
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/24/23 12:13:52.064
    STEP: get a list of Events with a label in the current namespace 02/24/23 12:13:52.086
    STEP: delete a list of events 02/24/23 12:13:52.091
    Feb 24 12:13:52.092: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/24/23 12:13:52.117
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 24 12:13:52.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1064" for this suite. 02/24/23 12:13:52.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:52.147
Feb 24 12:13:52.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:13:52.148
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:52.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:52.189
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 02/24/23 12:13:52.193
Feb 24 12:13:52.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-7624 api-versions'
Feb 24 12:13:52.335: INFO: stderr: ""
Feb 24 12:13:52.335: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperatingsystemmanager.k8c.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:13:52.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7624" for this suite. 02/24/23 12:13:52.342
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":296,"skipped":5297,"failed":0}
------------------------------
• [0.202 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:52.147
    Feb 24 12:13:52.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:13:52.148
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:52.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:52.189
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 02/24/23 12:13:52.193
    Feb 24 12:13:52.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-7624 api-versions'
    Feb 24 12:13:52.335: INFO: stderr: ""
    Feb 24 12:13:52.335: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperatingsystemmanager.k8c.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:13:52.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7624" for this suite. 02/24/23 12:13:52.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:52.359
Feb 24 12:13:52.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename security-context 02/24/23 12:13:52.361
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:52.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:52.39
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 12:13:52.394
Feb 24 12:13:52.404: INFO: Waiting up to 5m0s for pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a" in namespace "security-context-4112" to be "Succeeded or Failed"
Feb 24 12:13:52.414: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.956653ms
Feb 24 12:13:54.419: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015233284s
Feb 24 12:13:56.419: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015237905s
STEP: Saw pod success 02/24/23 12:13:56.419
Feb 24 12:13:56.419: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a" satisfied condition "Succeeded or Failed"
Feb 24 12:13:56.423: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod security-context-938f96ad-599f-44b0-8c62-71538e20c06a container test-container: <nil>
STEP: delete the pod 02/24/23 12:13:56.431
Feb 24 12:13:56.442: INFO: Waiting for pod security-context-938f96ad-599f-44b0-8c62-71538e20c06a to disappear
Feb 24 12:13:56.446: INFO: Pod security-context-938f96ad-599f-44b0-8c62-71538e20c06a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 24 12:13:56.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4112" for this suite. 02/24/23 12:13:56.457
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":297,"skipped":5431,"failed":0}
------------------------------
• [4.106 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:52.359
    Feb 24 12:13:52.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename security-context 02/24/23 12:13:52.361
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:52.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:52.39
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/24/23 12:13:52.394
    Feb 24 12:13:52.404: INFO: Waiting up to 5m0s for pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a" in namespace "security-context-4112" to be "Succeeded or Failed"
    Feb 24 12:13:52.414: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.956653ms
    Feb 24 12:13:54.419: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015233284s
    Feb 24 12:13:56.419: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015237905s
    STEP: Saw pod success 02/24/23 12:13:56.419
    Feb 24 12:13:56.419: INFO: Pod "security-context-938f96ad-599f-44b0-8c62-71538e20c06a" satisfied condition "Succeeded or Failed"
    Feb 24 12:13:56.423: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod security-context-938f96ad-599f-44b0-8c62-71538e20c06a container test-container: <nil>
    STEP: delete the pod 02/24/23 12:13:56.431
    Feb 24 12:13:56.442: INFO: Waiting for pod security-context-938f96ad-599f-44b0-8c62-71538e20c06a to disappear
    Feb 24 12:13:56.446: INFO: Pod security-context-938f96ad-599f-44b0-8c62-71538e20c06a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 24 12:13:56.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4112" for this suite. 02/24/23 12:13:56.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:13:56.475
Feb 24 12:13:56.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-runtime 02/24/23 12:13:56.476
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:56.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:56.51
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 02/24/23 12:13:56.515
STEP: wait for the container to reach Failed 02/24/23 12:13:56.526
STEP: get the container status 02/24/23 12:14:00.553
STEP: the container should be terminated 02/24/23 12:14:00.558
STEP: the termination message should be set 02/24/23 12:14:00.559
Feb 24 12:14:00.559: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/24/23 12:14:00.559
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 24 12:14:00.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8285" for this suite. 02/24/23 12:14:00.588
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":298,"skipped":5491,"failed":0}
------------------------------
• [4.122 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:13:56.475
    Feb 24 12:13:56.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-runtime 02/24/23 12:13:56.476
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:13:56.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:13:56.51
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 02/24/23 12:13:56.515
    STEP: wait for the container to reach Failed 02/24/23 12:13:56.526
    STEP: get the container status 02/24/23 12:14:00.553
    STEP: the container should be terminated 02/24/23 12:14:00.558
    STEP: the termination message should be set 02/24/23 12:14:00.559
    Feb 24 12:14:00.559: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/24/23 12:14:00.559
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 24 12:14:00.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8285" for this suite. 02/24/23 12:14:00.588
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:00.598
Feb 24 12:14:00.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 12:14:00.6
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:00.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:00.631
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/24/23 12:14:00.635
Feb 24 12:14:00.644: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2932  0c78a45f-2b32-460f-bdd9-f22a8467f6d5 39075 0 2023-02-24 12:14:00 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-24 12:14:00 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gnm6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gnm6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 24 12:14:00.644: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2932" to be "running and ready"
Feb 24 12:14:00.653: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93108ms
Feb 24 12:14:00.653: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:14:02.658: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014111831s
Feb 24 12:14:02.658: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 24 12:14:02.658: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/24/23 12:14:02.658
Feb 24 12:14:02.659: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2932 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:14:02.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:14:02.659: INFO: ExecWithOptions: Clientset creation
Feb 24 12:14:02.659: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2932/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/24/23 12:14:02.799
Feb 24 12:14:02.799: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2932 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:14:02.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:14:02.800: INFO: ExecWithOptions: Clientset creation
Feb 24 12:14:02.800: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2932/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:14:02.957: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 12:14:02.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2932" for this suite. 02/24/23 12:14:02.984
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":299,"skipped":5493,"failed":0}
------------------------------
• [2.397 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:00.598
    Feb 24 12:14:00.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 12:14:00.6
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:00.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:00.631
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/24/23 12:14:00.635
    Feb 24 12:14:00.644: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2932  0c78a45f-2b32-460f-bdd9-f22a8467f6d5 39075 0 2023-02-24 12:14:00 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-24 12:14:00 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gnm6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gnm6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 24 12:14:00.644: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2932" to be "running and ready"
    Feb 24 12:14:00.653: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.93108ms
    Feb 24 12:14:00.653: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:14:02.658: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.014111831s
    Feb 24 12:14:02.658: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 24 12:14:02.658: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/24/23 12:14:02.658
    Feb 24 12:14:02.659: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2932 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:14:02.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:14:02.659: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:14:02.659: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2932/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/24/23 12:14:02.799
    Feb 24 12:14:02.799: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2932 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:14:02.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:14:02.800: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:14:02.800: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2932/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:14:02.957: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 12:14:02.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2932" for this suite. 02/24/23 12:14:02.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:02.997
Feb 24 12:14:02.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:14:02.998
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:03.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:03.055
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 24 12:14:03.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:14:04.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4655" for this suite. 02/24/23 12:14:04.114
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":300,"skipped":5503,"failed":0}
------------------------------
• [1.125 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:02.997
    Feb 24 12:14:02.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:14:02.998
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:03.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:03.055
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 24 12:14:03.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:14:04.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4655" for this suite. 02/24/23 12:14:04.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:04.126
Feb 24 12:14:04.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 12:14:04.127
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:04.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:04.155
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 12:14:04.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-361" for this suite. 02/24/23 12:14:04.215
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":301,"skipped":5522,"failed":0}
------------------------------
• [0.097 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:04.126
    Feb 24 12:14:04.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 12:14:04.127
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:04.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:04.155
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 12:14:04.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-361" for this suite. 02/24/23 12:14:04.215
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:04.224
Feb 24 12:14:04.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 12:14:04.225
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:04.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:04.27
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/24/23 12:14:04.274
STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 12:14:04.28
STEP: delete the deployment 02/24/23 12:14:04.793
STEP: wait for all rs to be garbage collected 02/24/23 12:14:04.815
STEP: expected 0 rs, got 1 rs 02/24/23 12:14:04.824
STEP: expected 0 pods, got 2 pods 02/24/23 12:14:04.857
STEP: Gathering metrics 02/24/23 12:14:05.43
Feb 24 12:14:05.495: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 12:14:05.500: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.645639ms
Feb 24 12:14:05.500: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 12:14:05.500: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 12:14:05.590: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 12:14:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7445" for this suite. 02/24/23 12:14:05.601
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":302,"skipped":5525,"failed":0}
------------------------------
• [1.385 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:04.224
    Feb 24 12:14:04.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 12:14:04.225
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:04.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:04.27
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/24/23 12:14:04.274
    STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 12:14:04.28
    STEP: delete the deployment 02/24/23 12:14:04.793
    STEP: wait for all rs to be garbage collected 02/24/23 12:14:04.815
    STEP: expected 0 rs, got 1 rs 02/24/23 12:14:04.824
    STEP: expected 0 pods, got 2 pods 02/24/23 12:14:04.857
    STEP: Gathering metrics 02/24/23 12:14:05.43
    Feb 24 12:14:05.495: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 12:14:05.500: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.645639ms
    Feb 24 12:14:05.500: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 12:14:05.500: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 12:14:05.590: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 12:14:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7445" for this suite. 02/24/23 12:14:05.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:05.614
Feb 24 12:14:05.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 12:14:05.615
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:05.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:05.662
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-afcf0155-bf68-477c-b4a6-c0e3222f1986 02/24/23 12:14:05.666
STEP: Creating a pod to test consume configMaps 02/24/23 12:14:05.687
Feb 24 12:14:05.715: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9" in namespace "configmap-7896" to be "Succeeded or Failed"
Feb 24 12:14:05.735: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.24411ms
Feb 24 12:14:07.741: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026823221s
Feb 24 12:14:09.743: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028152685s
STEP: Saw pod success 02/24/23 12:14:09.743
Feb 24 12:14:09.743: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9" satisfied condition "Succeeded or Failed"
Feb 24 12:14:09.747: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:14:09.755
Feb 24 12:14:09.772: INFO: Waiting for pod pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9 to disappear
Feb 24 12:14:09.777: INFO: Pod pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 12:14:09.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7896" for this suite. 02/24/23 12:14:09.787
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":303,"skipped":5553,"failed":0}
------------------------------
• [4.184 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:05.614
    Feb 24 12:14:05.615: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 12:14:05.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:05.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:05.662
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-afcf0155-bf68-477c-b4a6-c0e3222f1986 02/24/23 12:14:05.666
    STEP: Creating a pod to test consume configMaps 02/24/23 12:14:05.687
    Feb 24 12:14:05.715: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9" in namespace "configmap-7896" to be "Succeeded or Failed"
    Feb 24 12:14:05.735: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.24411ms
    Feb 24 12:14:07.741: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026823221s
    Feb 24 12:14:09.743: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028152685s
    STEP: Saw pod success 02/24/23 12:14:09.743
    Feb 24 12:14:09.743: INFO: Pod "pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9" satisfied condition "Succeeded or Failed"
    Feb 24 12:14:09.747: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:14:09.755
    Feb 24 12:14:09.772: INFO: Waiting for pod pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9 to disappear
    Feb 24 12:14:09.777: INFO: Pod pod-configmaps-45d80be8-2033-40e0-884c-8bb73a4a1ee9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 12:14:09.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7896" for this suite. 02/24/23 12:14:09.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:09.801
Feb 24 12:14:09.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replicaset 02/24/23 12:14:09.802
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:09.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:09.832
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/24/23 12:14:09.841
STEP: Verify that the required pods have come up. 02/24/23 12:14:09.847
Feb 24 12:14:09.851: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 24 12:14:14.857: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/24/23 12:14:14.857
STEP: Getting /status 02/24/23 12:14:14.857
Feb 24 12:14:14.862: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/24/23 12:14:14.862
Feb 24 12:14:14.874: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/24/23 12:14:14.874
Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: ADDED
Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.878: INFO: Found replicaset test-rs in namespace replicaset-9140 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 24 12:14:14.878: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/24/23 12:14:14.878
Feb 24 12:14:14.879: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 24 12:14:14.887: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/24/23 12:14:14.887
Feb 24 12:14:14.891: INFO: Observed &ReplicaSet event: ADDED
Feb 24 12:14:14.891: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.892: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.893: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.893: INFO: Observed replicaset test-rs in namespace replicaset-9140 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 24 12:14:14.893: INFO: Observed &ReplicaSet event: MODIFIED
Feb 24 12:14:14.893: INFO: Found replicaset test-rs in namespace replicaset-9140 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 24 12:14:14.893: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 24 12:14:14.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9140" for this suite. 02/24/23 12:14:14.901
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":304,"skipped":5572,"failed":0}
------------------------------
• [SLOW TEST] [5.111 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:09.801
    Feb 24 12:14:09.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replicaset 02/24/23 12:14:09.802
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:09.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:09.832
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/24/23 12:14:09.841
    STEP: Verify that the required pods have come up. 02/24/23 12:14:09.847
    Feb 24 12:14:09.851: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 24 12:14:14.857: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/24/23 12:14:14.857
    STEP: Getting /status 02/24/23 12:14:14.857
    Feb 24 12:14:14.862: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/24/23 12:14:14.862
    Feb 24 12:14:14.874: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/24/23 12:14:14.874
    Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: ADDED
    Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.878: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.878: INFO: Found replicaset test-rs in namespace replicaset-9140 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 24 12:14:14.878: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/24/23 12:14:14.878
    Feb 24 12:14:14.879: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 24 12:14:14.887: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/24/23 12:14:14.887
    Feb 24 12:14:14.891: INFO: Observed &ReplicaSet event: ADDED
    Feb 24 12:14:14.891: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.892: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.893: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.893: INFO: Observed replicaset test-rs in namespace replicaset-9140 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 24 12:14:14.893: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 24 12:14:14.893: INFO: Found replicaset test-rs in namespace replicaset-9140 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 24 12:14:14.893: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 24 12:14:14.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9140" for this suite. 02/24/23 12:14:14.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:14.915
Feb 24 12:14:14.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-pred 02/24/23 12:14:14.92
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:14.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:14.953
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 24 12:14:14.957: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 12:14:14.972: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 12:14:14.977: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
Feb 24 12:14:14.989: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:14.989: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:14:14.989: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:14:14.989: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
Feb 24 12:14:14.989: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:14:14.989: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:14:14.995: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:14:14.996: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:14.996: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:14:14.996: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:14.996: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:14:14.996: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:14.996: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 12:14:14.996: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:14.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:14:14.997: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 12:14:14.997: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
Feb 24 12:14:15.012: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:15.012: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:14:15.012: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:14:15.012: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
Feb 24 12:14:15.013: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:14:15.013: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:14:15.013: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:14:15.013: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:15.013: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:14:15.013: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:15.013: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:14:15.013: INFO: test-rs-ww54z from replicaset-9140 started at 2023-02-24 12:14:09 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:15.013: INFO: 	Container httpd ready: true, restart count 0
Feb 24 12:14:15.013: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:15.013: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:14:15.013: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 12:14:15.014: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
Feb 24 12:14:15.027: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:15.027: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:14:15.027: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:14:15.027: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
Feb 24 12:14:15.027: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:14:15.027: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:14:15.027: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:14:15.027: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:15.027: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:14:15.027: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 12:14:15.027: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:14:15.027: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:15.027: INFO: 	Container e2e ready: true, restart count 0
Feb 24 12:14:15.027: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:14:15.027: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:14:15.027: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:14:15.027: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 12:14:15.027
Feb 24 12:14:15.038: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6586" to be "running"
Feb 24 12:14:15.045: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286667ms
Feb 24 12:14:17.051: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013245769s
Feb 24 12:14:17.051: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 12:14:17.055
STEP: Trying to apply a random label on the found node. 02/24/23 12:14:17.068
STEP: verifying the node has the label kubernetes.io/e2e-9709234c-b38b-4a1c-9f7e-d116957fef06 42 02/24/23 12:14:17.083
STEP: Trying to relaunch the pod, now with labels. 02/24/23 12:14:17.087
Feb 24 12:14:17.094: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6586" to be "not pending"
Feb 24 12:14:17.100: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195999ms
Feb 24 12:14:19.107: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012876334s
Feb 24 12:14:19.107: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-9709234c-b38b-4a1c-9f7e-d116957fef06 off the node ip-172-31-216-47.eu-west-3.compute.internal 02/24/23 12:14:19.112
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9709234c-b38b-4a1c-9f7e-d116957fef06 02/24/23 12:14:19.131
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 24 12:14:19.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6586" for this suite. 02/24/23 12:14:19.143
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":305,"skipped":5578,"failed":0}
------------------------------
• [4.237 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:14.915
    Feb 24 12:14:14.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-pred 02/24/23 12:14:14.92
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:14.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:14.953
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 24 12:14:14.957: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 12:14:14.972: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 12:14:14.977: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
    Feb 24 12:14:14.989: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:14.989: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:14:14.989: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:14:14.989: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
    Feb 24 12:14:14.989: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:14:14.989: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:14:14.995: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:14:14.996: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:14.996: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:14:14.996: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:14.996: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:14:14.996: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:14.996: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 12:14:14.996: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:14.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:14:14.997: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 12:14:14.997: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
    Feb 24 12:14:15.012: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:15.012: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:14:15.012: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:14:15.012: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
    Feb 24 12:14:15.013: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:15.013: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:15.013: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: test-rs-ww54z from replicaset-9140 started at 2023-02-24 12:14:09 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:15.013: INFO: 	Container httpd ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:15.013: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:14:15.013: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 12:14:15.014: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
    Feb 24 12:14:15.027: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:15.027: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
    Feb 24 12:14:15.027: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:15.027: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 12:14:15.027: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:15.027: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:14:15.027: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:14:15.027: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/24/23 12:14:15.027
    Feb 24 12:14:15.038: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6586" to be "running"
    Feb 24 12:14:15.045: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286667ms
    Feb 24 12:14:17.051: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013245769s
    Feb 24 12:14:17.051: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/24/23 12:14:17.055
    STEP: Trying to apply a random label on the found node. 02/24/23 12:14:17.068
    STEP: verifying the node has the label kubernetes.io/e2e-9709234c-b38b-4a1c-9f7e-d116957fef06 42 02/24/23 12:14:17.083
    STEP: Trying to relaunch the pod, now with labels. 02/24/23 12:14:17.087
    Feb 24 12:14:17.094: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6586" to be "not pending"
    Feb 24 12:14:17.100: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.195999ms
    Feb 24 12:14:19.107: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.012876334s
    Feb 24 12:14:19.107: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-9709234c-b38b-4a1c-9f7e-d116957fef06 off the node ip-172-31-216-47.eu-west-3.compute.internal 02/24/23 12:14:19.112
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-9709234c-b38b-4a1c-9f7e-d116957fef06 02/24/23 12:14:19.131
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 12:14:19.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6586" for this suite. 02/24/23 12:14:19.143
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:19.152
Feb 24 12:14:19.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 12:14:19.154
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:19.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:19.197
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 12:14:19.201
Feb 24 12:14:19.213: INFO: Waiting up to 5m0s for pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29" in namespace "emptydir-6755" to be "Succeeded or Failed"
Feb 24 12:14:19.221: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.173677ms
Feb 24 12:14:21.226: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013381013s
Feb 24 12:14:23.227: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014272169s
STEP: Saw pod success 02/24/23 12:14:23.227
Feb 24 12:14:23.227: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29" satisfied condition "Succeeded or Failed"
Feb 24 12:14:23.232: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-8e822fc8-0502-4d8b-b058-559baeb90c29 container test-container: <nil>
STEP: delete the pod 02/24/23 12:14:23.24
Feb 24 12:14:23.270: INFO: Waiting for pod pod-8e822fc8-0502-4d8b-b058-559baeb90c29 to disappear
Feb 24 12:14:23.275: INFO: Pod pod-8e822fc8-0502-4d8b-b058-559baeb90c29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 12:14:23.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6755" for this suite. 02/24/23 12:14:23.283
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5597,"failed":0}
------------------------------
• [4.139 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:19.152
    Feb 24 12:14:19.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:14:19.154
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:19.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:19.197
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/24/23 12:14:19.201
    Feb 24 12:14:19.213: INFO: Waiting up to 5m0s for pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29" in namespace "emptydir-6755" to be "Succeeded or Failed"
    Feb 24 12:14:19.221: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.173677ms
    Feb 24 12:14:21.226: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013381013s
    Feb 24 12:14:23.227: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014272169s
    STEP: Saw pod success 02/24/23 12:14:23.227
    Feb 24 12:14:23.227: INFO: Pod "pod-8e822fc8-0502-4d8b-b058-559baeb90c29" satisfied condition "Succeeded or Failed"
    Feb 24 12:14:23.232: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod pod-8e822fc8-0502-4d8b-b058-559baeb90c29 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:14:23.24
    Feb 24 12:14:23.270: INFO: Waiting for pod pod-8e822fc8-0502-4d8b-b058-559baeb90c29 to disappear
    Feb 24 12:14:23.275: INFO: Pod pod-8e822fc8-0502-4d8b-b058-559baeb90c29 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:14:23.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6755" for this suite. 02/24/23 12:14:23.283
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:14:23.292
Feb 24 12:14:23.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 12:14:23.294
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:23.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:23.345
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3781 02/24/23 12:14:23.349
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 02/24/23 12:14:23.363
Feb 24 12:14:23.383: INFO: Found 0 stateful pods, waiting for 3
Feb 24 12:14:33.389: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:14:33.389: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:14:33.389: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/24/23 12:14:33.406
Feb 24 12:14:33.428: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/24/23 12:14:33.428
STEP: Not applying an update when the partition is greater than the number of replicas 02/24/23 12:14:43.451
STEP: Performing a canary update 02/24/23 12:14:43.451
Feb 24 12:14:43.474: INFO: Updating stateful set ss2
Feb 24 12:14:43.484: INFO: Waiting for Pod statefulset-3781/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 02/24/23 12:14:53.496
Feb 24 12:14:53.543: INFO: Found 1 stateful pods, waiting for 3
Feb 24 12:15:03.550: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:15:03.550: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:15:03.550: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/24/23 12:15:03.56
Feb 24 12:15:03.584: INFO: Updating stateful set ss2
Feb 24 12:15:03.593: INFO: Waiting for Pod statefulset-3781/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 24 12:15:13.627: INFO: Updating stateful set ss2
Feb 24 12:15:13.637: INFO: Waiting for StatefulSet statefulset-3781/ss2 to complete update
Feb 24 12:15:13.638: INFO: Waiting for Pod statefulset-3781/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 12:15:23.647: INFO: Deleting all statefulset in ns statefulset-3781
Feb 24 12:15:23.651: INFO: Scaling statefulset ss2 to 0
Feb 24 12:15:33.673: INFO: Waiting for statefulset status.replicas updated to 0
Feb 24 12:15:33.677: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 12:15:33.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3781" for this suite. 02/24/23 12:15:33.704
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":307,"skipped":5597,"failed":0}
------------------------------
• [SLOW TEST] [70.420 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:14:23.292
    Feb 24 12:14:23.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 12:14:23.294
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:14:23.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:14:23.345
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3781 02/24/23 12:14:23.349
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 02/24/23 12:14:23.363
    Feb 24 12:14:23.383: INFO: Found 0 stateful pods, waiting for 3
    Feb 24 12:14:33.389: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:14:33.389: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:14:33.389: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/24/23 12:14:33.406
    Feb 24 12:14:33.428: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/24/23 12:14:33.428
    STEP: Not applying an update when the partition is greater than the number of replicas 02/24/23 12:14:43.451
    STEP: Performing a canary update 02/24/23 12:14:43.451
    Feb 24 12:14:43.474: INFO: Updating stateful set ss2
    Feb 24 12:14:43.484: INFO: Waiting for Pod statefulset-3781/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 02/24/23 12:14:53.496
    Feb 24 12:14:53.543: INFO: Found 1 stateful pods, waiting for 3
    Feb 24 12:15:03.550: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:15:03.550: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:15:03.550: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/24/23 12:15:03.56
    Feb 24 12:15:03.584: INFO: Updating stateful set ss2
    Feb 24 12:15:03.593: INFO: Waiting for Pod statefulset-3781/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 24 12:15:13.627: INFO: Updating stateful set ss2
    Feb 24 12:15:13.637: INFO: Waiting for StatefulSet statefulset-3781/ss2 to complete update
    Feb 24 12:15:13.638: INFO: Waiting for Pod statefulset-3781/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 12:15:23.647: INFO: Deleting all statefulset in ns statefulset-3781
    Feb 24 12:15:23.651: INFO: Scaling statefulset ss2 to 0
    Feb 24 12:15:33.673: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 24 12:15:33.677: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 12:15:33.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3781" for this suite. 02/24/23 12:15:33.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:15:33.713
Feb 24 12:15:33.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-webhook 02/24/23 12:15:33.714
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:33.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:33.747
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/24/23 12:15:33.751
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 12:15:34.239
STEP: Deploying the custom resource conversion webhook pod 02/24/23 12:15:34.253
STEP: Wait for the deployment to be ready 02/24/23 12:15:34.268
Feb 24 12:15:34.281: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:15:36.295
STEP: Verifying the service has paired with the endpoint 02/24/23 12:15:36.322
Feb 24 12:15:37.323: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 24 12:15:37.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Creating a v1 custom resource 02/24/23 12:15:39.913
STEP: Create a v2 custom resource 02/24/23 12:15:39.933
STEP: List CRs in v1 02/24/23 12:15:39.986
STEP: List CRs in v2 02/24/23 12:15:40.007
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:15:40.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7765" for this suite. 02/24/23 12:15:40.554
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":308,"skipped":5604,"failed":0}
------------------------------
• [SLOW TEST] [6.921 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:15:33.713
    Feb 24 12:15:33.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-webhook 02/24/23 12:15:33.714
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:33.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:33.747
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/24/23 12:15:33.751
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/24/23 12:15:34.239
    STEP: Deploying the custom resource conversion webhook pod 02/24/23 12:15:34.253
    STEP: Wait for the deployment to be ready 02/24/23 12:15:34.268
    Feb 24 12:15:34.281: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:15:36.295
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:15:36.322
    Feb 24 12:15:37.323: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 24 12:15:37.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Creating a v1 custom resource 02/24/23 12:15:39.913
    STEP: Create a v2 custom resource 02/24/23 12:15:39.933
    STEP: List CRs in v1 02/24/23 12:15:39.986
    STEP: List CRs in v2 02/24/23 12:15:40.007
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:15:40.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7765" for this suite. 02/24/23 12:15:40.554
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:15:40.636
Feb 24 12:15:40.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:15:40.639
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:40.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:40.683
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:15:40.696
Feb 24 12:15:40.720: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8212" to be "running and ready"
Feb 24 12:15:40.726: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.815719ms
Feb 24 12:15:40.726: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:15:42.731: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010480081s
Feb 24 12:15:42.731: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 24 12:15:42.731: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 02/24/23 12:15:42.735
Feb 24 12:15:42.742: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8212" to be "running and ready"
Feb 24 12:15:42.756: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 13.472952ms
Feb 24 12:15:42.756: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:15:44.762: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019608238s
Feb 24 12:15:44.762: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 24 12:15:44.763: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/24/23 12:15:44.766
STEP: delete the pod with lifecycle hook 02/24/23 12:15:44.78
Feb 24 12:15:44.790: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 12:15:44.797: INFO: Pod pod-with-poststart-http-hook still exists
Feb 24 12:15:46.798: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 12:15:46.803: INFO: Pod pod-with-poststart-http-hook still exists
Feb 24 12:15:48.799: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 24 12:15:48.804: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 24 12:15:48.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8212" for this suite. 02/24/23 12:15:48.813
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":309,"skipped":5611,"failed":0}
------------------------------
• [SLOW TEST] [8.188 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:15:40.636
    Feb 24 12:15:40.638: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/24/23 12:15:40.639
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:40.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:40.683
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/24/23 12:15:40.696
    Feb 24 12:15:40.720: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8212" to be "running and ready"
    Feb 24 12:15:40.726: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.815719ms
    Feb 24 12:15:40.726: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:15:42.731: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010480081s
    Feb 24 12:15:42.731: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 24 12:15:42.731: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 02/24/23 12:15:42.735
    Feb 24 12:15:42.742: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8212" to be "running and ready"
    Feb 24 12:15:42.756: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 13.472952ms
    Feb 24 12:15:42.756: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:15:44.762: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019608238s
    Feb 24 12:15:44.762: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 24 12:15:44.763: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/24/23 12:15:44.766
    STEP: delete the pod with lifecycle hook 02/24/23 12:15:44.78
    Feb 24 12:15:44.790: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 24 12:15:44.797: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 24 12:15:46.798: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 24 12:15:46.803: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 24 12:15:48.799: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 24 12:15:48.804: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 24 12:15:48.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8212" for this suite. 02/24/23 12:15:48.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:15:48.829
Feb 24 12:15:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:15:48.83
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:48.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:48.858
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 02/24/23 12:15:48.862
Feb 24 12:15:48.873: INFO: Waiting up to 5m0s for pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c" in namespace "projected-7689" to be "running and ready"
Feb 24 12:15:48.878: INFO: Pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812002ms
Feb 24 12:15:48.878: INFO: The phase of Pod annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:15:50.883: INFO: Pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010214071s
Feb 24 12:15:50.883: INFO: The phase of Pod annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c is Running (Ready = true)
Feb 24 12:15:50.883: INFO: Pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c" satisfied condition "running and ready"
Feb 24 12:15:51.420: INFO: Successfully updated pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:15:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7689" for this suite. 02/24/23 12:15:55.461
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":310,"skipped":5643,"failed":0}
------------------------------
• [SLOW TEST] [6.640 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:15:48.829
    Feb 24 12:15:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:15:48.83
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:48.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:48.858
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 02/24/23 12:15:48.862
    Feb 24 12:15:48.873: INFO: Waiting up to 5m0s for pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c" in namespace "projected-7689" to be "running and ready"
    Feb 24 12:15:48.878: INFO: Pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812002ms
    Feb 24 12:15:48.878: INFO: The phase of Pod annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:15:50.883: INFO: Pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010214071s
    Feb 24 12:15:50.883: INFO: The phase of Pod annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c is Running (Ready = true)
    Feb 24 12:15:50.883: INFO: Pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c" satisfied condition "running and ready"
    Feb 24 12:15:51.420: INFO: Successfully updated pod "annotationupdated3958a56-eb3b-4a49-b3a2-0005f142263c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:15:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7689" for this suite. 02/24/23 12:15:55.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:15:55.471
Feb 24 12:15:55.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:15:55.472
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:55.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:55.499
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 02/24/23 12:15:55.503
STEP: Getting a ResourceQuota 02/24/23 12:15:55.508
STEP: Updating a ResourceQuota 02/24/23 12:15:55.514
STEP: Verifying a ResourceQuota was modified 02/24/23 12:15:55.52
STEP: Deleting a ResourceQuota 02/24/23 12:15:55.524
STEP: Verifying the deleted ResourceQuota 02/24/23 12:15:55.538
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 12:15:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9572" for this suite. 02/24/23 12:15:55.55
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":311,"skipped":5659,"failed":0}
------------------------------
• [0.089 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:15:55.471
    Feb 24 12:15:55.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:15:55.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:55.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:55.499
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 02/24/23 12:15:55.503
    STEP: Getting a ResourceQuota 02/24/23 12:15:55.508
    STEP: Updating a ResourceQuota 02/24/23 12:15:55.514
    STEP: Verifying a ResourceQuota was modified 02/24/23 12:15:55.52
    STEP: Deleting a ResourceQuota 02/24/23 12:15:55.524
    STEP: Verifying the deleted ResourceQuota 02/24/23 12:15:55.538
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 12:15:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9572" for this suite. 02/24/23 12:15:55.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:15:55.565
Feb 24 12:15:55.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename downward-api 02/24/23 12:15:55.566
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:55.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:55.592
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:15:55.603
Feb 24 12:15:55.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee" in namespace "downward-api-6220" to be "Succeeded or Failed"
Feb 24 12:15:55.618: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489684ms
Feb 24 12:15:57.623: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010946338s
Feb 24 12:15:59.624: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011690711s
STEP: Saw pod success 02/24/23 12:15:59.624
Feb 24 12:15:59.624: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee" satisfied condition "Succeeded or Failed"
Feb 24 12:15:59.628: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee container client-container: <nil>
STEP: delete the pod 02/24/23 12:15:59.643
Feb 24 12:15:59.656: INFO: Waiting for pod downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee to disappear
Feb 24 12:15:59.666: INFO: Pod downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 24 12:15:59.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6220" for this suite. 02/24/23 12:15:59.675
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":312,"skipped":5719,"failed":0}
------------------------------
• [4.118 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:15:55.565
    Feb 24 12:15:55.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename downward-api 02/24/23 12:15:55.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:55.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:55.592
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:15:55.603
    Feb 24 12:15:55.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee" in namespace "downward-api-6220" to be "Succeeded or Failed"
    Feb 24 12:15:55.618: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489684ms
    Feb 24 12:15:57.623: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010946338s
    Feb 24 12:15:59.624: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011690711s
    STEP: Saw pod success 02/24/23 12:15:59.624
    Feb 24 12:15:59.624: INFO: Pod "downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee" satisfied condition "Succeeded or Failed"
    Feb 24 12:15:59.628: INFO: Trying to get logs from node ip-172-31-215-124.eu-west-3.compute.internal pod downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee container client-container: <nil>
    STEP: delete the pod 02/24/23 12:15:59.643
    Feb 24 12:15:59.656: INFO: Waiting for pod downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee to disappear
    Feb 24 12:15:59.666: INFO: Pod downwardapi-volume-a290f822-e82d-42a1-9574-906f22c3caee no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 24 12:15:59.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6220" for this suite. 02/24/23 12:15:59.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:15:59.684
Feb 24 12:15:59.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:15:59.687
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:59.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:59.72
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 02/24/23 12:15:59.726
Feb 24 12:15:59.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1526 create -f -'
Feb 24 12:16:01.061: INFO: stderr: ""
Feb 24 12:16:01.061: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/24/23 12:16:01.061
Feb 24 12:16:01.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1526 diff -f -'
Feb 24 12:16:02.570: INFO: rc: 1
Feb 24 12:16:02.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1526 delete -f -'
Feb 24 12:16:02.659: INFO: stderr: ""
Feb 24 12:16:02.659: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:16:02.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1526" for this suite. 02/24/23 12:16:02.67
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":313,"skipped":5735,"failed":0}
------------------------------
• [3.000 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:15:59.684
    Feb 24 12:15:59.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:15:59.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:15:59.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:15:59.72
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 02/24/23 12:15:59.726
    Feb 24 12:15:59.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1526 create -f -'
    Feb 24 12:16:01.061: INFO: stderr: ""
    Feb 24 12:16:01.061: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/24/23 12:16:01.061
    Feb 24 12:16:01.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1526 diff -f -'
    Feb 24 12:16:02.570: INFO: rc: 1
    Feb 24 12:16:02.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1526 delete -f -'
    Feb 24 12:16:02.659: INFO: stderr: ""
    Feb 24 12:16:02.659: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:16:02.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1526" for this suite. 02/24/23 12:16:02.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:16:02.685
Feb 24 12:16:02.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 12:16:02.692
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:02.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:02.739
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/24/23 12:16:02.756
STEP: delete the rc 02/24/23 12:16:07.778
STEP: wait for the rc to be deleted 02/24/23 12:16:07.787
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/24/23 12:16:12.793
STEP: Gathering metrics 02/24/23 12:16:42.808
Feb 24 12:16:42.840: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 12:16:42.845: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.727693ms
Feb 24 12:16:42.845: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 12:16:42.845: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 12:16:42.985: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 24 12:16:42.985: INFO: Deleting pod "simpletest.rc-2bxjn" in namespace "gc-8454"
Feb 24 12:16:43.002: INFO: Deleting pod "simpletest.rc-2jqs9" in namespace "gc-8454"
Feb 24 12:16:43.016: INFO: Deleting pod "simpletest.rc-2lbvs" in namespace "gc-8454"
Feb 24 12:16:43.046: INFO: Deleting pod "simpletest.rc-2lqs9" in namespace "gc-8454"
Feb 24 12:16:43.062: INFO: Deleting pod "simpletest.rc-2wl6m" in namespace "gc-8454"
Feb 24 12:16:43.077: INFO: Deleting pod "simpletest.rc-2zf7k" in namespace "gc-8454"
Feb 24 12:16:43.093: INFO: Deleting pod "simpletest.rc-48ht4" in namespace "gc-8454"
Feb 24 12:16:43.128: INFO: Deleting pod "simpletest.rc-4gddt" in namespace "gc-8454"
Feb 24 12:16:43.152: INFO: Deleting pod "simpletest.rc-4wjkk" in namespace "gc-8454"
Feb 24 12:16:43.171: INFO: Deleting pod "simpletest.rc-5jwtn" in namespace "gc-8454"
Feb 24 12:16:43.188: INFO: Deleting pod "simpletest.rc-5l4cn" in namespace "gc-8454"
Feb 24 12:16:43.224: INFO: Deleting pod "simpletest.rc-64zx7" in namespace "gc-8454"
Feb 24 12:16:43.274: INFO: Deleting pod "simpletest.rc-6cn4m" in namespace "gc-8454"
Feb 24 12:16:43.290: INFO: Deleting pod "simpletest.rc-6kpp2" in namespace "gc-8454"
Feb 24 12:16:43.313: INFO: Deleting pod "simpletest.rc-7rlv7" in namespace "gc-8454"
Feb 24 12:16:43.338: INFO: Deleting pod "simpletest.rc-878z9" in namespace "gc-8454"
Feb 24 12:16:43.370: INFO: Deleting pod "simpletest.rc-8t4nl" in namespace "gc-8454"
Feb 24 12:16:43.419: INFO: Deleting pod "simpletest.rc-94ldf" in namespace "gc-8454"
Feb 24 12:16:43.457: INFO: Deleting pod "simpletest.rc-99rsw" in namespace "gc-8454"
Feb 24 12:16:43.476: INFO: Deleting pod "simpletest.rc-b8lls" in namespace "gc-8454"
Feb 24 12:16:43.513: INFO: Deleting pod "simpletest.rc-b9hk4" in namespace "gc-8454"
Feb 24 12:16:43.544: INFO: Deleting pod "simpletest.rc-bjjqj" in namespace "gc-8454"
Feb 24 12:16:43.563: INFO: Deleting pod "simpletest.rc-blj56" in namespace "gc-8454"
Feb 24 12:16:43.581: INFO: Deleting pod "simpletest.rc-bp9d8" in namespace "gc-8454"
Feb 24 12:16:43.595: INFO: Deleting pod "simpletest.rc-bscw6" in namespace "gc-8454"
Feb 24 12:16:43.617: INFO: Deleting pod "simpletest.rc-btj5g" in namespace "gc-8454"
Feb 24 12:16:43.647: INFO: Deleting pod "simpletest.rc-bwml7" in namespace "gc-8454"
Feb 24 12:16:43.666: INFO: Deleting pod "simpletest.rc-cpvqw" in namespace "gc-8454"
Feb 24 12:16:43.693: INFO: Deleting pod "simpletest.rc-csmsc" in namespace "gc-8454"
Feb 24 12:16:43.757: INFO: Deleting pod "simpletest.rc-d2m7t" in namespace "gc-8454"
Feb 24 12:16:43.827: INFO: Deleting pod "simpletest.rc-d2z6t" in namespace "gc-8454"
Feb 24 12:16:43.845: INFO: Deleting pod "simpletest.rc-d4m4q" in namespace "gc-8454"
Feb 24 12:16:43.860: INFO: Deleting pod "simpletest.rc-d7k8f" in namespace "gc-8454"
Feb 24 12:16:43.957: INFO: Deleting pod "simpletest.rc-dfzwl" in namespace "gc-8454"
Feb 24 12:16:43.989: INFO: Deleting pod "simpletest.rc-dl5c6" in namespace "gc-8454"
Feb 24 12:16:44.021: INFO: Deleting pod "simpletest.rc-dm4lv" in namespace "gc-8454"
Feb 24 12:16:44.037: INFO: Deleting pod "simpletest.rc-dsv2x" in namespace "gc-8454"
Feb 24 12:16:44.056: INFO: Deleting pod "simpletest.rc-dw6g5" in namespace "gc-8454"
Feb 24 12:16:44.077: INFO: Deleting pod "simpletest.rc-dx2pd" in namespace "gc-8454"
Feb 24 12:16:44.095: INFO: Deleting pod "simpletest.rc-fbq5z" in namespace "gc-8454"
Feb 24 12:16:44.114: INFO: Deleting pod "simpletest.rc-fhnzj" in namespace "gc-8454"
Feb 24 12:16:44.153: INFO: Deleting pod "simpletest.rc-fq52w" in namespace "gc-8454"
Feb 24 12:16:44.165: INFO: Deleting pod "simpletest.rc-frpss" in namespace "gc-8454"
Feb 24 12:16:44.179: INFO: Deleting pod "simpletest.rc-ftphg" in namespace "gc-8454"
Feb 24 12:16:44.192: INFO: Deleting pod "simpletest.rc-ftpjj" in namespace "gc-8454"
Feb 24 12:16:44.212: INFO: Deleting pod "simpletest.rc-gffxz" in namespace "gc-8454"
Feb 24 12:16:44.236: INFO: Deleting pod "simpletest.rc-gh9wg" in namespace "gc-8454"
Feb 24 12:16:44.266: INFO: Deleting pod "simpletest.rc-grssw" in namespace "gc-8454"
Feb 24 12:16:44.279: INFO: Deleting pod "simpletest.rc-h9ln2" in namespace "gc-8454"
Feb 24 12:16:44.294: INFO: Deleting pod "simpletest.rc-hf7hh" in namespace "gc-8454"
Feb 24 12:16:44.422: INFO: Deleting pod "simpletest.rc-hr98b" in namespace "gc-8454"
Feb 24 12:16:44.436: INFO: Deleting pod "simpletest.rc-jnk5l" in namespace "gc-8454"
Feb 24 12:16:44.464: INFO: Deleting pod "simpletest.rc-jz5kd" in namespace "gc-8454"
Feb 24 12:16:44.478: INFO: Deleting pod "simpletest.rc-k8v94" in namespace "gc-8454"
Feb 24 12:16:44.518: INFO: Deleting pod "simpletest.rc-kdkxk" in namespace "gc-8454"
Feb 24 12:16:44.550: INFO: Deleting pod "simpletest.rc-kjxxn" in namespace "gc-8454"
Feb 24 12:16:44.565: INFO: Deleting pod "simpletest.rc-krgz8" in namespace "gc-8454"
Feb 24 12:16:44.591: INFO: Deleting pod "simpletest.rc-kvs59" in namespace "gc-8454"
Feb 24 12:16:44.626: INFO: Deleting pod "simpletest.rc-l42b4" in namespace "gc-8454"
Feb 24 12:16:44.670: INFO: Deleting pod "simpletest.rc-l7bc4" in namespace "gc-8454"
Feb 24 12:16:44.703: INFO: Deleting pod "simpletest.rc-lfv5q" in namespace "gc-8454"
Feb 24 12:16:44.743: INFO: Deleting pod "simpletest.rc-lg9lj" in namespace "gc-8454"
Feb 24 12:16:44.766: INFO: Deleting pod "simpletest.rc-ln889" in namespace "gc-8454"
Feb 24 12:16:44.783: INFO: Deleting pod "simpletest.rc-lsbtl" in namespace "gc-8454"
Feb 24 12:16:44.802: INFO: Deleting pod "simpletest.rc-ltxp7" in namespace "gc-8454"
Feb 24 12:16:44.825: INFO: Deleting pod "simpletest.rc-m65m8" in namespace "gc-8454"
Feb 24 12:16:44.841: INFO: Deleting pod "simpletest.rc-m8pbq" in namespace "gc-8454"
Feb 24 12:16:44.854: INFO: Deleting pod "simpletest.rc-m9dd6" in namespace "gc-8454"
Feb 24 12:16:44.870: INFO: Deleting pod "simpletest.rc-mqldz" in namespace "gc-8454"
Feb 24 12:16:44.882: INFO: Deleting pod "simpletest.rc-nf6hv" in namespace "gc-8454"
Feb 24 12:16:44.894: INFO: Deleting pod "simpletest.rc-nncss" in namespace "gc-8454"
Feb 24 12:16:44.910: INFO: Deleting pod "simpletest.rc-nx9qq" in namespace "gc-8454"
Feb 24 12:16:44.926: INFO: Deleting pod "simpletest.rc-p7ggr" in namespace "gc-8454"
Feb 24 12:16:44.955: INFO: Deleting pod "simpletest.rc-qfsmw" in namespace "gc-8454"
Feb 24 12:16:44.983: INFO: Deleting pod "simpletest.rc-qksvd" in namespace "gc-8454"
Feb 24 12:16:45.005: INFO: Deleting pod "simpletest.rc-qlkmp" in namespace "gc-8454"
Feb 24 12:16:45.035: INFO: Deleting pod "simpletest.rc-qplmh" in namespace "gc-8454"
Feb 24 12:16:45.058: INFO: Deleting pod "simpletest.rc-qqhql" in namespace "gc-8454"
Feb 24 12:16:45.131: INFO: Deleting pod "simpletest.rc-qqtcd" in namespace "gc-8454"
Feb 24 12:16:45.147: INFO: Deleting pod "simpletest.rc-qr2fl" in namespace "gc-8454"
Feb 24 12:16:45.175: INFO: Deleting pod "simpletest.rc-qx5bs" in namespace "gc-8454"
Feb 24 12:16:45.207: INFO: Deleting pod "simpletest.rc-rhz99" in namespace "gc-8454"
Feb 24 12:16:45.251: INFO: Deleting pod "simpletest.rc-rj28h" in namespace "gc-8454"
Feb 24 12:16:45.290: INFO: Deleting pod "simpletest.rc-t4dgk" in namespace "gc-8454"
Feb 24 12:16:45.316: INFO: Deleting pod "simpletest.rc-t56c8" in namespace "gc-8454"
Feb 24 12:16:45.336: INFO: Deleting pod "simpletest.rc-tj58t" in namespace "gc-8454"
Feb 24 12:16:45.355: INFO: Deleting pod "simpletest.rc-tnvn4" in namespace "gc-8454"
Feb 24 12:16:45.381: INFO: Deleting pod "simpletest.rc-tqv9t" in namespace "gc-8454"
Feb 24 12:16:45.400: INFO: Deleting pod "simpletest.rc-vn95w" in namespace "gc-8454"
Feb 24 12:16:45.415: INFO: Deleting pod "simpletest.rc-w7ctp" in namespace "gc-8454"
Feb 24 12:16:45.439: INFO: Deleting pod "simpletest.rc-w8lhv" in namespace "gc-8454"
Feb 24 12:16:45.461: INFO: Deleting pod "simpletest.rc-wfbr7" in namespace "gc-8454"
Feb 24 12:16:45.484: INFO: Deleting pod "simpletest.rc-x5vck" in namespace "gc-8454"
Feb 24 12:16:45.504: INFO: Deleting pod "simpletest.rc-xgdwd" in namespace "gc-8454"
Feb 24 12:16:45.518: INFO: Deleting pod "simpletest.rc-xzd6f" in namespace "gc-8454"
Feb 24 12:16:45.543: INFO: Deleting pod "simpletest.rc-zj2rl" in namespace "gc-8454"
Feb 24 12:16:45.564: INFO: Deleting pod "simpletest.rc-ztflg" in namespace "gc-8454"
Feb 24 12:16:45.588: INFO: Deleting pod "simpletest.rc-zxg57" in namespace "gc-8454"
Feb 24 12:16:45.607: INFO: Deleting pod "simpletest.rc-zz4wp" in namespace "gc-8454"
Feb 24 12:16:45.639: INFO: Deleting pod "simpletest.rc-zzp64" in namespace "gc-8454"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 12:16:45.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8454" for this suite. 02/24/23 12:16:45.683
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":314,"skipped":5742,"failed":0}
------------------------------
• [SLOW TEST] [43.015 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:16:02.685
    Feb 24 12:16:02.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 12:16:02.692
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:02.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:02.739
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/24/23 12:16:02.756
    STEP: delete the rc 02/24/23 12:16:07.778
    STEP: wait for the rc to be deleted 02/24/23 12:16:07.787
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/24/23 12:16:12.793
    STEP: Gathering metrics 02/24/23 12:16:42.808
    Feb 24 12:16:42.840: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 12:16:42.845: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.727693ms
    Feb 24 12:16:42.845: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 12:16:42.845: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 12:16:42.985: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 24 12:16:42.985: INFO: Deleting pod "simpletest.rc-2bxjn" in namespace "gc-8454"
    Feb 24 12:16:43.002: INFO: Deleting pod "simpletest.rc-2jqs9" in namespace "gc-8454"
    Feb 24 12:16:43.016: INFO: Deleting pod "simpletest.rc-2lbvs" in namespace "gc-8454"
    Feb 24 12:16:43.046: INFO: Deleting pod "simpletest.rc-2lqs9" in namespace "gc-8454"
    Feb 24 12:16:43.062: INFO: Deleting pod "simpletest.rc-2wl6m" in namespace "gc-8454"
    Feb 24 12:16:43.077: INFO: Deleting pod "simpletest.rc-2zf7k" in namespace "gc-8454"
    Feb 24 12:16:43.093: INFO: Deleting pod "simpletest.rc-48ht4" in namespace "gc-8454"
    Feb 24 12:16:43.128: INFO: Deleting pod "simpletest.rc-4gddt" in namespace "gc-8454"
    Feb 24 12:16:43.152: INFO: Deleting pod "simpletest.rc-4wjkk" in namespace "gc-8454"
    Feb 24 12:16:43.171: INFO: Deleting pod "simpletest.rc-5jwtn" in namespace "gc-8454"
    Feb 24 12:16:43.188: INFO: Deleting pod "simpletest.rc-5l4cn" in namespace "gc-8454"
    Feb 24 12:16:43.224: INFO: Deleting pod "simpletest.rc-64zx7" in namespace "gc-8454"
    Feb 24 12:16:43.274: INFO: Deleting pod "simpletest.rc-6cn4m" in namespace "gc-8454"
    Feb 24 12:16:43.290: INFO: Deleting pod "simpletest.rc-6kpp2" in namespace "gc-8454"
    Feb 24 12:16:43.313: INFO: Deleting pod "simpletest.rc-7rlv7" in namespace "gc-8454"
    Feb 24 12:16:43.338: INFO: Deleting pod "simpletest.rc-878z9" in namespace "gc-8454"
    Feb 24 12:16:43.370: INFO: Deleting pod "simpletest.rc-8t4nl" in namespace "gc-8454"
    Feb 24 12:16:43.419: INFO: Deleting pod "simpletest.rc-94ldf" in namespace "gc-8454"
    Feb 24 12:16:43.457: INFO: Deleting pod "simpletest.rc-99rsw" in namespace "gc-8454"
    Feb 24 12:16:43.476: INFO: Deleting pod "simpletest.rc-b8lls" in namespace "gc-8454"
    Feb 24 12:16:43.513: INFO: Deleting pod "simpletest.rc-b9hk4" in namespace "gc-8454"
    Feb 24 12:16:43.544: INFO: Deleting pod "simpletest.rc-bjjqj" in namespace "gc-8454"
    Feb 24 12:16:43.563: INFO: Deleting pod "simpletest.rc-blj56" in namespace "gc-8454"
    Feb 24 12:16:43.581: INFO: Deleting pod "simpletest.rc-bp9d8" in namespace "gc-8454"
    Feb 24 12:16:43.595: INFO: Deleting pod "simpletest.rc-bscw6" in namespace "gc-8454"
    Feb 24 12:16:43.617: INFO: Deleting pod "simpletest.rc-btj5g" in namespace "gc-8454"
    Feb 24 12:16:43.647: INFO: Deleting pod "simpletest.rc-bwml7" in namespace "gc-8454"
    Feb 24 12:16:43.666: INFO: Deleting pod "simpletest.rc-cpvqw" in namespace "gc-8454"
    Feb 24 12:16:43.693: INFO: Deleting pod "simpletest.rc-csmsc" in namespace "gc-8454"
    Feb 24 12:16:43.757: INFO: Deleting pod "simpletest.rc-d2m7t" in namespace "gc-8454"
    Feb 24 12:16:43.827: INFO: Deleting pod "simpletest.rc-d2z6t" in namespace "gc-8454"
    Feb 24 12:16:43.845: INFO: Deleting pod "simpletest.rc-d4m4q" in namespace "gc-8454"
    Feb 24 12:16:43.860: INFO: Deleting pod "simpletest.rc-d7k8f" in namespace "gc-8454"
    Feb 24 12:16:43.957: INFO: Deleting pod "simpletest.rc-dfzwl" in namespace "gc-8454"
    Feb 24 12:16:43.989: INFO: Deleting pod "simpletest.rc-dl5c6" in namespace "gc-8454"
    Feb 24 12:16:44.021: INFO: Deleting pod "simpletest.rc-dm4lv" in namespace "gc-8454"
    Feb 24 12:16:44.037: INFO: Deleting pod "simpletest.rc-dsv2x" in namespace "gc-8454"
    Feb 24 12:16:44.056: INFO: Deleting pod "simpletest.rc-dw6g5" in namespace "gc-8454"
    Feb 24 12:16:44.077: INFO: Deleting pod "simpletest.rc-dx2pd" in namespace "gc-8454"
    Feb 24 12:16:44.095: INFO: Deleting pod "simpletest.rc-fbq5z" in namespace "gc-8454"
    Feb 24 12:16:44.114: INFO: Deleting pod "simpletest.rc-fhnzj" in namespace "gc-8454"
    Feb 24 12:16:44.153: INFO: Deleting pod "simpletest.rc-fq52w" in namespace "gc-8454"
    Feb 24 12:16:44.165: INFO: Deleting pod "simpletest.rc-frpss" in namespace "gc-8454"
    Feb 24 12:16:44.179: INFO: Deleting pod "simpletest.rc-ftphg" in namespace "gc-8454"
    Feb 24 12:16:44.192: INFO: Deleting pod "simpletest.rc-ftpjj" in namespace "gc-8454"
    Feb 24 12:16:44.212: INFO: Deleting pod "simpletest.rc-gffxz" in namespace "gc-8454"
    Feb 24 12:16:44.236: INFO: Deleting pod "simpletest.rc-gh9wg" in namespace "gc-8454"
    Feb 24 12:16:44.266: INFO: Deleting pod "simpletest.rc-grssw" in namespace "gc-8454"
    Feb 24 12:16:44.279: INFO: Deleting pod "simpletest.rc-h9ln2" in namespace "gc-8454"
    Feb 24 12:16:44.294: INFO: Deleting pod "simpletest.rc-hf7hh" in namespace "gc-8454"
    Feb 24 12:16:44.422: INFO: Deleting pod "simpletest.rc-hr98b" in namespace "gc-8454"
    Feb 24 12:16:44.436: INFO: Deleting pod "simpletest.rc-jnk5l" in namespace "gc-8454"
    Feb 24 12:16:44.464: INFO: Deleting pod "simpletest.rc-jz5kd" in namespace "gc-8454"
    Feb 24 12:16:44.478: INFO: Deleting pod "simpletest.rc-k8v94" in namespace "gc-8454"
    Feb 24 12:16:44.518: INFO: Deleting pod "simpletest.rc-kdkxk" in namespace "gc-8454"
    Feb 24 12:16:44.550: INFO: Deleting pod "simpletest.rc-kjxxn" in namespace "gc-8454"
    Feb 24 12:16:44.565: INFO: Deleting pod "simpletest.rc-krgz8" in namespace "gc-8454"
    Feb 24 12:16:44.591: INFO: Deleting pod "simpletest.rc-kvs59" in namespace "gc-8454"
    Feb 24 12:16:44.626: INFO: Deleting pod "simpletest.rc-l42b4" in namespace "gc-8454"
    Feb 24 12:16:44.670: INFO: Deleting pod "simpletest.rc-l7bc4" in namespace "gc-8454"
    Feb 24 12:16:44.703: INFO: Deleting pod "simpletest.rc-lfv5q" in namespace "gc-8454"
    Feb 24 12:16:44.743: INFO: Deleting pod "simpletest.rc-lg9lj" in namespace "gc-8454"
    Feb 24 12:16:44.766: INFO: Deleting pod "simpletest.rc-ln889" in namespace "gc-8454"
    Feb 24 12:16:44.783: INFO: Deleting pod "simpletest.rc-lsbtl" in namespace "gc-8454"
    Feb 24 12:16:44.802: INFO: Deleting pod "simpletest.rc-ltxp7" in namespace "gc-8454"
    Feb 24 12:16:44.825: INFO: Deleting pod "simpletest.rc-m65m8" in namespace "gc-8454"
    Feb 24 12:16:44.841: INFO: Deleting pod "simpletest.rc-m8pbq" in namespace "gc-8454"
    Feb 24 12:16:44.854: INFO: Deleting pod "simpletest.rc-m9dd6" in namespace "gc-8454"
    Feb 24 12:16:44.870: INFO: Deleting pod "simpletest.rc-mqldz" in namespace "gc-8454"
    Feb 24 12:16:44.882: INFO: Deleting pod "simpletest.rc-nf6hv" in namespace "gc-8454"
    Feb 24 12:16:44.894: INFO: Deleting pod "simpletest.rc-nncss" in namespace "gc-8454"
    Feb 24 12:16:44.910: INFO: Deleting pod "simpletest.rc-nx9qq" in namespace "gc-8454"
    Feb 24 12:16:44.926: INFO: Deleting pod "simpletest.rc-p7ggr" in namespace "gc-8454"
    Feb 24 12:16:44.955: INFO: Deleting pod "simpletest.rc-qfsmw" in namespace "gc-8454"
    Feb 24 12:16:44.983: INFO: Deleting pod "simpletest.rc-qksvd" in namespace "gc-8454"
    Feb 24 12:16:45.005: INFO: Deleting pod "simpletest.rc-qlkmp" in namespace "gc-8454"
    Feb 24 12:16:45.035: INFO: Deleting pod "simpletest.rc-qplmh" in namespace "gc-8454"
    Feb 24 12:16:45.058: INFO: Deleting pod "simpletest.rc-qqhql" in namespace "gc-8454"
    Feb 24 12:16:45.131: INFO: Deleting pod "simpletest.rc-qqtcd" in namespace "gc-8454"
    Feb 24 12:16:45.147: INFO: Deleting pod "simpletest.rc-qr2fl" in namespace "gc-8454"
    Feb 24 12:16:45.175: INFO: Deleting pod "simpletest.rc-qx5bs" in namespace "gc-8454"
    Feb 24 12:16:45.207: INFO: Deleting pod "simpletest.rc-rhz99" in namespace "gc-8454"
    Feb 24 12:16:45.251: INFO: Deleting pod "simpletest.rc-rj28h" in namespace "gc-8454"
    Feb 24 12:16:45.290: INFO: Deleting pod "simpletest.rc-t4dgk" in namespace "gc-8454"
    Feb 24 12:16:45.316: INFO: Deleting pod "simpletest.rc-t56c8" in namespace "gc-8454"
    Feb 24 12:16:45.336: INFO: Deleting pod "simpletest.rc-tj58t" in namespace "gc-8454"
    Feb 24 12:16:45.355: INFO: Deleting pod "simpletest.rc-tnvn4" in namespace "gc-8454"
    Feb 24 12:16:45.381: INFO: Deleting pod "simpletest.rc-tqv9t" in namespace "gc-8454"
    Feb 24 12:16:45.400: INFO: Deleting pod "simpletest.rc-vn95w" in namespace "gc-8454"
    Feb 24 12:16:45.415: INFO: Deleting pod "simpletest.rc-w7ctp" in namespace "gc-8454"
    Feb 24 12:16:45.439: INFO: Deleting pod "simpletest.rc-w8lhv" in namespace "gc-8454"
    Feb 24 12:16:45.461: INFO: Deleting pod "simpletest.rc-wfbr7" in namespace "gc-8454"
    Feb 24 12:16:45.484: INFO: Deleting pod "simpletest.rc-x5vck" in namespace "gc-8454"
    Feb 24 12:16:45.504: INFO: Deleting pod "simpletest.rc-xgdwd" in namespace "gc-8454"
    Feb 24 12:16:45.518: INFO: Deleting pod "simpletest.rc-xzd6f" in namespace "gc-8454"
    Feb 24 12:16:45.543: INFO: Deleting pod "simpletest.rc-zj2rl" in namespace "gc-8454"
    Feb 24 12:16:45.564: INFO: Deleting pod "simpletest.rc-ztflg" in namespace "gc-8454"
    Feb 24 12:16:45.588: INFO: Deleting pod "simpletest.rc-zxg57" in namespace "gc-8454"
    Feb 24 12:16:45.607: INFO: Deleting pod "simpletest.rc-zz4wp" in namespace "gc-8454"
    Feb 24 12:16:45.639: INFO: Deleting pod "simpletest.rc-zzp64" in namespace "gc-8454"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 12:16:45.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8454" for this suite. 02/24/23 12:16:45.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:16:45.724
Feb 24 12:16:45.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:16:45.726
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:45.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:45.758
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 02/24/23 12:16:45.762
Feb 24 12:16:45.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 create -f -'
Feb 24 12:16:46.760: INFO: stderr: ""
Feb 24 12:16:46.760: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 12:16:46.76
Feb 24 12:16:46.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 12:16:46.960: INFO: stderr: ""
Feb 24 12:16:46.960: INFO: stdout: "update-demo-nautilus-6mpxc update-demo-nautilus-n67gd "
Feb 24 12:16:46.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 12:16:47.205: INFO: stderr: ""
Feb 24 12:16:47.205: INFO: stdout: ""
Feb 24 12:16:47.205: INFO: update-demo-nautilus-6mpxc is created but not running
Feb 24 12:16:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 12:16:52.298: INFO: stderr: ""
Feb 24 12:16:52.298: INFO: stdout: "update-demo-nautilus-6mpxc update-demo-nautilus-n67gd "
Feb 24 12:16:52.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 12:16:52.376: INFO: stderr: ""
Feb 24 12:16:52.376: INFO: stdout: ""
Feb 24 12:16:52.376: INFO: update-demo-nautilus-6mpxc is created but not running
Feb 24 12:16:57.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 24 12:16:57.452: INFO: stderr: ""
Feb 24 12:16:57.452: INFO: stdout: "update-demo-nautilus-6mpxc update-demo-nautilus-n67gd "
Feb 24 12:16:57.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 12:16:57.521: INFO: stderr: ""
Feb 24 12:16:57.521: INFO: stdout: "true"
Feb 24 12:16:57.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 12:16:57.597: INFO: stderr: ""
Feb 24 12:16:57.597: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 12:16:57.597: INFO: validating pod update-demo-nautilus-6mpxc
Feb 24 12:16:57.603: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 12:16:57.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 12:16:57.603: INFO: update-demo-nautilus-6mpxc is verified up and running
Feb 24 12:16:57.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-n67gd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 24 12:16:57.675: INFO: stderr: ""
Feb 24 12:16:57.675: INFO: stdout: "true"
Feb 24 12:16:57.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-n67gd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 24 12:16:57.749: INFO: stderr: ""
Feb 24 12:16:57.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 24 12:16:57.749: INFO: validating pod update-demo-nautilus-n67gd
Feb 24 12:16:57.758: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 24 12:16:57.758: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 24 12:16:57.758: INFO: update-demo-nautilus-n67gd is verified up and running
STEP: using delete to clean up resources 02/24/23 12:16:57.758
Feb 24 12:16:57.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 delete --grace-period=0 --force -f -'
Feb 24 12:16:57.846: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 24 12:16:57.846: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 24 12:16:57.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get rc,svc -l name=update-demo --no-headers'
Feb 24 12:16:57.976: INFO: stderr: "No resources found in kubectl-879 namespace.\n"
Feb 24 12:16:57.976: INFO: stdout: ""
Feb 24 12:16:57.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 24 12:16:58.078: INFO: stderr: ""
Feb 24 12:16:58.078: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:16:58.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-879" for this suite. 02/24/23 12:16:58.087
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":315,"skipped":5766,"failed":0}
------------------------------
• [SLOW TEST] [12.375 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:16:45.724
    Feb 24 12:16:45.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:16:45.726
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:45.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:45.758
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 02/24/23 12:16:45.762
    Feb 24 12:16:45.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 create -f -'
    Feb 24 12:16:46.760: INFO: stderr: ""
    Feb 24 12:16:46.760: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/24/23 12:16:46.76
    Feb 24 12:16:46.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 12:16:46.960: INFO: stderr: ""
    Feb 24 12:16:46.960: INFO: stdout: "update-demo-nautilus-6mpxc update-demo-nautilus-n67gd "
    Feb 24 12:16:46.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 12:16:47.205: INFO: stderr: ""
    Feb 24 12:16:47.205: INFO: stdout: ""
    Feb 24 12:16:47.205: INFO: update-demo-nautilus-6mpxc is created but not running
    Feb 24 12:16:52.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 12:16:52.298: INFO: stderr: ""
    Feb 24 12:16:52.298: INFO: stdout: "update-demo-nautilus-6mpxc update-demo-nautilus-n67gd "
    Feb 24 12:16:52.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 12:16:52.376: INFO: stderr: ""
    Feb 24 12:16:52.376: INFO: stdout: ""
    Feb 24 12:16:52.376: INFO: update-demo-nautilus-6mpxc is created but not running
    Feb 24 12:16:57.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 24 12:16:57.452: INFO: stderr: ""
    Feb 24 12:16:57.452: INFO: stdout: "update-demo-nautilus-6mpxc update-demo-nautilus-n67gd "
    Feb 24 12:16:57.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 12:16:57.521: INFO: stderr: ""
    Feb 24 12:16:57.521: INFO: stdout: "true"
    Feb 24 12:16:57.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-6mpxc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 12:16:57.597: INFO: stderr: ""
    Feb 24 12:16:57.597: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 12:16:57.597: INFO: validating pod update-demo-nautilus-6mpxc
    Feb 24 12:16:57.603: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 12:16:57.603: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 12:16:57.603: INFO: update-demo-nautilus-6mpxc is verified up and running
    Feb 24 12:16:57.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-n67gd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 24 12:16:57.675: INFO: stderr: ""
    Feb 24 12:16:57.675: INFO: stdout: "true"
    Feb 24 12:16:57.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods update-demo-nautilus-n67gd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 24 12:16:57.749: INFO: stderr: ""
    Feb 24 12:16:57.749: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 24 12:16:57.749: INFO: validating pod update-demo-nautilus-n67gd
    Feb 24 12:16:57.758: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 24 12:16:57.758: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 24 12:16:57.758: INFO: update-demo-nautilus-n67gd is verified up and running
    STEP: using delete to clean up resources 02/24/23 12:16:57.758
    Feb 24 12:16:57.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 delete --grace-period=0 --force -f -'
    Feb 24 12:16:57.846: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 24 12:16:57.846: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 24 12:16:57.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get rc,svc -l name=update-demo --no-headers'
    Feb 24 12:16:57.976: INFO: stderr: "No resources found in kubectl-879 namespace.\n"
    Feb 24 12:16:57.976: INFO: stdout: ""
    Feb 24 12:16:57.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-879 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 24 12:16:58.078: INFO: stderr: ""
    Feb 24 12:16:58.078: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:16:58.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-879" for this suite. 02/24/23 12:16:58.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:16:58.101
Feb 24 12:16:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:16:58.101
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:58.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:58.194
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:16:58.218
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:16:58.711
STEP: Deploying the webhook pod 02/24/23 12:16:58.72
STEP: Wait for the deployment to be ready 02/24/23 12:16:58.736
Feb 24 12:16:58.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:17:00.768
STEP: Verifying the service has paired with the endpoint 02/24/23 12:17:00.796
Feb 24 12:17:01.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 02/24/23 12:17:01.925
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:17:01.987
STEP: Deleting the collection of validation webhooks 02/24/23 12:17:02.056
STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:17:02.143
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:17:02.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4562" for this suite. 02/24/23 12:17:02.179
STEP: Destroying namespace "webhook-4562-markers" for this suite. 02/24/23 12:17:02.196
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":316,"skipped":5798,"failed":0}
------------------------------
• [4.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:16:58.101
    Feb 24 12:16:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:16:58.101
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:16:58.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:16:58.194
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:16:58.218
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:16:58.711
    STEP: Deploying the webhook pod 02/24/23 12:16:58.72
    STEP: Wait for the deployment to be ready 02/24/23 12:16:58.736
    Feb 24 12:16:58.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:17:00.768
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:17:00.796
    Feb 24 12:17:01.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 02/24/23 12:17:01.925
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:17:01.987
    STEP: Deleting the collection of validation webhooks 02/24/23 12:17:02.056
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/24/23 12:17:02.143
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:17:02.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4562" for this suite. 02/24/23 12:17:02.179
    STEP: Destroying namespace "webhook-4562-markers" for this suite. 02/24/23 12:17:02.196
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:17:02.361
Feb 24 12:17:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename container-probe 02/24/23 12:17:02.363
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:02.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:02.427
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 in namespace container-probe-8031 02/24/23 12:17:02.434
Feb 24 12:17:02.466: INFO: Waiting up to 5m0s for pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0" in namespace "container-probe-8031" to be "not pending"
Feb 24 12:17:02.494: INFO: Pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.740252ms
Feb 24 12:17:04.499: INFO: Pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.032452958s
Feb 24 12:17:04.499: INFO: Pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0" satisfied condition "not pending"
Feb 24 12:17:04.499: INFO: Started pod busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 in namespace container-probe-8031
STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:17:04.499
Feb 24 12:17:04.503: INFO: Initial restart count of pod busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 is 0
Feb 24 12:17:54.658: INFO: Restart count of pod container-probe-8031/busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 is now 1 (50.155020898s elapsed)
STEP: deleting the pod 02/24/23 12:17:54.658
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 24 12:17:54.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8031" for this suite. 02/24/23 12:17:54.694
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":317,"skipped":5843,"failed":0}
------------------------------
• [SLOW TEST] [52.343 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:17:02.361
    Feb 24 12:17:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename container-probe 02/24/23 12:17:02.363
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:02.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:02.427
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 in namespace container-probe-8031 02/24/23 12:17:02.434
    Feb 24 12:17:02.466: INFO: Waiting up to 5m0s for pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0" in namespace "container-probe-8031" to be "not pending"
    Feb 24 12:17:02.494: INFO: Pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.740252ms
    Feb 24 12:17:04.499: INFO: Pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0": Phase="Running", Reason="", readiness=true. Elapsed: 2.032452958s
    Feb 24 12:17:04.499: INFO: Pod "busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0" satisfied condition "not pending"
    Feb 24 12:17:04.499: INFO: Started pod busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 in namespace container-probe-8031
    STEP: checking the pod's current state and verifying that restartCount is present 02/24/23 12:17:04.499
    Feb 24 12:17:04.503: INFO: Initial restart count of pod busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 is 0
    Feb 24 12:17:54.658: INFO: Restart count of pod container-probe-8031/busybox-a6e0fe2b-339d-480e-873e-21d524faf4a0 is now 1 (50.155020898s elapsed)
    STEP: deleting the pod 02/24/23 12:17:54.658
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 24 12:17:54.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8031" for this suite. 02/24/23 12:17:54.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:17:54.712
Feb 24 12:17:54.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename statefulset 02/24/23 12:17:54.712
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:54.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:54.747
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2228 02/24/23 12:17:54.754
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Feb 24 12:17:54.882: INFO: Found 0 stateful pods, waiting for 1
Feb 24 12:18:04.888: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/24/23 12:18:04.896
W0224 12:18:04.905359      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 24 12:18:04.914: INFO: Found 1 stateful pods, waiting for 2
Feb 24 12:18:14.920: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 24 12:18:14.920: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/24/23 12:18:14.931
STEP: Delete all of the StatefulSets 02/24/23 12:18:14.935
STEP: Verify that StatefulSets have been deleted 02/24/23 12:18:14.944
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 24 12:18:14.948: INFO: Deleting all statefulset in ns statefulset-2228
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 24 12:18:14.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2228" for this suite. 02/24/23 12:18:14.968
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":318,"skipped":5932,"failed":0}
------------------------------
• [SLOW TEST] [20.271 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:17:54.712
    Feb 24 12:17:54.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename statefulset 02/24/23 12:17:54.712
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:17:54.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:17:54.747
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2228 02/24/23 12:17:54.754
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Feb 24 12:17:54.882: INFO: Found 0 stateful pods, waiting for 1
    Feb 24 12:18:04.888: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/24/23 12:18:04.896
    W0224 12:18:04.905359      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 24 12:18:04.914: INFO: Found 1 stateful pods, waiting for 2
    Feb 24 12:18:14.920: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 24 12:18:14.920: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/24/23 12:18:14.931
    STEP: Delete all of the StatefulSets 02/24/23 12:18:14.935
    STEP: Verify that StatefulSets have been deleted 02/24/23 12:18:14.944
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 24 12:18:14.948: INFO: Deleting all statefulset in ns statefulset-2228
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 24 12:18:14.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2228" for this suite. 02/24/23 12:18:14.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:14.985
Feb 24 12:18:14.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:18:14.986
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:15.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:15.061
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-f641a90e-6152-4aff-9030-e2b1f95f01d2 02/24/23 12:18:15.066
STEP: Creating a pod to test consume configMaps 02/24/23 12:18:15.082
Feb 24 12:18:15.099: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377" in namespace "projected-132" to be "Succeeded or Failed"
Feb 24 12:18:15.111: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377": Phase="Pending", Reason="", readiness=false. Elapsed: 11.655326ms
Feb 24 12:18:17.117: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01777056s
Feb 24 12:18:19.117: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017811307s
STEP: Saw pod success 02/24/23 12:18:19.117
Feb 24 12:18:19.118: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377" satisfied condition "Succeeded or Failed"
Feb 24 12:18:19.122: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377 container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/24/23 12:18:19.138
Feb 24 12:18:19.152: INFO: Waiting for pod pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377 to disappear
Feb 24 12:18:19.156: INFO: Pod pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 12:18:19.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-132" for this suite. 02/24/23 12:18:19.165
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":319,"skipped":5941,"failed":0}
------------------------------
• [4.189 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:14.985
    Feb 24 12:18:14.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:18:14.986
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:15.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:15.061
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-f641a90e-6152-4aff-9030-e2b1f95f01d2 02/24/23 12:18:15.066
    STEP: Creating a pod to test consume configMaps 02/24/23 12:18:15.082
    Feb 24 12:18:15.099: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377" in namespace "projected-132" to be "Succeeded or Failed"
    Feb 24 12:18:15.111: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377": Phase="Pending", Reason="", readiness=false. Elapsed: 11.655326ms
    Feb 24 12:18:17.117: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01777056s
    Feb 24 12:18:19.117: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017811307s
    STEP: Saw pod success 02/24/23 12:18:19.117
    Feb 24 12:18:19.118: INFO: Pod "pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377" satisfied condition "Succeeded or Failed"
    Feb 24 12:18:19.122: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:18:19.138
    Feb 24 12:18:19.152: INFO: Waiting for pod pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377 to disappear
    Feb 24 12:18:19.156: INFO: Pod pod-projected-configmaps-f95be90d-40e7-44bc-a423-b173b0ba8377 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 12:18:19.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-132" for this suite. 02/24/23 12:18:19.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:19.184
Feb 24 12:18:19.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:18:19.185
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.235
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Feb 24 12:18:19.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2508 version'
Feb 24 12:18:19.308: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 24 12:18:19.308: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:18:19.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2508" for this suite. 02/24/23 12:18:19.317
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":320,"skipped":6004,"failed":0}
------------------------------
• [0.140 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:19.184
    Feb 24 12:18:19.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:18:19.185
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.235
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Feb 24 12:18:19.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-2508 version'
    Feb 24 12:18:19.308: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 24 12:18:19.308: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:18:19.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2508" for this suite. 02/24/23 12:18:19.317
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:19.324
Feb 24 12:18:19.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename events 02/24/23 12:18:19.326
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.355
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/24/23 12:18:19.362
STEP: listing events in all namespaces 02/24/23 12:18:19.372
STEP: listing events in test namespace 02/24/23 12:18:19.377
STEP: listing events with field selection filtering on source 02/24/23 12:18:19.381
STEP: listing events with field selection filtering on reportingController 02/24/23 12:18:19.386
STEP: getting the test event 02/24/23 12:18:19.39
STEP: patching the test event 02/24/23 12:18:19.395
STEP: getting the test event 02/24/23 12:18:19.406
STEP: updating the test event 02/24/23 12:18:19.41
STEP: getting the test event 02/24/23 12:18:19.42
STEP: deleting the test event 02/24/23 12:18:19.425
STEP: listing events in all namespaces 02/24/23 12:18:19.436
STEP: listing events in test namespace 02/24/23 12:18:19.441
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 24 12:18:19.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1116" for this suite. 02/24/23 12:18:19.451
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":321,"skipped":6005,"failed":0}
------------------------------
• [0.134 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:19.324
    Feb 24 12:18:19.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename events 02/24/23 12:18:19.326
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.355
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/24/23 12:18:19.362
    STEP: listing events in all namespaces 02/24/23 12:18:19.372
    STEP: listing events in test namespace 02/24/23 12:18:19.377
    STEP: listing events with field selection filtering on source 02/24/23 12:18:19.381
    STEP: listing events with field selection filtering on reportingController 02/24/23 12:18:19.386
    STEP: getting the test event 02/24/23 12:18:19.39
    STEP: patching the test event 02/24/23 12:18:19.395
    STEP: getting the test event 02/24/23 12:18:19.406
    STEP: updating the test event 02/24/23 12:18:19.41
    STEP: getting the test event 02/24/23 12:18:19.42
    STEP: deleting the test event 02/24/23 12:18:19.425
    STEP: listing events in all namespaces 02/24/23 12:18:19.436
    STEP: listing events in test namespace 02/24/23 12:18:19.441
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 24 12:18:19.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1116" for this suite. 02/24/23 12:18:19.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:19.462
Feb 24 12:18:19.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:18:19.463
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.491
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/24/23 12:18:19.495
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/24/23 12:18:19.497
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 12:18:19.497
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/24/23 12:18:19.497
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/24/23 12:18:19.499
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 12:18:19.499
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 12:18:19.5
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:18:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5634" for this suite. 02/24/23 12:18:19.506
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":322,"skipped":6018,"failed":0}
------------------------------
• [0.052 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:19.462
    Feb 24 12:18:19.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename custom-resource-definition 02/24/23 12:18:19.463
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.491
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/24/23 12:18:19.495
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/24/23 12:18:19.497
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/24/23 12:18:19.497
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/24/23 12:18:19.497
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/24/23 12:18:19.499
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 12:18:19.499
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/24/23 12:18:19.5
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:18:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5634" for this suite. 02/24/23 12:18:19.506
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:19.515
Feb 24 12:18:19.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename disruption 02/24/23 12:18:19.516
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.55
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 02/24/23 12:18:19.562
STEP: Updating PodDisruptionBudget status 02/24/23 12:18:21.572
STEP: Waiting for all pods to be running 02/24/23 12:18:21.582
Feb 24 12:18:21.589: INFO: running pods: 0 < 1
STEP: locating a running pod 02/24/23 12:18:23.594
STEP: Waiting for the pdb to be processed 02/24/23 12:18:23.608
STEP: Patching PodDisruptionBudget status 02/24/23 12:18:23.617
STEP: Waiting for the pdb to be processed 02/24/23 12:18:23.631
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 24 12:18:23.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9648" for this suite. 02/24/23 12:18:23.644
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":323,"skipped":6021,"failed":0}
------------------------------
• [4.137 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:19.515
    Feb 24 12:18:19.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename disruption 02/24/23 12:18:19.516
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:19.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:19.55
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 02/24/23 12:18:19.562
    STEP: Updating PodDisruptionBudget status 02/24/23 12:18:21.572
    STEP: Waiting for all pods to be running 02/24/23 12:18:21.582
    Feb 24 12:18:21.589: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/24/23 12:18:23.594
    STEP: Waiting for the pdb to be processed 02/24/23 12:18:23.608
    STEP: Patching PodDisruptionBudget status 02/24/23 12:18:23.617
    STEP: Waiting for the pdb to be processed 02/24/23 12:18:23.631
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 24 12:18:23.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9648" for this suite. 02/24/23 12:18:23.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:23.654
Feb 24 12:18:23.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:18:23.656
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:23.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:23.685
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 24 12:18:23.698: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab" in namespace "kubelet-test-606" to be "running and ready"
Feb 24 12:18:23.705: INFO: Pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.648504ms
Feb 24 12:18:23.705: INFO: The phase of Pod busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:18:25.711: INFO: Pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab": Phase="Running", Reason="", readiness=true. Elapsed: 2.012805459s
Feb 24 12:18:25.711: INFO: The phase of Pod busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab is Running (Ready = true)
Feb 24 12:18:25.711: INFO: Pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 24 12:18:25.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-606" for this suite. 02/24/23 12:18:25.735
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":324,"skipped":6032,"failed":0}
------------------------------
• [2.093 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:23.654
    Feb 24 12:18:23.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubelet-test 02/24/23 12:18:23.656
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:23.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:23.685
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 24 12:18:23.698: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab" in namespace "kubelet-test-606" to be "running and ready"
    Feb 24 12:18:23.705: INFO: Pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.648504ms
    Feb 24 12:18:23.705: INFO: The phase of Pod busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:18:25.711: INFO: Pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab": Phase="Running", Reason="", readiness=true. Elapsed: 2.012805459s
    Feb 24 12:18:25.711: INFO: The phase of Pod busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab is Running (Ready = true)
    Feb 24 12:18:25.711: INFO: Pod "busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 24 12:18:25.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-606" for this suite. 02/24/23 12:18:25.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:25.756
Feb 24 12:18:25.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename sched-pred 02/24/23 12:18:25.758
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:25.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:25.782
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 24 12:18:25.789: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 24 12:18:25.806: INFO: Waiting for terminating namespaces to be deleted...
Feb 24 12:18:25.812: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
Feb 24 12:18:25.827: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.827: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:18:25.828: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:18:25.828: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
Feb 24 12:18:25.828: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:18:25.828: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:18:25.828: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:18:25.828: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.828: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:18:25.828: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.828: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:18:25.828: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.828: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 24 12:18:25.828: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:18:25.828: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 12:18:25.828: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
Feb 24 12:18:25.839: INFO: pod-0 from disruption-9648 started at 2023-02-24 12:18:21 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container donothing ready: true, restart count 0
Feb 24 12:18:25.839: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:18:25.839: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:18:25.839: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:18:25.839: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:18:25.839: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:18:25.839: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:18:25.839: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:18:25.839: INFO: busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab from kubelet-test-606 started at 2023-02-24 12:18:23 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab ready: true, restart count 0
Feb 24 12:18:25.839: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.839: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:18:25.839: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 24 12:18:25.839: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
Feb 24 12:18:25.865: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.865: INFO: 	Container calico-node ready: true, restart count 0
Feb 24 12:18:25.865: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 24 12:18:25.865: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
Feb 24 12:18:25.865: INFO: 	Container ebs-plugin ready: true, restart count 0
Feb 24 12:18:25.865: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 24 12:18:25.865: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 24 12:18:25.865: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.865: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 24 12:18:25.865: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
Feb 24 12:18:25.865: INFO: 	Container node-cache ready: true, restart count 0
Feb 24 12:18:25.865: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.865: INFO: 	Container e2e ready: true, restart count 0
Feb 24 12:18:25.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:18:25.865: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
Feb 24 12:18:25.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 24 12:18:25.865: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ip-172-31-215-124.eu-west-3.compute.internal 02/24/23 12:18:25.895
STEP: verifying the node has the label node ip-172-31-216-47.eu-west-3.compute.internal 02/24/23 12:18:25.913
STEP: verifying the node has the label node ip-172-31-217-191.eu-west-3.compute.internal 02/24/23 12:18:25.931
Feb 24 12:18:25.958: INFO: Pod pod-0 requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.958: INFO: Pod canal-29gqs requesting resource cpu=250m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.958: INFO: Pod canal-khjqb requesting resource cpu=250m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.958: INFO: Pod canal-spws5 requesting resource cpu=250m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod ebs-csi-node-75wwf requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod ebs-csi-node-j9x4j requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod ebs-csi-node-tlfc7 requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod kube-proxy-bxx6d requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod kube-proxy-jmqgp requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod kube-proxy-wnnjv requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod node-local-dns-7l6g8 requesting resource cpu=25m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod node-local-dns-hcm9m requesting resource cpu=25m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod node-local-dns-vwq9p requesting resource cpu=25m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.959: INFO: Pod busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.960: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.960: INFO: Pod sonobuoy-e2e-job-8fb5ffbe6d554bfc requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 02/24/23 12:18:25.96
Feb 24 12:18:25.960: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-215-124.eu-west-3.compute.internal
Feb 24 12:18:25.972: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-216-47.eu-west-3.compute.internal
Feb 24 12:18:25.983: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-217-191.eu-west-3.compute.internal
Feb 24 12:18:25.991: INFO: Waiting up to 5m0s for pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc" in namespace "sched-pred-9189" to be "running"
Feb 24 12:18:26.020: INFO: Pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.604848ms
Feb 24 12:18:28.026: INFO: Pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc": Phase="Running", Reason="", readiness=true. Elapsed: 2.027787186s
Feb 24 12:18:28.026: INFO: Pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc" satisfied condition "running"
Feb 24 12:18:28.026: INFO: Waiting up to 5m0s for pod "filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb" in namespace "sched-pred-9189" to be "running"
Feb 24 12:18:28.030: INFO: Pod "filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.557575ms
Feb 24 12:18:28.030: INFO: Pod "filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb" satisfied condition "running"
Feb 24 12:18:28.030: INFO: Waiting up to 5m0s for pod "filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b" in namespace "sched-pred-9189" to be "running"
Feb 24 12:18:28.035: INFO: Pod "filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.115332ms
Feb 24 12:18:28.035: INFO: Pod "filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/24/23 12:18:28.035
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e64544211b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9189/filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb to ip-172-31-216-47.eu-west-3.compute.internal] 02/24/23 12:18:28.039
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e66c1c2485], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e66dbbb1be], Reason = [Created], Message = [Created container filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e675d19cab], Reason = [Started], Message = [Started container filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e645cf2fc9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9189/filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b to ip-172-31-217-191.eu-west-3.compute.internal] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e66c9e8aaa], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e66dc0b1e2], Reason = [Created], Message = [Created container filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e671f93771], Reason = [Started], Message = [Started container filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e64485bd4f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9189/filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc to ip-172-31-215-124.eu-west-3.compute.internal] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e66c7c134f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e66e6fbf4d], Reason = [Created], Message = [Created container filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e673ea2474], Reason = [Started], Message = [Started container filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc] 02/24/23 12:18:28.04
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1746c1e6bff8e59b], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 02/24/23 12:18:28.055
STEP: removing the label node off the node ip-172-31-215-124.eu-west-3.compute.internal 02/24/23 12:18:29.057
STEP: verifying the node doesn't have the label node 02/24/23 12:18:29.072
STEP: removing the label node off the node ip-172-31-216-47.eu-west-3.compute.internal 02/24/23 12:18:29.078
STEP: verifying the node doesn't have the label node 02/24/23 12:18:29.102
STEP: removing the label node off the node ip-172-31-217-191.eu-west-3.compute.internal 02/24/23 12:18:29.108
STEP: verifying the node doesn't have the label node 02/24/23 12:18:29.14
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 24 12:18:29.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9189" for this suite. 02/24/23 12:18:29.157
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":325,"skipped":6054,"failed":0}
------------------------------
• [3.417 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:25.756
    Feb 24 12:18:25.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename sched-pred 02/24/23 12:18:25.758
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:25.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:25.782
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 24 12:18:25.789: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 24 12:18:25.806: INFO: Waiting for terminating namespaces to be deleted...
    Feb 24 12:18:25.812: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-215-124.eu-west-3.compute.internal before test
    Feb 24 12:18:25.827: INFO: canal-spws5 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.827: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: ebs-csi-node-j9x4j from kube-system started at 2023-02-24 10:58:32 +0000 UTC (3 container statuses recorded)
    Feb 24 12:18:25.828: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: kube-proxy-bxx6d from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.828: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: node-local-dns-7l6g8 from kube-system started at 2023-02-24 10:58:32 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.828: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: sonobuoy from sonobuoy started at 2023-02-24 11:01:08 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.828: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 12:18:25.828: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-216-47.eu-west-3.compute.internal before test
    Feb 24 12:18:25.839: INFO: pod-0 from disruption-9648 started at 2023-02-24 12:18:21 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container donothing ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: canal-khjqb from kube-system started at 2023-02-24 10:58:33 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: ebs-csi-node-75wwf from kube-system started at 2023-02-24 10:58:33 +0000 UTC (3 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: kube-proxy-jmqgp from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: node-local-dns-hcm9m from kube-system started at 2023-02-24 10:58:33 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab from kubelet-test-606 started at 2023-02-24 12:18:23 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.839: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 24 12:18:25.839: INFO: 
    Logging pods the apiserver thinks is on node ip-172-31-217-191.eu-west-3.compute.internal before test
    Feb 24 12:18:25.865: INFO: canal-29gqs from kube-system started at 2023-02-24 10:58:44 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.865: INFO: 	Container calico-node ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: 	Container kube-flannel ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: ebs-csi-node-tlfc7 from kube-system started at 2023-02-24 10:58:44 +0000 UTC (3 container statuses recorded)
    Feb 24 12:18:25.865: INFO: 	Container ebs-plugin ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: kube-proxy-wnnjv from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.865: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: node-local-dns-vwq9p from kube-system started at 2023-02-24 10:58:44 +0000 UTC (1 container statuses recorded)
    Feb 24 12:18:25.865: INFO: 	Container node-cache ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: sonobuoy-e2e-job-8fb5ffbe6d554bfc from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.865: INFO: 	Container e2e ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 from sonobuoy started at 2023-02-24 11:01:12 +0000 UTC (2 container statuses recorded)
    Feb 24 12:18:25.865: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 24 12:18:25.865: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ip-172-31-215-124.eu-west-3.compute.internal 02/24/23 12:18:25.895
    STEP: verifying the node has the label node ip-172-31-216-47.eu-west-3.compute.internal 02/24/23 12:18:25.913
    STEP: verifying the node has the label node ip-172-31-217-191.eu-west-3.compute.internal 02/24/23 12:18:25.931
    Feb 24 12:18:25.958: INFO: Pod pod-0 requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.958: INFO: Pod canal-29gqs requesting resource cpu=250m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.958: INFO: Pod canal-khjqb requesting resource cpu=250m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.958: INFO: Pod canal-spws5 requesting resource cpu=250m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod ebs-csi-node-75wwf requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod ebs-csi-node-j9x4j requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod ebs-csi-node-tlfc7 requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod kube-proxy-bxx6d requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod kube-proxy-jmqgp requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod kube-proxy-wnnjv requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod node-local-dns-7l6g8 requesting resource cpu=25m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod node-local-dns-hcm9m requesting resource cpu=25m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod node-local-dns-vwq9p requesting resource cpu=25m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.959: INFO: Pod busybox-readonly-fsf08068be-10ce-47c6-a16a-e256f0b39eab requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.960: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.960: INFO: Pod sonobuoy-e2e-job-8fb5ffbe6d554bfc requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp requesting resource cpu=0m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72231a48044239-k49r6 requesting resource cpu=0m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72231a48044239-sflsb requesting resource cpu=0m on Node ip-172-31-216-47.eu-west-3.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 02/24/23 12:18:25.96
    Feb 24 12:18:25.960: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-215-124.eu-west-3.compute.internal
    Feb 24 12:18:25.972: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-216-47.eu-west-3.compute.internal
    Feb 24 12:18:25.983: INFO: Creating a pod which consumes cpu=927m on Node ip-172-31-217-191.eu-west-3.compute.internal
    Feb 24 12:18:25.991: INFO: Waiting up to 5m0s for pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc" in namespace "sched-pred-9189" to be "running"
    Feb 24 12:18:26.020: INFO: Pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.604848ms
    Feb 24 12:18:28.026: INFO: Pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc": Phase="Running", Reason="", readiness=true. Elapsed: 2.027787186s
    Feb 24 12:18:28.026: INFO: Pod "filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc" satisfied condition "running"
    Feb 24 12:18:28.026: INFO: Waiting up to 5m0s for pod "filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb" in namespace "sched-pred-9189" to be "running"
    Feb 24 12:18:28.030: INFO: Pod "filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.557575ms
    Feb 24 12:18:28.030: INFO: Pod "filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb" satisfied condition "running"
    Feb 24 12:18:28.030: INFO: Waiting up to 5m0s for pod "filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b" in namespace "sched-pred-9189" to be "running"
    Feb 24 12:18:28.035: INFO: Pod "filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.115332ms
    Feb 24 12:18:28.035: INFO: Pod "filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/24/23 12:18:28.035
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e64544211b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9189/filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb to ip-172-31-216-47.eu-west-3.compute.internal] 02/24/23 12:18:28.039
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e66c1c2485], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e66dbbb1be], Reason = [Created], Message = [Created container filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb.1746c1e675d19cab], Reason = [Started], Message = [Started container filler-pod-210638c2-d3c5-45d2-a0b1-922d6070c7bb] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e645cf2fc9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9189/filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b to ip-172-31-217-191.eu-west-3.compute.internal] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e66c9e8aaa], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e66dc0b1e2], Reason = [Created], Message = [Created container filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b.1746c1e671f93771], Reason = [Started], Message = [Started container filler-pod-5b209d89-ab83-442f-8129-22c2ea28bf2b] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e64485bd4f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9189/filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc to ip-172-31-215-124.eu-west-3.compute.internal] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e66c7c134f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e66e6fbf4d], Reason = [Created], Message = [Created container filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc.1746c1e673ea2474], Reason = [Started], Message = [Started container filler-pod-802916ab-3435-439c-a446-6a4adcd4fadc] 02/24/23 12:18:28.04
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1746c1e6bff8e59b], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 02/24/23 12:18:28.055
    STEP: removing the label node off the node ip-172-31-215-124.eu-west-3.compute.internal 02/24/23 12:18:29.057
    STEP: verifying the node doesn't have the label node 02/24/23 12:18:29.072
    STEP: removing the label node off the node ip-172-31-216-47.eu-west-3.compute.internal 02/24/23 12:18:29.078
    STEP: verifying the node doesn't have the label node 02/24/23 12:18:29.102
    STEP: removing the label node off the node ip-172-31-217-191.eu-west-3.compute.internal 02/24/23 12:18:29.108
    STEP: verifying the node doesn't have the label node 02/24/23 12:18:29.14
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 12:18:29.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9189" for this suite. 02/24/23 12:18:29.157
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:29.192
Feb 24 12:18:29.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:18:29.196
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:29.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:29.238
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-8c213ec9-82eb-4f0b-a449-ccf834592187 02/24/23 12:18:29.256
STEP: Creating secret with name s-test-opt-upd-b042b8fa-ff3a-4e12-b83c-97678956ea37 02/24/23 12:18:29.263
STEP: Creating the pod 02/24/23 12:18:29.271
Feb 24 12:18:29.284: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e" in namespace "projected-693" to be "running and ready"
Feb 24 12:18:29.296: INFO: Pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.079976ms
Feb 24 12:18:29.296: INFO: The phase of Pod pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:18:31.313: INFO: Pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e": Phase="Running", Reason="", readiness=true. Elapsed: 2.029064443s
Feb 24 12:18:31.313: INFO: The phase of Pod pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e is Running (Ready = true)
Feb 24 12:18:31.313: INFO: Pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8c213ec9-82eb-4f0b-a449-ccf834592187 02/24/23 12:18:31.37
STEP: Updating secret s-test-opt-upd-b042b8fa-ff3a-4e12-b83c-97678956ea37 02/24/23 12:18:31.379
STEP: Creating secret with name s-test-opt-create-5d6e2875-c758-4fc0-83a3-989154595cbb 02/24/23 12:18:31.387
STEP: waiting to observe update in volume 02/24/23 12:18:31.397
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 24 12:18:35.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-693" for this suite. 02/24/23 12:18:35.46
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":326,"skipped":6060,"failed":0}
------------------------------
• [SLOW TEST] [6.278 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:29.192
    Feb 24 12:18:29.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:18:29.196
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:29.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:29.238
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-8c213ec9-82eb-4f0b-a449-ccf834592187 02/24/23 12:18:29.256
    STEP: Creating secret with name s-test-opt-upd-b042b8fa-ff3a-4e12-b83c-97678956ea37 02/24/23 12:18:29.263
    STEP: Creating the pod 02/24/23 12:18:29.271
    Feb 24 12:18:29.284: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e" in namespace "projected-693" to be "running and ready"
    Feb 24 12:18:29.296: INFO: Pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.079976ms
    Feb 24 12:18:29.296: INFO: The phase of Pod pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:18:31.313: INFO: Pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e": Phase="Running", Reason="", readiness=true. Elapsed: 2.029064443s
    Feb 24 12:18:31.313: INFO: The phase of Pod pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e is Running (Ready = true)
    Feb 24 12:18:31.313: INFO: Pod "pod-projected-secrets-44fa7b75-ca82-446a-955b-995b59f5b85e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8c213ec9-82eb-4f0b-a449-ccf834592187 02/24/23 12:18:31.37
    STEP: Updating secret s-test-opt-upd-b042b8fa-ff3a-4e12-b83c-97678956ea37 02/24/23 12:18:31.379
    STEP: Creating secret with name s-test-opt-create-5d6e2875-c758-4fc0-83a3-989154595cbb 02/24/23 12:18:31.387
    STEP: waiting to observe update in volume 02/24/23 12:18:31.397
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 24 12:18:35.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-693" for this suite. 02/24/23 12:18:35.46
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:18:35.471
Feb 24 12:18:35.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename cronjob 02/24/23 12:18:35.472
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:35.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:35.5
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/24/23 12:18:35.504
STEP: Ensuring more than one job is running at a time 02/24/23 12:18:35.511
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/24/23 12:20:01.517
STEP: Removing cronjob 02/24/23 12:20:01.521
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 24 12:20:01.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2137" for this suite. 02/24/23 12:20:01.56
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":327,"skipped":6062,"failed":0}
------------------------------
• [SLOW TEST] [86.116 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:18:35.471
    Feb 24 12:18:35.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename cronjob 02/24/23 12:18:35.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:18:35.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:18:35.5
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/24/23 12:18:35.504
    STEP: Ensuring more than one job is running at a time 02/24/23 12:18:35.511
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/24/23 12:20:01.517
    STEP: Removing cronjob 02/24/23 12:20:01.521
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 24 12:20:01.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2137" for this suite. 02/24/23 12:20:01.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:20:01.59
Feb 24 12:20:01.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 12:20:01.594
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:02.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:02.096
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 12:20:02.115
Feb 24 12:20:02.129: INFO: Waiting up to 5m0s for pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474" in namespace "emptydir-9416" to be "Succeeded or Failed"
Feb 24 12:20:02.139: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Pending", Reason="", readiness=false. Elapsed: 10.260542ms
Feb 24 12:20:04.145: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015391391s
Feb 24 12:20:06.147: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017630727s
Feb 24 12:20:08.146: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016470092s
STEP: Saw pod success 02/24/23 12:20:08.146
Feb 24 12:20:08.146: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474" satisfied condition "Succeeded or Failed"
Feb 24 12:20:08.152: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-19b3d2d6-f7db-475b-9176-729a60e26474 container test-container: <nil>
STEP: delete the pod 02/24/23 12:20:08.165
Feb 24 12:20:08.180: INFO: Waiting for pod pod-19b3d2d6-f7db-475b-9176-729a60e26474 to disappear
Feb 24 12:20:08.184: INFO: Pod pod-19b3d2d6-f7db-475b-9176-729a60e26474 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 12:20:08.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9416" for this suite. 02/24/23 12:20:08.192
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":328,"skipped":6096,"failed":0}
------------------------------
• [SLOW TEST] [6.609 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:20:01.59
    Feb 24 12:20:01.591: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:20:01.594
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:02.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:02.096
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/24/23 12:20:02.115
    Feb 24 12:20:02.129: INFO: Waiting up to 5m0s for pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474" in namespace "emptydir-9416" to be "Succeeded or Failed"
    Feb 24 12:20:02.139: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Pending", Reason="", readiness=false. Elapsed: 10.260542ms
    Feb 24 12:20:04.145: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015391391s
    Feb 24 12:20:06.147: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017630727s
    Feb 24 12:20:08.146: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016470092s
    STEP: Saw pod success 02/24/23 12:20:08.146
    Feb 24 12:20:08.146: INFO: Pod "pod-19b3d2d6-f7db-475b-9176-729a60e26474" satisfied condition "Succeeded or Failed"
    Feb 24 12:20:08.152: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-19b3d2d6-f7db-475b-9176-729a60e26474 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:20:08.165
    Feb 24 12:20:08.180: INFO: Waiting for pod pod-19b3d2d6-f7db-475b-9176-729a60e26474 to disappear
    Feb 24 12:20:08.184: INFO: Pod pod-19b3d2d6-f7db-475b-9176-729a60e26474 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:20:08.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9416" for this suite. 02/24/23 12:20:08.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:20:08.202
Feb 24 12:20:08.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename taint-multiple-pods 02/24/23 12:20:08.203
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:08.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:08.24
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Feb 24 12:20:08.248: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 24 12:21:08.311: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Feb 24 12:21:08.317: INFO: Starting informer...
STEP: Starting pods... 02/24/23 12:21:08.317
Feb 24 12:21:08.543: INFO: Pod1 is running on ip-172-31-216-47.eu-west-3.compute.internal. Tainting Node
Feb 24 12:21:08.754: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3456" to be "running"
Feb 24 12:21:08.759: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.780724ms
Feb 24 12:21:10.764: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010140296s
Feb 24 12:21:10.764: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 24 12:21:10.764: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3456" to be "running"
Feb 24 12:21:10.769: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.545937ms
Feb 24 12:21:10.769: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 24 12:21:10.769: INFO: Pod2 is running on ip-172-31-216-47.eu-west-3.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 02/24/23 12:21:10.769
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:21:10.784
STEP: Waiting for Pod1 and Pod2 to be deleted 02/24/23 12:21:10.791
Feb 24 12:21:16.361: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 24 12:21:36.401: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:21:36.432
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Feb 24 12:21:36.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3456" for this suite. 02/24/23 12:21:36.456
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":329,"skipped":6120,"failed":0}
------------------------------
• [SLOW TEST] [88.270 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:20:08.202
    Feb 24 12:20:08.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename taint-multiple-pods 02/24/23 12:20:08.203
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:20:08.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:20:08.24
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Feb 24 12:20:08.248: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 24 12:21:08.311: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Feb 24 12:21:08.317: INFO: Starting informer...
    STEP: Starting pods... 02/24/23 12:21:08.317
    Feb 24 12:21:08.543: INFO: Pod1 is running on ip-172-31-216-47.eu-west-3.compute.internal. Tainting Node
    Feb 24 12:21:08.754: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3456" to be "running"
    Feb 24 12:21:08.759: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.780724ms
    Feb 24 12:21:10.764: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.010140296s
    Feb 24 12:21:10.764: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 24 12:21:10.764: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3456" to be "running"
    Feb 24 12:21:10.769: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.545937ms
    Feb 24 12:21:10.769: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 24 12:21:10.769: INFO: Pod2 is running on ip-172-31-216-47.eu-west-3.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 02/24/23 12:21:10.769
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:21:10.784
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/24/23 12:21:10.791
    Feb 24 12:21:16.361: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 24 12:21:36.401: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/24/23 12:21:36.432
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Feb 24 12:21:36.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-3456" for this suite. 02/24/23 12:21:36.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:21:36.476
Feb 24 12:21:36.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename gc 02/24/23 12:21:36.482
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:36.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:36.534
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/24/23 12:21:36.538
STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 12:21:36.547
STEP: delete the deployment 02/24/23 12:21:37.061
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/24/23 12:21:37.069
STEP: Gathering metrics 02/24/23 12:21:37.6
Feb 24 12:21:37.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
Feb 24 12:21:37.637: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.392209ms
Feb 24 12:21:37.637: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
Feb 24 12:21:37.637: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
Feb 24 12:21:37.732: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 24 12:21:37.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1720" for this suite. 02/24/23 12:21:37.738
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":330,"skipped":6132,"failed":0}
------------------------------
• [1.270 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:21:36.476
    Feb 24 12:21:36.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename gc 02/24/23 12:21:36.482
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:36.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:36.534
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/24/23 12:21:36.538
    STEP: Wait for the Deployment to create new ReplicaSet 02/24/23 12:21:36.547
    STEP: delete the deployment 02/24/23 12:21:37.061
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/24/23 12:21:37.069
    STEP: Gathering metrics 02/24/23 12:21:37.6
    Feb 24 12:21:37.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" in namespace "kube-system" to be "running and ready"
    Feb 24 12:21:37.637: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 4.392209ms
    Feb 24 12:21:37.637: INFO: The phase of Pod kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal is Running (Ready = true)
    Feb 24 12:21:37.637: INFO: Pod "kube-controller-manager-ip-172-31-217-245.eu-west-3.compute.internal" satisfied condition "running and ready"
    Feb 24 12:21:37.732: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 24 12:21:37.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1720" for this suite. 02/24/23 12:21:37.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:21:37.749
Feb 24 12:21:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:21:37.75
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:37.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:37.774
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 02/24/23 12:21:37.778
STEP: Ensuring ResourceQuota status is calculated 02/24/23 12:21:37.784
STEP: Creating a ResourceQuota with not best effort scope 02/24/23 12:21:39.79
STEP: Ensuring ResourceQuota status is calculated 02/24/23 12:21:39.798
STEP: Creating a best-effort pod 02/24/23 12:21:41.804
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/24/23 12:21:41.819
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/24/23 12:21:43.829
STEP: Deleting the pod 02/24/23 12:21:45.836
STEP: Ensuring resource quota status released the pod usage 02/24/23 12:21:45.851
STEP: Creating a not best-effort pod 02/24/23 12:21:47.857
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/24/23 12:21:47.867
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/24/23 12:21:49.874
STEP: Deleting the pod 02/24/23 12:21:51.879
STEP: Ensuring resource quota status released the pod usage 02/24/23 12:21:51.894
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 12:21:53.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1676" for this suite. 02/24/23 12:21:53.907
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":331,"skipped":6163,"failed":0}
------------------------------
• [SLOW TEST] [16.166 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:21:37.749
    Feb 24 12:21:37.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:21:37.75
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:37.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:37.774
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 02/24/23 12:21:37.778
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 12:21:37.784
    STEP: Creating a ResourceQuota with not best effort scope 02/24/23 12:21:39.79
    STEP: Ensuring ResourceQuota status is calculated 02/24/23 12:21:39.798
    STEP: Creating a best-effort pod 02/24/23 12:21:41.804
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/24/23 12:21:41.819
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/24/23 12:21:43.829
    STEP: Deleting the pod 02/24/23 12:21:45.836
    STEP: Ensuring resource quota status released the pod usage 02/24/23 12:21:45.851
    STEP: Creating a not best-effort pod 02/24/23 12:21:47.857
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/24/23 12:21:47.867
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/24/23 12:21:49.874
    STEP: Deleting the pod 02/24/23 12:21:51.879
    STEP: Ensuring resource quota status released the pod usage 02/24/23 12:21:51.894
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 12:21:53.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1676" for this suite. 02/24/23 12:21:53.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:21:53.916
Feb 24 12:21:53.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:21:53.918
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:53.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:53.945
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 02/24/23 12:21:53.949
STEP: Getting a ResourceQuota 02/24/23 12:21:53.954
STEP: Listing all ResourceQuotas with LabelSelector 02/24/23 12:21:53.959
STEP: Patching the ResourceQuota 02/24/23 12:21:53.963
STEP: Deleting a Collection of ResourceQuotas 02/24/23 12:21:53.97
STEP: Verifying the deleted ResourceQuota 02/24/23 12:21:53.982
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 12:21:53.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5972" for this suite. 02/24/23 12:21:53.995
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":332,"skipped":6168,"failed":0}
------------------------------
• [0.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:21:53.916
    Feb 24 12:21:53.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:21:53.918
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:53.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:53.945
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 02/24/23 12:21:53.949
    STEP: Getting a ResourceQuota 02/24/23 12:21:53.954
    STEP: Listing all ResourceQuotas with LabelSelector 02/24/23 12:21:53.959
    STEP: Patching the ResourceQuota 02/24/23 12:21:53.963
    STEP: Deleting a Collection of ResourceQuotas 02/24/23 12:21:53.97
    STEP: Verifying the deleted ResourceQuota 02/24/23 12:21:53.982
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 12:21:53.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5972" for this suite. 02/24/23 12:21:53.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:21:54.009
Feb 24 12:21:54.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 12:21:54.012
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:54.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:54.037
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4577.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/24/23 12:21:54.041
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4577.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/24/23 12:21:54.041
STEP: creating a pod to probe /etc/hosts 02/24/23 12:21:54.041
STEP: submitting the pod to kubernetes 02/24/23 12:21:54.041
Feb 24 12:21:54.053: INFO: Waiting up to 15m0s for pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb" in namespace "dns-4577" to be "running"
Feb 24 12:21:54.060: INFO: Pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.3656ms
Feb 24 12:21:56.067: INFO: Pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013975123s
Feb 24 12:21:56.067: INFO: Pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:21:56.067
STEP: looking for the results for each expected name from probers 02/24/23 12:21:56.071
Feb 24 12:21:56.097: INFO: DNS probes using dns-4577/dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb succeeded

STEP: deleting the pod 02/24/23 12:21:56.097
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 12:21:56.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4577" for this suite. 02/24/23 12:21:56.119
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":333,"skipped":6178,"failed":0}
------------------------------
• [2.118 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:21:54.009
    Feb 24 12:21:54.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 12:21:54.012
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:54.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:54.037
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4577.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/24/23 12:21:54.041
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4577.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4577.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/24/23 12:21:54.041
    STEP: creating a pod to probe /etc/hosts 02/24/23 12:21:54.041
    STEP: submitting the pod to kubernetes 02/24/23 12:21:54.041
    Feb 24 12:21:54.053: INFO: Waiting up to 15m0s for pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb" in namespace "dns-4577" to be "running"
    Feb 24 12:21:54.060: INFO: Pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.3656ms
    Feb 24 12:21:56.067: INFO: Pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.013975123s
    Feb 24 12:21:56.067: INFO: Pod "dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:21:56.067
    STEP: looking for the results for each expected name from probers 02/24/23 12:21:56.071
    Feb 24 12:21:56.097: INFO: DNS probes using dns-4577/dns-test-b7f96ea2-9799-479a-964f-2abe47dfe4eb succeeded

    STEP: deleting the pod 02/24/23 12:21:56.097
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 12:21:56.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4577" for this suite. 02/24/23 12:21:56.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:21:56.132
Feb 24 12:21:56.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 12:21:56.133
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:56.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:56.161
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8134 02/24/23 12:21:56.164
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 12:21:56.189
STEP: creating service externalsvc in namespace services-8134 02/24/23 12:21:56.189
STEP: creating replication controller externalsvc in namespace services-8134 02/24/23 12:21:56.23
I0224 12:21:56.257186      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8134, replica count: 2
I0224 12:21:59.308389      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/24/23 12:21:59.322
Feb 24 12:21:59.394: INFO: Creating new exec pod
Feb 24 12:21:59.410: INFO: Waiting up to 5m0s for pod "execpodlncch" in namespace "services-8134" to be "running"
Feb 24 12:21:59.424: INFO: Pod "execpodlncch": Phase="Pending", Reason="", readiness=false. Elapsed: 13.668172ms
Feb 24 12:22:01.431: INFO: Pod "execpodlncch": Phase="Running", Reason="", readiness=true. Elapsed: 2.020519904s
Feb 24 12:22:01.431: INFO: Pod "execpodlncch" satisfied condition "running"
Feb 24 12:22:01.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-8134 exec execpodlncch -- /bin/sh -x -c nslookup clusterip-service.services-8134.svc.cluster.local'
Feb 24 12:22:01.669: INFO: stderr: "+ nslookup clusterip-service.services-8134.svc.cluster.local\n"
Feb 24 12:22:01.669: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8134.svc.cluster.local\tcanonical name = externalsvc.services-8134.svc.cluster.local.\nName:\texternalsvc.services-8134.svc.cluster.local\nAddress: 10.107.242.211\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8134, will wait for the garbage collector to delete the pods 02/24/23 12:22:01.669
Feb 24 12:22:01.733: INFO: Deleting ReplicationController externalsvc took: 8.482353ms
Feb 24 12:22:01.835: INFO: Terminating ReplicationController externalsvc pods took: 101.761119ms
Feb 24 12:22:03.860: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 12:22:03.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8134" for this suite. 02/24/23 12:22:03.892
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":334,"skipped":6195,"failed":0}
------------------------------
• [SLOW TEST] [7.773 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:21:56.132
    Feb 24 12:21:56.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 12:21:56.133
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:21:56.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:21:56.161
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8134 02/24/23 12:21:56.164
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/24/23 12:21:56.189
    STEP: creating service externalsvc in namespace services-8134 02/24/23 12:21:56.189
    STEP: creating replication controller externalsvc in namespace services-8134 02/24/23 12:21:56.23
    I0224 12:21:56.257186      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8134, replica count: 2
    I0224 12:21:59.308389      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/24/23 12:21:59.322
    Feb 24 12:21:59.394: INFO: Creating new exec pod
    Feb 24 12:21:59.410: INFO: Waiting up to 5m0s for pod "execpodlncch" in namespace "services-8134" to be "running"
    Feb 24 12:21:59.424: INFO: Pod "execpodlncch": Phase="Pending", Reason="", readiness=false. Elapsed: 13.668172ms
    Feb 24 12:22:01.431: INFO: Pod "execpodlncch": Phase="Running", Reason="", readiness=true. Elapsed: 2.020519904s
    Feb 24 12:22:01.431: INFO: Pod "execpodlncch" satisfied condition "running"
    Feb 24 12:22:01.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-8134 exec execpodlncch -- /bin/sh -x -c nslookup clusterip-service.services-8134.svc.cluster.local'
    Feb 24 12:22:01.669: INFO: stderr: "+ nslookup clusterip-service.services-8134.svc.cluster.local\n"
    Feb 24 12:22:01.669: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-8134.svc.cluster.local\tcanonical name = externalsvc.services-8134.svc.cluster.local.\nName:\texternalsvc.services-8134.svc.cluster.local\nAddress: 10.107.242.211\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8134, will wait for the garbage collector to delete the pods 02/24/23 12:22:01.669
    Feb 24 12:22:01.733: INFO: Deleting ReplicationController externalsvc took: 8.482353ms
    Feb 24 12:22:01.835: INFO: Terminating ReplicationController externalsvc pods took: 101.761119ms
    Feb 24 12:22:03.860: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 12:22:03.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8134" for this suite. 02/24/23 12:22:03.892
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:22:03.923
Feb 24 12:22:03.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename configmap 02/24/23 12:22:03.928
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:03.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:03.987
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-64d6a394-f3a0-4c21-9076-f66bf52c888b 02/24/23 12:22:03.997
STEP: Creating a pod to test consume configMaps 02/24/23 12:22:04.006
Feb 24 12:22:04.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78" in namespace "configmap-1146" to be "Succeeded or Failed"
Feb 24 12:22:04.025: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.171365ms
Feb 24 12:22:06.039: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021964341s
Feb 24 12:22:08.031: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014041719s
STEP: Saw pod success 02/24/23 12:22:08.031
Feb 24 12:22:08.031: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78" satisfied condition "Succeeded or Failed"
Feb 24 12:22:08.036: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78 container configmap-volume-test: <nil>
STEP: delete the pod 02/24/23 12:22:08.05
Feb 24 12:22:08.080: INFO: Waiting for pod pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78 to disappear
Feb 24 12:22:08.086: INFO: Pod pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 24 12:22:08.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1146" for this suite. 02/24/23 12:22:08.095
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":335,"skipped":6205,"failed":0}
------------------------------
• [4.180 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:22:03.923
    Feb 24 12:22:03.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename configmap 02/24/23 12:22:03.928
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:03.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:03.987
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-64d6a394-f3a0-4c21-9076-f66bf52c888b 02/24/23 12:22:03.997
    STEP: Creating a pod to test consume configMaps 02/24/23 12:22:04.006
    Feb 24 12:22:04.017: INFO: Waiting up to 5m0s for pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78" in namespace "configmap-1146" to be "Succeeded or Failed"
    Feb 24 12:22:04.025: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.171365ms
    Feb 24 12:22:06.039: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021964341s
    Feb 24 12:22:08.031: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014041719s
    STEP: Saw pod success 02/24/23 12:22:08.031
    Feb 24 12:22:08.031: INFO: Pod "pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78" satisfied condition "Succeeded or Failed"
    Feb 24 12:22:08.036: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78 container configmap-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:22:08.05
    Feb 24 12:22:08.080: INFO: Waiting for pod pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78 to disappear
    Feb 24 12:22:08.086: INFO: Pod pod-configmaps-dd0434e5-606a-4489-9088-ce472837bd78 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 24 12:22:08.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1146" for this suite. 02/24/23 12:22:08.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:22:08.112
Feb 24 12:22:08.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:22:08.113
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:08.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:08.142
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 24 12:22:08.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9285" for this suite. 02/24/23 12:22:08.161
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":336,"skipped":6251,"failed":0}
------------------------------
• [0.057 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:22:08.112
    Feb 24 12:22:08.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename runtimeclass 02/24/23 12:22:08.113
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:08.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:08.142
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 24 12:22:08.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9285" for this suite. 02/24/23 12:22:08.161
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:22:08.169
Feb 24 12:22:08.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:22:08.172
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:08.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:08.199
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-a5cf7c0f-0992-4ff5-8428-8b1619c1bc40 02/24/23 12:22:08.203
STEP: Creating a pod to test consume configMaps 02/24/23 12:22:08.21
Feb 24 12:22:08.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172" in namespace "projected-4593" to be "Succeeded or Failed"
Feb 24 12:22:08.237: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172": Phase="Pending", Reason="", readiness=false. Elapsed: 8.25759ms
Feb 24 12:22:10.243: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013973135s
Feb 24 12:22:12.244: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014715862s
STEP: Saw pod success 02/24/23 12:22:12.244
Feb 24 12:22:12.244: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172" satisfied condition "Succeeded or Failed"
Feb 24 12:22:12.251: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172 container agnhost-container: <nil>
STEP: delete the pod 02/24/23 12:22:12.262
Feb 24 12:22:12.280: INFO: Waiting for pod pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172 to disappear
Feb 24 12:22:12.288: INFO: Pod pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 12:22:12.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4593" for this suite. 02/24/23 12:22:12.301
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":337,"skipped":6254,"failed":0}
------------------------------
• [4.154 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:22:08.169
    Feb 24 12:22:08.169: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:22:08.172
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:08.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:08.199
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-a5cf7c0f-0992-4ff5-8428-8b1619c1bc40 02/24/23 12:22:08.203
    STEP: Creating a pod to test consume configMaps 02/24/23 12:22:08.21
    Feb 24 12:22:08.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172" in namespace "projected-4593" to be "Succeeded or Failed"
    Feb 24 12:22:08.237: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172": Phase="Pending", Reason="", readiness=false. Elapsed: 8.25759ms
    Feb 24 12:22:10.243: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013973135s
    Feb 24 12:22:12.244: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014715862s
    STEP: Saw pod success 02/24/23 12:22:12.244
    Feb 24 12:22:12.244: INFO: Pod "pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172" satisfied condition "Succeeded or Failed"
    Feb 24 12:22:12.251: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172 container agnhost-container: <nil>
    STEP: delete the pod 02/24/23 12:22:12.262
    Feb 24 12:22:12.280: INFO: Waiting for pod pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172 to disappear
    Feb 24 12:22:12.288: INFO: Pod pod-projected-configmaps-8e474cac-105d-4a30-86bd-42b235c99172 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 12:22:12.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4593" for this suite. 02/24/23 12:22:12.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:22:12.327
Feb 24 12:22:12.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename pod-network-test 02/24/23 12:22:12.329
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:12.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:12.367
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-978 02/24/23 12:22:12.373
STEP: creating a selector 02/24/23 12:22:12.373
STEP: Creating the service pods in kubernetes 02/24/23 12:22:12.373
Feb 24 12:22:12.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 24 12:22:12.424: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-978" to be "running and ready"
Feb 24 12:22:12.434: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.01556ms
Feb 24 12:22:12.434: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:22:14.441: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015891686s
Feb 24 12:22:14.441: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:16.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014517118s
Feb 24 12:22:16.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:18.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015123365s
Feb 24 12:22:18.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:20.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014840485s
Feb 24 12:22:20.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:22.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015018035s
Feb 24 12:22:22.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:24.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014407433s
Feb 24 12:22:24.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:26.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015079369s
Feb 24 12:22:26.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:28.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014800925s
Feb 24 12:22:28.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:30.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014379576s
Feb 24 12:22:30.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:32.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014603655s
Feb 24 12:22:32.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 24 12:22:34.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014469305s
Feb 24 12:22:34.439: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 24 12:22:34.439: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 24 12:22:34.443: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-978" to be "running and ready"
Feb 24 12:22:34.447: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.083008ms
Feb 24 12:22:34.448: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 24 12:22:34.448: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 24 12:22:34.451: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-978" to be "running and ready"
Feb 24 12:22:34.456: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.957834ms
Feb 24 12:22:34.456: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 24 12:22:34.456: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/24/23 12:22:34.46
Feb 24 12:22:34.475: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-978" to be "running"
Feb 24 12:22:34.483: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.910992ms
Feb 24 12:22:36.488: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01300889s
Feb 24 12:22:36.488: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 24 12:22:36.493: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-978" to be "running"
Feb 24 12:22:36.497: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.307336ms
Feb 24 12:22:36.497: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 24 12:22:36.501: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 24 12:22:36.501: INFO: Going to poll 10.244.4.240 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 24 12:22:36.506: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.240:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-978 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:22:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:22:36.507: INFO: ExecWithOptions: Clientset creation
Feb 24 12:22:36.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-978/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.4.240%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:22:36.615: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 24 12:22:36.615: INFO: Going to poll 10.244.3.104 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 24 12:22:36.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.104:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-978 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:22:36.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:22:36.624: INFO: ExecWithOptions: Clientset creation
Feb 24 12:22:36.624: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-978/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.104%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:22:36.710: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 24 12:22:36.710: INFO: Going to poll 10.244.5.159 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 24 12:22:36.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-978 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 24 12:22:36.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
Feb 24 12:22:36.716: INFO: ExecWithOptions: Clientset creation
Feb 24 12:22:36.716: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-978/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.5.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 24 12:22:36.795: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 24 12:22:36.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-978" for this suite. 02/24/23 12:22:36.804
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":338,"skipped":6264,"failed":0}
------------------------------
• [SLOW TEST] [24.483 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:22:12.327
    Feb 24 12:22:12.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename pod-network-test 02/24/23 12:22:12.329
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:12.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:12.367
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-978 02/24/23 12:22:12.373
    STEP: creating a selector 02/24/23 12:22:12.373
    STEP: Creating the service pods in kubernetes 02/24/23 12:22:12.373
    Feb 24 12:22:12.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 24 12:22:12.424: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-978" to be "running and ready"
    Feb 24 12:22:12.434: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.01556ms
    Feb 24 12:22:12.434: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:22:14.441: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015891686s
    Feb 24 12:22:14.441: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:16.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014517118s
    Feb 24 12:22:16.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:18.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015123365s
    Feb 24 12:22:18.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:20.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014840485s
    Feb 24 12:22:20.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:22.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015018035s
    Feb 24 12:22:22.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:24.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014407433s
    Feb 24 12:22:24.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:26.440: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015079369s
    Feb 24 12:22:26.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:28.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014800925s
    Feb 24 12:22:28.440: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:30.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014379576s
    Feb 24 12:22:30.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:32.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014603655s
    Feb 24 12:22:32.439: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 24 12:22:34.439: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014469305s
    Feb 24 12:22:34.439: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 24 12:22:34.439: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 24 12:22:34.443: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-978" to be "running and ready"
    Feb 24 12:22:34.447: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.083008ms
    Feb 24 12:22:34.448: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 24 12:22:34.448: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 24 12:22:34.451: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-978" to be "running and ready"
    Feb 24 12:22:34.456: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.957834ms
    Feb 24 12:22:34.456: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 24 12:22:34.456: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/24/23 12:22:34.46
    Feb 24 12:22:34.475: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-978" to be "running"
    Feb 24 12:22:34.483: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.910992ms
    Feb 24 12:22:36.488: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01300889s
    Feb 24 12:22:36.488: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 24 12:22:36.493: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-978" to be "running"
    Feb 24 12:22:36.497: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.307336ms
    Feb 24 12:22:36.497: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 24 12:22:36.501: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 24 12:22:36.501: INFO: Going to poll 10.244.4.240 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 12:22:36.506: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.240:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-978 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:22:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:22:36.507: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:22:36.507: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-978/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.4.240%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:22:36.615: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 24 12:22:36.615: INFO: Going to poll 10.244.3.104 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 12:22:36.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.104:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-978 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:22:36.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:22:36.624: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:22:36.624: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-978/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.3.104%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:22:36.710: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 24 12:22:36.710: INFO: Going to poll 10.244.5.159 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 24 12:22:36.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.159:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-978 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 24 12:22:36.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    Feb 24 12:22:36.716: INFO: ExecWithOptions: Clientset creation
    Feb 24 12:22:36.716: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-978/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.5.159%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 24 12:22:36.795: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 24 12:22:36.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-978" for this suite. 02/24/23 12:22:36.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:22:36.82
Feb 24 12:22:36.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename services 02/24/23 12:22:36.821
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:36.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:36.852
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7962 02/24/23 12:22:36.856
STEP: changing the ExternalName service to type=ClusterIP 02/24/23 12:22:36.862
STEP: creating replication controller externalname-service in namespace services-7962 02/24/23 12:22:36.959
I0224 12:22:36.966711      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7962, replica count: 2
I0224 12:22:40.018706      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 24 12:22:40.018: INFO: Creating new exec pod
Feb 24 12:22:40.027: INFO: Waiting up to 5m0s for pod "execpod2slrr" in namespace "services-7962" to be "running"
Feb 24 12:22:40.033: INFO: Pod "execpod2slrr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479014ms
Feb 24 12:22:42.041: INFO: Pod "execpod2slrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.014054129s
Feb 24 12:22:42.047: INFO: Pod "execpod2slrr" satisfied condition "running"
Feb 24 12:22:43.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 12:22:43.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:43.189: INFO: stdout: ""
Feb 24 12:22:44.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 12:22:44.364: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:44.364: INFO: stdout: ""
Feb 24 12:22:45.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 12:22:45.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:45.438: INFO: stdout: ""
Feb 24 12:22:46.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 12:22:46.393: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:46.393: INFO: stdout: ""
Feb 24 12:22:47.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 24 12:22:47.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:47.395: INFO: stdout: "externalname-service-tbfvk"
Feb 24 12:22:47.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
Feb 24 12:22:47.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.71.55 80\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:47.634: INFO: stdout: ""
Feb 24 12:22:48.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
Feb 24 12:22:48.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.71.55 80\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:48.785: INFO: stdout: ""
Feb 24 12:22:49.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
Feb 24 12:22:49.848: INFO: stderr: "+ nc -v -t -w 2 10.99.71.55 80\n+ echo hostName\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:49.848: INFO: stdout: ""
Feb 24 12:22:50.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
Feb 24 12:22:50.818: INFO: stderr: "+ nc -v -t -w 2 10.99.71.55 80\n+ echo hostName\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
Feb 24 12:22:50.818: INFO: stdout: "externalname-service-tbfvk"
Feb 24 12:22:50.819: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 24 12:22:50.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7962" for this suite. 02/24/23 12:22:50.855
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":339,"skipped":6286,"failed":0}
------------------------------
• [SLOW TEST] [14.049 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:22:36.82
    Feb 24 12:22:36.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename services 02/24/23 12:22:36.821
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:36.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:36.852
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7962 02/24/23 12:22:36.856
    STEP: changing the ExternalName service to type=ClusterIP 02/24/23 12:22:36.862
    STEP: creating replication controller externalname-service in namespace services-7962 02/24/23 12:22:36.959
    I0224 12:22:36.966711      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7962, replica count: 2
    I0224 12:22:40.018706      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 24 12:22:40.018: INFO: Creating new exec pod
    Feb 24 12:22:40.027: INFO: Waiting up to 5m0s for pod "execpod2slrr" in namespace "services-7962" to be "running"
    Feb 24 12:22:40.033: INFO: Pod "execpod2slrr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479014ms
    Feb 24 12:22:42.041: INFO: Pod "execpod2slrr": Phase="Running", Reason="", readiness=true. Elapsed: 2.014054129s
    Feb 24 12:22:42.047: INFO: Pod "execpod2slrr" satisfied condition "running"
    Feb 24 12:22:43.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 12:22:43.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:43.189: INFO: stdout: ""
    Feb 24 12:22:44.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 12:22:44.364: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:44.364: INFO: stdout: ""
    Feb 24 12:22:45.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 12:22:45.438: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:45.438: INFO: stdout: ""
    Feb 24 12:22:46.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 12:22:46.393: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:46.393: INFO: stdout: ""
    Feb 24 12:22:47.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 24 12:22:47.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:47.395: INFO: stdout: "externalname-service-tbfvk"
    Feb 24 12:22:47.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
    Feb 24 12:22:47.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.71.55 80\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:47.634: INFO: stdout: ""
    Feb 24 12:22:48.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
    Feb 24 12:22:48.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.71.55 80\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:48.785: INFO: stdout: ""
    Feb 24 12:22:49.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
    Feb 24 12:22:49.848: INFO: stderr: "+ nc -v -t -w 2 10.99.71.55 80\n+ echo hostName\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:49.848: INFO: stdout: ""
    Feb 24 12:22:50.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=services-7962 exec execpod2slrr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.71.55 80'
    Feb 24 12:22:50.818: INFO: stderr: "+ nc -v -t -w 2 10.99.71.55 80\n+ echo hostName\nConnection to 10.99.71.55 80 port [tcp/http] succeeded!\n"
    Feb 24 12:22:50.818: INFO: stdout: "externalname-service-tbfvk"
    Feb 24 12:22:50.819: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 24 12:22:50.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7962" for this suite. 02/24/23 12:22:50.855
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:22:50.87
Feb 24 12:22:50.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename cronjob 02/24/23 12:22:50.872
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:50.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:50.914
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/24/23 12:22:50.919
STEP: Ensuring a job is scheduled 02/24/23 12:22:50.927
STEP: Ensuring exactly one is scheduled 02/24/23 12:23:00.933
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 12:23:00.94
STEP: Ensuring no more jobs are scheduled 02/24/23 12:23:00.945
STEP: Removing cronjob 02/24/23 12:28:00.955
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 24 12:28:00.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5939" for this suite. 02/24/23 12:28:00.971
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":340,"skipped":6304,"failed":0}
------------------------------
• [SLOW TEST] [310.110 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:22:50.87
    Feb 24 12:22:50.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename cronjob 02/24/23 12:22:50.872
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:22:50.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:22:50.914
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/24/23 12:22:50.919
    STEP: Ensuring a job is scheduled 02/24/23 12:22:50.927
    STEP: Ensuring exactly one is scheduled 02/24/23 12:23:00.933
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/24/23 12:23:00.94
    STEP: Ensuring no more jobs are scheduled 02/24/23 12:23:00.945
    STEP: Removing cronjob 02/24/23 12:28:00.955
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 24 12:28:00.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5939" for this suite. 02/24/23 12:28:00.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:00.981
Feb 24 12:28:00.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:28:00.982
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:01.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:01.054
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Feb 24 12:28:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 12:28:07.097
Feb 24 12:28:07.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 create -f -'
Feb 24 12:28:07.848: INFO: stderr: ""
Feb 24 12:28:07.848: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 24 12:28:07.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 delete e2e-test-crd-publish-openapi-7778-crds test-cr'
Feb 24 12:28:07.954: INFO: stderr: ""
Feb 24 12:28:07.954: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 24 12:28:07.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 apply -f -'
Feb 24 12:28:08.331: INFO: stderr: ""
Feb 24 12:28:08.331: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 24 12:28:08.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 delete e2e-test-crd-publish-openapi-7778-crds test-cr'
Feb 24 12:28:08.496: INFO: stderr: ""
Feb 24 12:28:08.496: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/24/23 12:28:08.496
Feb 24 12:28:08.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 explain e2e-test-crd-publish-openapi-7778-crds'
Feb 24 12:28:09.599: INFO: stderr: ""
Feb 24 12:28:09.599: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7778-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:28:13.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3770" for this suite. 02/24/23 12:28:13.17
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":341,"skipped":6317,"failed":0}
------------------------------
• [SLOW TEST] [12.202 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:00.981
    Feb 24 12:28:00.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-publish-openapi 02/24/23 12:28:00.982
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:01.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:01.054
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Feb 24 12:28:01.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/24/23 12:28:07.097
    Feb 24 12:28:07.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 create -f -'
    Feb 24 12:28:07.848: INFO: stderr: ""
    Feb 24 12:28:07.848: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 24 12:28:07.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 delete e2e-test-crd-publish-openapi-7778-crds test-cr'
    Feb 24 12:28:07.954: INFO: stderr: ""
    Feb 24 12:28:07.954: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 24 12:28:07.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 apply -f -'
    Feb 24 12:28:08.331: INFO: stderr: ""
    Feb 24 12:28:08.331: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 24 12:28:08.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 --namespace=crd-publish-openapi-3770 delete e2e-test-crd-publish-openapi-7778-crds test-cr'
    Feb 24 12:28:08.496: INFO: stderr: ""
    Feb 24 12:28:08.496: INFO: stdout: "e2e-test-crd-publish-openapi-7778-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/24/23 12:28:08.496
    Feb 24 12:28:08.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=crd-publish-openapi-3770 explain e2e-test-crd-publish-openapi-7778-crds'
    Feb 24 12:28:09.599: INFO: stderr: ""
    Feb 24 12:28:09.599: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7778-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:28:13.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3770" for this suite. 02/24/23 12:28:13.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:13.183
Feb 24 12:28:13.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:28:13.184
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:13.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:13.208
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 02/24/23 12:28:13.211
STEP: Creating a ResourceQuota 02/24/23 12:28:18.222
STEP: Ensuring resource quota status is calculated 02/24/23 12:28:18.23
STEP: Creating a Service 02/24/23 12:28:20.258
STEP: Creating a NodePort Service 02/24/23 12:28:20.367
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/24/23 12:28:20.485
STEP: Ensuring resource quota status captures service creation 02/24/23 12:28:20.576
STEP: Deleting Services 02/24/23 12:28:22.583
STEP: Ensuring resource quota status released usage 02/24/23 12:28:22.795
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 12:28:24.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-382" for this suite. 02/24/23 12:28:24.813
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":342,"skipped":6322,"failed":0}
------------------------------
• [SLOW TEST] [11.648 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:13.183
    Feb 24 12:28:13.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:28:13.184
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:13.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:13.208
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 02/24/23 12:28:13.211
    STEP: Creating a ResourceQuota 02/24/23 12:28:18.222
    STEP: Ensuring resource quota status is calculated 02/24/23 12:28:18.23
    STEP: Creating a Service 02/24/23 12:28:20.258
    STEP: Creating a NodePort Service 02/24/23 12:28:20.367
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/24/23 12:28:20.485
    STEP: Ensuring resource quota status captures service creation 02/24/23 12:28:20.576
    STEP: Deleting Services 02/24/23 12:28:22.583
    STEP: Ensuring resource quota status released usage 02/24/23 12:28:22.795
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 12:28:24.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-382" for this suite. 02/24/23 12:28:24.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:24.832
Feb 24 12:28:24.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 12:28:24.833
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:24.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:24.91
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 02/24/23 12:28:24.912
Feb 24 12:28:24.935: INFO: Waiting up to 5m0s for pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e" in namespace "var-expansion-3395" to be "Succeeded or Failed"
Feb 24 12:28:24.954: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.290023ms
Feb 24 12:28:26.960: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024548084s
Feb 24 12:28:28.960: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02486184s
STEP: Saw pod success 02/24/23 12:28:28.96
Feb 24 12:28:28.961: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e" satisfied condition "Succeeded or Failed"
Feb 24 12:28:28.965: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e container dapi-container: <nil>
STEP: delete the pod 02/24/23 12:28:28.983
Feb 24 12:28:29.005: INFO: Waiting for pod var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e to disappear
Feb 24 12:28:29.010: INFO: Pod var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 12:28:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3395" for this suite. 02/24/23 12:28:29.017
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":343,"skipped":6327,"failed":0}
------------------------------
• [4.195 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:24.832
    Feb 24 12:28:24.833: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 12:28:24.833
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:24.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:24.91
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 02/24/23 12:28:24.912
    Feb 24 12:28:24.935: INFO: Waiting up to 5m0s for pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e" in namespace "var-expansion-3395" to be "Succeeded or Failed"
    Feb 24 12:28:24.954: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.290023ms
    Feb 24 12:28:26.960: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024548084s
    Feb 24 12:28:28.960: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02486184s
    STEP: Saw pod success 02/24/23 12:28:28.96
    Feb 24 12:28:28.961: INFO: Pod "var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e" satisfied condition "Succeeded or Failed"
    Feb 24 12:28:28.965: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e container dapi-container: <nil>
    STEP: delete the pod 02/24/23 12:28:28.983
    Feb 24 12:28:29.005: INFO: Waiting for pod var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e to disappear
    Feb 24 12:28:29.010: INFO: Pod var-expansion-eb68bd81-3418-4ce5-ba23-42d970eef62e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 12:28:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3395" for this suite. 02/24/23 12:28:29.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:29.03
Feb 24 12:28:29.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename endpointslice 02/24/23 12:28:29.031
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:29.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:29.057
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Feb 24 12:28:29.081: INFO: Endpoints addresses: [172.31.215.128 172.31.216.150 172.31.217.245] , ports: [6443]
Feb 24 12:28:29.081: INFO: EndpointSlices addresses: [172.31.215.128 172.31.216.150 172.31.217.245] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 24 12:28:29.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6601" for this suite. 02/24/23 12:28:29.087
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":344,"skipped":6347,"failed":0}
------------------------------
• [0.076 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:29.03
    Feb 24 12:28:29.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename endpointslice 02/24/23 12:28:29.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:29.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:29.057
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Feb 24 12:28:29.081: INFO: Endpoints addresses: [172.31.215.128 172.31.216.150 172.31.217.245] , ports: [6443]
    Feb 24 12:28:29.081: INFO: EndpointSlices addresses: [172.31.215.128 172.31.216.150 172.31.217.245] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 24 12:28:29.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6601" for this suite. 02/24/23 12:28:29.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:29.107
Feb 24 12:28:29.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename job 02/24/23 12:28:29.108
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:29.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:29.135
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 02/24/23 12:28:29.137
STEP: Ensuring job reaches completions 02/24/23 12:28:29.145
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 24 12:28:41.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3189" for this suite. 02/24/23 12:28:41.176
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":345,"skipped":6355,"failed":0}
------------------------------
• [SLOW TEST] [12.082 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:29.107
    Feb 24 12:28:29.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename job 02/24/23 12:28:29.108
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:29.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:29.135
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 02/24/23 12:28:29.137
    STEP: Ensuring job reaches completions 02/24/23 12:28:29.145
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 24 12:28:41.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3189" for this suite. 02/24/23 12:28:41.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:41.193
Feb 24 12:28:41.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:28:41.194
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:41.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:41.222
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:28:41.248
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:28:41.678
STEP: Deploying the webhook pod 02/24/23 12:28:41.693
STEP: Wait for the deployment to be ready 02/24/23 12:28:41.718
Feb 24 12:28:41.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:28:43.755
STEP: Verifying the service has paired with the endpoint 02/24/23 12:28:43.857
Feb 24 12:28:44.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/24/23 12:28:44.864
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:44.864
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/24/23 12:28:44.884
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/24/23 12:28:45.899
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:45.899
STEP: Having no error when timeout is longer than webhook latency 02/24/23 12:28:47.024
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:47.025
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/24/23 12:28:52.08
STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:52.08
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:28:57.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8592" for this suite. 02/24/23 12:28:57.139
STEP: Destroying namespace "webhook-8592-markers" for this suite. 02/24/23 12:28:57.149
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":346,"skipped":6376,"failed":0}
------------------------------
• [SLOW TEST] [16.092 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:41.193
    Feb 24 12:28:41.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:28:41.194
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:41.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:41.222
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:28:41.248
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:28:41.678
    STEP: Deploying the webhook pod 02/24/23 12:28:41.693
    STEP: Wait for the deployment to be ready 02/24/23 12:28:41.718
    Feb 24 12:28:41.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:28:43.755
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:28:43.857
    Feb 24 12:28:44.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/24/23 12:28:44.864
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:44.864
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/24/23 12:28:44.884
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/24/23 12:28:45.899
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:45.899
    STEP: Having no error when timeout is longer than webhook latency 02/24/23 12:28:47.024
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:47.025
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/24/23 12:28:52.08
    STEP: Registering slow webhook via the AdmissionRegistration API 02/24/23 12:28:52.08
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:28:57.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8592" for this suite. 02/24/23 12:28:57.139
    STEP: Destroying namespace "webhook-8592-markers" for this suite. 02/24/23 12:28:57.149
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:28:57.287
Feb 24 12:28:57.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:28:57.288
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:57.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:57.329
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f407ddca-98be-46a9-8a2e-b8b94907153a 02/24/23 12:28:57.351
STEP: Creating the pod 02/24/23 12:28:57.361
Feb 24 12:28:57.385: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb" in namespace "projected-1601" to be "running and ready"
Feb 24 12:28:57.394: INFO: Pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.001349ms
Feb 24 12:28:57.394: INFO: The phase of Pod pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:28:59.400: INFO: Pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014303933s
Feb 24 12:28:59.400: INFO: The phase of Pod pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb is Running (Ready = true)
Feb 24 12:28:59.400: INFO: Pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-f407ddca-98be-46a9-8a2e-b8b94907153a 02/24/23 12:28:59.414
STEP: waiting to observe update in volume 02/24/23 12:28:59.423
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 24 12:29:01.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1601" for this suite. 02/24/23 12:29:01.456
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":347,"skipped":6410,"failed":0}
------------------------------
• [4.182 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:28:57.287
    Feb 24 12:28:57.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:28:57.288
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:28:57.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:28:57.329
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-f407ddca-98be-46a9-8a2e-b8b94907153a 02/24/23 12:28:57.351
    STEP: Creating the pod 02/24/23 12:28:57.361
    Feb 24 12:28:57.385: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb" in namespace "projected-1601" to be "running and ready"
    Feb 24 12:28:57.394: INFO: Pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.001349ms
    Feb 24 12:28:57.394: INFO: The phase of Pod pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:28:59.400: INFO: Pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014303933s
    Feb 24 12:28:59.400: INFO: The phase of Pod pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb is Running (Ready = true)
    Feb 24 12:28:59.400: INFO: Pod "pod-projected-configmaps-bf242c81-1fb9-49b3-ae13-9c939ffae2fb" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-f407ddca-98be-46a9-8a2e-b8b94907153a 02/24/23 12:28:59.414
    STEP: waiting to observe update in volume 02/24/23 12:28:59.423
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 24 12:29:01.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1601" for this suite. 02/24/23 12:29:01.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:29:01.471
Feb 24 12:29:01.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename secrets 02/24/23 12:29:01.474
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:01.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:01.512
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-3b80ac0b-e149-4557-980f-52078f2981b1 02/24/23 12:29:01.515
STEP: Creating a pod to test consume secrets 02/24/23 12:29:01.527
Feb 24 12:29:01.556: INFO: Waiting up to 5m0s for pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc" in namespace "secrets-4695" to be "Succeeded or Failed"
Feb 24 12:29:01.614: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc": Phase="Pending", Reason="", readiness=false. Elapsed: 57.76682ms
Feb 24 12:29:03.621: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc": Phase="Running", Reason="", readiness=false. Elapsed: 2.064120449s
Feb 24 12:29:05.622: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065781004s
STEP: Saw pod success 02/24/23 12:29:05.622
Feb 24 12:29:05.623: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc" satisfied condition "Succeeded or Failed"
Feb 24 12:29:05.627: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc container secret-volume-test: <nil>
STEP: delete the pod 02/24/23 12:29:05.641
Feb 24 12:29:05.666: INFO: Waiting for pod pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc to disappear
Feb 24 12:29:05.676: INFO: Pod pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 24 12:29:05.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4695" for this suite. 02/24/23 12:29:05.692
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":348,"skipped":6436,"failed":0}
------------------------------
• [4.236 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:29:01.471
    Feb 24 12:29:01.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename secrets 02/24/23 12:29:01.474
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:01.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:01.512
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-3b80ac0b-e149-4557-980f-52078f2981b1 02/24/23 12:29:01.515
    STEP: Creating a pod to test consume secrets 02/24/23 12:29:01.527
    Feb 24 12:29:01.556: INFO: Waiting up to 5m0s for pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc" in namespace "secrets-4695" to be "Succeeded or Failed"
    Feb 24 12:29:01.614: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc": Phase="Pending", Reason="", readiness=false. Elapsed: 57.76682ms
    Feb 24 12:29:03.621: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc": Phase="Running", Reason="", readiness=false. Elapsed: 2.064120449s
    Feb 24 12:29:05.622: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065781004s
    STEP: Saw pod success 02/24/23 12:29:05.622
    Feb 24 12:29:05.623: INFO: Pod "pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc" satisfied condition "Succeeded or Failed"
    Feb 24 12:29:05.627: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc container secret-volume-test: <nil>
    STEP: delete the pod 02/24/23 12:29:05.641
    Feb 24 12:29:05.666: INFO: Waiting for pod pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc to disappear
    Feb 24 12:29:05.676: INFO: Pod pod-secrets-47d0e9e8-5a17-4180-8492-c68f7b4076fc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 24 12:29:05.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4695" for this suite. 02/24/23 12:29:05.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:29:05.727
Feb 24 12:29:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename init-container 02/24/23 12:29:05.728
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:05.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:05.792
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 02/24/23 12:29:05.813
Feb 24 12:29:05.814: INFO: PodSpec: initContainers in spec.initContainers
Feb 24 12:29:47.869: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c0330e56-fb24-4477-9853-0347bdd6e9ed", GenerateName:"", Namespace:"init-container-5032", SelfLink:"", UID:"8ccb9671-8446-48e4-8db3-c5b9015e438c", ResourceVersion:"46722", Generation:0, CreationTimestamp:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"814122492"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b58b3e6e49ddb3a655329d69d34dd379c3aa7925be88ddc321eca544530db6e5", "cni.projectcalico.org/podIP":"10.244.4.242/32", "cni.projectcalico.org/podIPs":"10.244.4.242/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003b8ba0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 29, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003b8be8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 29, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003b8c30), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-xzxjv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004fc2300), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xzxjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xzxjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xzxjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f0abf0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-215-124.eu-west-3.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003e86620), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f0ad30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f0ad50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f0ad58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f0ad5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004956240), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.215.124", PodIP:"10.244.4.242", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.4.242"}}, StartTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003e86700)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003e86770)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://006bbcb8b8dec126fe7b00c0caf7748fcab5354e466a9445049b46e8542905e0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004fc2380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004fc2360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f0addf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 24 12:29:47.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5032" for this suite. 02/24/23 12:29:47.877
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":349,"skipped":6463,"failed":0}
------------------------------
• [SLOW TEST] [42.164 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:29:05.727
    Feb 24 12:29:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename init-container 02/24/23 12:29:05.728
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:05.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:05.792
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 02/24/23 12:29:05.813
    Feb 24 12:29:05.814: INFO: PodSpec: initContainers in spec.initContainers
    Feb 24 12:29:47.869: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c0330e56-fb24-4477-9853-0347bdd6e9ed", GenerateName:"", Namespace:"init-container-5032", SelfLink:"", UID:"8ccb9671-8446-48e4-8db3-c5b9015e438c", ResourceVersion:"46722", Generation:0, CreationTimestamp:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"814122492"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b58b3e6e49ddb3a655329d69d34dd379c3aa7925be88ddc321eca544530db6e5", "cni.projectcalico.org/podIP":"10.244.4.242/32", "cni.projectcalico.org/podIPs":"10.244.4.242/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003b8ba0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 29, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003b8be8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 24, 12, 29, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0003b8c30), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-xzxjv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004fc2300), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xzxjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xzxjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-xzxjv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f0abf0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-215-124.eu-west-3.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003e86620), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f0ad30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f0ad50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f0ad58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f0ad5c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004956240), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.215.124", PodIP:"10.244.4.242", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.4.242"}}, StartTime:time.Date(2023, time.February, 24, 12, 29, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003e86700)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003e86770)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://006bbcb8b8dec126fe7b00c0caf7748fcab5354e466a9445049b46e8542905e0", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004fc2380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004fc2360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f0addf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 24 12:29:47.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5032" for this suite. 02/24/23 12:29:47.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:29:47.9
Feb 24 12:29:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename endpointslice 02/24/23 12:29:47.902
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:47.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:47.924
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 02/24/23 12:29:47.926
STEP: getting /apis/discovery.k8s.io 02/24/23 12:29:47.928
STEP: getting /apis/discovery.k8s.iov1 02/24/23 12:29:47.928
STEP: creating 02/24/23 12:29:47.929
STEP: getting 02/24/23 12:29:47.952
STEP: listing 02/24/23 12:29:47.957
STEP: watching 02/24/23 12:29:47.962
Feb 24 12:29:47.962: INFO: starting watch
STEP: cluster-wide listing 02/24/23 12:29:47.963
STEP: cluster-wide watching 02/24/23 12:29:47.969
Feb 24 12:29:47.970: INFO: starting watch
STEP: patching 02/24/23 12:29:47.971
STEP: updating 02/24/23 12:29:47.979
Feb 24 12:29:48.001: INFO: waiting for watch events with expected annotations
Feb 24 12:29:48.001: INFO: saw patched and updated annotations
STEP: deleting 02/24/23 12:29:48.001
STEP: deleting a collection 02/24/23 12:29:48.033
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 24 12:29:48.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1322" for this suite. 02/24/23 12:29:48.07
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":350,"skipped":6482,"failed":0}
------------------------------
• [0.182 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:29:47.9
    Feb 24 12:29:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename endpointslice 02/24/23 12:29:47.902
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:47.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:47.924
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 02/24/23 12:29:47.926
    STEP: getting /apis/discovery.k8s.io 02/24/23 12:29:47.928
    STEP: getting /apis/discovery.k8s.iov1 02/24/23 12:29:47.928
    STEP: creating 02/24/23 12:29:47.929
    STEP: getting 02/24/23 12:29:47.952
    STEP: listing 02/24/23 12:29:47.957
    STEP: watching 02/24/23 12:29:47.962
    Feb 24 12:29:47.962: INFO: starting watch
    STEP: cluster-wide listing 02/24/23 12:29:47.963
    STEP: cluster-wide watching 02/24/23 12:29:47.969
    Feb 24 12:29:47.970: INFO: starting watch
    STEP: patching 02/24/23 12:29:47.971
    STEP: updating 02/24/23 12:29:47.979
    Feb 24 12:29:48.001: INFO: waiting for watch events with expected annotations
    Feb 24 12:29:48.001: INFO: saw patched and updated annotations
    STEP: deleting 02/24/23 12:29:48.001
    STEP: deleting a collection 02/24/23 12:29:48.033
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 24 12:29:48.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1322" for this suite. 02/24/23 12:29:48.07
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:29:48.084
Feb 24 12:29:48.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:29:48.085
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:48.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:48.118
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 12:29:48.12
Feb 24 12:29:48.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1587 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 24 12:29:48.211: INFO: stderr: ""
Feb 24 12:29:48.211: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/24/23 12:29:48.211
Feb 24 12:29:48.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1587 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Feb 24 12:29:48.965: INFO: stderr: ""
Feb 24 12:29:48.965: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 12:29:48.965
Feb 24 12:29:48.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1587 delete pods e2e-test-httpd-pod'
Feb 24 12:29:51.577: INFO: stderr: ""
Feb 24 12:29:51.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:29:51.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1587" for this suite. 02/24/23 12:29:51.586
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":351,"skipped":6484,"failed":0}
------------------------------
• [3.525 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:29:48.084
    Feb 24 12:29:48.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:29:48.085
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:48.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:48.118
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 12:29:48.12
    Feb 24 12:29:48.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1587 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 24 12:29:48.211: INFO: stderr: ""
    Feb 24 12:29:48.211: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/24/23 12:29:48.211
    Feb 24 12:29:48.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1587 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Feb 24 12:29:48.965: INFO: stderr: ""
    Feb 24 12:29:48.965: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/24/23 12:29:48.965
    Feb 24 12:29:48.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-1587 delete pods e2e-test-httpd-pod'
    Feb 24 12:29:51.577: INFO: stderr: ""
    Feb 24 12:29:51.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:29:51.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1587" for this suite. 02/24/23 12:29:51.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:29:51.609
Feb 24 12:29:51.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename kubectl 02/24/23 12:29:51.615
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:51.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:51.657
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Feb 24 12:29:51.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 create -f -'
Feb 24 12:29:52.589: INFO: stderr: ""
Feb 24 12:29:52.589: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 24 12:29:52.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 create -f -'
Feb 24 12:29:52.852: INFO: stderr: ""
Feb 24 12:29:52.852: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/24/23 12:29:52.852
Feb 24 12:29:53.858: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 12:29:53.858: INFO: Found 1 / 1
Feb 24 12:29:53.858: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 24 12:29:53.862: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 24 12:29:53.862: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 24 12:29:53.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe pod agnhost-primary-sk7pw'
Feb 24 12:29:53.988: INFO: stderr: ""
Feb 24 12:29:53.988: INFO: stdout: "Name:             agnhost-primary-sk7pw\nNamespace:        kubectl-6335\nPriority:         0\nService Account:  default\nNode:             ip-172-31-216-47.eu-west-3.compute.internal/172.31.216.47\nStart Time:       Fri, 24 Feb 2023 12:29:52 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: efd3eb808df20c5406da48fb7cfd55f5e958242222286e8a2c60e757c37c2eed\n                  cni.projectcalico.org/podIP: 10.244.3.117/32\n                  cni.projectcalico.org/podIPs: 10.244.3.117/32\nStatus:           Running\nIP:               10.244.3.117\nIPs:\n  IP:           10.244.3.117\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://1e145f065fefe30a8453f3ca718b1bbdb360f4e884efeeb087a5e2ea74a1db30\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 24 Feb 2023 12:29:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tzdpd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-tzdpd:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-6335/agnhost-primary-sk7pw to ip-172-31-216-47.eu-west-3.compute.internal\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Feb 24 12:29:53.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe rc agnhost-primary'
Feb 24 12:29:54.142: INFO: stderr: ""
Feb 24 12:29:54.142: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6335\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-sk7pw\n"
Feb 24 12:29:54.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe service agnhost-primary'
Feb 24 12:29:54.236: INFO: stderr: ""
Feb 24 12:29:54.236: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6335\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.135.202\nIPs:               10.100.135.202\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.117:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 24 12:29:54.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe node ip-172-31-215-124.eu-west-3.compute.internal'
Feb 24 12:29:54.370: INFO: stderr: ""
Feb 24 12:29:54.370: INFO: stdout: "Name:               ip-172-31-215-124.eu-west-3.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-3\n                    failure-domain.beta.kubernetes.io/zone=eu-west-3a\n                    isSpotInstance=false\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-215-124\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=241307b4-7d30-4219-8883-1be236a767be\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.ebs.csi.aws.com/zone=eu-west-3a\n                    topology.kubernetes.io/region=eu-west-3\n                    topology.kubernetes.io/zone=eu-west-3a\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    workerset=k8s-conformance-1-eu-west-3a\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.31.215.124\n                    cluster.k8s.io/machine: kube-system/k8s-conformance-1-eu-west-3a-d86474544-gxr5g\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-06306eb2e448af4bb\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"4a:a4:f1:5e:2b:94\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.215.124\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.215.124/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.4.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 24 Feb 2023 10:58:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-215-124.eu-west-3.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 24 Feb 2023 12:29:44 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 24 Feb 2023 10:59:04 +0000   Fri, 24 Feb 2023 10:59:04 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:51 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.215.124\n  ExternalIP:   52.47.169.136\n  Hostname:     ip-172-31-215-124.eu-west-3.compute.internal\n  InternalDNS:  ip-172-31-215-124.eu-west-3.compute.internal\n  ExternalDNS:  ec2-52-47-169-136.eu-west-3.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           50620216Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3956688Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         1600m\n  ephemeral-storage:           44504107341\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3444688Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec24419d7d5423e3e1c61e0954d0a3f7\n  System UUID:                ec24419d-7d54-23e3-e1c6-1e0954d0a3f7\n  Boot ID:                    7095506a-42e6-4872-9dbc-7fb61cf1f3ba\n  Kernel Version:             5.15.0-1030-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.244.4.0/24\nPodCIDRs:                     10.244.4.0/24\nProviderID:                   aws:///eu-west-3a/i-06306eb2e448af4bb\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-spws5                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 ebs-csi-node-j9x4j                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 kube-proxy-bxx6d                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 node-local-dns-7l6g8                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         91m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         275m (17%)  0 (0%)\n  memory                      5Mi (0%)    0 (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Feb 24 12:29:54.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe namespace kubectl-6335'
Feb 24 12:29:54.459: INFO: stderr: ""
Feb 24 12:29:54.459: INFO: stdout: "Name:         kubectl-6335\nLabels:       e2e-framework=kubectl\n              e2e-run=3953cf66-917a-4eb2-b776-6d0d4ab06829\n              kubernetes.io/metadata.name=kubectl-6335\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 24 12:29:54.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6335" for this suite. 02/24/23 12:29:54.465
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":352,"skipped":6493,"failed":0}
------------------------------
• [2.866 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:29:51.609
    Feb 24 12:29:51.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename kubectl 02/24/23 12:29:51.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:51.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:51.657
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Feb 24 12:29:51.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 create -f -'
    Feb 24 12:29:52.589: INFO: stderr: ""
    Feb 24 12:29:52.589: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 24 12:29:52.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 create -f -'
    Feb 24 12:29:52.852: INFO: stderr: ""
    Feb 24 12:29:52.852: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/24/23 12:29:52.852
    Feb 24 12:29:53.858: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 12:29:53.858: INFO: Found 1 / 1
    Feb 24 12:29:53.858: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 24 12:29:53.862: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 24 12:29:53.862: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 24 12:29:53.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe pod agnhost-primary-sk7pw'
    Feb 24 12:29:53.988: INFO: stderr: ""
    Feb 24 12:29:53.988: INFO: stdout: "Name:             agnhost-primary-sk7pw\nNamespace:        kubectl-6335\nPriority:         0\nService Account:  default\nNode:             ip-172-31-216-47.eu-west-3.compute.internal/172.31.216.47\nStart Time:       Fri, 24 Feb 2023 12:29:52 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: efd3eb808df20c5406da48fb7cfd55f5e958242222286e8a2c60e757c37c2eed\n                  cni.projectcalico.org/podIP: 10.244.3.117/32\n                  cni.projectcalico.org/podIPs: 10.244.3.117/32\nStatus:           Running\nIP:               10.244.3.117\nIPs:\n  IP:           10.244.3.117\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://1e145f065fefe30a8453f3ca718b1bbdb360f4e884efeeb087a5e2ea74a1db30\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 24 Feb 2023 12:29:53 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tzdpd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-tzdpd:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-6335/agnhost-primary-sk7pw to ip-172-31-216-47.eu-west-3.compute.internal\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Feb 24 12:29:53.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe rc agnhost-primary'
    Feb 24 12:29:54.142: INFO: stderr: ""
    Feb 24 12:29:54.142: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6335\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-sk7pw\n"
    Feb 24 12:29:54.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe service agnhost-primary'
    Feb 24 12:29:54.236: INFO: stderr: ""
    Feb 24 12:29:54.236: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6335\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.135.202\nIPs:               10.100.135.202\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.117:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 24 12:29:54.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe node ip-172-31-215-124.eu-west-3.compute.internal'
    Feb 24 12:29:54.370: INFO: stderr: ""
    Feb 24 12:29:54.370: INFO: stdout: "Name:               ip-172-31-215-124.eu-west-3.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-3\n                    failure-domain.beta.kubernetes.io/zone=eu-west-3a\n                    isSpotInstance=false\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-215-124\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=241307b4-7d30-4219-8883-1be236a767be\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.ebs.csi.aws.com/zone=eu-west-3a\n                    topology.kubernetes.io/region=eu-west-3\n                    topology.kubernetes.io/zone=eu-west-3a\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    workerset=k8s-conformance-1-eu-west-3a\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 172.31.215.124\n                    cluster.k8s.io/machine: kube-system/k8s-conformance-1-eu-west-3a-d86474544-gxr5g\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-06306eb2e448af4bb\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"4a:a4:f1:5e:2b:94\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.215.124\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.215.124/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.4.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 24 Feb 2023 10:58:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-215-124.eu-west-3.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 24 Feb 2023 12:29:44 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 24 Feb 2023 10:59:04 +0000   Fri, 24 Feb 2023 10:59:04 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 24 Feb 2023 12:29:36 +0000   Fri, 24 Feb 2023 10:58:51 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.215.124\n  ExternalIP:   52.47.169.136\n  Hostname:     ip-172-31-215-124.eu-west-3.compute.internal\n  InternalDNS:  ip-172-31-215-124.eu-west-3.compute.internal\n  ExternalDNS:  ec2-52-47-169-136.eu-west-3.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           50620216Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3956688Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         1600m\n  ephemeral-storage:           44504107341\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3444688Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec24419d7d5423e3e1c61e0954d0a3f7\n  System UUID:                ec24419d-7d54-23e3-e1c6-1e0954d0a3f7\n  Boot ID:                    7095506a-42e6-4872-9dbc-7fb61cf1f3ba\n  Kernel Version:             5.15.0-1030-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.244.4.0/24\nPodCIDRs:                     10.244.4.0/24\nProviderID:                   aws:///eu-west-3a/i-06306eb2e448af4bb\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-spws5                                                250m (15%)    0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 ebs-csi-node-j9x4j                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 kube-proxy-bxx6d                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 node-local-dns-7l6g8                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         91m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-df72231a48044239-5zhnp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         275m (17%)  0 (0%)\n  memory                      5Mi (0%)    0 (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
    Feb 24 12:29:54.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1678170937 --namespace=kubectl-6335 describe namespace kubectl-6335'
    Feb 24 12:29:54.459: INFO: stderr: ""
    Feb 24 12:29:54.459: INFO: stdout: "Name:         kubectl-6335\nLabels:       e2e-framework=kubectl\n              e2e-run=3953cf66-917a-4eb2-b776-6d0d4ab06829\n              kubernetes.io/metadata.name=kubectl-6335\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 24 12:29:54.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6335" for this suite. 02/24/23 12:29:54.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:29:54.475
Feb 24 12:29:54.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename webhook 02/24/23 12:29:54.476
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:54.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:54.499
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/24/23 12:29:54.517
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:29:54.99
STEP: Deploying the webhook pod 02/24/23 12:29:55
STEP: Wait for the deployment to be ready 02/24/23 12:29:55.019
Feb 24 12:29:55.041: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/24/23 12:29:57.056
STEP: Verifying the service has paired with the endpoint 02/24/23 12:29:57.098
Feb 24 12:29:58.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 12:29:58.105
STEP: create a pod that should be denied by the webhook 02/24/23 12:29:58.127
STEP: create a pod that causes the webhook to hang 02/24/23 12:29:58.141
STEP: create a configmap that should be denied by the webhook 02/24/23 12:30:08.15
STEP: create a configmap that should be admitted by the webhook 02/24/23 12:30:08.187
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 12:30:08.211
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 12:30:08.225
STEP: create a namespace that bypass the webhook 02/24/23 12:30:08.234
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/24/23 12:30:08.248
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:30:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3134" for this suite. 02/24/23 12:30:08.315
STEP: Destroying namespace "webhook-3134-markers" for this suite. 02/24/23 12:30:08.33
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":353,"skipped":6519,"failed":0}
------------------------------
• [SLOW TEST] [14.003 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:29:54.475
    Feb 24 12:29:54.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename webhook 02/24/23 12:29:54.476
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:29:54.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:29:54.499
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/24/23 12:29:54.517
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/24/23 12:29:54.99
    STEP: Deploying the webhook pod 02/24/23 12:29:55
    STEP: Wait for the deployment to be ready 02/24/23 12:29:55.019
    Feb 24 12:29:55.041: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/24/23 12:29:57.056
    STEP: Verifying the service has paired with the endpoint 02/24/23 12:29:57.098
    Feb 24 12:29:58.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 02/24/23 12:29:58.105
    STEP: create a pod that should be denied by the webhook 02/24/23 12:29:58.127
    STEP: create a pod that causes the webhook to hang 02/24/23 12:29:58.141
    STEP: create a configmap that should be denied by the webhook 02/24/23 12:30:08.15
    STEP: create a configmap that should be admitted by the webhook 02/24/23 12:30:08.187
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 12:30:08.211
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/24/23 12:30:08.225
    STEP: create a namespace that bypass the webhook 02/24/23 12:30:08.234
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/24/23 12:30:08.248
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:30:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3134" for this suite. 02/24/23 12:30:08.315
    STEP: Destroying namespace "webhook-3134-markers" for this suite. 02/24/23 12:30:08.33
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:30:08.485
Feb 24 12:30:08.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename replication-controller 02/24/23 12:30:08.491
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:30:08.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:30:08.529
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 02/24/23 12:30:08.546
Feb 24 12:30:08.565: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6265" to be "running and ready"
Feb 24 12:30:08.593: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 28.856978ms
Feb 24 12:30:08.594: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 24 12:30:10.605: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.039955218s
Feb 24 12:30:10.605: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 24 12:30:10.605: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/24/23 12:30:10.61
STEP: Then the orphan pod is adopted 02/24/23 12:30:10.622
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 24 12:30:11.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6265" for this suite. 02/24/23 12:30:11.654
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":354,"skipped":6536,"failed":0}
------------------------------
• [3.181 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:30:08.485
    Feb 24 12:30:08.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename replication-controller 02/24/23 12:30:08.491
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:30:08.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:30:08.529
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/24/23 12:30:08.546
    Feb 24 12:30:08.565: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6265" to be "running and ready"
    Feb 24 12:30:08.593: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 28.856978ms
    Feb 24 12:30:08.594: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 24 12:30:10.605: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.039955218s
    Feb 24 12:30:10.605: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 24 12:30:10.605: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/24/23 12:30:10.61
    STEP: Then the orphan pod is adopted 02/24/23 12:30:10.622
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 24 12:30:11.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6265" for this suite. 02/24/23 12:30:11.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:30:11.668
Feb 24 12:30:11.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename dns 02/24/23 12:30:11.669
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:30:11.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:30:11.704
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/24/23 12:30:11.707
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
 02/24/23 12:30:11.714
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
 02/24/23 12:30:11.715
STEP: creating a pod to probe DNS 02/24/23 12:30:11.715
STEP: submitting the pod to kubernetes 02/24/23 12:30:11.715
Feb 24 12:30:11.733: INFO: Waiting up to 15m0s for pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d" in namespace "dns-765" to be "running"
Feb 24 12:30:11.741: INFO: Pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.06057ms
Feb 24 12:30:13.751: INFO: Pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017640072s
Feb 24 12:30:13.751: INFO: Pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:30:13.751
STEP: looking for the results for each expected name from probers 02/24/23 12:30:13.761
Feb 24 12:30:13.781: INFO: DNS probes using dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d succeeded

STEP: deleting the pod 02/24/23 12:30:13.781
STEP: changing the externalName to bar.example.com 02/24/23 12:30:13.809
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
 02/24/23 12:30:13.827
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
 02/24/23 12:30:13.827
STEP: creating a second pod to probe DNS 02/24/23 12:30:13.827
STEP: submitting the pod to kubernetes 02/24/23 12:30:13.827
Feb 24 12:30:13.838: INFO: Waiting up to 15m0s for pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48" in namespace "dns-765" to be "running"
Feb 24 12:30:13.845: INFO: Pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48": Phase="Pending", Reason="", readiness=false. Elapsed: 7.338152ms
Feb 24 12:30:15.852: INFO: Pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48": Phase="Running", Reason="", readiness=true. Elapsed: 2.0137315s
Feb 24 12:30:15.852: INFO: Pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:30:15.852
STEP: looking for the results for each expected name from probers 02/24/23 12:30:15.858
Feb 24 12:30:15.867: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:15.874: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:15.874: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

Feb 24 12:30:20.882: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:20.890: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:20.890: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

Feb 24 12:30:25.883: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:25.889: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:25.889: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

Feb 24 12:30:30.884: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:30.895: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains '' instead of 'bar.example.com.'
Feb 24 12:30:30.895: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

Feb 24 12:30:35.883: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:35.890: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:35.890: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

Feb 24 12:30:40.881: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:40.887: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 24 12:30:40.887: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

Feb 24 12:30:45.889: INFO: DNS probes using dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 succeeded

STEP: deleting the pod 02/24/23 12:30:45.89
STEP: changing the service to type=ClusterIP 02/24/23 12:30:45.911
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
 02/24/23 12:30:46.019
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
 02/24/23 12:30:46.019
STEP: creating a third pod to probe DNS 02/24/23 12:30:46.019
STEP: submitting the pod to kubernetes 02/24/23 12:30:46.025
Feb 24 12:30:46.042: INFO: Waiting up to 15m0s for pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c" in namespace "dns-765" to be "running"
Feb 24 12:30:46.052: INFO: Pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.530517ms
Feb 24 12:30:48.062: INFO: Pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.019065085s
Feb 24 12:30:48.062: INFO: Pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c" satisfied condition "running"
STEP: retrieving the pod 02/24/23 12:30:48.062
STEP: looking for the results for each expected name from probers 02/24/23 12:30:48.067
Feb 24 12:30:48.081: INFO: DNS probes using dns-test-989115cf-eea9-464f-8fff-0f543e161f3c succeeded

STEP: deleting the pod 02/24/23 12:30:48.081
STEP: deleting the test externalName service 02/24/23 12:30:48.109
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 24 12:30:48.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-765" for this suite. 02/24/23 12:30:48.164
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":355,"skipped":6545,"failed":0}
------------------------------
• [SLOW TEST] [36.512 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:30:11.668
    Feb 24 12:30:11.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename dns 02/24/23 12:30:11.669
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:30:11.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:30:11.704
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/24/23 12:30:11.707
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
     02/24/23 12:30:11.714
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
     02/24/23 12:30:11.715
    STEP: creating a pod to probe DNS 02/24/23 12:30:11.715
    STEP: submitting the pod to kubernetes 02/24/23 12:30:11.715
    Feb 24 12:30:11.733: INFO: Waiting up to 15m0s for pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d" in namespace "dns-765" to be "running"
    Feb 24 12:30:11.741: INFO: Pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.06057ms
    Feb 24 12:30:13.751: INFO: Pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017640072s
    Feb 24 12:30:13.751: INFO: Pod "dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:30:13.751
    STEP: looking for the results for each expected name from probers 02/24/23 12:30:13.761
    Feb 24 12:30:13.781: INFO: DNS probes using dns-test-c9b361d0-34ba-4605-bde2-e6ca25d2272d succeeded

    STEP: deleting the pod 02/24/23 12:30:13.781
    STEP: changing the externalName to bar.example.com 02/24/23 12:30:13.809
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
     02/24/23 12:30:13.827
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
     02/24/23 12:30:13.827
    STEP: creating a second pod to probe DNS 02/24/23 12:30:13.827
    STEP: submitting the pod to kubernetes 02/24/23 12:30:13.827
    Feb 24 12:30:13.838: INFO: Waiting up to 15m0s for pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48" in namespace "dns-765" to be "running"
    Feb 24 12:30:13.845: INFO: Pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48": Phase="Pending", Reason="", readiness=false. Elapsed: 7.338152ms
    Feb 24 12:30:15.852: INFO: Pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48": Phase="Running", Reason="", readiness=true. Elapsed: 2.0137315s
    Feb 24 12:30:15.852: INFO: Pod "dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:30:15.852
    STEP: looking for the results for each expected name from probers 02/24/23 12:30:15.858
    Feb 24 12:30:15.867: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:15.874: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:15.874: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

    Feb 24 12:30:20.882: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:20.890: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:20.890: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

    Feb 24 12:30:25.883: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:25.889: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:25.889: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

    Feb 24 12:30:30.884: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:30.895: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains '' instead of 'bar.example.com.'
    Feb 24 12:30:30.895: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

    Feb 24 12:30:35.883: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:35.890: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:35.890: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

    Feb 24 12:30:40.881: INFO: File wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:40.887: INFO: File jessie_udp@dns-test-service-3.dns-765.svc.cluster.local from pod  dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 24 12:30:40.887: INFO: Lookups using dns-765/dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 failed for: [wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local jessie_udp@dns-test-service-3.dns-765.svc.cluster.local]

    Feb 24 12:30:45.889: INFO: DNS probes using dns-test-e06d1dfd-f7c2-41c6-b0b5-27020eed3c48 succeeded

    STEP: deleting the pod 02/24/23 12:30:45.89
    STEP: changing the service to type=ClusterIP 02/24/23 12:30:45.911
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
     02/24/23 12:30:46.019
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-765.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-765.svc.cluster.local; sleep 1; done
     02/24/23 12:30:46.019
    STEP: creating a third pod to probe DNS 02/24/23 12:30:46.019
    STEP: submitting the pod to kubernetes 02/24/23 12:30:46.025
    Feb 24 12:30:46.042: INFO: Waiting up to 15m0s for pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c" in namespace "dns-765" to be "running"
    Feb 24 12:30:46.052: INFO: Pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.530517ms
    Feb 24 12:30:48.062: INFO: Pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c": Phase="Running", Reason="", readiness=true. Elapsed: 2.019065085s
    Feb 24 12:30:48.062: INFO: Pod "dns-test-989115cf-eea9-464f-8fff-0f543e161f3c" satisfied condition "running"
    STEP: retrieving the pod 02/24/23 12:30:48.062
    STEP: looking for the results for each expected name from probers 02/24/23 12:30:48.067
    Feb 24 12:30:48.081: INFO: DNS probes using dns-test-989115cf-eea9-464f-8fff-0f543e161f3c succeeded

    STEP: deleting the pod 02/24/23 12:30:48.081
    STEP: deleting the test externalName service 02/24/23 12:30:48.109
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 24 12:30:48.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-765" for this suite. 02/24/23 12:30:48.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:30:48.187
Feb 24 12:30:48.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename crd-watch 02/24/23 12:30:48.193
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:30:48.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:30:48.262
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 24 12:30:48.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Creating first CR  02/24/23 12:30:50.958
Feb 24 12:30:50.966: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:30:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:30:50Z]] name:name1 resourceVersion:47251 uid:512c5a91-db15-45e6-8029-662b26567f6a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/24/23 12:31:00.967
Feb 24 12:31:00.975: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:31:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:00Z]] name:name2 resourceVersion:47330 uid:e2811b0c-6827-4292-8ff9-228eef1a5444] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/24/23 12:31:10.976
Feb 24 12:31:10.991: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:30:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:10Z]] name:name1 resourceVersion:47368 uid:512c5a91-db15-45e6-8029-662b26567f6a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/24/23 12:31:20.993
Feb 24 12:31:21.010: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:31:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:20Z]] name:name2 resourceVersion:47406 uid:e2811b0c-6827-4292-8ff9-228eef1a5444] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/24/23 12:31:31.011
Feb 24 12:31:31.033: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:30:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:10Z]] name:name1 resourceVersion:47443 uid:512c5a91-db15-45e6-8029-662b26567f6a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/24/23 12:31:41.034
Feb 24 12:31:41.044: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:31:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:20Z]] name:name2 resourceVersion:47480 uid:e2811b0c-6827-4292-8ff9-228eef1a5444] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 24 12:31:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-669" for this suite. 02/24/23 12:31:51.589
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":356,"skipped":6625,"failed":0}
------------------------------
• [SLOW TEST] [63.419 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:30:48.187
    Feb 24 12:30:48.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename crd-watch 02/24/23 12:30:48.193
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:30:48.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:30:48.262
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 24 12:30:48.296: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Creating first CR  02/24/23 12:30:50.958
    Feb 24 12:30:50.966: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:30:50Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:30:50Z]] name:name1 resourceVersion:47251 uid:512c5a91-db15-45e6-8029-662b26567f6a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/24/23 12:31:00.967
    Feb 24 12:31:00.975: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:31:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:00Z]] name:name2 resourceVersion:47330 uid:e2811b0c-6827-4292-8ff9-228eef1a5444] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/24/23 12:31:10.976
    Feb 24 12:31:10.991: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:30:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:10Z]] name:name1 resourceVersion:47368 uid:512c5a91-db15-45e6-8029-662b26567f6a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/24/23 12:31:20.993
    Feb 24 12:31:21.010: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:31:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:20Z]] name:name2 resourceVersion:47406 uid:e2811b0c-6827-4292-8ff9-228eef1a5444] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/24/23 12:31:31.011
    Feb 24 12:31:31.033: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:30:50Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:10Z]] name:name1 resourceVersion:47443 uid:512c5a91-db15-45e6-8029-662b26567f6a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/24/23 12:31:41.034
    Feb 24 12:31:41.044: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-24T12:31:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-24T12:31:20Z]] name:name2 resourceVersion:47480 uid:e2811b0c-6827-4292-8ff9-228eef1a5444] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 24 12:31:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-669" for this suite. 02/24/23 12:31:51.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:31:51.606
Feb 24 12:31:51.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename projected 02/24/23 12:31:51.607
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:51.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:51.649
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 02/24/23 12:31:51.652
Feb 24 12:31:51.675: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500" in namespace "projected-1929" to be "Succeeded or Failed"
Feb 24 12:31:51.694: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500": Phase="Pending", Reason="", readiness=false. Elapsed: 18.850786ms
Feb 24 12:31:53.700: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024768285s
Feb 24 12:31:55.702: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026803975s
STEP: Saw pod success 02/24/23 12:31:55.702
Feb 24 12:31:55.702: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500" satisfied condition "Succeeded or Failed"
Feb 24 12:31:55.707: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500 container client-container: <nil>
STEP: delete the pod 02/24/23 12:31:55.723
Feb 24 12:31:55.742: INFO: Waiting for pod downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500 to disappear
Feb 24 12:31:55.748: INFO: Pod downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 24 12:31:55.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1929" for this suite. 02/24/23 12:31:55.758
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":357,"skipped":6631,"failed":0}
------------------------------
• [4.187 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:31:51.606
    Feb 24 12:31:51.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename projected 02/24/23 12:31:51.607
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:51.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:51.649
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 02/24/23 12:31:51.652
    Feb 24 12:31:51.675: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500" in namespace "projected-1929" to be "Succeeded or Failed"
    Feb 24 12:31:51.694: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500": Phase="Pending", Reason="", readiness=false. Elapsed: 18.850786ms
    Feb 24 12:31:53.700: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024768285s
    Feb 24 12:31:55.702: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026803975s
    STEP: Saw pod success 02/24/23 12:31:55.702
    Feb 24 12:31:55.702: INFO: Pod "downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500" satisfied condition "Succeeded or Failed"
    Feb 24 12:31:55.707: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500 container client-container: <nil>
    STEP: delete the pod 02/24/23 12:31:55.723
    Feb 24 12:31:55.742: INFO: Waiting for pod downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500 to disappear
    Feb 24 12:31:55.748: INFO: Pod downwardapi-volume-2394a171-e85c-41b7-8741-cce8b10db500 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 24 12:31:55.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1929" for this suite. 02/24/23 12:31:55.758
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:31:55.799
Feb 24 12:31:55.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 12:31:55.801
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:55.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:55.912
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/24/23 12:31:55.927
STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:31:56.537
Feb 24 12:31:56.573: INFO: Pod name wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c: Found 0 pods out of 5
Feb 24 12:32:01.592: INFO: Pod name wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/24/23 12:32:01.592
Feb 24 12:32:01.592: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:01.597: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 5.263574ms
Feb 24 12:32:03.605: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254533s
Feb 24 12:32:05.604: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012109542s
Feb 24 12:32:07.604: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012018763s
Feb 24 12:32:09.607: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014487137s
Feb 24 12:32:11.603: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Running", Reason="", readiness=true. Elapsed: 10.011109355s
Feb 24 12:32:11.604: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j" satisfied condition "running"
Feb 24 12:32:11.604: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-dctjp" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:11.609: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-dctjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.999938ms
Feb 24 12:32:11.609: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-dctjp" satisfied condition "running"
Feb 24 12:32:11.609: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-kjjc9" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:11.615: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-kjjc9": Phase="Running", Reason="", readiness=true. Elapsed: 5.861381ms
Feb 24 12:32:11.616: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-kjjc9" satisfied condition "running"
Feb 24 12:32:11.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-mzqbj" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:11.621: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-mzqbj": Phase="Running", Reason="", readiness=true. Elapsed: 5.459886ms
Feb 24 12:32:11.621: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-mzqbj" satisfied condition "running"
Feb 24 12:32:11.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-p46rs" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:11.630: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-p46rs": Phase="Running", Reason="", readiness=true. Elapsed: 7.988244ms
Feb 24 12:32:11.630: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-p46rs" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c in namespace emptydir-wrapper-1882, will wait for the garbage collector to delete the pods 02/24/23 12:32:11.63
Feb 24 12:32:11.703: INFO: Deleting ReplicationController wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c took: 16.18975ms
Feb 24 12:32:11.809: INFO: Terminating ReplicationController wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c pods took: 105.806168ms
STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:32:15.418
Feb 24 12:32:15.443: INFO: Pod name wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff: Found 0 pods out of 5
Feb 24 12:32:20.456: INFO: Pod name wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/24/23 12:32:20.456
Feb 24 12:32:20.456: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:20.465: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024194ms
Feb 24 12:32:22.471: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015075054s
Feb 24 12:32:24.472: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016527276s
Feb 24 12:32:26.488: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032678132s
Feb 24 12:32:28.471: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015165068s
Feb 24 12:32:30.484: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Running", Reason="", readiness=true. Elapsed: 10.028418725s
Feb 24 12:32:30.484: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk" satisfied condition "running"
Feb 24 12:32:30.484: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-blqv2" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:30.490: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-blqv2": Phase="Running", Reason="", readiness=true. Elapsed: 5.899746ms
Feb 24 12:32:30.490: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-blqv2" satisfied condition "running"
Feb 24 12:32:30.490: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-dzvp7" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:30.496: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-dzvp7": Phase="Running", Reason="", readiness=true. Elapsed: 5.438889ms
Feb 24 12:32:30.496: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-dzvp7" satisfied condition "running"
Feb 24 12:32:30.496: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-hjvrd" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:30.503: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-hjvrd": Phase="Running", Reason="", readiness=true. Elapsed: 6.665823ms
Feb 24 12:32:30.503: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-hjvrd" satisfied condition "running"
Feb 24 12:32:30.503: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-qp9rv" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:30.508: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-qp9rv": Phase="Running", Reason="", readiness=true. Elapsed: 5.339433ms
Feb 24 12:32:30.508: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-qp9rv" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff in namespace emptydir-wrapper-1882, will wait for the garbage collector to delete the pods 02/24/23 12:32:30.508
Feb 24 12:32:30.577: INFO: Deleting ReplicationController wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff took: 10.888801ms
Feb 24 12:32:30.877: INFO: Terminating ReplicationController wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff pods took: 300.230928ms
STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:32:33.984
Feb 24 12:32:34.027: INFO: Pod name wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a: Found 0 pods out of 5
Feb 24 12:32:39.039: INFO: Pod name wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/24/23 12:32:39.039
Feb 24 12:32:39.039: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:39.045: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900604ms
Feb 24 12:32:41.054: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015533642s
Feb 24 12:32:43.051: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01233284s
Feb 24 12:32:45.052: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013487799s
Feb 24 12:32:47.056: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016989322s
Feb 24 12:32:49.051: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012389171s
Feb 24 12:32:49.051: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx" satisfied condition "running"
Feb 24 12:32:49.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-g2n7t" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:49.056: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-g2n7t": Phase="Running", Reason="", readiness=true. Elapsed: 4.950661ms
Feb 24 12:32:49.057: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-g2n7t" satisfied condition "running"
Feb 24 12:32:49.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-h88wr" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:49.062: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-h88wr": Phase="Running", Reason="", readiness=true. Elapsed: 4.985944ms
Feb 24 12:32:49.062: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-h88wr" satisfied condition "running"
Feb 24 12:32:49.062: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-p5s27" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:49.067: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-p5s27": Phase="Running", Reason="", readiness=true. Elapsed: 5.746246ms
Feb 24 12:32:49.068: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-p5s27" satisfied condition "running"
Feb 24 12:32:49.068: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-qjl5w" in namespace "emptydir-wrapper-1882" to be "running"
Feb 24 12:32:49.073: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-qjl5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.924535ms
Feb 24 12:32:49.073: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-qjl5w" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a in namespace emptydir-wrapper-1882, will wait for the garbage collector to delete the pods 02/24/23 12:32:49.073
Feb 24 12:32:49.139: INFO: Deleting ReplicationController wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a took: 10.41042ms
Feb 24 12:32:49.241: INFO: Terminating ReplicationController wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a pods took: 102.621247ms
STEP: Cleaning up the configMaps 02/24/23 12:32:53.642
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 24 12:32:54.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1882" for this suite. 02/24/23 12:32:54.289
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":358,"skipped":6634,"failed":0}
------------------------------
• [SLOW TEST] [58.523 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:31:55.799
    Feb 24 12:31:55.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir-wrapper 02/24/23 12:31:55.801
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:31:55.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:31:55.912
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/24/23 12:31:55.927
    STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:31:56.537
    Feb 24 12:31:56.573: INFO: Pod name wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c: Found 0 pods out of 5
    Feb 24 12:32:01.592: INFO: Pod name wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/24/23 12:32:01.592
    Feb 24 12:32:01.592: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:01.597: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 5.263574ms
    Feb 24 12:32:03.605: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01254533s
    Feb 24 12:32:05.604: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012109542s
    Feb 24 12:32:07.604: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012018763s
    Feb 24 12:32:09.607: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.014487137s
    Feb 24 12:32:11.603: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j": Phase="Running", Reason="", readiness=true. Elapsed: 10.011109355s
    Feb 24 12:32:11.604: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-87q4j" satisfied condition "running"
    Feb 24 12:32:11.604: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-dctjp" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:11.609: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-dctjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.999938ms
    Feb 24 12:32:11.609: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-dctjp" satisfied condition "running"
    Feb 24 12:32:11.609: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-kjjc9" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:11.615: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-kjjc9": Phase="Running", Reason="", readiness=true. Elapsed: 5.861381ms
    Feb 24 12:32:11.616: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-kjjc9" satisfied condition "running"
    Feb 24 12:32:11.616: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-mzqbj" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:11.621: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-mzqbj": Phase="Running", Reason="", readiness=true. Elapsed: 5.459886ms
    Feb 24 12:32:11.621: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-mzqbj" satisfied condition "running"
    Feb 24 12:32:11.622: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-p46rs" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:11.630: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-p46rs": Phase="Running", Reason="", readiness=true. Elapsed: 7.988244ms
    Feb 24 12:32:11.630: INFO: Pod "wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c-p46rs" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c in namespace emptydir-wrapper-1882, will wait for the garbage collector to delete the pods 02/24/23 12:32:11.63
    Feb 24 12:32:11.703: INFO: Deleting ReplicationController wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c took: 16.18975ms
    Feb 24 12:32:11.809: INFO: Terminating ReplicationController wrapped-volume-race-0f2b9ea8-e4a6-4fee-a7f4-c73ebb40ee4c pods took: 105.806168ms
    STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:32:15.418
    Feb 24 12:32:15.443: INFO: Pod name wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff: Found 0 pods out of 5
    Feb 24 12:32:20.456: INFO: Pod name wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/24/23 12:32:20.456
    Feb 24 12:32:20.456: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:20.465: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024194ms
    Feb 24 12:32:22.471: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015075054s
    Feb 24 12:32:24.472: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016527276s
    Feb 24 12:32:26.488: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032678132s
    Feb 24 12:32:28.471: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015165068s
    Feb 24 12:32:30.484: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk": Phase="Running", Reason="", readiness=true. Elapsed: 10.028418725s
    Feb 24 12:32:30.484: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-55gkk" satisfied condition "running"
    Feb 24 12:32:30.484: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-blqv2" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:30.490: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-blqv2": Phase="Running", Reason="", readiness=true. Elapsed: 5.899746ms
    Feb 24 12:32:30.490: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-blqv2" satisfied condition "running"
    Feb 24 12:32:30.490: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-dzvp7" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:30.496: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-dzvp7": Phase="Running", Reason="", readiness=true. Elapsed: 5.438889ms
    Feb 24 12:32:30.496: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-dzvp7" satisfied condition "running"
    Feb 24 12:32:30.496: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-hjvrd" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:30.503: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-hjvrd": Phase="Running", Reason="", readiness=true. Elapsed: 6.665823ms
    Feb 24 12:32:30.503: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-hjvrd" satisfied condition "running"
    Feb 24 12:32:30.503: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-qp9rv" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:30.508: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-qp9rv": Phase="Running", Reason="", readiness=true. Elapsed: 5.339433ms
    Feb 24 12:32:30.508: INFO: Pod "wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff-qp9rv" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff in namespace emptydir-wrapper-1882, will wait for the garbage collector to delete the pods 02/24/23 12:32:30.508
    Feb 24 12:32:30.577: INFO: Deleting ReplicationController wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff took: 10.888801ms
    Feb 24 12:32:30.877: INFO: Terminating ReplicationController wrapped-volume-race-92908b0b-0c25-4e5b-a478-c5e5fec3aeff pods took: 300.230928ms
    STEP: Creating RC which spawns configmap-volume pods 02/24/23 12:32:33.984
    Feb 24 12:32:34.027: INFO: Pod name wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a: Found 0 pods out of 5
    Feb 24 12:32:39.039: INFO: Pod name wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/24/23 12:32:39.039
    Feb 24 12:32:39.039: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:39.045: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900604ms
    Feb 24 12:32:41.054: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015533642s
    Feb 24 12:32:43.051: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01233284s
    Feb 24 12:32:45.052: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013487799s
    Feb 24 12:32:47.056: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016989322s
    Feb 24 12:32:49.051: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012389171s
    Feb 24 12:32:49.051: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-dhcjx" satisfied condition "running"
    Feb 24 12:32:49.051: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-g2n7t" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:49.056: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-g2n7t": Phase="Running", Reason="", readiness=true. Elapsed: 4.950661ms
    Feb 24 12:32:49.057: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-g2n7t" satisfied condition "running"
    Feb 24 12:32:49.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-h88wr" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:49.062: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-h88wr": Phase="Running", Reason="", readiness=true. Elapsed: 4.985944ms
    Feb 24 12:32:49.062: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-h88wr" satisfied condition "running"
    Feb 24 12:32:49.062: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-p5s27" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:49.067: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-p5s27": Phase="Running", Reason="", readiness=true. Elapsed: 5.746246ms
    Feb 24 12:32:49.068: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-p5s27" satisfied condition "running"
    Feb 24 12:32:49.068: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-qjl5w" in namespace "emptydir-wrapper-1882" to be "running"
    Feb 24 12:32:49.073: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-qjl5w": Phase="Running", Reason="", readiness=true. Elapsed: 4.924535ms
    Feb 24 12:32:49.073: INFO: Pod "wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a-qjl5w" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a in namespace emptydir-wrapper-1882, will wait for the garbage collector to delete the pods 02/24/23 12:32:49.073
    Feb 24 12:32:49.139: INFO: Deleting ReplicationController wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a took: 10.41042ms
    Feb 24 12:32:49.241: INFO: Terminating ReplicationController wrapped-volume-race-247f3d69-631b-462b-a63d-81d8a676702a pods took: 102.621247ms
    STEP: Cleaning up the configMaps 02/24/23 12:32:53.642
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:32:54.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1882" for this suite. 02/24/23 12:32:54.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:32:54.323
Feb 24 12:32:54.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename resourcequota 02/24/23 12:32:54.324
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:32:54.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:32:54.404
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 02/24/23 12:32:54.407
STEP: Creating a ResourceQuota 02/24/23 12:32:59.429
STEP: Ensuring resource quota status is calculated 02/24/23 12:32:59.439
STEP: Creating a ReplicaSet 02/24/23 12:33:01.445
STEP: Ensuring resource quota status captures replicaset creation 02/24/23 12:33:01.464
STEP: Deleting a ReplicaSet 02/24/23 12:33:03.498
STEP: Ensuring resource quota status released usage 02/24/23 12:33:03.564
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 24 12:33:05.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9580" for this suite. 02/24/23 12:33:05.576
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":359,"skipped":6642,"failed":0}
------------------------------
• [SLOW TEST] [11.266 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:32:54.323
    Feb 24 12:32:54.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename resourcequota 02/24/23 12:32:54.324
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:32:54.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:32:54.404
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 02/24/23 12:32:54.407
    STEP: Creating a ResourceQuota 02/24/23 12:32:59.429
    STEP: Ensuring resource quota status is calculated 02/24/23 12:32:59.439
    STEP: Creating a ReplicaSet 02/24/23 12:33:01.445
    STEP: Ensuring resource quota status captures replicaset creation 02/24/23 12:33:01.464
    STEP: Deleting a ReplicaSet 02/24/23 12:33:03.498
    STEP: Ensuring resource quota status released usage 02/24/23 12:33:03.564
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 24 12:33:05.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9580" for this suite. 02/24/23 12:33:05.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:33:05.595
Feb 24 12:33:05.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename limitrange 02/24/23 12:33:05.596
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:33:05.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:33:05.626
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 02/24/23 12:33:05.628
STEP: Setting up watch 02/24/23 12:33:05.628
STEP: Submitting a LimitRange 02/24/23 12:33:05.734
STEP: Verifying LimitRange creation was observed 02/24/23 12:33:05.746
STEP: Fetching the LimitRange to ensure it has proper values 02/24/23 12:33:05.746
Feb 24 12:33:05.754: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 24 12:33:05.755: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/24/23 12:33:05.755
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/24/23 12:33:05.766
Feb 24 12:33:05.774: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 24 12:33:05.774: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/24/23 12:33:05.774
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/24/23 12:33:05.791
Feb 24 12:33:05.801: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 24 12:33:05.802: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/24/23 12:33:05.802
STEP: Failing to create a Pod with more than max resources 02/24/23 12:33:05.808
STEP: Updating a LimitRange 02/24/23 12:33:05.81
STEP: Verifying LimitRange updating is effective 02/24/23 12:33:05.818
STEP: Creating a Pod with less than former min resources 02/24/23 12:33:07.824
STEP: Failing to create a Pod with more than max resources 02/24/23 12:33:07.837
STEP: Deleting a LimitRange 02/24/23 12:33:07.839
STEP: Verifying the LimitRange was deleted 02/24/23 12:33:07.859
Feb 24 12:33:12.865: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/24/23 12:33:12.865
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Feb 24 12:33:12.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-300" for this suite. 02/24/23 12:33:12.891
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":360,"skipped":6657,"failed":0}
------------------------------
• [SLOW TEST] [7.311 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:33:05.595
    Feb 24 12:33:05.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename limitrange 02/24/23 12:33:05.596
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:33:05.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:33:05.626
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 02/24/23 12:33:05.628
    STEP: Setting up watch 02/24/23 12:33:05.628
    STEP: Submitting a LimitRange 02/24/23 12:33:05.734
    STEP: Verifying LimitRange creation was observed 02/24/23 12:33:05.746
    STEP: Fetching the LimitRange to ensure it has proper values 02/24/23 12:33:05.746
    Feb 24 12:33:05.754: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 24 12:33:05.755: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/24/23 12:33:05.755
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/24/23 12:33:05.766
    Feb 24 12:33:05.774: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 24 12:33:05.774: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/24/23 12:33:05.774
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/24/23 12:33:05.791
    Feb 24 12:33:05.801: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 24 12:33:05.802: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/24/23 12:33:05.802
    STEP: Failing to create a Pod with more than max resources 02/24/23 12:33:05.808
    STEP: Updating a LimitRange 02/24/23 12:33:05.81
    STEP: Verifying LimitRange updating is effective 02/24/23 12:33:05.818
    STEP: Creating a Pod with less than former min resources 02/24/23 12:33:07.824
    STEP: Failing to create a Pod with more than max resources 02/24/23 12:33:07.837
    STEP: Deleting a LimitRange 02/24/23 12:33:07.839
    STEP: Verifying the LimitRange was deleted 02/24/23 12:33:07.859
    Feb 24 12:33:12.865: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/24/23 12:33:12.865
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Feb 24 12:33:12.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-300" for this suite. 02/24/23 12:33:12.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:33:12.909
Feb 24 12:33:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename emptydir 02/24/23 12:33:12.91
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:33:12.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:33:12.941
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 12:33:12.949
Feb 24 12:33:12.962: INFO: Waiting up to 5m0s for pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17" in namespace "emptydir-48" to be "Succeeded or Failed"
Feb 24 12:33:12.979: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17": Phase="Pending", Reason="", readiness=false. Elapsed: 16.730652ms
Feb 24 12:33:14.987: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024910638s
Feb 24 12:33:16.985: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02270363s
STEP: Saw pod success 02/24/23 12:33:16.985
Feb 24 12:33:16.985: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17" satisfied condition "Succeeded or Failed"
Feb 24 12:33:16.990: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-76696766-90d3-4f0b-90db-a4b8a3350d17 container test-container: <nil>
STEP: delete the pod 02/24/23 12:33:17.002
Feb 24 12:33:17.035: INFO: Waiting for pod pod-76696766-90d3-4f0b-90db-a4b8a3350d17 to disappear
Feb 24 12:33:17.049: INFO: Pod pod-76696766-90d3-4f0b-90db-a4b8a3350d17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 24 12:33:17.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-48" for this suite. 02/24/23 12:33:17.062
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":361,"skipped":6671,"failed":0}
------------------------------
• [4.171 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:33:12.909
    Feb 24 12:33:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename emptydir 02/24/23 12:33:12.91
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:33:12.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:33:12.941
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/24/23 12:33:12.949
    Feb 24 12:33:12.962: INFO: Waiting up to 5m0s for pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17" in namespace "emptydir-48" to be "Succeeded or Failed"
    Feb 24 12:33:12.979: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17": Phase="Pending", Reason="", readiness=false. Elapsed: 16.730652ms
    Feb 24 12:33:14.987: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024910638s
    Feb 24 12:33:16.985: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02270363s
    STEP: Saw pod success 02/24/23 12:33:16.985
    Feb 24 12:33:16.985: INFO: Pod "pod-76696766-90d3-4f0b-90db-a4b8a3350d17" satisfied condition "Succeeded or Failed"
    Feb 24 12:33:16.990: INFO: Trying to get logs from node ip-172-31-216-47.eu-west-3.compute.internal pod pod-76696766-90d3-4f0b-90db-a4b8a3350d17 container test-container: <nil>
    STEP: delete the pod 02/24/23 12:33:17.002
    Feb 24 12:33:17.035: INFO: Waiting for pod pod-76696766-90d3-4f0b-90db-a4b8a3350d17 to disappear
    Feb 24 12:33:17.049: INFO: Pod pod-76696766-90d3-4f0b-90db-a4b8a3350d17 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 24 12:33:17.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-48" for this suite. 02/24/23 12:33:17.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/24/23 12:33:17.083
Feb 24 12:33:17.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
STEP: Building a namespace api object, basename var-expansion 02/24/23 12:33:17.084
STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:33:17.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:33:17.112
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 02/24/23 12:33:17.115
Feb 24 12:33:17.128: INFO: Waiting up to 2m0s for pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" in namespace "var-expansion-8247" to be "running"
Feb 24 12:33:17.146: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.521889ms
Feb 24 12:33:19.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02468003s
Feb 24 12:33:21.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023743012s
Feb 24 12:33:23.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023843166s
Feb 24 12:33:25.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02363459s
Feb 24 12:33:27.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023085355s
Feb 24 12:33:29.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024419631s
Feb 24 12:33:31.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.025580012s
Feb 24 12:33:33.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024101343s
Feb 24 12:33:35.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023295824s
Feb 24 12:33:37.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.024227641s
Feb 24 12:33:39.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02393228s
Feb 24 12:33:41.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.023689268s
Feb 24 12:33:43.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.025241194s
Feb 24 12:33:45.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.029486563s
Feb 24 12:33:47.156: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.028021215s
Feb 24 12:33:49.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023615666s
Feb 24 12:33:51.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023998384s
Feb 24 12:33:53.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.022778429s
Feb 24 12:33:55.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.024893816s
Feb 24 12:33:57.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029285945s
Feb 24 12:33:59.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.024716256s
Feb 24 12:34:01.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025978229s
Feb 24 12:34:03.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023816137s
Feb 24 12:34:05.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024920997s
Feb 24 12:34:07.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02536288s
Feb 24 12:34:09.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.024629852s
Feb 24 12:34:11.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.025099298s
Feb 24 12:34:13.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.024337784s
Feb 24 12:34:15.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.02384399s
Feb 24 12:34:17.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023101201s
Feb 24 12:34:19.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.024076312s
Feb 24 12:34:21.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02541754s
Feb 24 12:34:23.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030001938s
Feb 24 12:34:25.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023204623s
Feb 24 12:34:27.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023481626s
Feb 24 12:34:29.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024230935s
Feb 24 12:34:31.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.023719291s
Feb 24 12:34:33.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025325688s
Feb 24 12:34:35.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023317549s
Feb 24 12:34:37.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023090795s
Feb 24 12:34:39.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.024084341s
Feb 24 12:34:41.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024494183s
Feb 24 12:34:43.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.024171953s
Feb 24 12:34:45.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.029305532s
Feb 24 12:34:47.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022920626s
Feb 24 12:34:49.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.022964553s
Feb 24 12:34:51.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.023409233s
Feb 24 12:34:53.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025339003s
Feb 24 12:34:55.155: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.026302795s
Feb 24 12:34:57.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024451885s
Feb 24 12:34:59.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023637184s
Feb 24 12:35:01.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025316747s
Feb 24 12:35:03.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.023340395s
Feb 24 12:35:05.160: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.031437375s
Feb 24 12:35:07.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025547595s
Feb 24 12:35:09.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.025888104s
Feb 24 12:35:11.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023909683s
Feb 24 12:35:13.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.023385797s
Feb 24 12:35:15.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02524027s
Feb 24 12:35:17.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022670743s
Feb 24 12:35:17.156: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027982291s
STEP: updating the pod 02/24/23 12:35:17.156
Feb 24 12:35:17.675: INFO: Successfully updated pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5"
STEP: waiting for pod running 02/24/23 12:35:17.676
Feb 24 12:35:17.676: INFO: Waiting up to 2m0s for pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" in namespace "var-expansion-8247" to be "running"
Feb 24 12:35:17.683: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961767ms
Feb 24 12:35:19.689: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013638422s
Feb 24 12:35:19.690: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" satisfied condition "running"
STEP: deleting the pod gracefully 02/24/23 12:35:19.69
Feb 24 12:35:19.690: INFO: Deleting pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" in namespace "var-expansion-8247"
Feb 24 12:35:19.703: INFO: Wait up to 5m0s for pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 24 12:35:51.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8247" for this suite. 02/24/23 12:35:51.74
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":362,"skipped":6694,"failed":0}
------------------------------
• [SLOW TEST] [154.670 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/24/23 12:33:17.083
    Feb 24 12:33:17.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1678170937
    STEP: Building a namespace api object, basename var-expansion 02/24/23 12:33:17.084
    STEP: Waiting for a default service account to be provisioned in namespace 02/24/23 12:33:17.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/24/23 12:33:17.112
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 02/24/23 12:33:17.115
    Feb 24 12:33:17.128: INFO: Waiting up to 2m0s for pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" in namespace "var-expansion-8247" to be "running"
    Feb 24 12:33:17.146: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.521889ms
    Feb 24 12:33:19.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02468003s
    Feb 24 12:33:21.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023743012s
    Feb 24 12:33:23.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023843166s
    Feb 24 12:33:25.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02363459s
    Feb 24 12:33:27.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023085355s
    Feb 24 12:33:29.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.024419631s
    Feb 24 12:33:31.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.025580012s
    Feb 24 12:33:33.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024101343s
    Feb 24 12:33:35.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023295824s
    Feb 24 12:33:37.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.024227641s
    Feb 24 12:33:39.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02393228s
    Feb 24 12:33:41.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.023689268s
    Feb 24 12:33:43.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.025241194s
    Feb 24 12:33:45.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.029486563s
    Feb 24 12:33:47.156: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.028021215s
    Feb 24 12:33:49.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023615666s
    Feb 24 12:33:51.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023998384s
    Feb 24 12:33:53.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.022778429s
    Feb 24 12:33:55.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.024893816s
    Feb 24 12:33:57.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029285945s
    Feb 24 12:33:59.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.024716256s
    Feb 24 12:34:01.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025978229s
    Feb 24 12:34:03.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.023816137s
    Feb 24 12:34:05.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024920997s
    Feb 24 12:34:07.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.02536288s
    Feb 24 12:34:09.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.024629852s
    Feb 24 12:34:11.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.025099298s
    Feb 24 12:34:13.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.024337784s
    Feb 24 12:34:15.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.02384399s
    Feb 24 12:34:17.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.023101201s
    Feb 24 12:34:19.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.024076312s
    Feb 24 12:34:21.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02541754s
    Feb 24 12:34:23.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030001938s
    Feb 24 12:34:25.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.023204623s
    Feb 24 12:34:27.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023481626s
    Feb 24 12:34:29.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024230935s
    Feb 24 12:34:31.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.023719291s
    Feb 24 12:34:33.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025325688s
    Feb 24 12:34:35.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.023317549s
    Feb 24 12:34:37.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.023090795s
    Feb 24 12:34:39.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.024084341s
    Feb 24 12:34:41.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.024494183s
    Feb 24 12:34:43.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.024171953s
    Feb 24 12:34:45.158: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.029305532s
    Feb 24 12:34:47.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.022920626s
    Feb 24 12:34:49.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.022964553s
    Feb 24 12:34:51.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.023409233s
    Feb 24 12:34:53.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025339003s
    Feb 24 12:34:55.155: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.026302795s
    Feb 24 12:34:57.153: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.024451885s
    Feb 24 12:34:59.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023637184s
    Feb 24 12:35:01.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025316747s
    Feb 24 12:35:03.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.023340395s
    Feb 24 12:35:05.160: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.031437375s
    Feb 24 12:35:07.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025547595s
    Feb 24 12:35:09.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.025888104s
    Feb 24 12:35:11.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023909683s
    Feb 24 12:35:13.152: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.023385797s
    Feb 24 12:35:15.154: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02524027s
    Feb 24 12:35:17.151: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022670743s
    Feb 24 12:35:17.156: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027982291s
    STEP: updating the pod 02/24/23 12:35:17.156
    Feb 24 12:35:17.675: INFO: Successfully updated pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5"
    STEP: waiting for pod running 02/24/23 12:35:17.676
    Feb 24 12:35:17.676: INFO: Waiting up to 2m0s for pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" in namespace "var-expansion-8247" to be "running"
    Feb 24 12:35:17.683: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.961767ms
    Feb 24 12:35:19.689: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013638422s
    Feb 24 12:35:19.690: INFO: Pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" satisfied condition "running"
    STEP: deleting the pod gracefully 02/24/23 12:35:19.69
    Feb 24 12:35:19.690: INFO: Deleting pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" in namespace "var-expansion-8247"
    Feb 24 12:35:19.703: INFO: Wait up to 5m0s for pod "var-expansion-2f11173c-8bfe-465a-ae6e-78649ae2d6d5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 24 12:35:51.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8247" for this suite. 02/24/23 12:35:51.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Feb 24 12:35:51.754: INFO: Running AfterSuite actions on all nodes
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Feb 24 12:35:51.754: INFO: Running AfterSuite actions on node 1
Feb 24 12:35:51.754: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 24 12:35:51.754: INFO: Running AfterSuite actions on all nodes
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Feb 24 12:35:51.754: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 24 12:35:51.754: INFO: Running AfterSuite actions on node 1
    Feb 24 12:35:51.754: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.069 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5663.055 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h34m23.388344196s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

