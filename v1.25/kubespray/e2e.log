I0212 11:07:43.188948      22 e2e.go:116] Starting e2e run "2c3b1601-a843-49d3-92af-7f94d0476c26" on Ginkgo node 1
Feb 12 11:07:43.202: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1676200063 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Feb 12 11:07:43.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:07:43.286: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 12 11:07:48.241: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 12 11:07:55.002: INFO: The status of Pod kube-apiserver-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:07:55.002: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Feb 12 11:07:55.002: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:07:55.002: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:07:55.002: INFO: kube-apiserver-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:47 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:47 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:07:55.002: INFO: 
Feb 12 11:07:58.873: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:07:58.873: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Feb 12 11:07:58.873: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:07:58.873: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:07:58.873: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:07:58.873: INFO: 
Feb 12 11:07:59.289: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:07:59.289: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (11 seconds elapsed)
Feb 12 11:07:59.289: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:07:59.289: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:07:59.289: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:07:59.289: INFO: 
Feb 12 11:08:01.511: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:01.511: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (13 seconds elapsed)
Feb 12 11:08:01.511: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:01.511: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:01.511: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:01.511: INFO: 
Feb 12 11:08:03.062: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:03.062: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (14 seconds elapsed)
Feb 12 11:08:03.062: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:03.062: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:03.062: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:03.063: INFO: 
Feb 12 11:08:05.076: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:05.076: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (16 seconds elapsed)
Feb 12 11:08:05.076: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:05.076: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:05.076: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:05.076: INFO: 
Feb 12 11:08:07.083: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:07.083: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (18 seconds elapsed)
Feb 12 11:08:07.083: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:07.083: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:07.083: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:07.083: INFO: 
Feb 12 11:08:09.090: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:09.090: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (20 seconds elapsed)
Feb 12 11:08:09.090: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:09.090: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:09.090: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:09.090: INFO: 
Feb 12 11:08:11.057: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:11.057: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (22 seconds elapsed)
Feb 12 11:08:11.057: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:11.057: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:11.057: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:11.057: INFO: 
Feb 12 11:08:13.026: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:13.026: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (24 seconds elapsed)
Feb 12 11:08:13.026: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:13.026: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:13.026: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:13.026: INFO: 
Feb 12 11:08:15.036: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:15.036: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (26 seconds elapsed)
Feb 12 11:08:15.036: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:15.036: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:15.036: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:15.036: INFO: 
Feb 12 11:08:31.927: INFO: The status of Pod kube-apiserver-kube-1 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:31.927: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:31.927: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (43 seconds elapsed)
Feb 12 11:08:31.927: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:31.927: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:31.927: INFO: kube-apiserver-kube-1           kube-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:27 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:27 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:31 +0000 UTC  }]
Feb 12 11:08:31.927: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:31.927: INFO: 
Feb 12 11:08:34.175: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:34.175: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:34.175: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (45 seconds elapsed)
Feb 12 11:08:34.175: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:34.175: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:34.175: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:34.175: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:34.175: INFO: 
Feb 12 11:08:35.104: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:35.105: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:35.105: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (46 seconds elapsed)
Feb 12 11:08:35.105: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:35.105: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:35.105: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:35.106: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:35.106: INFO: 
Feb 12 11:08:37.088: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:37.088: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:37.088: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (48 seconds elapsed)
Feb 12 11:08:37.088: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:37.088: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:37.088: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:37.088: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:37.089: INFO: 
Feb 12 11:08:39.084: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:39.085: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:39.085: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (50 seconds elapsed)
Feb 12 11:08:39.085: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:39.085: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:39.085: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:39.085: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:39.085: INFO: 
Feb 12 11:08:41.082: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:41.082: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (52 seconds elapsed)
Feb 12 11:08:41.082: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:41.082: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:41.082: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:41.082: INFO: 
Feb 12 11:08:43.028: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:43.028: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (54 seconds elapsed)
Feb 12 11:08:43.028: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:43.028: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:43.028: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:43.028: INFO: 
Feb 12 11:08:45.023: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:45.023: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (56 seconds elapsed)
Feb 12 11:08:45.023: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:45.023: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:45.023: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:45.023: INFO: 
Feb 12 11:08:47.055: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:47.055: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (58 seconds elapsed)
Feb 12 11:08:47.055: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:47.055: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:47.055: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:47.055: INFO: 
Feb 12 11:08:49.047: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:49.047: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (60 seconds elapsed)
Feb 12 11:08:49.047: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:49.047: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:49.047: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:49.048: INFO: 
Feb 12 11:08:51.177: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:51.177: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (62 seconds elapsed)
Feb 12 11:08:51.177: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:51.177: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:51.177: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:51.177: INFO: 
Feb 12 11:08:53.042: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:53.042: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (64 seconds elapsed)
Feb 12 11:08:53.042: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:53.042: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:53.042: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:53.042: INFO: 
Feb 12 11:08:55.041: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:55.041: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (66 seconds elapsed)
Feb 12 11:08:55.041: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:55.042: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:55.042: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:55.042: INFO: 
Feb 12 11:08:57.082: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:57.082: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (68 seconds elapsed)
Feb 12 11:08:57.082: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:57.082: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:57.082: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:57.083: INFO: 
Feb 12 11:08:59.079: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:08:59.079: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (70 seconds elapsed)
Feb 12 11:08:59.079: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:08:59.079: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:08:59.079: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:08:59.079: INFO: 
Feb 12 11:09:01.056: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:09:01.056: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (72 seconds elapsed)
Feb 12 11:09:01.057: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:09:01.057: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:09:01.057: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
Feb 12 11:09:01.057: INFO: 
Feb 12 11:09:03.023: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (74 seconds elapsed)
Feb 12 11:09:03.023: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 12 11:09:03.023: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 12 11:09:03.027: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 12 11:09:03.027: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 12 11:09:03.027: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Feb 12 11:09:03.027: INFO: e2e test version: v1.25.6
Feb 12 11:09:03.028: INFO: kube-apiserver version: v1.25.6
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Feb 12 11:09:03.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:09:03.040: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [79.757 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 12 11:07:43.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:07:43.286: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Feb 12 11:07:48.241: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 12 11:07:55.002: INFO: The status of Pod kube-apiserver-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:07:55.002: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
    Feb 12 11:07:55.002: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:07:55.002: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:07:55.002: INFO: kube-apiserver-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:47 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:47 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:07:55.002: INFO: 
    Feb 12 11:07:58.873: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:07:58.873: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
    Feb 12 11:07:58.873: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:07:58.873: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:07:58.873: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:07:58.873: INFO: 
    Feb 12 11:07:59.289: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:07:59.289: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (11 seconds elapsed)
    Feb 12 11:07:59.289: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:07:59.289: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:07:59.289: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:07:59.289: INFO: 
    Feb 12 11:08:01.511: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:01.511: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (13 seconds elapsed)
    Feb 12 11:08:01.511: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:01.511: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:01.511: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:01.511: INFO: 
    Feb 12 11:08:03.062: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:03.062: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (14 seconds elapsed)
    Feb 12 11:08:03.062: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:03.062: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:03.062: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:03.063: INFO: 
    Feb 12 11:08:05.076: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:05.076: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (16 seconds elapsed)
    Feb 12 11:08:05.076: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:05.076: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:05.076: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:05.076: INFO: 
    Feb 12 11:08:07.083: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:07.083: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (18 seconds elapsed)
    Feb 12 11:08:07.083: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:07.083: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:07.083: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:07.083: INFO: 
    Feb 12 11:08:09.090: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:09.090: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (20 seconds elapsed)
    Feb 12 11:08:09.090: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:09.090: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:09.090: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:09.090: INFO: 
    Feb 12 11:08:11.057: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:11.057: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (22 seconds elapsed)
    Feb 12 11:08:11.057: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:11.057: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:11.057: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:11.057: INFO: 
    Feb 12 11:08:13.026: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:13.026: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (24 seconds elapsed)
    Feb 12 11:08:13.026: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:13.026: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:13.026: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:13.026: INFO: 
    Feb 12 11:08:15.036: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:15.036: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (26 seconds elapsed)
    Feb 12 11:08:15.036: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:15.036: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:15.036: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:15.036: INFO: 
    Feb 12 11:08:31.927: INFO: The status of Pod kube-apiserver-kube-1 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:31.927: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:31.927: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (43 seconds elapsed)
    Feb 12 11:08:31.927: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:31.927: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:31.927: INFO: kube-apiserver-kube-1           kube-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:27 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:27 +0000 UTC ContainersNotReady containers with unready status: [kube-apiserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:31 +0000 UTC  }]
    Feb 12 11:08:31.927: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:31.927: INFO: 
    Feb 12 11:08:34.175: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:34.175: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:34.175: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (45 seconds elapsed)
    Feb 12 11:08:34.175: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:34.175: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:34.175: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:34.175: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:34.175: INFO: 
    Feb 12 11:08:35.104: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:35.105: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:35.105: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (46 seconds elapsed)
    Feb 12 11:08:35.105: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:35.105: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:35.105: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:35.106: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:35.106: INFO: 
    Feb 12 11:08:37.088: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:37.088: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:37.088: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (48 seconds elapsed)
    Feb 12 11:08:37.088: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:37.088: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:37.088: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:37.088: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:37.089: INFO: 
    Feb 12 11:08:39.084: INFO: The status of Pod kube-controller-manager-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:39.085: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:39.085: INFO: 18 / 20 pods in namespace 'kube-system' are running and ready (50 seconds elapsed)
    Feb 12 11:08:39.085: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:39.085: INFO: POD                             NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:39.085: INFO: kube-controller-manager-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:07:55 +0000 UTC ContainersNotReady containers with unready status: [kube-controller-manager]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:39.085: INFO: kube-scheduler-kube-2           kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:39.085: INFO: 
    Feb 12 11:08:41.082: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:41.082: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (52 seconds elapsed)
    Feb 12 11:08:41.082: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:41.082: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:41.082: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:41.082: INFO: 
    Feb 12 11:08:43.028: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:43.028: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (54 seconds elapsed)
    Feb 12 11:08:43.028: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:43.028: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:43.028: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:43.028: INFO: 
    Feb 12 11:08:45.023: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:45.023: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (56 seconds elapsed)
    Feb 12 11:08:45.023: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:45.023: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:45.023: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:45.023: INFO: 
    Feb 12 11:08:47.055: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:47.055: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (58 seconds elapsed)
    Feb 12 11:08:47.055: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:47.055: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:47.055: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:47.055: INFO: 
    Feb 12 11:08:49.047: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:49.047: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (60 seconds elapsed)
    Feb 12 11:08:49.047: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:49.047: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:49.047: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:49.048: INFO: 
    Feb 12 11:08:51.177: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:51.177: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (62 seconds elapsed)
    Feb 12 11:08:51.177: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:51.177: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:51.177: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:51.177: INFO: 
    Feb 12 11:08:53.042: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:53.042: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (64 seconds elapsed)
    Feb 12 11:08:53.042: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:53.042: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:53.042: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:53.042: INFO: 
    Feb 12 11:08:55.041: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:55.041: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (66 seconds elapsed)
    Feb 12 11:08:55.041: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:55.042: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:55.042: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:55.042: INFO: 
    Feb 12 11:08:57.082: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:57.082: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (68 seconds elapsed)
    Feb 12 11:08:57.082: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:57.082: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:57.082: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:57.083: INFO: 
    Feb 12 11:08:59.079: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:08:59.079: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (70 seconds elapsed)
    Feb 12 11:08:59.079: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:08:59.079: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:08:59.079: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:08:59.079: INFO: 
    Feb 12 11:09:01.056: INFO: The status of Pod kube-scheduler-kube-2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:09:01.056: INFO: 19 / 20 pods in namespace 'kube-system' are running and ready (72 seconds elapsed)
    Feb 12 11:09:01.057: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:09:01.057: INFO: POD                    NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:09:01.057: INFO: kube-scheduler-kube-2  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:08:25 +0000 UTC ContainersNotReady containers with unready status: [kube-scheduler]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 10:55:53 +0000 UTC  }]
    Feb 12 11:09:01.057: INFO: 
    Feb 12 11:09:03.023: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (74 seconds elapsed)
    Feb 12 11:09:03.023: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Feb 12 11:09:03.023: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 12 11:09:03.027: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Feb 12 11:09:03.027: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Feb 12 11:09:03.027: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Feb 12 11:09:03.027: INFO: e2e test version: v1.25.6
    Feb 12 11:09:03.028: INFO: kube-apiserver version: v1.25.6
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Feb 12 11:09:03.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:09:03.040: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:09:03.053
Feb 12 11:09:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:09:03.054
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:03.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:03.085
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 02/12/23 11:09:03.088
Feb 12 11:09:03.088: INFO: namespace kubectl-8844
Feb 12 11:09:03.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 create -f -'
Feb 12 11:09:03.725: INFO: stderr: ""
Feb 12 11:09:03.725: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/12/23 11:09:03.725
Feb 12 11:09:04.740: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:04.740: INFO: Found 0 / 1
Feb 12 11:09:05.738: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:05.738: INFO: Found 0 / 1
Feb 12 11:09:06.733: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:06.733: INFO: Found 0 / 1
Feb 12 11:09:08.093: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:08.093: INFO: Found 0 / 1
Feb 12 11:09:08.875: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:08.875: INFO: Found 0 / 1
Feb 12 11:09:09.729: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:09.729: INFO: Found 0 / 1
Feb 12 11:09:11.396: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:11.396: INFO: Found 0 / 1
Feb 12 11:09:11.730: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:11.730: INFO: Found 0 / 1
Feb 12 11:09:12.728: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:12.728: INFO: Found 0 / 1
Feb 12 11:09:13.741: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:13.741: INFO: Found 1 / 1
Feb 12 11:09:13.742: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 12 11:09:13.753: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:09:13.753: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 12 11:09:13.754: INFO: wait on agnhost-primary startup in kubectl-8844 
Feb 12 11:09:13.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 logs agnhost-primary-xghj5 agnhost-primary'
Feb 12 11:09:13.945: INFO: stderr: ""
Feb 12 11:09:13.945: INFO: stdout: "Paused\n"
STEP: exposing RC 02/12/23 11:09:13.945
Feb 12 11:09:13.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 12 11:09:14.081: INFO: stderr: ""
Feb 12 11:09:14.081: INFO: stdout: "service/rm2 exposed\n"
Feb 12 11:09:14.089: INFO: Service rm2 in namespace kubectl-8844 found.
STEP: exposing service 02/12/23 11:09:16.115
Feb 12 11:09:16.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 12 11:09:16.300: INFO: stderr: ""
Feb 12 11:09:16.301: INFO: stdout: "service/rm3 exposed\n"
Feb 12 11:09:16.308: INFO: Service rm3 in namespace kubectl-8844 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:09:18.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8844" for this suite. 02/12/23 11:09:18.329
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":1,"skipped":0,"failed":0}
------------------------------
 [SLOW TEST] [15.287 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:09:03.053
    Feb 12 11:09:03.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:09:03.054
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:03.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:03.085
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 02/12/23 11:09:03.088
    Feb 12 11:09:03.088: INFO: namespace kubectl-8844
    Feb 12 11:09:03.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 create -f -'
    Feb 12 11:09:03.725: INFO: stderr: ""
    Feb 12 11:09:03.725: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/12/23 11:09:03.725
    Feb 12 11:09:04.740: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:04.740: INFO: Found 0 / 1
    Feb 12 11:09:05.738: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:05.738: INFO: Found 0 / 1
    Feb 12 11:09:06.733: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:06.733: INFO: Found 0 / 1
    Feb 12 11:09:08.093: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:08.093: INFO: Found 0 / 1
    Feb 12 11:09:08.875: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:08.875: INFO: Found 0 / 1
    Feb 12 11:09:09.729: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:09.729: INFO: Found 0 / 1
    Feb 12 11:09:11.396: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:11.396: INFO: Found 0 / 1
    Feb 12 11:09:11.730: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:11.730: INFO: Found 0 / 1
    Feb 12 11:09:12.728: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:12.728: INFO: Found 0 / 1
    Feb 12 11:09:13.741: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:13.741: INFO: Found 1 / 1
    Feb 12 11:09:13.742: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 12 11:09:13.753: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:09:13.753: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 12 11:09:13.754: INFO: wait on agnhost-primary startup in kubectl-8844 
    Feb 12 11:09:13.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 logs agnhost-primary-xghj5 agnhost-primary'
    Feb 12 11:09:13.945: INFO: stderr: ""
    Feb 12 11:09:13.945: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/12/23 11:09:13.945
    Feb 12 11:09:13.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 12 11:09:14.081: INFO: stderr: ""
    Feb 12 11:09:14.081: INFO: stdout: "service/rm2 exposed\n"
    Feb 12 11:09:14.089: INFO: Service rm2 in namespace kubectl-8844 found.
    STEP: exposing service 02/12/23 11:09:16.115
    Feb 12 11:09:16.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8844 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 12 11:09:16.300: INFO: stderr: ""
    Feb 12 11:09:16.301: INFO: stdout: "service/rm3 exposed\n"
    Feb 12 11:09:16.308: INFO: Service rm3 in namespace kubectl-8844 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:09:18.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8844" for this suite. 02/12/23 11:09:18.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:09:18.341
Feb 12 11:09:18.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:09:18.342
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:18.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:18.37
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 12 11:09:18.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:09:29.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7116" for this suite. 02/12/23 11:09:29.223
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":2,"skipped":19,"failed":0}
------------------------------
 [SLOW TEST] [10.904 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:09:18.341
    Feb 12 11:09:18.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:09:18.342
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:18.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:18.37
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 12 11:09:18.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:09:29.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7116" for this suite. 02/12/23 11:09:29.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:09:29.247
Feb 12 11:09:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename limitrange 02/12/23 11:09:29.248
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:29.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:29.305
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 02/12/23 11:09:29.309
STEP: Setting up watch 02/12/23 11:09:29.309
STEP: Submitting a LimitRange 02/12/23 11:09:29.419
STEP: Verifying LimitRange creation was observed 02/12/23 11:09:29.425
STEP: Fetching the LimitRange to ensure it has proper values 02/12/23 11:09:29.429
Feb 12 11:09:29.432: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 12 11:09:29.432: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/12/23 11:09:29.432
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/12/23 11:09:29.438
Feb 12 11:09:29.458: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 12 11:09:29.458: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/12/23 11:09:29.458
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/12/23 11:09:29.473
Feb 12 11:09:29.480: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 12 11:09:29.480: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/12/23 11:09:29.48
STEP: Failing to create a Pod with more than max resources 02/12/23 11:09:29.483
STEP: Updating a LimitRange 02/12/23 11:09:29.486
STEP: Verifying LimitRange updating is effective 02/12/23 11:09:29.496
STEP: Creating a Pod with less than former min resources 02/12/23 11:09:31.5
STEP: Failing to create a Pod with more than max resources 02/12/23 11:09:31.507
STEP: Deleting a LimitRange 02/12/23 11:09:31.51
STEP: Verifying the LimitRange was deleted 02/12/23 11:09:31.524
Feb 12 11:09:36.545: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/12/23 11:09:36.545
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Feb 12 11:09:36.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4163" for this suite. 02/12/23 11:09:36.625
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":3,"skipped":43,"failed":0}
------------------------------
 [SLOW TEST] [7.450 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:09:29.247
    Feb 12 11:09:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename limitrange 02/12/23 11:09:29.248
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:29.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:29.305
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 02/12/23 11:09:29.309
    STEP: Setting up watch 02/12/23 11:09:29.309
    STEP: Submitting a LimitRange 02/12/23 11:09:29.419
    STEP: Verifying LimitRange creation was observed 02/12/23 11:09:29.425
    STEP: Fetching the LimitRange to ensure it has proper values 02/12/23 11:09:29.429
    Feb 12 11:09:29.432: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 12 11:09:29.432: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/12/23 11:09:29.432
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/12/23 11:09:29.438
    Feb 12 11:09:29.458: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 12 11:09:29.458: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/12/23 11:09:29.458
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/12/23 11:09:29.473
    Feb 12 11:09:29.480: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 12 11:09:29.480: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/12/23 11:09:29.48
    STEP: Failing to create a Pod with more than max resources 02/12/23 11:09:29.483
    STEP: Updating a LimitRange 02/12/23 11:09:29.486
    STEP: Verifying LimitRange updating is effective 02/12/23 11:09:29.496
    STEP: Creating a Pod with less than former min resources 02/12/23 11:09:31.5
    STEP: Failing to create a Pod with more than max resources 02/12/23 11:09:31.507
    STEP: Deleting a LimitRange 02/12/23 11:09:31.51
    STEP: Verifying the LimitRange was deleted 02/12/23 11:09:31.524
    Feb 12 11:09:36.545: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/12/23 11:09:36.545
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Feb 12 11:09:36.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-4163" for this suite. 02/12/23 11:09:36.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:09:36.702
Feb 12 11:09:36.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 11:09:36.705
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:36.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:36.784
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 12 11:09:36.840: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 12 11:09:44.265: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/12/23 11:09:44.265
Feb 12 11:09:44.266: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-wmg9g" in namespace "deployment-5519" to be "running"
Feb 12 11:09:47.376: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110271072s
Feb 12 11:09:54.917: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Pending", Reason="", readiness=false. Elapsed: 10.650758567s
Feb 12 11:09:55.416: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Pending", Reason="", readiness=false. Elapsed: 11.150007809s
Feb 12 11:09:57.382: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Running", Reason="", readiness=true. Elapsed: 13.115510785s
Feb 12 11:09:57.382: INFO: Pod "test-cleanup-controller-wmg9g" satisfied condition "running"
Feb 12 11:09:57.382: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/12/23 11:09:57.393
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 11:09:57.404: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5519  cc3e3a13-7cd2-4b9d-a407-6119c5245846 2834 1 2023-02-12 11:09:57 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-12 11:09:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c41228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 12 11:09:57.407: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 11:09:57.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5519" for this suite. 02/12/23 11:09:57.415
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":4,"skipped":69,"failed":0}
------------------------------
 [SLOW TEST] [20.728 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:09:36.702
    Feb 12 11:09:36.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 11:09:36.705
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:36.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:36.784
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 12 11:09:36.840: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 12 11:09:44.265: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/12/23 11:09:44.265
    Feb 12 11:09:44.266: INFO: Waiting up to 5m0s for pod "test-cleanup-controller-wmg9g" in namespace "deployment-5519" to be "running"
    Feb 12 11:09:47.376: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110271072s
    Feb 12 11:09:54.917: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Pending", Reason="", readiness=false. Elapsed: 10.650758567s
    Feb 12 11:09:55.416: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Pending", Reason="", readiness=false. Elapsed: 11.150007809s
    Feb 12 11:09:57.382: INFO: Pod "test-cleanup-controller-wmg9g": Phase="Running", Reason="", readiness=true. Elapsed: 13.115510785s
    Feb 12 11:09:57.382: INFO: Pod "test-cleanup-controller-wmg9g" satisfied condition "running"
    Feb 12 11:09:57.382: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/12/23 11:09:57.393
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 11:09:57.404: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5519  cc3e3a13-7cd2-4b9d-a407-6119c5245846 2834 1 2023-02-12 11:09:57 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-02-12 11:09:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002c41228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 12 11:09:57.407: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 11:09:57.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5519" for this suite. 02/12/23 11:09:57.415
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:09:57.431
Feb 12 11:09:57.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename proxy 02/12/23 11:09:57.432
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:57.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:57.474
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/12/23 11:09:57.49
STEP: creating replication controller proxy-service-mcm2r in namespace proxy-5126 02/12/23 11:09:57.49
I0212 11:09:57.506259      22 runners.go:193] Created replication controller with name: proxy-service-mcm2r, namespace: proxy-5126, replica count: 1
I0212 11:09:58.557892      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 11:09:59.558919      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 11:10:00.559605      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 11:10:01.559769      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0212 11:10:02.560681      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 11:10:02.565: INFO: setup took 5.088827328s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/12/23 11:10:02.565
Feb 12 11:10:02.577: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 11.962905ms)
Feb 12 11:10:02.577: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 12.023715ms)
Feb 12 11:10:02.577: INFO: (0) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 12.23669ms)
Feb 12 11:10:02.578: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.64534ms)
Feb 12 11:10:02.578: INFO: (0) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 12.740784ms)
Feb 12 11:10:02.579: INFO: (0) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 14.02582ms)
Feb 12 11:10:02.579: INFO: (0) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 13.792524ms)
Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 23.352528ms)
Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 23.438089ms)
Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 23.29194ms)
Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 23.377642ms)
Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 25.3468ms)
Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 25.427558ms)
Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 25.433433ms)
Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 25.418826ms)
Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 24.722848ms)
Feb 12 11:10:02.601: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 10.828321ms)
Feb 12 11:10:02.602: INFO: (1) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 10.180162ms)
Feb 12 11:10:02.602: INFO: (1) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.207217ms)
Feb 12 11:10:02.603: INFO: (1) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 10.934611ms)
Feb 12 11:10:02.605: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 14.455586ms)
Feb 12 11:10:02.605: INFO: (1) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 13.739217ms)
Feb 12 11:10:02.605: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 14.35322ms)
Feb 12 11:10:02.606: INFO: (1) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 15.079784ms)
Feb 12 11:10:02.606: INFO: (1) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 15.085344ms)
Feb 12 11:10:02.611: INFO: (1) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 20.493497ms)
Feb 12 11:10:02.614: INFO: (1) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 23.373141ms)
Feb 12 11:10:02.620: INFO: (1) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 28.048885ms)
Feb 12 11:10:02.620: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 28.140512ms)
Feb 12 11:10:02.620: INFO: (1) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 29.243412ms)
Feb 12 11:10:02.624: INFO: (1) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 32.298173ms)
Feb 12 11:10:02.628: INFO: (1) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 37.78732ms)
Feb 12 11:10:02.641: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 12.848818ms)
Feb 12 11:10:02.642: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.818175ms)
Feb 12 11:10:02.642: INFO: (2) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 12.485161ms)
Feb 12 11:10:02.642: INFO: (2) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 12.722128ms)
Feb 12 11:10:02.648: INFO: (2) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 19.241033ms)
Feb 12 11:10:02.648: INFO: (2) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 18.60443ms)
Feb 12 11:10:02.649: INFO: (2) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 20.63612ms)
Feb 12 11:10:02.649: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 20.210713ms)
Feb 12 11:10:02.649: INFO: (2) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 20.490921ms)
Feb 12 11:10:02.651: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 22.404172ms)
Feb 12 11:10:02.651: INFO: (2) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 22.154566ms)
Feb 12 11:10:02.651: INFO: (2) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 21.599531ms)
Feb 12 11:10:02.652: INFO: (2) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 22.214102ms)
Feb 12 11:10:02.659: INFO: (2) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 28.942269ms)
Feb 12 11:10:02.660: INFO: (2) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 31.310525ms)
Feb 12 11:10:02.661: INFO: (2) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 31.106705ms)
Feb 12 11:10:02.674: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.383878ms)
Feb 12 11:10:02.674: INFO: (3) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 13.607198ms)
Feb 12 11:10:02.674: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 13.627611ms)
Feb 12 11:10:02.675: INFO: (3) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 14.592797ms)
Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 20.160572ms)
Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 20.232019ms)
Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 20.207412ms)
Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 20.244554ms)
Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 20.458087ms)
Feb 12 11:10:02.684: INFO: (3) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 22.931075ms)
Feb 12 11:10:02.684: INFO: (3) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 22.83508ms)
Feb 12 11:10:02.685: INFO: (3) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 24.017554ms)
Feb 12 11:10:02.685: INFO: (3) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 24.083158ms)
Feb 12 11:10:02.688: INFO: (3) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 27.520213ms)
Feb 12 11:10:02.690: INFO: (3) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 28.732777ms)
Feb 12 11:10:02.690: INFO: (3) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 28.984169ms)
Feb 12 11:10:02.696: INFO: (4) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 5.74228ms)
Feb 12 11:10:02.696: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 4.858998ms)
Feb 12 11:10:02.696: INFO: (4) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 5.439663ms)
Feb 12 11:10:02.698: INFO: (4) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.892026ms)
Feb 12 11:10:02.698: INFO: (4) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 7.233426ms)
Feb 12 11:10:02.701: INFO: (4) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 10.923038ms)
Feb 12 11:10:02.702: INFO: (4) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 10.853227ms)
Feb 12 11:10:02.702: INFO: (4) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 10.756551ms)
Feb 12 11:10:02.703: INFO: (4) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 12.831759ms)
Feb 12 11:10:02.704: INFO: (4) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 12.733187ms)
Feb 12 11:10:02.709: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 18.395665ms)
Feb 12 11:10:02.709: INFO: (4) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 18.845932ms)
Feb 12 11:10:02.709: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 19.09171ms)
Feb 12 11:10:02.711: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 20.065933ms)
Feb 12 11:10:02.713: INFO: (4) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 22.679577ms)
Feb 12 11:10:02.715: INFO: (4) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 25.412621ms)
Feb 12 11:10:02.727: INFO: (5) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 11.556248ms)
Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 11.755229ms)
Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 12.106494ms)
Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 12.154024ms)
Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 12.267748ms)
Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.25497ms)
Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.632978ms)
Feb 12 11:10:02.730: INFO: (5) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 14.651645ms)
Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 19.087242ms)
Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 19.049563ms)
Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 19.245084ms)
Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 19.154665ms)
Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 19.379172ms)
Feb 12 11:10:02.738: INFO: (5) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 21.859704ms)
Feb 12 11:10:02.739: INFO: (5) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 23.145232ms)
Feb 12 11:10:02.739: INFO: (5) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 23.719502ms)
Feb 12 11:10:02.747: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 6.956245ms)
Feb 12 11:10:02.747: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 7.054552ms)
Feb 12 11:10:02.747: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 6.9869ms)
Feb 12 11:10:02.749: INFO: (6) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 8.797635ms)
Feb 12 11:10:02.750: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 9.817995ms)
Feb 12 11:10:02.750: INFO: (6) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 9.94007ms)
Feb 12 11:10:02.750: INFO: (6) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 10.448842ms)
Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 10.866478ms)
Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.054116ms)
Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 10.979696ms)
Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 10.738134ms)
Feb 12 11:10:02.752: INFO: (6) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 12.592672ms)
Feb 12 11:10:02.752: INFO: (6) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 12.414134ms)
Feb 12 11:10:02.752: INFO: (6) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 12.469913ms)
Feb 12 11:10:02.753: INFO: (6) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 13.929954ms)
Feb 12 11:10:02.754: INFO: (6) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 14.519283ms)
Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.172836ms)
Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.58098ms)
Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.300616ms)
Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 10.361156ms)
Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 10.756596ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 16.893639ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 17.15844ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 17.194607ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 17.785121ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 17.464295ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 17.686883ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 17.627185ms)
Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 17.65876ms)
Feb 12 11:10:02.775: INFO: (7) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 20.833802ms)
Feb 12 11:10:02.775: INFO: (7) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 20.601692ms)
Feb 12 11:10:02.782: INFO: (7) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 26.982622ms)
Feb 12 11:10:02.795: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 13.585434ms)
Feb 12 11:10:02.796: INFO: (8) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 14.547151ms)
Feb 12 11:10:02.798: INFO: (8) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 15.397657ms)
Feb 12 11:10:02.798: INFO: (8) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 16.063916ms)
Feb 12 11:10:02.798: INFO: (8) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 15.744349ms)
Feb 12 11:10:02.801: INFO: (8) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 19.41029ms)
Feb 12 11:10:02.801: INFO: (8) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 18.555072ms)
Feb 12 11:10:02.802: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 19.585997ms)
Feb 12 11:10:02.802: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 19.60726ms)
Feb 12 11:10:02.804: INFO: (8) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 21.399724ms)
Feb 12 11:10:02.813: INFO: (8) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 30.439407ms)
Feb 12 11:10:02.813: INFO: (8) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 30.731392ms)
Feb 12 11:10:02.818: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 35.173217ms)
Feb 12 11:10:02.818: INFO: (8) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 34.895552ms)
Feb 12 11:10:02.821: INFO: (8) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 39.269786ms)
Feb 12 11:10:02.824: INFO: (8) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 41.864919ms)
Feb 12 11:10:02.830: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 5.770868ms)
Feb 12 11:10:02.832: INFO: (9) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 7.773997ms)
Feb 12 11:10:02.832: INFO: (9) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 7.942521ms)
Feb 12 11:10:02.833: INFO: (9) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 8.578727ms)
Feb 12 11:10:02.834: INFO: (9) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 9.292391ms)
Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 13.554707ms)
Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 13.549768ms)
Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 13.455602ms)
Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 13.595658ms)
Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.716027ms)
Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 13.62213ms)
Feb 12 11:10:02.839: INFO: (9) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 14.61591ms)
Feb 12 11:10:02.841: INFO: (9) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 15.899564ms)
Feb 12 11:10:02.841: INFO: (9) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 15.945568ms)
Feb 12 11:10:02.841: INFO: (9) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 15.908615ms)
Feb 12 11:10:02.844: INFO: (9) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.860719ms)
Feb 12 11:10:02.854: INFO: (10) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 10.693586ms)
Feb 12 11:10:02.855: INFO: (10) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 10.955149ms)
Feb 12 11:10:02.855: INFO: (10) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.31256ms)
Feb 12 11:10:02.856: INFO: (10) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.918537ms)
Feb 12 11:10:02.856: INFO: (10) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 12.085537ms)
Feb 12 11:10:02.861: INFO: (10) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 17.628398ms)
Feb 12 11:10:02.861: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 17.682799ms)
Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 17.668358ms)
Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 17.694599ms)
Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 17.778699ms)
Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 17.777325ms)
Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 18.244437ms)
Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.446989ms)
Feb 12 11:10:02.864: INFO: (10) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 20.042456ms)
Feb 12 11:10:02.865: INFO: (10) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 20.887253ms)
Feb 12 11:10:02.866: INFO: (10) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 22.082415ms)
Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 6.543239ms)
Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 6.807259ms)
Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 7.124902ms)
Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 7.217982ms)
Feb 12 11:10:02.876: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 9.990977ms)
Feb 12 11:10:02.876: INFO: (11) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 9.920017ms)
Feb 12 11:10:02.876: INFO: (11) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 9.985407ms)
Feb 12 11:10:02.877: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.309804ms)
Feb 12 11:10:02.878: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 11.491063ms)
Feb 12 11:10:02.878: INFO: (11) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.491923ms)
Feb 12 11:10:02.885: INFO: (11) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 19.269272ms)
Feb 12 11:10:02.885: INFO: (11) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 19.37654ms)
Feb 12 11:10:02.885: INFO: (11) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 19.327013ms)
Feb 12 11:10:02.886: INFO: (11) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 19.388553ms)
Feb 12 11:10:02.886: INFO: (11) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 19.562368ms)
Feb 12 11:10:02.887: INFO: (11) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 21.433125ms)
Feb 12 11:10:02.894: INFO: (12) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 6.990129ms)
Feb 12 11:10:02.897: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.569815ms)
Feb 12 11:10:02.899: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.563852ms)
Feb 12 11:10:02.900: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.874661ms)
Feb 12 11:10:02.900: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.97668ms)
Feb 12 11:10:02.901: INFO: (12) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 13.617515ms)
Feb 12 11:10:02.901: INFO: (12) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 13.407417ms)
Feb 12 11:10:02.901: INFO: (12) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.517172ms)
Feb 12 11:10:02.902: INFO: (12) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 14.406372ms)
Feb 12 11:10:02.902: INFO: (12) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 14.533011ms)
Feb 12 11:10:02.903: INFO: (12) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 14.815242ms)
Feb 12 11:10:02.903: INFO: (12) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 15.353129ms)
Feb 12 11:10:02.906: INFO: (12) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 18.457959ms)
Feb 12 11:10:02.907: INFO: (12) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 19.166989ms)
Feb 12 11:10:02.907: INFO: (12) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 19.284643ms)
Feb 12 11:10:02.908: INFO: (12) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 20.208906ms)
Feb 12 11:10:02.920: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.668732ms)
Feb 12 11:10:02.920: INFO: (13) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 11.724868ms)
Feb 12 11:10:02.923: INFO: (13) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 14.847123ms)
Feb 12 11:10:02.924: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 15.431415ms)
Feb 12 11:10:02.925: INFO: (13) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 16.683555ms)
Feb 12 11:10:02.926: INFO: (13) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 17.977013ms)
Feb 12 11:10:02.926: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 17.853705ms)
Feb 12 11:10:02.926: INFO: (13) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 17.714171ms)
Feb 12 11:10:02.927: INFO: (13) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 18.932124ms)
Feb 12 11:10:02.933: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 24.755779ms)
Feb 12 11:10:02.933: INFO: (13) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 24.597456ms)
Feb 12 11:10:02.933: INFO: (13) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 25.026189ms)
Feb 12 11:10:02.934: INFO: (13) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 25.527535ms)
Feb 12 11:10:02.936: INFO: (13) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 28.102847ms)
Feb 12 11:10:02.942: INFO: (13) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 33.345701ms)
Feb 12 11:10:02.942: INFO: (13) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 33.398983ms)
Feb 12 11:10:02.948: INFO: (14) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 6.063829ms)
Feb 12 11:10:02.952: INFO: (14) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 8.721464ms)
Feb 12 11:10:02.952: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.186956ms)
Feb 12 11:10:02.952: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.049449ms)
Feb 12 11:10:02.953: INFO: (14) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.470154ms)
Feb 12 11:10:02.953: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.692901ms)
Feb 12 11:10:02.953: INFO: (14) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 11.688744ms)
Feb 12 11:10:02.958: INFO: (14) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 15.696102ms)
Feb 12 11:10:02.958: INFO: (14) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 15.069135ms)
Feb 12 11:10:02.958: INFO: (14) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 15.323675ms)
Feb 12 11:10:02.959: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 16.263853ms)
Feb 12 11:10:02.959: INFO: (14) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 16.683867ms)
Feb 12 11:10:02.959: INFO: (14) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 16.386541ms)
Feb 12 11:10:02.962: INFO: (14) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.631466ms)
Feb 12 11:10:02.962: INFO: (14) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 19.737891ms)
Feb 12 11:10:02.963: INFO: (14) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 20.324724ms)
Feb 12 11:10:02.974: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.413435ms)
Feb 12 11:10:02.974: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 10.822334ms)
Feb 12 11:10:02.975: INFO: (15) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.005339ms)
Feb 12 11:10:02.975: INFO: (15) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.182025ms)
Feb 12 11:10:02.975: INFO: (15) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.682454ms)
Feb 12 11:10:02.976: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.684008ms)
Feb 12 11:10:02.978: INFO: (15) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 14.369206ms)
Feb 12 11:10:02.978: INFO: (15) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 14.235178ms)
Feb 12 11:10:02.982: INFO: (15) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 18.460863ms)
Feb 12 11:10:02.982: INFO: (15) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 18.204534ms)
Feb 12 11:10:02.983: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 19.295538ms)
Feb 12 11:10:02.983: INFO: (15) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 19.668166ms)
Feb 12 11:10:02.985: INFO: (15) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 21.996427ms)
Feb 12 11:10:02.988: INFO: (15) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 24.673236ms)
Feb 12 11:10:02.990: INFO: (15) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 26.190802ms)
Feb 12 11:10:02.990: INFO: (15) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 26.495007ms)
Feb 12 11:10:02.996: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 5.882767ms)
Feb 12 11:10:02.996: INFO: (16) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 6.187086ms)
Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 9.573359ms)
Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 9.463578ms)
Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 9.692665ms)
Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 9.927633ms)
Feb 12 11:10:03.001: INFO: (16) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 9.649846ms)
Feb 12 11:10:03.001: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.83945ms)
Feb 12 11:10:03.001: INFO: (16) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.034089ms)
Feb 12 11:10:03.003: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.144136ms)
Feb 12 11:10:03.003: INFO: (16) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 12.358871ms)
Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 13.175856ms)
Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 13.746928ms)
Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 13.647597ms)
Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 13.4815ms)
Feb 12 11:10:03.006: INFO: (16) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 15.229038ms)
Feb 12 11:10:03.012: INFO: (17) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 6.524257ms)
Feb 12 11:10:03.014: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 7.625659ms)
Feb 12 11:10:03.014: INFO: (17) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.697734ms)
Feb 12 11:10:03.014: INFO: (17) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 8.405367ms)
Feb 12 11:10:03.015: INFO: (17) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 8.670976ms)
Feb 12 11:10:03.015: INFO: (17) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 9.131553ms)
Feb 12 11:10:03.016: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.6621ms)
Feb 12 11:10:03.016: INFO: (17) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 9.765639ms)
Feb 12 11:10:03.018: INFO: (17) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 12.229696ms)
Feb 12 11:10:03.019: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 13.495669ms)
Feb 12 11:10:03.020: INFO: (17) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 13.659665ms)
Feb 12 11:10:03.019: INFO: (17) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.578781ms)
Feb 12 11:10:03.020: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 13.790253ms)
Feb 12 11:10:03.021: INFO: (17) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 14.785595ms)
Feb 12 11:10:03.021: INFO: (17) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 15.174203ms)
Feb 12 11:10:03.023: INFO: (17) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 16.756733ms)
Feb 12 11:10:03.030: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.186657ms)
Feb 12 11:10:03.031: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 7.744307ms)
Feb 12 11:10:03.033: INFO: (18) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 9.695753ms)
Feb 12 11:10:03.033: INFO: (18) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 9.649925ms)
Feb 12 11:10:03.033: INFO: (18) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 10.168885ms)
Feb 12 11:10:03.034: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 11.182614ms)
Feb 12 11:10:03.034: INFO: (18) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.540047ms)
Feb 12 11:10:03.034: INFO: (18) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.667819ms)
Feb 12 11:10:03.035: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.586713ms)
Feb 12 11:10:03.035: INFO: (18) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 11.626142ms)
Feb 12 11:10:03.035: INFO: (18) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 12.28186ms)
Feb 12 11:10:03.036: INFO: (18) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 13.473624ms)
Feb 12 11:10:03.037: INFO: (18) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 13.521133ms)
Feb 12 11:10:03.039: INFO: (18) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 16.125297ms)
Feb 12 11:10:03.040: INFO: (18) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 16.916558ms)
Feb 12 11:10:03.042: INFO: (18) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 18.849258ms)
Feb 12 11:10:03.048: INFO: (19) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 6.437868ms)
Feb 12 11:10:03.049: INFO: (19) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.050848ms)
Feb 12 11:10:03.051: INFO: (19) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 8.174736ms)
Feb 12 11:10:03.051: INFO: (19) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 8.745333ms)
Feb 12 11:10:03.052: INFO: (19) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 10.04641ms)
Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 11.113115ms)
Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 11.314962ms)
Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.396548ms)
Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.943574ms)
Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.805524ms)
Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 12.033013ms)
Feb 12 11:10:03.057: INFO: (19) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 14.926865ms)
Feb 12 11:10:03.057: INFO: (19) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 14.778715ms)
Feb 12 11:10:03.058: INFO: (19) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 15.357813ms)
Feb 12 11:10:03.058: INFO: (19) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 15.474557ms)
Feb 12 11:10:03.061: INFO: (19) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.721094ms)
STEP: deleting ReplicationController proxy-service-mcm2r in namespace proxy-5126, will wait for the garbage collector to delete the pods 02/12/23 11:10:03.061
Feb 12 11:10:03.131: INFO: Deleting ReplicationController proxy-service-mcm2r took: 13.059476ms
Feb 12 11:10:03.232: INFO: Terminating ReplicationController proxy-service-mcm2r pods took: 101.117938ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 12 11:10:06.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5126" for this suite. 02/12/23 11:10:06.14
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":5,"skipped":71,"failed":0}
------------------------------
 [SLOW TEST] [8.720 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:09:57.431
    Feb 12 11:09:57.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename proxy 02/12/23 11:09:57.432
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:09:57.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:09:57.474
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/12/23 11:09:57.49
    STEP: creating replication controller proxy-service-mcm2r in namespace proxy-5126 02/12/23 11:09:57.49
    I0212 11:09:57.506259      22 runners.go:193] Created replication controller with name: proxy-service-mcm2r, namespace: proxy-5126, replica count: 1
    I0212 11:09:58.557892      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 11:09:59.558919      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 11:10:00.559605      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 11:10:01.559769      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0212 11:10:02.560681      22 runners.go:193] proxy-service-mcm2r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 11:10:02.565: INFO: setup took 5.088827328s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/12/23 11:10:02.565
    Feb 12 11:10:02.577: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 11.962905ms)
    Feb 12 11:10:02.577: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 12.023715ms)
    Feb 12 11:10:02.577: INFO: (0) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 12.23669ms)
    Feb 12 11:10:02.578: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.64534ms)
    Feb 12 11:10:02.578: INFO: (0) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 12.740784ms)
    Feb 12 11:10:02.579: INFO: (0) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 14.02582ms)
    Feb 12 11:10:02.579: INFO: (0) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 13.792524ms)
    Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 23.352528ms)
    Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 23.438089ms)
    Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 23.29194ms)
    Feb 12 11:10:02.588: INFO: (0) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 23.377642ms)
    Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 25.3468ms)
    Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 25.427558ms)
    Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 25.433433ms)
    Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 25.418826ms)
    Feb 12 11:10:02.591: INFO: (0) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 24.722848ms)
    Feb 12 11:10:02.601: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 10.828321ms)
    Feb 12 11:10:02.602: INFO: (1) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 10.180162ms)
    Feb 12 11:10:02.602: INFO: (1) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.207217ms)
    Feb 12 11:10:02.603: INFO: (1) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 10.934611ms)
    Feb 12 11:10:02.605: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 14.455586ms)
    Feb 12 11:10:02.605: INFO: (1) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 13.739217ms)
    Feb 12 11:10:02.605: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 14.35322ms)
    Feb 12 11:10:02.606: INFO: (1) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 15.079784ms)
    Feb 12 11:10:02.606: INFO: (1) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 15.085344ms)
    Feb 12 11:10:02.611: INFO: (1) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 20.493497ms)
    Feb 12 11:10:02.614: INFO: (1) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 23.373141ms)
    Feb 12 11:10:02.620: INFO: (1) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 28.048885ms)
    Feb 12 11:10:02.620: INFO: (1) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 28.140512ms)
    Feb 12 11:10:02.620: INFO: (1) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 29.243412ms)
    Feb 12 11:10:02.624: INFO: (1) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 32.298173ms)
    Feb 12 11:10:02.628: INFO: (1) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 37.78732ms)
    Feb 12 11:10:02.641: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 12.848818ms)
    Feb 12 11:10:02.642: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.818175ms)
    Feb 12 11:10:02.642: INFO: (2) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 12.485161ms)
    Feb 12 11:10:02.642: INFO: (2) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 12.722128ms)
    Feb 12 11:10:02.648: INFO: (2) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 19.241033ms)
    Feb 12 11:10:02.648: INFO: (2) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 18.60443ms)
    Feb 12 11:10:02.649: INFO: (2) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 20.63612ms)
    Feb 12 11:10:02.649: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 20.210713ms)
    Feb 12 11:10:02.649: INFO: (2) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 20.490921ms)
    Feb 12 11:10:02.651: INFO: (2) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 22.404172ms)
    Feb 12 11:10:02.651: INFO: (2) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 22.154566ms)
    Feb 12 11:10:02.651: INFO: (2) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 21.599531ms)
    Feb 12 11:10:02.652: INFO: (2) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 22.214102ms)
    Feb 12 11:10:02.659: INFO: (2) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 28.942269ms)
    Feb 12 11:10:02.660: INFO: (2) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 31.310525ms)
    Feb 12 11:10:02.661: INFO: (2) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 31.106705ms)
    Feb 12 11:10:02.674: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.383878ms)
    Feb 12 11:10:02.674: INFO: (3) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 13.607198ms)
    Feb 12 11:10:02.674: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 13.627611ms)
    Feb 12 11:10:02.675: INFO: (3) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 14.592797ms)
    Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 20.160572ms)
    Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 20.232019ms)
    Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 20.207412ms)
    Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 20.244554ms)
    Feb 12 11:10:02.681: INFO: (3) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 20.458087ms)
    Feb 12 11:10:02.684: INFO: (3) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 22.931075ms)
    Feb 12 11:10:02.684: INFO: (3) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 22.83508ms)
    Feb 12 11:10:02.685: INFO: (3) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 24.017554ms)
    Feb 12 11:10:02.685: INFO: (3) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 24.083158ms)
    Feb 12 11:10:02.688: INFO: (3) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 27.520213ms)
    Feb 12 11:10:02.690: INFO: (3) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 28.732777ms)
    Feb 12 11:10:02.690: INFO: (3) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 28.984169ms)
    Feb 12 11:10:02.696: INFO: (4) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 5.74228ms)
    Feb 12 11:10:02.696: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 4.858998ms)
    Feb 12 11:10:02.696: INFO: (4) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 5.439663ms)
    Feb 12 11:10:02.698: INFO: (4) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.892026ms)
    Feb 12 11:10:02.698: INFO: (4) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 7.233426ms)
    Feb 12 11:10:02.701: INFO: (4) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 10.923038ms)
    Feb 12 11:10:02.702: INFO: (4) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 10.853227ms)
    Feb 12 11:10:02.702: INFO: (4) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 10.756551ms)
    Feb 12 11:10:02.703: INFO: (4) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 12.831759ms)
    Feb 12 11:10:02.704: INFO: (4) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 12.733187ms)
    Feb 12 11:10:02.709: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 18.395665ms)
    Feb 12 11:10:02.709: INFO: (4) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 18.845932ms)
    Feb 12 11:10:02.709: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 19.09171ms)
    Feb 12 11:10:02.711: INFO: (4) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 20.065933ms)
    Feb 12 11:10:02.713: INFO: (4) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 22.679577ms)
    Feb 12 11:10:02.715: INFO: (4) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 25.412621ms)
    Feb 12 11:10:02.727: INFO: (5) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 11.556248ms)
    Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 11.755229ms)
    Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 12.106494ms)
    Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 12.154024ms)
    Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 12.267748ms)
    Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.25497ms)
    Feb 12 11:10:02.728: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.632978ms)
    Feb 12 11:10:02.730: INFO: (5) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 14.651645ms)
    Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 19.087242ms)
    Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 19.049563ms)
    Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 19.245084ms)
    Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 19.154665ms)
    Feb 12 11:10:02.735: INFO: (5) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 19.379172ms)
    Feb 12 11:10:02.738: INFO: (5) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 21.859704ms)
    Feb 12 11:10:02.739: INFO: (5) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 23.145232ms)
    Feb 12 11:10:02.739: INFO: (5) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 23.719502ms)
    Feb 12 11:10:02.747: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 6.956245ms)
    Feb 12 11:10:02.747: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 7.054552ms)
    Feb 12 11:10:02.747: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 6.9869ms)
    Feb 12 11:10:02.749: INFO: (6) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 8.797635ms)
    Feb 12 11:10:02.750: INFO: (6) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 9.817995ms)
    Feb 12 11:10:02.750: INFO: (6) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 9.94007ms)
    Feb 12 11:10:02.750: INFO: (6) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 10.448842ms)
    Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 10.866478ms)
    Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.054116ms)
    Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 10.979696ms)
    Feb 12 11:10:02.751: INFO: (6) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 10.738134ms)
    Feb 12 11:10:02.752: INFO: (6) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 12.592672ms)
    Feb 12 11:10:02.752: INFO: (6) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 12.414134ms)
    Feb 12 11:10:02.752: INFO: (6) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 12.469913ms)
    Feb 12 11:10:02.753: INFO: (6) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 13.929954ms)
    Feb 12 11:10:02.754: INFO: (6) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 14.519283ms)
    Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.172836ms)
    Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.58098ms)
    Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.300616ms)
    Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 10.361156ms)
    Feb 12 11:10:02.765: INFO: (7) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 10.756596ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 16.893639ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 17.15844ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 17.194607ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 17.785121ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 17.464295ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 17.686883ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 17.627185ms)
    Feb 12 11:10:02.772: INFO: (7) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 17.65876ms)
    Feb 12 11:10:02.775: INFO: (7) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 20.833802ms)
    Feb 12 11:10:02.775: INFO: (7) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 20.601692ms)
    Feb 12 11:10:02.782: INFO: (7) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 26.982622ms)
    Feb 12 11:10:02.795: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 13.585434ms)
    Feb 12 11:10:02.796: INFO: (8) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 14.547151ms)
    Feb 12 11:10:02.798: INFO: (8) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 15.397657ms)
    Feb 12 11:10:02.798: INFO: (8) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 16.063916ms)
    Feb 12 11:10:02.798: INFO: (8) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 15.744349ms)
    Feb 12 11:10:02.801: INFO: (8) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 19.41029ms)
    Feb 12 11:10:02.801: INFO: (8) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 18.555072ms)
    Feb 12 11:10:02.802: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 19.585997ms)
    Feb 12 11:10:02.802: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 19.60726ms)
    Feb 12 11:10:02.804: INFO: (8) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 21.399724ms)
    Feb 12 11:10:02.813: INFO: (8) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 30.439407ms)
    Feb 12 11:10:02.813: INFO: (8) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 30.731392ms)
    Feb 12 11:10:02.818: INFO: (8) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 35.173217ms)
    Feb 12 11:10:02.818: INFO: (8) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 34.895552ms)
    Feb 12 11:10:02.821: INFO: (8) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 39.269786ms)
    Feb 12 11:10:02.824: INFO: (8) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 41.864919ms)
    Feb 12 11:10:02.830: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 5.770868ms)
    Feb 12 11:10:02.832: INFO: (9) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 7.773997ms)
    Feb 12 11:10:02.832: INFO: (9) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 7.942521ms)
    Feb 12 11:10:02.833: INFO: (9) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 8.578727ms)
    Feb 12 11:10:02.834: INFO: (9) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 9.292391ms)
    Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 13.554707ms)
    Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 13.549768ms)
    Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 13.455602ms)
    Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 13.595658ms)
    Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.716027ms)
    Feb 12 11:10:02.838: INFO: (9) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 13.62213ms)
    Feb 12 11:10:02.839: INFO: (9) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 14.61591ms)
    Feb 12 11:10:02.841: INFO: (9) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 15.899564ms)
    Feb 12 11:10:02.841: INFO: (9) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 15.945568ms)
    Feb 12 11:10:02.841: INFO: (9) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 15.908615ms)
    Feb 12 11:10:02.844: INFO: (9) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.860719ms)
    Feb 12 11:10:02.854: INFO: (10) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 10.693586ms)
    Feb 12 11:10:02.855: INFO: (10) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 10.955149ms)
    Feb 12 11:10:02.855: INFO: (10) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.31256ms)
    Feb 12 11:10:02.856: INFO: (10) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.918537ms)
    Feb 12 11:10:02.856: INFO: (10) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 12.085537ms)
    Feb 12 11:10:02.861: INFO: (10) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 17.628398ms)
    Feb 12 11:10:02.861: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 17.682799ms)
    Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 17.668358ms)
    Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 17.694599ms)
    Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 17.778699ms)
    Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 17.777325ms)
    Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 18.244437ms)
    Feb 12 11:10:02.862: INFO: (10) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.446989ms)
    Feb 12 11:10:02.864: INFO: (10) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 20.042456ms)
    Feb 12 11:10:02.865: INFO: (10) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 20.887253ms)
    Feb 12 11:10:02.866: INFO: (10) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 22.082415ms)
    Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 6.543239ms)
    Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 6.807259ms)
    Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 7.124902ms)
    Feb 12 11:10:02.873: INFO: (11) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 7.217982ms)
    Feb 12 11:10:02.876: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 9.990977ms)
    Feb 12 11:10:02.876: INFO: (11) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 9.920017ms)
    Feb 12 11:10:02.876: INFO: (11) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 9.985407ms)
    Feb 12 11:10:02.877: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.309804ms)
    Feb 12 11:10:02.878: INFO: (11) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 11.491063ms)
    Feb 12 11:10:02.878: INFO: (11) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.491923ms)
    Feb 12 11:10:02.885: INFO: (11) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 19.269272ms)
    Feb 12 11:10:02.885: INFO: (11) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 19.37654ms)
    Feb 12 11:10:02.885: INFO: (11) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 19.327013ms)
    Feb 12 11:10:02.886: INFO: (11) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 19.388553ms)
    Feb 12 11:10:02.886: INFO: (11) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 19.562368ms)
    Feb 12 11:10:02.887: INFO: (11) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 21.433125ms)
    Feb 12 11:10:02.894: INFO: (12) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 6.990129ms)
    Feb 12 11:10:02.897: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.569815ms)
    Feb 12 11:10:02.899: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.563852ms)
    Feb 12 11:10:02.900: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.874661ms)
    Feb 12 11:10:02.900: INFO: (12) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.97668ms)
    Feb 12 11:10:02.901: INFO: (12) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 13.617515ms)
    Feb 12 11:10:02.901: INFO: (12) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 13.407417ms)
    Feb 12 11:10:02.901: INFO: (12) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.517172ms)
    Feb 12 11:10:02.902: INFO: (12) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 14.406372ms)
    Feb 12 11:10:02.902: INFO: (12) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 14.533011ms)
    Feb 12 11:10:02.903: INFO: (12) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 14.815242ms)
    Feb 12 11:10:02.903: INFO: (12) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 15.353129ms)
    Feb 12 11:10:02.906: INFO: (12) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 18.457959ms)
    Feb 12 11:10:02.907: INFO: (12) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 19.166989ms)
    Feb 12 11:10:02.907: INFO: (12) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 19.284643ms)
    Feb 12 11:10:02.908: INFO: (12) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 20.208906ms)
    Feb 12 11:10:02.920: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.668732ms)
    Feb 12 11:10:02.920: INFO: (13) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 11.724868ms)
    Feb 12 11:10:02.923: INFO: (13) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 14.847123ms)
    Feb 12 11:10:02.924: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 15.431415ms)
    Feb 12 11:10:02.925: INFO: (13) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 16.683555ms)
    Feb 12 11:10:02.926: INFO: (13) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 17.977013ms)
    Feb 12 11:10:02.926: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 17.853705ms)
    Feb 12 11:10:02.926: INFO: (13) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 17.714171ms)
    Feb 12 11:10:02.927: INFO: (13) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 18.932124ms)
    Feb 12 11:10:02.933: INFO: (13) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 24.755779ms)
    Feb 12 11:10:02.933: INFO: (13) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 24.597456ms)
    Feb 12 11:10:02.933: INFO: (13) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 25.026189ms)
    Feb 12 11:10:02.934: INFO: (13) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 25.527535ms)
    Feb 12 11:10:02.936: INFO: (13) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 28.102847ms)
    Feb 12 11:10:02.942: INFO: (13) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 33.345701ms)
    Feb 12 11:10:02.942: INFO: (13) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 33.398983ms)
    Feb 12 11:10:02.948: INFO: (14) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 6.063829ms)
    Feb 12 11:10:02.952: INFO: (14) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 8.721464ms)
    Feb 12 11:10:02.952: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.186956ms)
    Feb 12 11:10:02.952: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.049449ms)
    Feb 12 11:10:02.953: INFO: (14) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.470154ms)
    Feb 12 11:10:02.953: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.692901ms)
    Feb 12 11:10:02.953: INFO: (14) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 11.688744ms)
    Feb 12 11:10:02.958: INFO: (14) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 15.696102ms)
    Feb 12 11:10:02.958: INFO: (14) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 15.069135ms)
    Feb 12 11:10:02.958: INFO: (14) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 15.323675ms)
    Feb 12 11:10:02.959: INFO: (14) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 16.263853ms)
    Feb 12 11:10:02.959: INFO: (14) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 16.683867ms)
    Feb 12 11:10:02.959: INFO: (14) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 16.386541ms)
    Feb 12 11:10:02.962: INFO: (14) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.631466ms)
    Feb 12 11:10:02.962: INFO: (14) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 19.737891ms)
    Feb 12 11:10:02.963: INFO: (14) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 20.324724ms)
    Feb 12 11:10:02.974: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 10.413435ms)
    Feb 12 11:10:02.974: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 10.822334ms)
    Feb 12 11:10:02.975: INFO: (15) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.005339ms)
    Feb 12 11:10:02.975: INFO: (15) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.182025ms)
    Feb 12 11:10:02.975: INFO: (15) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.682454ms)
    Feb 12 11:10:02.976: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.684008ms)
    Feb 12 11:10:02.978: INFO: (15) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 14.369206ms)
    Feb 12 11:10:02.978: INFO: (15) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 14.235178ms)
    Feb 12 11:10:02.982: INFO: (15) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 18.460863ms)
    Feb 12 11:10:02.982: INFO: (15) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 18.204534ms)
    Feb 12 11:10:02.983: INFO: (15) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 19.295538ms)
    Feb 12 11:10:02.983: INFO: (15) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 19.668166ms)
    Feb 12 11:10:02.985: INFO: (15) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 21.996427ms)
    Feb 12 11:10:02.988: INFO: (15) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 24.673236ms)
    Feb 12 11:10:02.990: INFO: (15) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 26.190802ms)
    Feb 12 11:10:02.990: INFO: (15) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 26.495007ms)
    Feb 12 11:10:02.996: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 5.882767ms)
    Feb 12 11:10:02.996: INFO: (16) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 6.187086ms)
    Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 9.573359ms)
    Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 9.463578ms)
    Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 9.692665ms)
    Feb 12 11:10:03.000: INFO: (16) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 9.927633ms)
    Feb 12 11:10:03.001: INFO: (16) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 9.649846ms)
    Feb 12 11:10:03.001: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.83945ms)
    Feb 12 11:10:03.001: INFO: (16) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 10.034089ms)
    Feb 12 11:10:03.003: INFO: (16) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 12.144136ms)
    Feb 12 11:10:03.003: INFO: (16) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 12.358871ms)
    Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 13.175856ms)
    Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 13.746928ms)
    Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 13.647597ms)
    Feb 12 11:10:03.004: INFO: (16) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 13.4815ms)
    Feb 12 11:10:03.006: INFO: (16) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 15.229038ms)
    Feb 12 11:10:03.012: INFO: (17) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 6.524257ms)
    Feb 12 11:10:03.014: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 7.625659ms)
    Feb 12 11:10:03.014: INFO: (17) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.697734ms)
    Feb 12 11:10:03.014: INFO: (17) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 8.405367ms)
    Feb 12 11:10:03.015: INFO: (17) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 8.670976ms)
    Feb 12 11:10:03.015: INFO: (17) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 9.131553ms)
    Feb 12 11:10:03.016: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 9.6621ms)
    Feb 12 11:10:03.016: INFO: (17) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 9.765639ms)
    Feb 12 11:10:03.018: INFO: (17) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 12.229696ms)
    Feb 12 11:10:03.019: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 13.495669ms)
    Feb 12 11:10:03.020: INFO: (17) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 13.659665ms)
    Feb 12 11:10:03.019: INFO: (17) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 13.578781ms)
    Feb 12 11:10:03.020: INFO: (17) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 13.790253ms)
    Feb 12 11:10:03.021: INFO: (17) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 14.785595ms)
    Feb 12 11:10:03.021: INFO: (17) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 15.174203ms)
    Feb 12 11:10:03.023: INFO: (17) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 16.756733ms)
    Feb 12 11:10:03.030: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.186657ms)
    Feb 12 11:10:03.031: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 7.744307ms)
    Feb 12 11:10:03.033: INFO: (18) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 9.695753ms)
    Feb 12 11:10:03.033: INFO: (18) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 9.649925ms)
    Feb 12 11:10:03.033: INFO: (18) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 10.168885ms)
    Feb 12 11:10:03.034: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 11.182614ms)
    Feb 12 11:10:03.034: INFO: (18) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.540047ms)
    Feb 12 11:10:03.034: INFO: (18) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.667819ms)
    Feb 12 11:10:03.035: INFO: (18) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.586713ms)
    Feb 12 11:10:03.035: INFO: (18) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 11.626142ms)
    Feb 12 11:10:03.035: INFO: (18) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 12.28186ms)
    Feb 12 11:10:03.036: INFO: (18) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 13.473624ms)
    Feb 12 11:10:03.037: INFO: (18) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 13.521133ms)
    Feb 12 11:10:03.039: INFO: (18) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 16.125297ms)
    Feb 12 11:10:03.040: INFO: (18) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 16.916558ms)
    Feb 12 11:10:03.042: INFO: (18) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 18.849258ms)
    Feb 12 11:10:03.048: INFO: (19) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:443/proxy/tlsrewritem... (200; 6.437868ms)
    Feb 12 11:10:03.049: INFO: (19) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:160/proxy/: foo (200; 7.050848ms)
    Feb 12 11:10:03.051: INFO: (19) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:162/proxy/: bar (200; 8.174736ms)
    Feb 12 11:10:03.051: INFO: (19) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:462/proxy/: tls qux (200; 8.745333ms)
    Feb 12 11:10:03.052: INFO: (19) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname2/proxy/: bar (200; 10.04641ms)
    Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:1080/proxy/rewriteme">test<... (200; 11.113115ms)
    Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/https:proxy-service-mcm2r-297w2:460/proxy/: tls baz (200; 11.314962ms)
    Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/http:proxy-service-mcm2r-297w2:1080/proxy/rewriteme">... (200; 11.396548ms)
    Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:160/proxy/: foo (200; 11.943574ms)
    Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2:162/proxy/: bar (200; 11.805524ms)
    Feb 12 11:10:03.054: INFO: (19) /api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/: <a href="/api/v1/namespaces/proxy-5126/pods/proxy-service-mcm2r-297w2/proxy/rewriteme">test</a> (200; 12.033013ms)
    Feb 12 11:10:03.057: INFO: (19) /api/v1/namespaces/proxy-5126/services/proxy-service-mcm2r:portname1/proxy/: foo (200; 14.926865ms)
    Feb 12 11:10:03.057: INFO: (19) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname1/proxy/: tls baz (200; 14.778715ms)
    Feb 12 11:10:03.058: INFO: (19) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname2/proxy/: bar (200; 15.357813ms)
    Feb 12 11:10:03.058: INFO: (19) /api/v1/namespaces/proxy-5126/services/https:proxy-service-mcm2r:tlsportname2/proxy/: tls qux (200; 15.474557ms)
    Feb 12 11:10:03.061: INFO: (19) /api/v1/namespaces/proxy-5126/services/http:proxy-service-mcm2r:portname1/proxy/: foo (200; 18.721094ms)
    STEP: deleting ReplicationController proxy-service-mcm2r in namespace proxy-5126, will wait for the garbage collector to delete the pods 02/12/23 11:10:03.061
    Feb 12 11:10:03.131: INFO: Deleting ReplicationController proxy-service-mcm2r took: 13.059476ms
    Feb 12 11:10:03.232: INFO: Terminating ReplicationController proxy-service-mcm2r pods took: 101.117938ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 12 11:10:06.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5126" for this suite. 02/12/23 11:10:06.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:10:06.154
Feb 12 11:10:06.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename job 02/12/23 11:10:06.155
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:06.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:06.2
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 02/12/23 11:10:06.204
STEP: Ensure pods equal to paralellism count is attached to the job 02/12/23 11:10:06.215
STEP: patching /status 02/12/23 11:10:12.233
STEP: updating /status 02/12/23 11:10:12.277
STEP: get /status 02/12/23 11:10:12.301
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 12 11:10:12.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-565" for this suite. 02/12/23 11:10:12.315
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":6,"skipped":93,"failed":0}
------------------------------
 [SLOW TEST] [6.180 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:10:06.154
    Feb 12 11:10:06.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename job 02/12/23 11:10:06.155
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:06.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:06.2
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 02/12/23 11:10:06.204
    STEP: Ensure pods equal to paralellism count is attached to the job 02/12/23 11:10:06.215
    STEP: patching /status 02/12/23 11:10:12.233
    STEP: updating /status 02/12/23 11:10:12.277
    STEP: get /status 02/12/23 11:10:12.301
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 12 11:10:12.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-565" for this suite. 02/12/23 11:10:12.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:10:12.335
Feb 12 11:10:12.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:10:12.335
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:12.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:12.393
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 02/12/23 11:10:12.43
STEP: submitting the pod to kubernetes 02/12/23 11:10:12.43
Feb 12 11:10:12.470: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" in namespace "pods-595" to be "running and ready"
Feb 12 11:10:12.475: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904505ms
Feb 12 11:10:12.475: INFO: The phase of Pod pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:10:14.490: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019562957s
Feb 12 11:10:14.490: INFO: The phase of Pod pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a is Running (Ready = true)
Feb 12 11:10:14.490: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/12/23 11:10:14.5
STEP: updating the pod 02/12/23 11:10:14.515
Feb 12 11:10:15.047: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a"
Feb 12 11:10:15.047: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" in namespace "pods-595" to be "terminated with reason DeadlineExceeded"
Feb 12 11:10:15.051: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=true. Elapsed: 4.191998ms
Feb 12 11:10:17.055: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007922444s
Feb 12 11:10:19.634: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=false. Elapsed: 4.586526055s
Feb 12 11:10:21.413: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.365361585s
Feb 12 11:10:21.413: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:10:21.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-595" for this suite. 02/12/23 11:10:21.432
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":7,"skipped":131,"failed":0}
------------------------------
 [SLOW TEST] [9.273 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:10:12.335
    Feb 12 11:10:12.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:10:12.335
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:12.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:12.393
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 02/12/23 11:10:12.43
    STEP: submitting the pod to kubernetes 02/12/23 11:10:12.43
    Feb 12 11:10:12.470: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" in namespace "pods-595" to be "running and ready"
    Feb 12 11:10:12.475: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904505ms
    Feb 12 11:10:12.475: INFO: The phase of Pod pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:10:14.490: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.019562957s
    Feb 12 11:10:14.490: INFO: The phase of Pod pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a is Running (Ready = true)
    Feb 12 11:10:14.490: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/12/23 11:10:14.5
    STEP: updating the pod 02/12/23 11:10:14.515
    Feb 12 11:10:15.047: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a"
    Feb 12 11:10:15.047: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" in namespace "pods-595" to be "terminated with reason DeadlineExceeded"
    Feb 12 11:10:15.051: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=true. Elapsed: 4.191998ms
    Feb 12 11:10:17.055: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.007922444s
    Feb 12 11:10:19.634: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Running", Reason="", readiness=false. Elapsed: 4.586526055s
    Feb 12 11:10:21.413: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.365361585s
    Feb 12 11:10:21.413: INFO: Pod "pod-update-activedeadlineseconds-b835895f-556c-4725-9162-f8ced08eef7a" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:10:21.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-595" for this suite. 02/12/23 11:10:21.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:10:21.608
Feb 12 11:10:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:10:21.609
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:21.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:21.845
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-abcc3fd0-9a4d-4a4e-b817-2c3958d4bb55 02/12/23 11:10:21.853
STEP: Creating a pod to test consume configMaps 02/12/23 11:10:21.894
Feb 12 11:10:21.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85" in namespace "configmap-9006" to be "Succeeded or Failed"
Feb 12 11:10:22.005: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Pending", Reason="", readiness=false. Elapsed: 9.300727ms
Feb 12 11:10:24.022: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026054424s
Feb 12 11:10:26.017: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02105022s
Feb 12 11:10:28.015: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018685342s
STEP: Saw pod success 02/12/23 11:10:28.015
Feb 12 11:10:28.015: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85" satisfied condition "Succeeded or Failed"
Feb 12 11:10:28.019: INFO: Trying to get logs from node kube-3 pod pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:10:28.043
Feb 12 11:10:28.068: INFO: Waiting for pod pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85 to disappear
Feb 12 11:10:28.072: INFO: Pod pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:10:28.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9006" for this suite. 02/12/23 11:10:28.076
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":8,"skipped":145,"failed":0}
------------------------------
 [SLOW TEST] [6.478 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:10:21.608
    Feb 12 11:10:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:10:21.609
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:21.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:21.845
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-abcc3fd0-9a4d-4a4e-b817-2c3958d4bb55 02/12/23 11:10:21.853
    STEP: Creating a pod to test consume configMaps 02/12/23 11:10:21.894
    Feb 12 11:10:21.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85" in namespace "configmap-9006" to be "Succeeded or Failed"
    Feb 12 11:10:22.005: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Pending", Reason="", readiness=false. Elapsed: 9.300727ms
    Feb 12 11:10:24.022: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026054424s
    Feb 12 11:10:26.017: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02105022s
    Feb 12 11:10:28.015: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018685342s
    STEP: Saw pod success 02/12/23 11:10:28.015
    Feb 12 11:10:28.015: INFO: Pod "pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85" satisfied condition "Succeeded or Failed"
    Feb 12 11:10:28.019: INFO: Trying to get logs from node kube-3 pod pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:10:28.043
    Feb 12 11:10:28.068: INFO: Waiting for pod pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85 to disappear
    Feb 12 11:10:28.072: INFO: Pod pod-configmaps-0a650512-0716-484a-8187-5caafbc43f85 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:10:28.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9006" for this suite. 02/12/23 11:10:28.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:10:28.091
Feb 12 11:10:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename subpath 02/12/23 11:10:28.092
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:28.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:28.115
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/12/23 11:10:28.12
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-v9rj 02/12/23 11:10:28.131
STEP: Creating a pod to test atomic-volume-subpath 02/12/23 11:10:28.131
Feb 12 11:10:28.142: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v9rj" in namespace "subpath-9798" to be "Succeeded or Failed"
Feb 12 11:10:28.149: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Pending", Reason="", readiness=false. Elapsed: 7.182018ms
Feb 12 11:10:30.163: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 2.02133777s
Feb 12 11:10:32.162: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 4.020153245s
Feb 12 11:10:34.160: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 6.018196125s
Feb 12 11:10:36.163: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 8.021142365s
Feb 12 11:10:38.156: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 10.014031558s
Feb 12 11:10:40.160: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 12.01801545s
Feb 12 11:10:42.159: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 14.017163498s
Feb 12 11:10:44.154: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 16.011564688s
Feb 12 11:10:46.164: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 18.021723408s
Feb 12 11:10:48.154: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 20.012380667s
Feb 12 11:10:50.154: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=false. Elapsed: 22.012108191s
Feb 12 11:10:52.155: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013035617s
STEP: Saw pod success 02/12/23 11:10:52.155
Feb 12 11:10:52.155: INFO: Pod "pod-subpath-test-configmap-v9rj" satisfied condition "Succeeded or Failed"
Feb 12 11:10:52.160: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-v9rj container test-container-subpath-configmap-v9rj: <nil>
STEP: delete the pod 02/12/23 11:10:52.168
Feb 12 11:10:52.187: INFO: Waiting for pod pod-subpath-test-configmap-v9rj to disappear
Feb 12 11:10:52.190: INFO: Pod pod-subpath-test-configmap-v9rj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v9rj 02/12/23 11:10:52.19
Feb 12 11:10:52.190: INFO: Deleting pod "pod-subpath-test-configmap-v9rj" in namespace "subpath-9798"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 12 11:10:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9798" for this suite. 02/12/23 11:10:52.196
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":9,"skipped":182,"failed":0}
------------------------------
 [SLOW TEST] [24.113 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:10:28.091
    Feb 12 11:10:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename subpath 02/12/23 11:10:28.092
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:28.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:28.115
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/12/23 11:10:28.12
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-v9rj 02/12/23 11:10:28.131
    STEP: Creating a pod to test atomic-volume-subpath 02/12/23 11:10:28.131
    Feb 12 11:10:28.142: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v9rj" in namespace "subpath-9798" to be "Succeeded or Failed"
    Feb 12 11:10:28.149: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Pending", Reason="", readiness=false. Elapsed: 7.182018ms
    Feb 12 11:10:30.163: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 2.02133777s
    Feb 12 11:10:32.162: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 4.020153245s
    Feb 12 11:10:34.160: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 6.018196125s
    Feb 12 11:10:36.163: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 8.021142365s
    Feb 12 11:10:38.156: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 10.014031558s
    Feb 12 11:10:40.160: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 12.01801545s
    Feb 12 11:10:42.159: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 14.017163498s
    Feb 12 11:10:44.154: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 16.011564688s
    Feb 12 11:10:46.164: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 18.021723408s
    Feb 12 11:10:48.154: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=true. Elapsed: 20.012380667s
    Feb 12 11:10:50.154: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Running", Reason="", readiness=false. Elapsed: 22.012108191s
    Feb 12 11:10:52.155: INFO: Pod "pod-subpath-test-configmap-v9rj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013035617s
    STEP: Saw pod success 02/12/23 11:10:52.155
    Feb 12 11:10:52.155: INFO: Pod "pod-subpath-test-configmap-v9rj" satisfied condition "Succeeded or Failed"
    Feb 12 11:10:52.160: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-v9rj container test-container-subpath-configmap-v9rj: <nil>
    STEP: delete the pod 02/12/23 11:10:52.168
    Feb 12 11:10:52.187: INFO: Waiting for pod pod-subpath-test-configmap-v9rj to disappear
    Feb 12 11:10:52.190: INFO: Pod pod-subpath-test-configmap-v9rj no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-v9rj 02/12/23 11:10:52.19
    Feb 12 11:10:52.190: INFO: Deleting pod "pod-subpath-test-configmap-v9rj" in namespace "subpath-9798"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 12 11:10:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9798" for this suite. 02/12/23 11:10:52.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:10:52.205
Feb 12 11:10:52.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:10:52.206
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:52.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:52.231
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-58f84def-97b4-46e2-8bbd-97d86637dd19 02/12/23 11:10:52.233
STEP: Creating a pod to test consume secrets 02/12/23 11:10:52.238
Feb 12 11:10:52.247: INFO: Waiting up to 5m0s for pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5" in namespace "secrets-3946" to be "Succeeded or Failed"
Feb 12 11:10:52.260: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.568503ms
Feb 12 11:10:54.264: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016970571s
Feb 12 11:10:56.291: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Running", Reason="", readiness=false. Elapsed: 4.043488444s
Feb 12 11:10:58.265: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017812482s
STEP: Saw pod success 02/12/23 11:10:58.265
Feb 12 11:10:58.265: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5" satisfied condition "Succeeded or Failed"
Feb 12 11:10:58.269: INFO: Trying to get logs from node kube-3 pod pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5 container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 11:10:58.279
Feb 12 11:10:58.357: INFO: Waiting for pod pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5 to disappear
Feb 12 11:10:58.360: INFO: Pod pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:10:58.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3946" for this suite. 02/12/23 11:10:58.364
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":10,"skipped":187,"failed":0}
------------------------------
 [SLOW TEST] [6.214 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:10:52.205
    Feb 12 11:10:52.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:10:52.206
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:52.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:52.231
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-58f84def-97b4-46e2-8bbd-97d86637dd19 02/12/23 11:10:52.233
    STEP: Creating a pod to test consume secrets 02/12/23 11:10:52.238
    Feb 12 11:10:52.247: INFO: Waiting up to 5m0s for pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5" in namespace "secrets-3946" to be "Succeeded or Failed"
    Feb 12 11:10:52.260: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.568503ms
    Feb 12 11:10:54.264: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016970571s
    Feb 12 11:10:56.291: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Running", Reason="", readiness=false. Elapsed: 4.043488444s
    Feb 12 11:10:58.265: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017812482s
    STEP: Saw pod success 02/12/23 11:10:58.265
    Feb 12 11:10:58.265: INFO: Pod "pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5" satisfied condition "Succeeded or Failed"
    Feb 12 11:10:58.269: INFO: Trying to get logs from node kube-3 pod pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5 container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 11:10:58.279
    Feb 12 11:10:58.357: INFO: Waiting for pod pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5 to disappear
    Feb 12 11:10:58.360: INFO: Pod pod-secrets-2dc697fd-1769-4d62-a6d8-31d0f16662e5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:10:58.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3946" for this suite. 02/12/23 11:10:58.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:10:58.42
Feb 12 11:10:58.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-pred 02/12/23 11:10:58.421
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:58.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:58.493
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 12 11:10:58.496: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 11:10:58.503: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 11:10:58.506: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Feb 12 11:10:58.511: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container calico-node ready: true, restart count 1
Feb 12 11:10:58.511: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container coredns ready: true, restart count 0
Feb 12 11:10:58.511: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container kube-apiserver ready: true, restart count 2
Feb 12 11:10:58.511: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container kube-controller-manager ready: true, restart count 5
Feb 12 11:10:58.511: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:10:58.511: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container kube-scheduler ready: true, restart count 4
Feb 12 11:10:58.511: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:10:58.511: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:10:58.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:10:58.511: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:10:58.511: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Feb 12 11:10:58.516: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.516: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 11:10:58.517: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container coredns ready: true, restart count 0
Feb 12 11:10:58.517: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container autoscaler ready: true, restart count 0
Feb 12 11:10:58.517: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 12 11:10:58.517: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container kube-controller-manager ready: true, restart count 6
Feb 12 11:10:58.517: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:10:58.517: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container kube-scheduler ready: true, restart count 5
Feb 12 11:10:58.517: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:10:58.517: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:10:58.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:10:58.517: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:10:58.517: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Feb 12 11:10:58.523: INFO: calico-kube-controllers-75748cc9fd-2rtj6 from kube-system started at 2023-02-12 11:00:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 12 11:10:58.523: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 11:10:58.523: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:10:58.523: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container nginx-proxy ready: true, restart count 0
Feb 12 11:10:58.523: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:10:58.523: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 11:10:58.523: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container e2e ready: true, restart count 0
Feb 12 11:10:58.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:10:58.523: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:10:58.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:10:58.523: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/12/23 11:10:58.523
Feb 12 11:10:58.536: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5272" to be "running"
Feb 12 11:10:58.541: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281581ms
Feb 12 11:11:00.547: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010319005s
Feb 12 11:11:00.547: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/12/23 11:11:00.552
STEP: Trying to apply a random label on the found node. 02/12/23 11:11:00.587
STEP: verifying the node has the label kubernetes.io/e2e-ef71c4ad-df2a-46c9-9399-7f31a1d3ffae 42 02/12/23 11:11:00.603
STEP: Trying to relaunch the pod, now with labels. 02/12/23 11:11:00.614
Feb 12 11:11:00.622: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5272" to be "not pending"
Feb 12 11:11:00.632: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.912364ms
Feb 12 11:11:06.719: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096225495s
Feb 12 11:11:08.637: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 8.014181581s
Feb 12 11:11:08.637: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-ef71c4ad-df2a-46c9-9399-7f31a1d3ffae off the node kube-3 02/12/23 11:11:08.641
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ef71c4ad-df2a-46c9-9399-7f31a1d3ffae 02/12/23 11:11:08.674
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:11:08.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5272" for this suite. 02/12/23 11:11:08.681
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":11,"skipped":199,"failed":0}
------------------------------
 [SLOW TEST] [10.269 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:10:58.42
    Feb 12 11:10:58.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-pred 02/12/23 11:10:58.421
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:10:58.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:10:58.493
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 12 11:10:58.496: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 12 11:10:58.503: INFO: Waiting for terminating namespaces to be deleted...
    Feb 12 11:10:58.506: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Feb 12 11:10:58.511: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container calico-node ready: true, restart count 1
    Feb 12 11:10:58.511: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 11:10:58.511: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container kube-apiserver ready: true, restart count 2
    Feb 12 11:10:58.511: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Feb 12 11:10:58.511: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:10:58.511: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container kube-scheduler ready: true, restart count 4
    Feb 12 11:10:58.511: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:10:58.511: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:10:58.511: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:10:58.511: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:10:58.511: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Feb 12 11:10:58.516: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.516: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container autoscaler ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container kube-apiserver ready: true, restart count 1
    Feb 12 11:10:58.517: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container kube-controller-manager ready: true, restart count 6
    Feb 12 11:10:58.517: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container kube-scheduler ready: true, restart count 5
    Feb 12 11:10:58.517: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:10:58.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:10:58.517: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Feb 12 11:10:58.523: INFO: calico-kube-controllers-75748cc9fd-2rtj6 from kube-system started at 2023-02-12 11:00:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container nginx-proxy ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container e2e ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:10:58.523: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:10:58.523: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/12/23 11:10:58.523
    Feb 12 11:10:58.536: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5272" to be "running"
    Feb 12 11:10:58.541: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281581ms
    Feb 12 11:11:00.547: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010319005s
    Feb 12 11:11:00.547: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/12/23 11:11:00.552
    STEP: Trying to apply a random label on the found node. 02/12/23 11:11:00.587
    STEP: verifying the node has the label kubernetes.io/e2e-ef71c4ad-df2a-46c9-9399-7f31a1d3ffae 42 02/12/23 11:11:00.603
    STEP: Trying to relaunch the pod, now with labels. 02/12/23 11:11:00.614
    Feb 12 11:11:00.622: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-5272" to be "not pending"
    Feb 12 11:11:00.632: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 8.912364ms
    Feb 12 11:11:06.719: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096225495s
    Feb 12 11:11:08.637: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 8.014181581s
    Feb 12 11:11:08.637: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-ef71c4ad-df2a-46c9-9399-7f31a1d3ffae off the node kube-3 02/12/23 11:11:08.641
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ef71c4ad-df2a-46c9-9399-7f31a1d3ffae 02/12/23 11:11:08.674
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:11:08.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5272" for this suite. 02/12/23 11:11:08.681
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:08.689
Feb 12 11:11:08.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:11:08.689
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:08.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:08.713
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 02/12/23 11:11:08.716
Feb 12 11:11:08.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7275 api-versions'
Feb 12 11:11:08.773: INFO: stderr: ""
Feb 12 11:11:08.773: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:11:08.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7275" for this suite. 02/12/23 11:11:08.777
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":12,"skipped":202,"failed":0}
------------------------------
 [0.099 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:08.689
    Feb 12 11:11:08.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:11:08.689
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:08.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:08.713
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 02/12/23 11:11:08.716
    Feb 12 11:11:08.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7275 api-versions'
    Feb 12 11:11:08.773: INFO: stderr: ""
    Feb 12 11:11:08.773: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:11:08.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7275" for this suite. 02/12/23 11:11:08.777
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:08.789
Feb 12 11:11:08.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename job 02/12/23 11:11:08.79
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:08.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:08.822
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 02/12/23 11:11:08.825
STEP: Ensuring job reaches completions 02/12/23 11:11:08.834
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 12 11:11:24.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6254" for this suite. 02/12/23 11:11:24.842
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":13,"skipped":203,"failed":0}
------------------------------
 [SLOW TEST] [16.060 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:08.789
    Feb 12 11:11:08.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename job 02/12/23 11:11:08.79
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:08.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:08.822
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 02/12/23 11:11:08.825
    STEP: Ensuring job reaches completions 02/12/23 11:11:08.834
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 12 11:11:24.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-6254" for this suite. 02/12/23 11:11:24.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:24.849
Feb 12 11:11:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:11:24.85
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:24.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:24.873
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:11:24.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5781" for this suite. 02/12/23 11:11:24.889
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":14,"skipped":208,"failed":0}
------------------------------
 [0.051 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:24.849
    Feb 12 11:11:24.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:11:24.85
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:24.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:24.873
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:11:24.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5781" for this suite. 02/12/23 11:11:24.889
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:24.901
Feb 12 11:11:24.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubelet-test 02/12/23 11:11:24.902
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:24.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:24.926
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 12 11:11:24.936: INFO: Waiting up to 5m0s for pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa" in namespace "kubelet-test-2154" to be "running and ready"
Feb 12 11:11:24.948: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.863301ms
Feb 12 11:11:24.948: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:11:27.333: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397446342s
Feb 12 11:11:27.333: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:11:28.976: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040069363s
Feb 12 11:11:28.976: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:11:30.960: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Running", Reason="", readiness=true. Elapsed: 6.024051911s
Feb 12 11:11:30.960: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Running (Ready = true)
Feb 12 11:11:30.960: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 12 11:11:30.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2154" for this suite. 02/12/23 11:11:31.003
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":15,"skipped":221,"failed":0}
------------------------------
 [SLOW TEST] [6.117 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:24.901
    Feb 12 11:11:24.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubelet-test 02/12/23 11:11:24.902
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:24.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:24.926
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 12 11:11:24.936: INFO: Waiting up to 5m0s for pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa" in namespace "kubelet-test-2154" to be "running and ready"
    Feb 12 11:11:24.948: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.863301ms
    Feb 12 11:11:24.948: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:11:27.333: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397446342s
    Feb 12 11:11:27.333: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:11:28.976: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040069363s
    Feb 12 11:11:28.976: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:11:30.960: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa": Phase="Running", Reason="", readiness=true. Elapsed: 6.024051911s
    Feb 12 11:11:30.960: INFO: The phase of Pod busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa is Running (Ready = true)
    Feb 12 11:11:30.960: INFO: Pod "busybox-scheduling-74f7381d-367f-4e90-b15b-95d9f73129fa" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 12 11:11:30.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2154" for this suite. 02/12/23 11:11:31.003
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:31.018
Feb 12 11:11:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 11:11:31.019
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:31.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:31.044
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Feb 12 11:11:31.066: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/12/23 11:11:31.071
Feb 12 11:11:31.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:31.075: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/12/23 11:11:31.075
Feb 12 11:11:31.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:31.108: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:32.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:32.126: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:33.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:33.118: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:34.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:34.113: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:38.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:38.197: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:39.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:39.112: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:44.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:44.221: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:48.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 12 11:11:48.902: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/12/23 11:11:48.94
Feb 12 11:11:49.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 12 11:11:49.183: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 12 11:11:50.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:50.193: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/12/23 11:11:50.193
Feb 12 11:11:50.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:50.226: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:51.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:51.229: INFO: Node kube-2 is running 0 daemon pod, expected 1
Feb 12 11:11:52.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 12 11:11:52.241: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:11:52.259
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1033, will wait for the garbage collector to delete the pods 02/12/23 11:11:52.259
Feb 12 11:11:52.332: INFO: Deleting DaemonSet.extensions daemon-set took: 12.327408ms
Feb 12 11:11:52.432: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.679062ms
Feb 12 11:11:55.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:11:55.138: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 12 11:11:55.144: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3663"},"items":null}

Feb 12 11:11:55.147: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:11:55.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1033" for this suite. 02/12/23 11:11:55.177
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":16,"skipped":225,"failed":0}
------------------------------
 [SLOW TEST] [24.168 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:31.018
    Feb 12 11:11:31.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 11:11:31.019
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:31.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:31.044
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Feb 12 11:11:31.066: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/12/23 11:11:31.071
    Feb 12 11:11:31.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:31.075: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/12/23 11:11:31.075
    Feb 12 11:11:31.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:31.108: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:32.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:32.126: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:33.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:33.118: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:34.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:34.113: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:38.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:38.197: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:39.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:39.112: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:44.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:44.221: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:48.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 12 11:11:48.902: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/12/23 11:11:48.94
    Feb 12 11:11:49.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 12 11:11:49.183: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb 12 11:11:50.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:50.193: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/12/23 11:11:50.193
    Feb 12 11:11:50.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:50.226: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:51.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:51.229: INFO: Node kube-2 is running 0 daemon pod, expected 1
    Feb 12 11:11:52.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 12 11:11:52.241: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:11:52.259
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1033, will wait for the garbage collector to delete the pods 02/12/23 11:11:52.259
    Feb 12 11:11:52.332: INFO: Deleting DaemonSet.extensions daemon-set took: 12.327408ms
    Feb 12 11:11:52.432: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.679062ms
    Feb 12 11:11:55.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:11:55.138: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 12 11:11:55.144: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3663"},"items":null}

    Feb 12 11:11:55.147: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3663"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:11:55.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1033" for this suite. 02/12/23 11:11:55.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:55.188
Feb 12 11:11:55.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename init-container 02/12/23 11:11:55.189
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:55.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:55.218
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 02/12/23 11:11:55.222
Feb 12 11:11:55.222: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 11:11:59.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6875" for this suite. 02/12/23 11:11:59.624
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":17,"skipped":256,"failed":0}
------------------------------
 [4.470 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:55.188
    Feb 12 11:11:55.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename init-container 02/12/23 11:11:55.189
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:55.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:55.218
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 02/12/23 11:11:55.222
    Feb 12 11:11:55.222: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 11:11:59.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6875" for this suite. 02/12/23 11:11:59.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:11:59.66
Feb 12 11:11:59.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename disruption 02/12/23 11:11:59.662
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:59.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:59.923
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 02/12/23 11:12:00.002
STEP: Waiting for all pods to be running 02/12/23 11:12:02.076
Feb 12 11:12:02.087: INFO: running pods: 0 < 3
Feb 12 11:12:05.046: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 12 11:12:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1041" for this suite. 02/12/23 11:12:06.109
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":18,"skipped":267,"failed":0}
------------------------------
 [SLOW TEST] [6.466 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:11:59.66
    Feb 12 11:11:59.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename disruption 02/12/23 11:11:59.662
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:11:59.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:11:59.923
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 02/12/23 11:12:00.002
    STEP: Waiting for all pods to be running 02/12/23 11:12:02.076
    Feb 12 11:12:02.087: INFO: running pods: 0 < 3
    Feb 12 11:12:05.046: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 12 11:12:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1041" for this suite. 02/12/23 11:12:06.109
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:06.128
Feb 12 11:12:06.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename ingress 02/12/23 11:12:06.13
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:06.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:06.161
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/12/23 11:12:06.164
STEP: getting /apis/networking.k8s.io 02/12/23 11:12:06.167
STEP: getting /apis/networking.k8s.iov1 02/12/23 11:12:06.168
STEP: creating 02/12/23 11:12:06.168
STEP: getting 02/12/23 11:12:06.196
STEP: listing 02/12/23 11:12:06.199
STEP: watching 02/12/23 11:12:06.201
Feb 12 11:12:06.202: INFO: starting watch
STEP: cluster-wide listing 02/12/23 11:12:06.202
STEP: cluster-wide watching 02/12/23 11:12:06.205
Feb 12 11:12:06.206: INFO: starting watch
STEP: patching 02/12/23 11:12:06.207
STEP: updating 02/12/23 11:12:06.222
Feb 12 11:12:06.236: INFO: waiting for watch events with expected annotations
Feb 12 11:12:06.236: INFO: saw patched and updated annotations
STEP: patching /status 02/12/23 11:12:06.236
STEP: updating /status 02/12/23 11:12:06.269
STEP: get /status 02/12/23 11:12:06.301
STEP: deleting 02/12/23 11:12:06.306
STEP: deleting a collection 02/12/23 11:12:06.333
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Feb 12 11:12:06.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7165" for this suite. 02/12/23 11:12:06.356
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":19,"skipped":271,"failed":0}
------------------------------
 [0.237 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:06.128
    Feb 12 11:12:06.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename ingress 02/12/23 11:12:06.13
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:06.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:06.161
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/12/23 11:12:06.164
    STEP: getting /apis/networking.k8s.io 02/12/23 11:12:06.167
    STEP: getting /apis/networking.k8s.iov1 02/12/23 11:12:06.168
    STEP: creating 02/12/23 11:12:06.168
    STEP: getting 02/12/23 11:12:06.196
    STEP: listing 02/12/23 11:12:06.199
    STEP: watching 02/12/23 11:12:06.201
    Feb 12 11:12:06.202: INFO: starting watch
    STEP: cluster-wide listing 02/12/23 11:12:06.202
    STEP: cluster-wide watching 02/12/23 11:12:06.205
    Feb 12 11:12:06.206: INFO: starting watch
    STEP: patching 02/12/23 11:12:06.207
    STEP: updating 02/12/23 11:12:06.222
    Feb 12 11:12:06.236: INFO: waiting for watch events with expected annotations
    Feb 12 11:12:06.236: INFO: saw patched and updated annotations
    STEP: patching /status 02/12/23 11:12:06.236
    STEP: updating /status 02/12/23 11:12:06.269
    STEP: get /status 02/12/23 11:12:06.301
    STEP: deleting 02/12/23 11:12:06.306
    STEP: deleting a collection 02/12/23 11:12:06.333
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Feb 12 11:12:06.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-7165" for this suite. 02/12/23 11:12:06.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:06.366
Feb 12 11:12:06.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename security-context-test 02/12/23 11:12:06.366
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:06.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:06.395
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Feb 12 11:12:06.418: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2" in namespace "security-context-test-5323" to be "Succeeded or Failed"
Feb 12 11:12:06.425: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178668ms
Feb 12 11:12:08.439: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020626606s
Feb 12 11:12:10.436: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017801041s
Feb 12 11:12:10.436: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2" satisfied condition "Succeeded or Failed"
Feb 12 11:12:10.460: INFO: Got logs for pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 12 11:12:10.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5323" for this suite. 02/12/23 11:12:10.472
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":20,"skipped":282,"failed":0}
------------------------------
 [4.114 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:06.366
    Feb 12 11:12:06.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename security-context-test 02/12/23 11:12:06.366
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:06.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:06.395
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Feb 12 11:12:06.418: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2" in namespace "security-context-test-5323" to be "Succeeded or Failed"
    Feb 12 11:12:06.425: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.178668ms
    Feb 12 11:12:08.439: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020626606s
    Feb 12 11:12:10.436: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017801041s
    Feb 12 11:12:10.436: INFO: Pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:10.460: INFO: Got logs for pod "busybox-privileged-false-79db3774-0103-42fc-ab06-ff00ce9477e2": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 12 11:12:10.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5323" for this suite. 02/12/23 11:12:10.472
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:10.48
Feb 12 11:12:10.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:12:10.482
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:10.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:10.507
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-9cdb756e-5b9d-4b61-94ae-299109a75e57 02/12/23 11:12:10.513
STEP: Creating a pod to test consume configMaps 02/12/23 11:12:10.521
Feb 12 11:12:10.530: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4" in namespace "projected-7824" to be "Succeeded or Failed"
Feb 12 11:12:10.534: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168128ms
Feb 12 11:12:12.542: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012297684s
Feb 12 11:12:14.575: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045158003s
Feb 12 11:12:16.538: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008730385s
STEP: Saw pod success 02/12/23 11:12:16.539
Feb 12 11:12:16.539: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4" satisfied condition "Succeeded or Failed"
Feb 12 11:12:16.542: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:12:16.55
Feb 12 11:12:16.570: INFO: Waiting for pod pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4 to disappear
Feb 12 11:12:16.575: INFO: Pod pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 11:12:16.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7824" for this suite. 02/12/23 11:12:16.578
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":21,"skipped":283,"failed":0}
------------------------------
 [SLOW TEST] [6.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:10.48
    Feb 12 11:12:10.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:12:10.482
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:10.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:10.507
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-9cdb756e-5b9d-4b61-94ae-299109a75e57 02/12/23 11:12:10.513
    STEP: Creating a pod to test consume configMaps 02/12/23 11:12:10.521
    Feb 12 11:12:10.530: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4" in namespace "projected-7824" to be "Succeeded or Failed"
    Feb 12 11:12:10.534: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168128ms
    Feb 12 11:12:12.542: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012297684s
    Feb 12 11:12:14.575: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045158003s
    Feb 12 11:12:16.538: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008730385s
    STEP: Saw pod success 02/12/23 11:12:16.539
    Feb 12 11:12:16.539: INFO: Pod "pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:16.542: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:12:16.55
    Feb 12 11:12:16.570: INFO: Waiting for pod pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4 to disappear
    Feb 12 11:12:16.575: INFO: Pod pod-projected-configmaps-b83d4e71-5c70-48dc-9898-5fce873825a4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 11:12:16.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7824" for this suite. 02/12/23 11:12:16.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:16.586
Feb 12 11:12:16.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:12:16.587
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:16.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:16.61
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-166190be-3599-48cb-835b-faeef253876a 02/12/23 11:12:16.612
STEP: Creating a pod to test consume configMaps 02/12/23 11:12:16.616
Feb 12 11:12:16.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91" in namespace "projected-2817" to be "Succeeded or Failed"
Feb 12 11:12:16.630: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Pending", Reason="", readiness=false. Elapsed: 5.876012ms
Feb 12 11:12:18.635: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010508628s
Feb 12 11:12:20.686: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061247711s
Feb 12 11:12:22.642: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017219313s
STEP: Saw pod success 02/12/23 11:12:22.642
Feb 12 11:12:22.642: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91" satisfied condition "Succeeded or Failed"
Feb 12 11:12:22.656: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:12:22.678
Feb 12 11:12:22.763: INFO: Waiting for pod pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91 to disappear
Feb 12 11:12:22.772: INFO: Pod pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 11:12:22.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2817" for this suite. 02/12/23 11:12:22.779
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":22,"skipped":310,"failed":0}
------------------------------
 [SLOW TEST] [6.228 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:16.586
    Feb 12 11:12:16.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:12:16.587
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:16.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:16.61
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-166190be-3599-48cb-835b-faeef253876a 02/12/23 11:12:16.612
    STEP: Creating a pod to test consume configMaps 02/12/23 11:12:16.616
    Feb 12 11:12:16.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91" in namespace "projected-2817" to be "Succeeded or Failed"
    Feb 12 11:12:16.630: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Pending", Reason="", readiness=false. Elapsed: 5.876012ms
    Feb 12 11:12:18.635: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010508628s
    Feb 12 11:12:20.686: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061247711s
    Feb 12 11:12:22.642: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017219313s
    STEP: Saw pod success 02/12/23 11:12:22.642
    Feb 12 11:12:22.642: INFO: Pod "pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:22.656: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:12:22.678
    Feb 12 11:12:22.763: INFO: Waiting for pod pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91 to disappear
    Feb 12 11:12:22.772: INFO: Pod pod-projected-configmaps-1eb9082d-3880-4186-8dea-fa3c216c2d91 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 11:12:22.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2817" for this suite. 02/12/23 11:12:22.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:22.818
Feb 12 11:12:22.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:12:22.819
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:22.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:22.884
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 02/12/23 11:12:22.938
Feb 12 11:12:22.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8611 create -f -'
Feb 12 11:12:23.672: INFO: stderr: ""
Feb 12 11:12:23.672: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/12/23 11:12:23.672
Feb 12 11:12:25.005: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:12:25.006: INFO: Found 0 / 1
Feb 12 11:12:25.685: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:12:25.685: INFO: Found 0 / 1
Feb 12 11:12:26.690: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:12:26.690: INFO: Found 1 / 1
Feb 12 11:12:26.690: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/12/23 11:12:26.69
Feb 12 11:12:26.702: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:12:26.702: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 12 11:12:26.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8611 patch pod agnhost-primary-v8p6c -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 12 11:12:26.864: INFO: stderr: ""
Feb 12 11:12:26.864: INFO: stdout: "pod/agnhost-primary-v8p6c patched\n"
STEP: checking annotations 02/12/23 11:12:26.864
Feb 12 11:12:26.868: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:12:26.868: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:12:26.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8611" for this suite. 02/12/23 11:12:26.872
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":23,"skipped":327,"failed":0}
------------------------------
 [4.065 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:22.818
    Feb 12 11:12:22.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:12:22.819
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:22.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:22.884
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 02/12/23 11:12:22.938
    Feb 12 11:12:22.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8611 create -f -'
    Feb 12 11:12:23.672: INFO: stderr: ""
    Feb 12 11:12:23.672: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/12/23 11:12:23.672
    Feb 12 11:12:25.005: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:12:25.006: INFO: Found 0 / 1
    Feb 12 11:12:25.685: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:12:25.685: INFO: Found 0 / 1
    Feb 12 11:12:26.690: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:12:26.690: INFO: Found 1 / 1
    Feb 12 11:12:26.690: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/12/23 11:12:26.69
    Feb 12 11:12:26.702: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:12:26.702: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 12 11:12:26.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8611 patch pod agnhost-primary-v8p6c -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 12 11:12:26.864: INFO: stderr: ""
    Feb 12 11:12:26.864: INFO: stdout: "pod/agnhost-primary-v8p6c patched\n"
    STEP: checking annotations 02/12/23 11:12:26.864
    Feb 12 11:12:26.868: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:12:26.868: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:12:26.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8611" for this suite. 02/12/23 11:12:26.872
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:26.883
Feb 12 11:12:26.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:12:26.884
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:26.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:26.904
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:12:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2711" for this suite. 02/12/23 11:12:28.31
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":24,"skipped":327,"failed":0}
------------------------------
 [1.488 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:26.883
    Feb 12 11:12:26.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:12:26.884
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:26.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:26.904
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:12:28.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2711" for this suite. 02/12/23 11:12:28.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:28.372
Feb 12 11:12:28.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:12:28.373
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:28.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:28.462
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:12:28.472
Feb 12 11:12:28.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c" in namespace "projected-8711" to be "Succeeded or Failed"
Feb 12 11:12:28.587: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.949036ms
Feb 12 11:12:30.600: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030122658s
Feb 12 11:12:32.592: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02135928s
Feb 12 11:12:34.603: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03229026s
STEP: Saw pod success 02/12/23 11:12:34.603
Feb 12 11:12:34.603: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c" satisfied condition "Succeeded or Failed"
Feb 12 11:12:34.615: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c container client-container: <nil>
STEP: delete the pod 02/12/23 11:12:34.633
Feb 12 11:12:34.661: INFO: Waiting for pod downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c to disappear
Feb 12 11:12:34.676: INFO: Pod downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:12:34.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8711" for this suite. 02/12/23 11:12:34.681
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":25,"skipped":341,"failed":0}
------------------------------
 [SLOW TEST] [6.321 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:28.372
    Feb 12 11:12:28.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:12:28.373
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:28.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:28.462
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:12:28.472
    Feb 12 11:12:28.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c" in namespace "projected-8711" to be "Succeeded or Failed"
    Feb 12 11:12:28.587: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.949036ms
    Feb 12 11:12:30.600: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030122658s
    Feb 12 11:12:32.592: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02135928s
    Feb 12 11:12:34.603: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03229026s
    STEP: Saw pod success 02/12/23 11:12:34.603
    Feb 12 11:12:34.603: INFO: Pod "downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:34.615: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c container client-container: <nil>
    STEP: delete the pod 02/12/23 11:12:34.633
    Feb 12 11:12:34.661: INFO: Waiting for pod downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c to disappear
    Feb 12 11:12:34.676: INFO: Pod downwardapi-volume-27cc85b8-47c7-490d-af5a-bcce3ea0f51c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:12:34.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8711" for this suite. 02/12/23 11:12:34.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:34.695
Feb 12 11:12:34.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename security-context 02/12/23 11:12:34.695
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:34.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:34.717
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/12/23 11:12:34.719
Feb 12 11:12:34.730: INFO: Waiting up to 5m0s for pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5" in namespace "security-context-7858" to be "Succeeded or Failed"
Feb 12 11:12:34.733: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.628624ms
Feb 12 11:12:36.741: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011403637s
Feb 12 11:12:38.746: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016495576s
STEP: Saw pod success 02/12/23 11:12:38.747
Feb 12 11:12:38.748: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5" satisfied condition "Succeeded or Failed"
Feb 12 11:12:38.762: INFO: Trying to get logs from node kube-3 pod security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5 container test-container: <nil>
STEP: delete the pod 02/12/23 11:12:38.779
Feb 12 11:12:38.813: INFO: Waiting for pod security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5 to disappear
Feb 12 11:12:38.816: INFO: Pod security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 12 11:12:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7858" for this suite. 02/12/23 11:12:38.819
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":26,"skipped":359,"failed":0}
------------------------------
 [4.132 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:34.695
    Feb 12 11:12:34.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename security-context 02/12/23 11:12:34.695
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:34.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:34.717
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/12/23 11:12:34.719
    Feb 12 11:12:34.730: INFO: Waiting up to 5m0s for pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5" in namespace "security-context-7858" to be "Succeeded or Failed"
    Feb 12 11:12:34.733: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.628624ms
    Feb 12 11:12:36.741: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011403637s
    Feb 12 11:12:38.746: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016495576s
    STEP: Saw pod success 02/12/23 11:12:38.747
    Feb 12 11:12:38.748: INFO: Pod "security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:38.762: INFO: Trying to get logs from node kube-3 pod security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5 container test-container: <nil>
    STEP: delete the pod 02/12/23 11:12:38.779
    Feb 12 11:12:38.813: INFO: Waiting for pod security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5 to disappear
    Feb 12 11:12:38.816: INFO: Pod security-context-08d3ab10-f08f-45b0-9cbc-f60af230d7c5 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 12 11:12:38.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-7858" for this suite. 02/12/23 11:12:38.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:38.832
Feb 12 11:12:38.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename certificates 02/12/23 11:12:38.833
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:38.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:38.858
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/12/23 11:12:39.318
STEP: getting /apis/certificates.k8s.io 02/12/23 11:12:39.32
STEP: getting /apis/certificates.k8s.io/v1 02/12/23 11:12:39.321
STEP: creating 02/12/23 11:12:39.321
STEP: getting 02/12/23 11:12:39.339
STEP: listing 02/12/23 11:12:39.342
STEP: watching 02/12/23 11:12:39.344
Feb 12 11:12:39.344: INFO: starting watch
STEP: patching 02/12/23 11:12:39.345
STEP: updating 02/12/23 11:12:39.356
Feb 12 11:12:39.361: INFO: waiting for watch events with expected annotations
Feb 12 11:12:39.361: INFO: saw patched and updated annotations
STEP: getting /approval 02/12/23 11:12:39.361
STEP: patching /approval 02/12/23 11:12:39.365
STEP: updating /approval 02/12/23 11:12:39.372
STEP: getting /status 02/12/23 11:12:39.377
STEP: patching /status 02/12/23 11:12:39.381
STEP: updating /status 02/12/23 11:12:39.389
STEP: deleting 02/12/23 11:12:39.396
STEP: deleting a collection 02/12/23 11:12:39.407
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:12:39.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9761" for this suite. 02/12/23 11:12:39.425
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":27,"skipped":465,"failed":0}
------------------------------
 [0.600 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:38.832
    Feb 12 11:12:38.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename certificates 02/12/23 11:12:38.833
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:38.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:38.858
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/12/23 11:12:39.318
    STEP: getting /apis/certificates.k8s.io 02/12/23 11:12:39.32
    STEP: getting /apis/certificates.k8s.io/v1 02/12/23 11:12:39.321
    STEP: creating 02/12/23 11:12:39.321
    STEP: getting 02/12/23 11:12:39.339
    STEP: listing 02/12/23 11:12:39.342
    STEP: watching 02/12/23 11:12:39.344
    Feb 12 11:12:39.344: INFO: starting watch
    STEP: patching 02/12/23 11:12:39.345
    STEP: updating 02/12/23 11:12:39.356
    Feb 12 11:12:39.361: INFO: waiting for watch events with expected annotations
    Feb 12 11:12:39.361: INFO: saw patched and updated annotations
    STEP: getting /approval 02/12/23 11:12:39.361
    STEP: patching /approval 02/12/23 11:12:39.365
    STEP: updating /approval 02/12/23 11:12:39.372
    STEP: getting /status 02/12/23 11:12:39.377
    STEP: patching /status 02/12/23 11:12:39.381
    STEP: updating /status 02/12/23 11:12:39.389
    STEP: deleting 02/12/23 11:12:39.396
    STEP: deleting a collection 02/12/23 11:12:39.407
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:12:39.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-9761" for this suite. 02/12/23 11:12:39.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:39.44
Feb 12 11:12:39.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:12:39.441
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:39.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:39.465
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  02/12/23 11:12:39.468
Feb 12 11:12:39.480: INFO: Waiting up to 5m0s for pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8" in namespace "svcaccounts-9834" to be "Succeeded or Failed"
Feb 12 11:12:39.484: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705474ms
Feb 12 11:12:41.504: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024173332s
Feb 12 11:12:43.500: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019597795s
STEP: Saw pod success 02/12/23 11:12:43.5
Feb 12 11:12:43.500: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8" satisfied condition "Succeeded or Failed"
Feb 12 11:12:43.511: INFO: Trying to get logs from node kube-3 pod test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:12:43.533
Feb 12 11:12:43.561: INFO: Waiting for pod test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8 to disappear
Feb 12 11:12:43.564: INFO: Pod test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 12 11:12:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9834" for this suite. 02/12/23 11:12:43.567
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":28,"skipped":514,"failed":0}
------------------------------
 [4.135 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:39.44
    Feb 12 11:12:39.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:12:39.441
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:39.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:39.465
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  02/12/23 11:12:39.468
    Feb 12 11:12:39.480: INFO: Waiting up to 5m0s for pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8" in namespace "svcaccounts-9834" to be "Succeeded or Failed"
    Feb 12 11:12:39.484: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705474ms
    Feb 12 11:12:41.504: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024173332s
    Feb 12 11:12:43.500: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019597795s
    STEP: Saw pod success 02/12/23 11:12:43.5
    Feb 12 11:12:43.500: INFO: Pod "test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:43.511: INFO: Trying to get logs from node kube-3 pod test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:12:43.533
    Feb 12 11:12:43.561: INFO: Waiting for pod test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8 to disappear
    Feb 12 11:12:43.564: INFO: Pod test-pod-7bc11f95-6314-46fb-8b6f-59489d5b8aa8 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 12 11:12:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9834" for this suite. 02/12/23 11:12:43.567
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:43.575
Feb 12 11:12:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename podtemplate 02/12/23 11:12:43.576
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:43.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:43.598
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 12 11:12:43.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2206" for this suite. 02/12/23 11:12:43.629
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":29,"skipped":516,"failed":0}
------------------------------
 [0.060 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:43.575
    Feb 12 11:12:43.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename podtemplate 02/12/23 11:12:43.576
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:43.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:43.598
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 12 11:12:43.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2206" for this suite. 02/12/23 11:12:43.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:43.637
Feb 12 11:12:43.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:12:43.638
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:43.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:43.661
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 12 11:12:43.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:12:49.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4188" for this suite. 02/12/23 11:12:49.71
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":30,"skipped":530,"failed":0}
------------------------------
 [SLOW TEST] [6.091 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:43.637
    Feb 12 11:12:43.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:12:43.638
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:43.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:43.661
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 12 11:12:43.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:12:49.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4188" for this suite. 02/12/23 11:12:49.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:49.729
Feb 12 11:12:49.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:12:49.731
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:49.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:49.761
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/12/23 11:12:49.763
Feb 12 11:12:49.780: INFO: Waiting up to 5m0s for pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b" in namespace "emptydir-7619" to be "Succeeded or Failed"
Feb 12 11:12:49.789: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.329906ms
Feb 12 11:12:51.803: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021723157s
Feb 12 11:12:53.801: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019733264s
STEP: Saw pod success 02/12/23 11:12:53.801
Feb 12 11:12:53.801: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b" satisfied condition "Succeeded or Failed"
Feb 12 11:12:53.810: INFO: Trying to get logs from node kube-3 pod pod-e859ca35-107c-4f22-922d-c17b72fbcd5b container test-container: <nil>
STEP: delete the pod 02/12/23 11:12:53.823
Feb 12 11:12:53.845: INFO: Waiting for pod pod-e859ca35-107c-4f22-922d-c17b72fbcd5b to disappear
Feb 12 11:12:53.849: INFO: Pod pod-e859ca35-107c-4f22-922d-c17b72fbcd5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:12:53.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7619" for this suite. 02/12/23 11:12:53.852
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":31,"skipped":539,"failed":0}
------------------------------
 [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:49.729
    Feb 12 11:12:49.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:12:49.731
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:49.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:49.761
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/12/23 11:12:49.763
    Feb 12 11:12:49.780: INFO: Waiting up to 5m0s for pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b" in namespace "emptydir-7619" to be "Succeeded or Failed"
    Feb 12 11:12:49.789: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.329906ms
    Feb 12 11:12:51.803: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021723157s
    Feb 12 11:12:53.801: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019733264s
    STEP: Saw pod success 02/12/23 11:12:53.801
    Feb 12 11:12:53.801: INFO: Pod "pod-e859ca35-107c-4f22-922d-c17b72fbcd5b" satisfied condition "Succeeded or Failed"
    Feb 12 11:12:53.810: INFO: Trying to get logs from node kube-3 pod pod-e859ca35-107c-4f22-922d-c17b72fbcd5b container test-container: <nil>
    STEP: delete the pod 02/12/23 11:12:53.823
    Feb 12 11:12:53.845: INFO: Waiting for pod pod-e859ca35-107c-4f22-922d-c17b72fbcd5b to disappear
    Feb 12 11:12:53.849: INFO: Pod pod-e859ca35-107c-4f22-922d-c17b72fbcd5b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:12:53.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7619" for this suite. 02/12/23 11:12:53.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:12:53.862
Feb 12 11:12:53.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replication-controller 02/12/23 11:12:53.864
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:53.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:53.886
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9 02/12/23 11:12:53.888
Feb 12 11:12:53.898: INFO: Pod name my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9: Found 0 pods out of 1
Feb 12 11:12:58.909: INFO: Pod name my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9: Found 1 pods out of 1
Feb 12 11:12:58.909: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9" are running
Feb 12 11:12:58.909: INFO: Waiting up to 5m0s for pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5" in namespace "replication-controller-3971" to be "running"
Feb 12 11:12:58.914: INFO: Pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5": Phase="Running", Reason="", readiness=true. Elapsed: 5.427712ms
Feb 12 11:12:58.914: INFO: Pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5" satisfied condition "running"
Feb 12 11:12:58.914: INFO: Pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:53 +0000 UTC Reason: Message:}])
Feb 12 11:12:58.914: INFO: Trying to dial the pod
Feb 12 11:13:04.061: INFO: Controller my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9: Got expected result from replica 1 [my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5]: "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 12 11:13:04.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3971" for this suite. 02/12/23 11:13:04.071
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":32,"skipped":547,"failed":0}
------------------------------
 [SLOW TEST] [10.230 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:12:53.862
    Feb 12 11:12:53.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replication-controller 02/12/23 11:12:53.864
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:12:53.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:12:53.886
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9 02/12/23 11:12:53.888
    Feb 12 11:12:53.898: INFO: Pod name my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9: Found 0 pods out of 1
    Feb 12 11:12:58.909: INFO: Pod name my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9: Found 1 pods out of 1
    Feb 12 11:12:58.909: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9" are running
    Feb 12 11:12:58.909: INFO: Waiting up to 5m0s for pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5" in namespace "replication-controller-3971" to be "running"
    Feb 12 11:12:58.914: INFO: Pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5": Phase="Running", Reason="", readiness=true. Elapsed: 5.427712ms
    Feb 12 11:12:58.914: INFO: Pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5" satisfied condition "running"
    Feb 12 11:12:58.914: INFO: Pod "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:12:53 +0000 UTC Reason: Message:}])
    Feb 12 11:12:58.914: INFO: Trying to dial the pod
    Feb 12 11:13:04.061: INFO: Controller my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9: Got expected result from replica 1 [my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5]: "my-hostname-basic-acc11be0-dfd5-4726-b9ce-f802bf249ea9-wn9v5", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 12 11:13:04.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3971" for this suite. 02/12/23 11:13:04.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:13:04.098
Feb 12 11:13:04.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename subpath 02/12/23 11:13:04.099
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:04.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:04.317
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/12/23 11:13:04.323
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-2m76 02/12/23 11:13:04.364
STEP: Creating a pod to test atomic-volume-subpath 02/12/23 11:13:04.364
Feb 12 11:13:04.471: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2m76" in namespace "subpath-9001" to be "Succeeded or Failed"
Feb 12 11:13:04.524: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Pending", Reason="", readiness=false. Elapsed: 53.178346ms
Feb 12 11:13:06.529: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 2.058040281s
Feb 12 11:13:08.529: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 4.05806s
Feb 12 11:13:10.532: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 6.061071798s
Feb 12 11:13:12.535: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 8.064373915s
Feb 12 11:13:14.539: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 10.068385382s
Feb 12 11:13:17.037: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 12.566781834s
Feb 12 11:13:18.536: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 14.065736321s
Feb 12 11:13:20.872: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 16.401752935s
Feb 12 11:13:22.539: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 18.068494365s
Feb 12 11:13:24.540: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 20.069207341s
Feb 12 11:13:26.530: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=false. Elapsed: 22.059747109s
Feb 12 11:13:28.529: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.058586594s
STEP: Saw pod success 02/12/23 11:13:28.53
Feb 12 11:13:28.530: INFO: Pod "pod-subpath-test-configmap-2m76" satisfied condition "Succeeded or Failed"
Feb 12 11:13:28.534: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-2m76 container test-container-subpath-configmap-2m76: <nil>
STEP: delete the pod 02/12/23 11:13:28.543
Feb 12 11:13:28.570: INFO: Waiting for pod pod-subpath-test-configmap-2m76 to disappear
Feb 12 11:13:28.575: INFO: Pod pod-subpath-test-configmap-2m76 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2m76 02/12/23 11:13:28.575
Feb 12 11:13:28.575: INFO: Deleting pod "pod-subpath-test-configmap-2m76" in namespace "subpath-9001"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 12 11:13:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9001" for this suite. 02/12/23 11:13:28.583
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":33,"skipped":583,"failed":0}
------------------------------
 [SLOW TEST] [24.494 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:13:04.098
    Feb 12 11:13:04.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename subpath 02/12/23 11:13:04.099
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:04.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:04.317
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/12/23 11:13:04.323
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-2m76 02/12/23 11:13:04.364
    STEP: Creating a pod to test atomic-volume-subpath 02/12/23 11:13:04.364
    Feb 12 11:13:04.471: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2m76" in namespace "subpath-9001" to be "Succeeded or Failed"
    Feb 12 11:13:04.524: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Pending", Reason="", readiness=false. Elapsed: 53.178346ms
    Feb 12 11:13:06.529: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 2.058040281s
    Feb 12 11:13:08.529: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 4.05806s
    Feb 12 11:13:10.532: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 6.061071798s
    Feb 12 11:13:12.535: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 8.064373915s
    Feb 12 11:13:14.539: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 10.068385382s
    Feb 12 11:13:17.037: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 12.566781834s
    Feb 12 11:13:18.536: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 14.065736321s
    Feb 12 11:13:20.872: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 16.401752935s
    Feb 12 11:13:22.539: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 18.068494365s
    Feb 12 11:13:24.540: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=true. Elapsed: 20.069207341s
    Feb 12 11:13:26.530: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Running", Reason="", readiness=false. Elapsed: 22.059747109s
    Feb 12 11:13:28.529: INFO: Pod "pod-subpath-test-configmap-2m76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.058586594s
    STEP: Saw pod success 02/12/23 11:13:28.53
    Feb 12 11:13:28.530: INFO: Pod "pod-subpath-test-configmap-2m76" satisfied condition "Succeeded or Failed"
    Feb 12 11:13:28.534: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-configmap-2m76 container test-container-subpath-configmap-2m76: <nil>
    STEP: delete the pod 02/12/23 11:13:28.543
    Feb 12 11:13:28.570: INFO: Waiting for pod pod-subpath-test-configmap-2m76 to disappear
    Feb 12 11:13:28.575: INFO: Pod pod-subpath-test-configmap-2m76 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-2m76 02/12/23 11:13:28.575
    Feb 12 11:13:28.575: INFO: Deleting pod "pod-subpath-test-configmap-2m76" in namespace "subpath-9001"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 12 11:13:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9001" for this suite. 02/12/23 11:13:28.583
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:13:28.593
Feb 12 11:13:28.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:13:28.593
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:28.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:28.615
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Feb 12 11:13:28.630: INFO: Waiting up to 5m0s for pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185" in namespace "svcaccounts-3954" to be "running"
Feb 12 11:13:28.634: INFO: Pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185": Phase="Pending", Reason="", readiness=false. Elapsed: 3.126139ms
Feb 12 11:13:30.638: INFO: Pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185": Phase="Running", Reason="", readiness=true. Elapsed: 2.007150564s
Feb 12 11:13:30.638: INFO: Pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185" satisfied condition "running"
STEP: reading a file in the container 02/12/23 11:13:30.638
Feb 12 11:13:30.638: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3954 pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/12/23 11:13:30.744
Feb 12 11:13:30.744: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3954 pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/12/23 11:13:30.866
Feb 12 11:13:30.866: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3954 pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 12 11:13:30.971: INFO: Got root ca configmap in namespace "svcaccounts-3954"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 12 11:13:30.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3954" for this suite. 02/12/23 11:13:30.976
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":34,"skipped":587,"failed":0}
------------------------------
 [2.390 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:13:28.593
    Feb 12 11:13:28.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:13:28.593
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:28.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:28.615
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Feb 12 11:13:28.630: INFO: Waiting up to 5m0s for pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185" in namespace "svcaccounts-3954" to be "running"
    Feb 12 11:13:28.634: INFO: Pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185": Phase="Pending", Reason="", readiness=false. Elapsed: 3.126139ms
    Feb 12 11:13:30.638: INFO: Pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185": Phase="Running", Reason="", readiness=true. Elapsed: 2.007150564s
    Feb 12 11:13:30.638: INFO: Pod "pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185" satisfied condition "running"
    STEP: reading a file in the container 02/12/23 11:13:30.638
    Feb 12 11:13:30.638: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3954 pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/12/23 11:13:30.744
    Feb 12 11:13:30.744: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3954 pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/12/23 11:13:30.866
    Feb 12 11:13:30.866: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3954 pod-service-account-17930796-7b76-4967-b410-fbb1f37d3185 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 12 11:13:30.971: INFO: Got root ca configmap in namespace "svcaccounts-3954"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 12 11:13:30.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3954" for this suite. 02/12/23 11:13:30.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:13:30.983
Feb 12 11:13:30.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-webhook 02/12/23 11:13:30.983
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:31.003
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/12/23 11:13:31.005
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/12/23 11:13:31.275
STEP: Deploying the custom resource conversion webhook pod 02/12/23 11:13:31.29
STEP: Wait for the deployment to be ready 02/12/23 11:13:31.302
Feb 12 11:13:31.313: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 11:13:33.341
STEP: Verifying the service has paired with the endpoint 02/12/23 11:13:33.361
Feb 12 11:13:34.361: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 12 11:13:34.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Creating a v1 custom resource 02/12/23 11:13:42.012
STEP: Create a v2 custom resource 02/12/23 11:13:42.035
STEP: List CRs in v1 02/12/23 11:13:42.116
STEP: List CRs in v2 02/12/23 11:13:42.122
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:13:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3213" for this suite. 02/12/23 11:13:42.666
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":35,"skipped":602,"failed":0}
------------------------------
 [SLOW TEST] [11.815 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:13:30.983
    Feb 12 11:13:30.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-webhook 02/12/23 11:13:30.983
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:31.003
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/12/23 11:13:31.005
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/12/23 11:13:31.275
    STEP: Deploying the custom resource conversion webhook pod 02/12/23 11:13:31.29
    STEP: Wait for the deployment to be ready 02/12/23 11:13:31.302
    Feb 12 11:13:31.313: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 11:13:33.341
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:13:33.361
    Feb 12 11:13:34.361: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 12 11:13:34.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Creating a v1 custom resource 02/12/23 11:13:42.012
    STEP: Create a v2 custom resource 02/12/23 11:13:42.035
    STEP: List CRs in v1 02/12/23 11:13:42.116
    STEP: List CRs in v2 02/12/23 11:13:42.122
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:13:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3213" for this suite. 02/12/23 11:13:42.666
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:13:42.807
Feb 12 11:13:42.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename endpointslice 02/12/23 11:13:42.809
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:42.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:42.918
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 02/12/23 11:13:53.229
STEP: referencing matching pods with named port 02/12/23 11:13:58.236
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/12/23 11:14:03.243
STEP: recreating EndpointSlices after they've been deleted 02/12/23 11:14:08.255
Feb 12 11:14:08.283: INFO: EndpointSlice for Service endpointslice-2918/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 12 11:14:18.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2918" for this suite. 02/12/23 11:14:18.435
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":36,"skipped":679,"failed":0}
------------------------------
 [SLOW TEST] [35.676 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:13:42.807
    Feb 12 11:13:42.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename endpointslice 02/12/23 11:13:42.809
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:13:42.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:13:42.918
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 02/12/23 11:13:53.229
    STEP: referencing matching pods with named port 02/12/23 11:13:58.236
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/12/23 11:14:03.243
    STEP: recreating EndpointSlices after they've been deleted 02/12/23 11:14:08.255
    Feb 12 11:14:08.283: INFO: EndpointSlice for Service endpointslice-2918/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 12 11:14:18.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2918" for this suite. 02/12/23 11:14:18.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:14:18.488
Feb 12 11:14:18.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:14:18.489
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:14:18.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:14:18.636
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:14:18.64
Feb 12 11:14:18.662: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e" in namespace "projected-1464" to be "Succeeded or Failed"
Feb 12 11:14:18.677: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.318483ms
Feb 12 11:14:20.684: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021409373s
Feb 12 11:14:22.683: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020928063s
STEP: Saw pod success 02/12/23 11:14:22.683
Feb 12 11:14:22.683: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e" satisfied condition "Succeeded or Failed"
Feb 12 11:14:22.686: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e container client-container: <nil>
STEP: delete the pod 02/12/23 11:14:22.692
Feb 12 11:14:22.711: INFO: Waiting for pod downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e to disappear
Feb 12 11:14:22.714: INFO: Pod downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:14:22.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1464" for this suite. 02/12/23 11:14:22.718
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":37,"skipped":689,"failed":0}
------------------------------
 [4.238 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:14:18.488
    Feb 12 11:14:18.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:14:18.489
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:14:18.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:14:18.636
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:14:18.64
    Feb 12 11:14:18.662: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e" in namespace "projected-1464" to be "Succeeded or Failed"
    Feb 12 11:14:18.677: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.318483ms
    Feb 12 11:14:20.684: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021409373s
    Feb 12 11:14:22.683: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020928063s
    STEP: Saw pod success 02/12/23 11:14:22.683
    Feb 12 11:14:22.683: INFO: Pod "downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e" satisfied condition "Succeeded or Failed"
    Feb 12 11:14:22.686: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e container client-container: <nil>
    STEP: delete the pod 02/12/23 11:14:22.692
    Feb 12 11:14:22.711: INFO: Waiting for pod downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e to disappear
    Feb 12 11:14:22.714: INFO: Pod downwardapi-volume-4cc581a7-c85e-41c1-937a-c50de842226e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:14:22.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1464" for this suite. 02/12/23 11:14:22.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:14:22.727
Feb 12 11:14:22.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pod-network-test 02/12/23 11:14:22.728
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:14:22.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:14:22.748
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-7569 02/12/23 11:14:22.75
STEP: creating a selector 02/12/23 11:14:22.75
STEP: Creating the service pods in kubernetes 02/12/23 11:14:22.75
Feb 12 11:14:22.750: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 11:14:22.797: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7569" to be "running and ready"
Feb 12 11:14:22.814: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.858254ms
Feb 12 11:14:22.814: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:24.819: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021945723s
Feb 12 11:14:24.819: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:26.821: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024513177s
Feb 12 11:14:26.821: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:32.538: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.741447079s
Feb 12 11:14:32.538: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:32.959: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.162511304s
Feb 12 11:14:32.959: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:34.826: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029401486s
Feb 12 11:14:34.826: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:37.572: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.775506551s
Feb 12 11:14:37.572: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:39.814: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.017384975s
Feb 12 11:14:39.814: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:40.851: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.054652189s
Feb 12 11:14:40.851: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:14:42.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029402844s
Feb 12 11:14:42.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:14:44.819: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.022270801s
Feb 12 11:14:44.819: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:14:46.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.029215248s
Feb 12 11:14:46.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:14:48.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.028979239s
Feb 12 11:14:48.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:14:50.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.029629181s
Feb 12 11:14:50.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:14:52.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.021289002s
Feb 12 11:14:52.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:14:54.828: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.031269092s
Feb 12 11:14:54.828: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 12 11:14:54.828: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 12 11:14:54.839: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7569" to be "running and ready"
Feb 12 11:14:54.852: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.670154ms
Feb 12 11:14:54.852: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 12 11:14:54.852: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 12 11:14:54.863: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7569" to be "running and ready"
Feb 12 11:14:54.871: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.830457ms
Feb 12 11:14:54.871: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 12 11:14:54.871: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/12/23 11:14:54.883
Feb 12 11:14:54.920: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7569" to be "running"
Feb 12 11:14:54.940: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.033924ms
Feb 12 11:14:56.948: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028270287s
Feb 12 11:14:56.948: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 12 11:14:56.958: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7569" to be "running"
Feb 12 11:14:56.967: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.826819ms
Feb 12 11:14:56.967: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 12 11:14:56.974: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 12 11:14:56.974: INFO: Going to poll 10.233.120.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 12 11:14:56.980: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:14:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:14:56.981: INFO: ExecWithOptions: Clientset creation
Feb 12 11:14:56.981: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7569/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.67+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 11:14:58.083: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 12 11:14:58.083: INFO: Going to poll 10.233.120.198 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 12 11:14:58.087: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:14:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:14:58.088: INFO: ExecWithOptions: Clientset creation
Feb 12 11:14:58.088: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7569/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.198+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 11:14:59.146: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 12 11:14:59.146: INFO: Going to poll 10.233.99.102 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Feb 12 11:14:59.159: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.99.102 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:14:59.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:14:59.160: INFO: ExecWithOptions: Clientset creation
Feb 12 11:14:59.161: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7569/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.99.102+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 11:15:00.284: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 12 11:15:00.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7569" for this suite. 02/12/23 11:15:00.29
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":38,"skipped":694,"failed":0}
------------------------------
 [SLOW TEST] [37.572 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:14:22.727
    Feb 12 11:14:22.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pod-network-test 02/12/23 11:14:22.728
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:14:22.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:14:22.748
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-7569 02/12/23 11:14:22.75
    STEP: creating a selector 02/12/23 11:14:22.75
    STEP: Creating the service pods in kubernetes 02/12/23 11:14:22.75
    Feb 12 11:14:22.750: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 12 11:14:22.797: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7569" to be "running and ready"
    Feb 12 11:14:22.814: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.858254ms
    Feb 12 11:14:22.814: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:24.819: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021945723s
    Feb 12 11:14:24.819: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:26.821: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024513177s
    Feb 12 11:14:26.821: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:32.538: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.741447079s
    Feb 12 11:14:32.538: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:32.959: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.162511304s
    Feb 12 11:14:32.959: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:34.826: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029401486s
    Feb 12 11:14:34.826: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:37.572: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.775506551s
    Feb 12 11:14:37.572: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:39.814: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.017384975s
    Feb 12 11:14:39.814: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:40.851: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.054652189s
    Feb 12 11:14:40.851: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:14:42.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029402844s
    Feb 12 11:14:42.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:14:44.819: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.022270801s
    Feb 12 11:14:44.819: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:14:46.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.029215248s
    Feb 12 11:14:46.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:14:48.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.028979239s
    Feb 12 11:14:48.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:14:50.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.029629181s
    Feb 12 11:14:50.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:14:52.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.021289002s
    Feb 12 11:14:52.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:14:54.828: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.031269092s
    Feb 12 11:14:54.828: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 12 11:14:54.828: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 12 11:14:54.839: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7569" to be "running and ready"
    Feb 12 11:14:54.852: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.670154ms
    Feb 12 11:14:54.852: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 12 11:14:54.852: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 12 11:14:54.863: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7569" to be "running and ready"
    Feb 12 11:14:54.871: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.830457ms
    Feb 12 11:14:54.871: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 12 11:14:54.871: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/12/23 11:14:54.883
    Feb 12 11:14:54.920: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7569" to be "running"
    Feb 12 11:14:54.940: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.033924ms
    Feb 12 11:14:56.948: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028270287s
    Feb 12 11:14:56.948: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 12 11:14:56.958: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7569" to be "running"
    Feb 12 11:14:56.967: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.826819ms
    Feb 12 11:14:56.967: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 12 11:14:56.974: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 12 11:14:56.974: INFO: Going to poll 10.233.120.67 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 12 11:14:56.980: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:14:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:14:56.981: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:14:56.981: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7569/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.67+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 11:14:58.083: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 12 11:14:58.083: INFO: Going to poll 10.233.120.198 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 12 11:14:58.087: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.120.198 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:14:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:14:58.088: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:14:58.088: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7569/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.120.198+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 11:14:59.146: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 12 11:14:59.146: INFO: Going to poll 10.233.99.102 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Feb 12 11:14:59.159: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.99.102 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:14:59.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:14:59.160: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:14:59.161: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7569/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.99.102+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 11:15:00.284: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 12 11:15:00.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7569" for this suite. 02/12/23 11:15:00.29
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:15:00.3
Feb 12 11:15:00.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename events 02/12/23 11:15:00.3
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:00.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:00.328
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/12/23 11:15:00.33
Feb 12 11:15:00.335: INFO: created test-event-1
Feb 12 11:15:00.340: INFO: created test-event-2
Feb 12 11:15:00.344: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/12/23 11:15:00.344
STEP: delete collection of events 02/12/23 11:15:00.347
Feb 12 11:15:00.347: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/12/23 11:15:00.375
Feb 12 11:15:00.375: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 12 11:15:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8816" for this suite. 02/12/23 11:15:00.38
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":39,"skipped":698,"failed":0}
------------------------------
 [0.088 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:15:00.3
    Feb 12 11:15:00.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename events 02/12/23 11:15:00.3
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:00.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:00.328
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/12/23 11:15:00.33
    Feb 12 11:15:00.335: INFO: created test-event-1
    Feb 12 11:15:00.340: INFO: created test-event-2
    Feb 12 11:15:00.344: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/12/23 11:15:00.344
    STEP: delete collection of events 02/12/23 11:15:00.347
    Feb 12 11:15:00.347: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/12/23 11:15:00.375
    Feb 12 11:15:00.375: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 12 11:15:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8816" for this suite. 02/12/23 11:15:00.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:15:00.395
Feb 12 11:15:00.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pod-network-test 02/12/23 11:15:00.396
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:00.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:00.417
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-8878 02/12/23 11:15:00.42
STEP: creating a selector 02/12/23 11:15:00.42
STEP: Creating the service pods in kubernetes 02/12/23 11:15:00.42
Feb 12 11:15:00.421: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 11:15:00.458: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8878" to be "running and ready"
Feb 12 11:15:00.470: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.623616ms
Feb 12 11:15:00.470: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:15:02.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.027241584s
Feb 12 11:15:02.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:15:06.770: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.31240175s
Feb 12 11:15:06.771: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:15:15.220: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.762884708s
Feb 12 11:15:15.221: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:15:18.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.297778903s
Feb 12 11:15:18.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:15:20.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024878614s
Feb 12 11:15:20.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 11:15:22.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023204052s
Feb 12 11:15:22.481: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 12 11:15:22.481: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 12 11:15:22.493: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8878" to be "running and ready"
Feb 12 11:15:22.506: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.225764ms
Feb 12 11:15:22.506: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 12 11:15:22.507: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 12 11:15:22.517: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8878" to be "running and ready"
Feb 12 11:15:22.529: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.384531ms
Feb 12 11:15:22.529: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 12 11:15:22.530: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/12/23 11:15:22.542
Feb 12 11:15:22.551: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8878" to be "running"
Feb 12 11:15:22.556: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868129ms
Feb 12 11:15:24.587: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035932626s
Feb 12 11:15:24.588: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 12 11:15:24.607: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 12 11:15:24.607: INFO: Breadth first check of 10.233.120.68 on host 10.2.20.101...
Feb 12 11:15:24.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.105:9080/dial?request=hostname&protocol=udp&host=10.233.120.68&port=8081&tries=1'] Namespace:pod-network-test-8878 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:15:24.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:15:24.614: INFO: ExecWithOptions: Clientset creation
Feb 12 11:15:24.614: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8878/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.68%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 12 11:15:24.716: INFO: Waiting for responses: map[]
Feb 12 11:15:24.716: INFO: reached 10.233.120.68 after 0/1 tries
Feb 12 11:15:24.716: INFO: Breadth first check of 10.233.120.199 on host 10.2.20.102...
Feb 12 11:15:24.721: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.105:9080/dial?request=hostname&protocol=udp&host=10.233.120.199&port=8081&tries=1'] Namespace:pod-network-test-8878 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:15:24.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:15:24.721: INFO: ExecWithOptions: Clientset creation
Feb 12 11:15:24.722: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8878/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 12 11:15:24.794: INFO: Waiting for responses: map[]
Feb 12 11:15:24.794: INFO: reached 10.233.120.199 after 0/1 tries
Feb 12 11:15:24.795: INFO: Breadth first check of 10.233.99.104 on host 10.2.20.103...
Feb 12 11:15:24.799: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.105:9080/dial?request=hostname&protocol=udp&host=10.233.99.104&port=8081&tries=1'] Namespace:pod-network-test-8878 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:15:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:15:24.799: INFO: ExecWithOptions: Clientset creation
Feb 12 11:15:24.799: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8878/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.99.104%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 12 11:15:24.876: INFO: Waiting for responses: map[]
Feb 12 11:15:24.876: INFO: reached 10.233.99.104 after 0/1 tries
Feb 12 11:15:24.876: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 12 11:15:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8878" for this suite. 02/12/23 11:15:24.884
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":40,"skipped":744,"failed":0}
------------------------------
 [SLOW TEST] [24.502 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:15:00.395
    Feb 12 11:15:00.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pod-network-test 02/12/23 11:15:00.396
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:00.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:00.417
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-8878 02/12/23 11:15:00.42
    STEP: creating a selector 02/12/23 11:15:00.42
    STEP: Creating the service pods in kubernetes 02/12/23 11:15:00.42
    Feb 12 11:15:00.421: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 12 11:15:00.458: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8878" to be "running and ready"
    Feb 12 11:15:00.470: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.623616ms
    Feb 12 11:15:00.470: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:15:02.485: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.027241584s
    Feb 12 11:15:02.485: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:15:06.770: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.31240175s
    Feb 12 11:15:06.771: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:15:15.220: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.762884708s
    Feb 12 11:15:15.221: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:15:18.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.297778903s
    Feb 12 11:15:18.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:15:20.482: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.024878614s
    Feb 12 11:15:20.482: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 11:15:22.481: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.023204052s
    Feb 12 11:15:22.481: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 12 11:15:22.481: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 12 11:15:22.493: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8878" to be "running and ready"
    Feb 12 11:15:22.506: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.225764ms
    Feb 12 11:15:22.506: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 12 11:15:22.507: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 12 11:15:22.517: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8878" to be "running and ready"
    Feb 12 11:15:22.529: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.384531ms
    Feb 12 11:15:22.529: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 12 11:15:22.530: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/12/23 11:15:22.542
    Feb 12 11:15:22.551: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8878" to be "running"
    Feb 12 11:15:22.556: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868129ms
    Feb 12 11:15:24.587: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035932626s
    Feb 12 11:15:24.588: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 12 11:15:24.607: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 12 11:15:24.607: INFO: Breadth first check of 10.233.120.68 on host 10.2.20.101...
    Feb 12 11:15:24.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.105:9080/dial?request=hostname&protocol=udp&host=10.233.120.68&port=8081&tries=1'] Namespace:pod-network-test-8878 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:15:24.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:15:24.614: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:15:24.614: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8878/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.68%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 12 11:15:24.716: INFO: Waiting for responses: map[]
    Feb 12 11:15:24.716: INFO: reached 10.233.120.68 after 0/1 tries
    Feb 12 11:15:24.716: INFO: Breadth first check of 10.233.120.199 on host 10.2.20.102...
    Feb 12 11:15:24.721: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.105:9080/dial?request=hostname&protocol=udp&host=10.233.120.199&port=8081&tries=1'] Namespace:pod-network-test-8878 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:15:24.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:15:24.721: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:15:24.722: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8878/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.120.199%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 12 11:15:24.794: INFO: Waiting for responses: map[]
    Feb 12 11:15:24.794: INFO: reached 10.233.120.199 after 0/1 tries
    Feb 12 11:15:24.795: INFO: Breadth first check of 10.233.99.104 on host 10.2.20.103...
    Feb 12 11:15:24.799: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.105:9080/dial?request=hostname&protocol=udp&host=10.233.99.104&port=8081&tries=1'] Namespace:pod-network-test-8878 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:15:24.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:15:24.799: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:15:24.799: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8878/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.105%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.99.104%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 12 11:15:24.876: INFO: Waiting for responses: map[]
    Feb 12 11:15:24.876: INFO: reached 10.233.99.104 after 0/1 tries
    Feb 12 11:15:24.876: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 12 11:15:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8878" for this suite. 02/12/23 11:15:24.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:15:24.898
Feb 12 11:15:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 11:15:24.898
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:24.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:24.921
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/12/23 11:15:24.922
Feb 12 11:15:24.933: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5449  a106c56d-592d-4094-979a-c12b5c9e8665 5054 0 2023-02-12 11:15:24 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-12 11:15:24 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvkz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvkz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 11:15:24.933: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5449" to be "running and ready"
Feb 12 11:15:24.942: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.702674ms
Feb 12 11:15:24.942: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:15:26.954: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.021108511s
Feb 12 11:15:26.954: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 12 11:15:26.954: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/12/23 11:15:26.954
Feb 12 11:15:26.954: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5449 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:15:26.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:15:26.956: INFO: ExecWithOptions: Clientset creation
Feb 12 11:15:26.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5449/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/12/23 11:15:27.093
Feb 12 11:15:27.093: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5449 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:15:27.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:15:27.094: INFO: ExecWithOptions: Clientset creation
Feb 12 11:15:27.094: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5449/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 11:15:27.231: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 11:15:27.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5449" for this suite. 02/12/23 11:15:27.256
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":41,"skipped":750,"failed":0}
------------------------------
 [2.367 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:15:24.898
    Feb 12 11:15:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 11:15:24.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:24.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:24.921
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/12/23 11:15:24.922
    Feb 12 11:15:24.933: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5449  a106c56d-592d-4094-979a-c12b5c9e8665 5054 0 2023-02-12 11:15:24 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-12 11:15:24 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvkz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvkz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 11:15:24.933: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5449" to be "running and ready"
    Feb 12 11:15:24.942: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 8.702674ms
    Feb 12 11:15:24.942: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:15:26.954: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.021108511s
    Feb 12 11:15:26.954: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 12 11:15:26.954: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/12/23 11:15:26.954
    Feb 12 11:15:26.954: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5449 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:15:26.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:15:26.956: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:15:26.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5449/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/12/23 11:15:27.093
    Feb 12 11:15:27.093: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5449 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:15:27.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:15:27.094: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:15:27.094: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-5449/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 11:15:27.231: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 11:15:27.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5449" for this suite. 02/12/23 11:15:27.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:15:27.266
Feb 12 11:15:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-runtime 02/12/23 11:15:27.267
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:27.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:27.289
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 02/12/23 11:15:27.292
STEP: wait for the container to reach Succeeded 02/12/23 11:15:27.301
STEP: get the container status 02/12/23 11:15:33.693
STEP: the container should be terminated 02/12/23 11:15:33.718
STEP: the termination message should be set 02/12/23 11:15:33.718
Feb 12 11:15:33.719: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/12/23 11:15:33.719
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 12 11:15:33.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8182" for this suite. 02/12/23 11:15:33.9
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":42,"skipped":810,"failed":0}
------------------------------
 [SLOW TEST] [6.675 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:15:27.266
    Feb 12 11:15:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-runtime 02/12/23 11:15:27.267
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:27.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:27.289
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 02/12/23 11:15:27.292
    STEP: wait for the container to reach Succeeded 02/12/23 11:15:27.301
    STEP: get the container status 02/12/23 11:15:33.693
    STEP: the container should be terminated 02/12/23 11:15:33.718
    STEP: the termination message should be set 02/12/23 11:15:33.718
    Feb 12 11:15:33.719: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/12/23 11:15:33.719
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 12 11:15:33.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8182" for this suite. 02/12/23 11:15:33.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:15:33.943
Feb 12 11:15:33.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:15:33.945
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:34.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:34.007
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6466 02/12/23 11:15:34.01
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/12/23 11:15:34.047
STEP: creating service externalsvc in namespace services-6466 02/12/23 11:15:34.047
STEP: creating replication controller externalsvc in namespace services-6466 02/12/23 11:15:34.083
I0212 11:15:34.098133      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6466, replica count: 2
I0212 11:15:37.149936      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/12/23 11:15:37.171
Feb 12 11:15:37.214: INFO: Creating new exec pod
Feb 12 11:15:37.235: INFO: Waiting up to 5m0s for pod "execpodcl9xf" in namespace "services-6466" to be "running"
Feb 12 11:15:37.249: INFO: Pod "execpodcl9xf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.902092ms
Feb 12 11:15:39.253: INFO: Pod "execpodcl9xf": Phase="Running", Reason="", readiness=true. Elapsed: 2.018052797s
Feb 12 11:15:39.253: INFO: Pod "execpodcl9xf" satisfied condition "running"
Feb 12 11:15:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6466 exec execpodcl9xf -- /bin/sh -x -c nslookup nodeport-service.services-6466.svc.cluster.local'
Feb 12 11:15:39.412: INFO: stderr: "+ nslookup nodeport-service.services-6466.svc.cluster.local\n"
Feb 12 11:15:39.412: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-6466.svc.cluster.local\tcanonical name = externalsvc.services-6466.svc.cluster.local.\nName:\texternalsvc.services-6466.svc.cluster.local\nAddress: 10.233.31.239\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6466, will wait for the garbage collector to delete the pods 02/12/23 11:15:39.412
Feb 12 11:15:39.476: INFO: Deleting ReplicationController externalsvc took: 10.803363ms
Feb 12 11:15:39.577: INFO: Terminating ReplicationController externalsvc pods took: 100.837525ms
Feb 12 11:15:43.545: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:15:43.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6466" for this suite. 02/12/23 11:15:43.724
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":43,"skipped":817,"failed":0}
------------------------------
 [SLOW TEST] [9.806 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:15:33.943
    Feb 12 11:15:33.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:15:33.945
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:34.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:34.007
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-6466 02/12/23 11:15:34.01
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/12/23 11:15:34.047
    STEP: creating service externalsvc in namespace services-6466 02/12/23 11:15:34.047
    STEP: creating replication controller externalsvc in namespace services-6466 02/12/23 11:15:34.083
    I0212 11:15:34.098133      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6466, replica count: 2
    I0212 11:15:37.149936      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/12/23 11:15:37.171
    Feb 12 11:15:37.214: INFO: Creating new exec pod
    Feb 12 11:15:37.235: INFO: Waiting up to 5m0s for pod "execpodcl9xf" in namespace "services-6466" to be "running"
    Feb 12 11:15:37.249: INFO: Pod "execpodcl9xf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.902092ms
    Feb 12 11:15:39.253: INFO: Pod "execpodcl9xf": Phase="Running", Reason="", readiness=true. Elapsed: 2.018052797s
    Feb 12 11:15:39.253: INFO: Pod "execpodcl9xf" satisfied condition "running"
    Feb 12 11:15:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6466 exec execpodcl9xf -- /bin/sh -x -c nslookup nodeport-service.services-6466.svc.cluster.local'
    Feb 12 11:15:39.412: INFO: stderr: "+ nslookup nodeport-service.services-6466.svc.cluster.local\n"
    Feb 12 11:15:39.412: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-6466.svc.cluster.local\tcanonical name = externalsvc.services-6466.svc.cluster.local.\nName:\texternalsvc.services-6466.svc.cluster.local\nAddress: 10.233.31.239\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6466, will wait for the garbage collector to delete the pods 02/12/23 11:15:39.412
    Feb 12 11:15:39.476: INFO: Deleting ReplicationController externalsvc took: 10.803363ms
    Feb 12 11:15:39.577: INFO: Terminating ReplicationController externalsvc pods took: 100.837525ms
    Feb 12 11:15:43.545: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:15:43.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6466" for this suite. 02/12/23 11:15:43.724
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:15:43.752
Feb 12 11:15:43.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-runtime 02/12/23 11:15:43.754
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:43.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:43.904
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/12/23 11:15:43.974
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/12/23 11:16:01.178
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/12/23 11:16:01.181
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/12/23 11:16:01.188
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/12/23 11:16:01.188
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/12/23 11:16:01.222
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/12/23 11:16:03.24
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/12/23 11:16:05.256
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/12/23 11:16:05.265
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/12/23 11:16:05.266
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/12/23 11:16:05.307
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/12/23 11:16:06.494
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/12/23 11:16:09.593
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/12/23 11:16:09.606
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/12/23 11:16:09.606
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 12 11:16:09.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3158" for this suite. 02/12/23 11:16:09.641
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":44,"skipped":823,"failed":0}
------------------------------
 [SLOW TEST] [25.897 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:15:43.752
    Feb 12 11:15:43.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-runtime 02/12/23 11:15:43.754
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:15:43.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:15:43.904
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/12/23 11:15:43.974
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/12/23 11:16:01.178
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/12/23 11:16:01.181
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/12/23 11:16:01.188
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/12/23 11:16:01.188
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/12/23 11:16:01.222
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/12/23 11:16:03.24
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/12/23 11:16:05.256
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/12/23 11:16:05.265
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/12/23 11:16:05.266
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/12/23 11:16:05.307
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/12/23 11:16:06.494
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/12/23 11:16:09.593
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/12/23 11:16:09.606
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/12/23 11:16:09.606
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 12 11:16:09.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3158" for this suite. 02/12/23 11:16:09.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:16:09.651
Feb 12 11:16:09.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:16:09.651
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:16:09.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:16:09.677
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 02/12/23 11:16:09.679
Feb 12 11:16:09.688: INFO: Waiting up to 5m0s for pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14" in namespace "projected-8431" to be "running and ready"
Feb 12 11:16:09.695: INFO: Pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.635832ms
Feb 12 11:16:09.695: INFO: The phase of Pod annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:16:11.709: INFO: Pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14": Phase="Running", Reason="", readiness=true. Elapsed: 2.020535459s
Feb 12 11:16:11.709: INFO: The phase of Pod annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14 is Running (Ready = true)
Feb 12 11:16:11.709: INFO: Pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14" satisfied condition "running and ready"
Feb 12 11:16:12.280: INFO: Successfully updated pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:16:16.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8431" for this suite. 02/12/23 11:16:16.353
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":45,"skipped":854,"failed":0}
------------------------------
 [SLOW TEST] [6.744 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:16:09.651
    Feb 12 11:16:09.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:16:09.651
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:16:09.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:16:09.677
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 02/12/23 11:16:09.679
    Feb 12 11:16:09.688: INFO: Waiting up to 5m0s for pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14" in namespace "projected-8431" to be "running and ready"
    Feb 12 11:16:09.695: INFO: Pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.635832ms
    Feb 12 11:16:09.695: INFO: The phase of Pod annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:16:11.709: INFO: Pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14": Phase="Running", Reason="", readiness=true. Elapsed: 2.020535459s
    Feb 12 11:16:11.709: INFO: The phase of Pod annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14 is Running (Ready = true)
    Feb 12 11:16:11.709: INFO: Pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14" satisfied condition "running and ready"
    Feb 12 11:16:12.280: INFO: Successfully updated pod "annotationupdatea397d5e4-9a49-439a-ac6f-a80b23fd1a14"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:16:16.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8431" for this suite. 02/12/23 11:16:16.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:16:16.399
Feb 12 11:16:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename watch 02/12/23 11:16:16.4
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:16:16.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:16:16.427
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/12/23 11:16:16.429
STEP: starting a background goroutine to produce watch events 02/12/23 11:16:16.431
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/12/23 11:16:16.431
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 12 11:16:19.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6228" for this suite. 02/12/23 11:16:19.256
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":46,"skipped":911,"failed":0}
------------------------------
 [2.914 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:16:16.399
    Feb 12 11:16:16.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename watch 02/12/23 11:16:16.4
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:16:16.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:16:16.427
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/12/23 11:16:16.429
    STEP: starting a background goroutine to produce watch events 02/12/23 11:16:16.431
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/12/23 11:16:16.431
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 12 11:16:19.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6228" for this suite. 02/12/23 11:16:19.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:16:19.314
Feb 12 11:16:19.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 11:16:19.315
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:16:19.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:16:19.339
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085 in namespace container-probe-8214 02/12/23 11:16:19.341
Feb 12 11:16:19.351: INFO: Waiting up to 5m0s for pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085" in namespace "container-probe-8214" to be "not pending"
Feb 12 11:16:19.356: INFO: Pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690427ms
Feb 12 11:16:21.360: INFO: Pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085": Phase="Running", Reason="", readiness=true. Elapsed: 2.009083548s
Feb 12 11:16:21.360: INFO: Pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085" satisfied condition "not pending"
Feb 12 11:16:21.360: INFO: Started pod busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085 in namespace container-probe-8214
STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:16:21.36
Feb 12 11:16:21.364: INFO: Initial restart count of pod busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085 is 0
STEP: deleting the pod 02/12/23 11:20:22.083
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 11:20:22.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8214" for this suite. 02/12/23 11:20:22.141
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":47,"skipped":917,"failed":0}
------------------------------
 [SLOW TEST] [242.836 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:16:19.314
    Feb 12 11:16:19.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 11:16:19.315
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:16:19.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:16:19.339
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085 in namespace container-probe-8214 02/12/23 11:16:19.341
    Feb 12 11:16:19.351: INFO: Waiting up to 5m0s for pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085" in namespace "container-probe-8214" to be "not pending"
    Feb 12 11:16:19.356: INFO: Pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690427ms
    Feb 12 11:16:21.360: INFO: Pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085": Phase="Running", Reason="", readiness=true. Elapsed: 2.009083548s
    Feb 12 11:16:21.360: INFO: Pod "busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085" satisfied condition "not pending"
    Feb 12 11:16:21.360: INFO: Started pod busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085 in namespace container-probe-8214
    STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:16:21.36
    Feb 12 11:16:21.364: INFO: Initial restart count of pod busybox-2b9ced6c-bbf9-408e-9b0b-6b0b8f106085 is 0
    STEP: deleting the pod 02/12/23 11:20:22.083
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 11:20:22.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8214" for this suite. 02/12/23 11:20:22.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:20:22.151
Feb 12 11:20:22.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:20:22.152
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:20:22.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:20:22.175
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-756a0888-156d-4063-b2be-d515ed0207ff 02/12/23 11:20:22.177
STEP: Creating a pod to test consume configMaps 02/12/23 11:20:22.182
Feb 12 11:20:22.189: INFO: Waiting up to 5m0s for pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31" in namespace "configmap-9527" to be "Succeeded or Failed"
Feb 12 11:20:22.193: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218735ms
Feb 12 11:20:24.208: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018943617s
Feb 12 11:20:26.205: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016599221s
STEP: Saw pod success 02/12/23 11:20:26.206
Feb 12 11:20:26.206: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31" satisfied condition "Succeeded or Failed"
Feb 12 11:20:26.219: INFO: Trying to get logs from node kube-3 pod pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31 container configmap-volume-test: <nil>
STEP: delete the pod 02/12/23 11:20:26.272
Feb 12 11:20:26.295: INFO: Waiting for pod pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31 to disappear
Feb 12 11:20:26.298: INFO: Pod pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:20:26.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9527" for this suite. 02/12/23 11:20:26.302
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":48,"skipped":926,"failed":0}
------------------------------
 [4.159 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:20:22.151
    Feb 12 11:20:22.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:20:22.152
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:20:22.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:20:22.175
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-756a0888-156d-4063-b2be-d515ed0207ff 02/12/23 11:20:22.177
    STEP: Creating a pod to test consume configMaps 02/12/23 11:20:22.182
    Feb 12 11:20:22.189: INFO: Waiting up to 5m0s for pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31" in namespace "configmap-9527" to be "Succeeded or Failed"
    Feb 12 11:20:22.193: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218735ms
    Feb 12 11:20:24.208: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018943617s
    Feb 12 11:20:26.205: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016599221s
    STEP: Saw pod success 02/12/23 11:20:26.206
    Feb 12 11:20:26.206: INFO: Pod "pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31" satisfied condition "Succeeded or Failed"
    Feb 12 11:20:26.219: INFO: Trying to get logs from node kube-3 pod pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31 container configmap-volume-test: <nil>
    STEP: delete the pod 02/12/23 11:20:26.272
    Feb 12 11:20:26.295: INFO: Waiting for pod pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31 to disappear
    Feb 12 11:20:26.298: INFO: Pod pod-configmaps-21cf1d7a-d6ea-4365-b21c-219c565c5e31 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:20:26.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9527" for this suite. 02/12/23 11:20:26.302
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:20:26.31
Feb 12 11:20:26.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:20:26.311
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:20:26.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:20:26.33
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 02/12/23 11:20:26.331
Feb 12 11:20:26.342: INFO: Waiting up to 5m0s for pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b" in namespace "downward-api-4057" to be "Succeeded or Failed"
Feb 12 11:20:26.352: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050443ms
Feb 12 11:20:28.358: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016239926s
Feb 12 11:20:30.362: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020140213s
STEP: Saw pod success 02/12/23 11:20:30.362
Feb 12 11:20:30.363: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b" satisfied condition "Succeeded or Failed"
Feb 12 11:20:30.369: INFO: Trying to get logs from node kube-3 pod downward-api-35114259-e127-4ad0-9b38-682339f0e11b container dapi-container: <nil>
STEP: delete the pod 02/12/23 11:20:30.385
Feb 12 11:20:30.409: INFO: Waiting for pod downward-api-35114259-e127-4ad0-9b38-682339f0e11b to disappear
Feb 12 11:20:30.411: INFO: Pod downward-api-35114259-e127-4ad0-9b38-682339f0e11b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 12 11:20:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4057" for this suite. 02/12/23 11:20:30.414
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":49,"skipped":929,"failed":0}
------------------------------
 [4.111 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:20:26.31
    Feb 12 11:20:26.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:20:26.311
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:20:26.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:20:26.33
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 02/12/23 11:20:26.331
    Feb 12 11:20:26.342: INFO: Waiting up to 5m0s for pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b" in namespace "downward-api-4057" to be "Succeeded or Failed"
    Feb 12 11:20:26.352: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.050443ms
    Feb 12 11:20:28.358: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016239926s
    Feb 12 11:20:30.362: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020140213s
    STEP: Saw pod success 02/12/23 11:20:30.362
    Feb 12 11:20:30.363: INFO: Pod "downward-api-35114259-e127-4ad0-9b38-682339f0e11b" satisfied condition "Succeeded or Failed"
    Feb 12 11:20:30.369: INFO: Trying to get logs from node kube-3 pod downward-api-35114259-e127-4ad0-9b38-682339f0e11b container dapi-container: <nil>
    STEP: delete the pod 02/12/23 11:20:30.385
    Feb 12 11:20:30.409: INFO: Waiting for pod downward-api-35114259-e127-4ad0-9b38-682339f0e11b to disappear
    Feb 12 11:20:30.411: INFO: Pod downward-api-35114259-e127-4ad0-9b38-682339f0e11b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 12 11:20:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4057" for this suite. 02/12/23 11:20:30.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:20:30.424
Feb 12 11:20:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-preemption 02/12/23 11:20:30.425
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:20:30.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:20:30.451
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 12 11:20:30.467: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 11:21:30.532: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:21:30.54
Feb 12 11:21:30.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-preemption-path 02/12/23 11:21:30.542
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:21:30.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:21:30.596
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 02/12/23 11:21:30.598
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/12/23 11:21:30.599
Feb 12 11:21:30.613: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5965" to be "running"
Feb 12 11:21:30.621: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364195ms
Feb 12 11:21:32.630: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016767363s
Feb 12 11:21:32.630: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/12/23 11:21:32.635
Feb 12 11:21:33.208: INFO: found a healthy node: kube-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Feb 12 11:21:44.328: INFO: pods created so far: [1 1 1]
Feb 12 11:21:44.329: INFO: length of pods created so far: 3
Feb 12 11:21:48.352: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Feb 12 11:21:55.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5965" for this suite. 02/12/23 11:21:55.36
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:21:55.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3188" for this suite. 02/12/23 11:21:55.41
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":50,"skipped":954,"failed":0}
------------------------------
 [SLOW TEST] [85.026 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:20:30.424
    Feb 12 11:20:30.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-preemption 02/12/23 11:20:30.425
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:20:30.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:20:30.451
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 12 11:20:30.467: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 12 11:21:30.532: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:21:30.54
    Feb 12 11:21:30.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-preemption-path 02/12/23 11:21:30.542
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:21:30.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:21:30.596
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 02/12/23 11:21:30.598
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/12/23 11:21:30.599
    Feb 12 11:21:30.613: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5965" to be "running"
    Feb 12 11:21:30.621: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364195ms
    Feb 12 11:21:32.630: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016767363s
    Feb 12 11:21:32.630: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/12/23 11:21:32.635
    Feb 12 11:21:33.208: INFO: found a healthy node: kube-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Feb 12 11:21:44.328: INFO: pods created so far: [1 1 1]
    Feb 12 11:21:44.329: INFO: length of pods created so far: 3
    Feb 12 11:21:48.352: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Feb 12 11:21:55.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-5965" for this suite. 02/12/23 11:21:55.36
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:21:55.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3188" for this suite. 02/12/23 11:21:55.41
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:21:55.451
Feb 12 11:21:55.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:21:55.452
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:21:55.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:21:55.477
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/12/23 11:21:55.48
Feb 12 11:21:55.490: INFO: Waiting up to 5m0s for pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee" in namespace "emptydir-3996" to be "Succeeded or Failed"
Feb 12 11:21:55.494: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105198ms
Feb 12 11:21:57.506: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016234949s
Feb 12 11:21:59.507: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016901747s
STEP: Saw pod success 02/12/23 11:21:59.507
Feb 12 11:21:59.507: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee" satisfied condition "Succeeded or Failed"
Feb 12 11:21:59.519: INFO: Trying to get logs from node kube-3 pod pod-86708cb2-f27c-4484-b069-22e541f7a7ee container test-container: <nil>
STEP: delete the pod 02/12/23 11:21:59.544
Feb 12 11:21:59.567: INFO: Waiting for pod pod-86708cb2-f27c-4484-b069-22e541f7a7ee to disappear
Feb 12 11:21:59.570: INFO: Pod pod-86708cb2-f27c-4484-b069-22e541f7a7ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:21:59.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3996" for this suite. 02/12/23 11:21:59.574
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":51,"skipped":974,"failed":0}
------------------------------
 [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:21:55.451
    Feb 12 11:21:55.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:21:55.452
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:21:55.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:21:55.477
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/12/23 11:21:55.48
    Feb 12 11:21:55.490: INFO: Waiting up to 5m0s for pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee" in namespace "emptydir-3996" to be "Succeeded or Failed"
    Feb 12 11:21:55.494: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105198ms
    Feb 12 11:21:57.506: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016234949s
    Feb 12 11:21:59.507: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016901747s
    STEP: Saw pod success 02/12/23 11:21:59.507
    Feb 12 11:21:59.507: INFO: Pod "pod-86708cb2-f27c-4484-b069-22e541f7a7ee" satisfied condition "Succeeded or Failed"
    Feb 12 11:21:59.519: INFO: Trying to get logs from node kube-3 pod pod-86708cb2-f27c-4484-b069-22e541f7a7ee container test-container: <nil>
    STEP: delete the pod 02/12/23 11:21:59.544
    Feb 12 11:21:59.567: INFO: Waiting for pod pod-86708cb2-f27c-4484-b069-22e541f7a7ee to disappear
    Feb 12 11:21:59.570: INFO: Pod pod-86708cb2-f27c-4484-b069-22e541f7a7ee no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:21:59.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3996" for this suite. 02/12/23 11:21:59.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:21:59.582
Feb 12 11:21:59.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:21:59.583
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:21:59.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:21:59.604
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 11:21:59.606
Feb 12 11:21:59.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 12 11:21:59.662: INFO: stderr: ""
Feb 12 11:21:59.662: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/12/23 11:21:59.662
STEP: verifying the pod e2e-test-httpd-pod was created 02/12/23 11:22:04.713
Feb 12 11:22:04.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 get pod e2e-test-httpd-pod -o json'
Feb 12 11:22:04.774: INFO: stderr: ""
Feb 12 11:22:04.774: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"75024c02ddf9ff9e9c6173ba6a744498313de6ddb11159a58ca467f2fbefcbed\",\n            \"cni.projectcalico.org/podIP\": \"10.233.99.123/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.99.123/32\"\n        },\n        \"creationTimestamp\": \"2023-02-12T11:21:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8958\",\n        \"resourceVersion\": \"6624\",\n        \"uid\": \"9418f145-58b4-4f29-91b7-77d2ad0733e9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-n27gl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-n27gl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:21:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:22:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:22:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:21:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a1e111ad13836e23e822e15f902703bf2df662a4322688a8741e51127a6ae3d8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-12T11:22:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.2.20.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.99.123\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.99.123\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-12T11:21:59Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/12/23 11:22:04.774
Feb 12 11:22:04.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 replace -f -'
Feb 12 11:22:06.812: INFO: stderr: ""
Feb 12 11:22:06.812: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/12/23 11:22:06.812
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Feb 12 11:22:08.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 delete pods e2e-test-httpd-pod'
Feb 12 11:22:10.172: INFO: stderr: ""
Feb 12 11:22:10.172: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:22:10.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8958" for this suite. 02/12/23 11:22:10.176
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":52,"skipped":1005,"failed":0}
------------------------------
 [SLOW TEST] [10.612 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:21:59.582
    Feb 12 11:21:59.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:21:59.583
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:21:59.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:21:59.604
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 11:21:59.606
    Feb 12 11:21:59.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 12 11:21:59.662: INFO: stderr: ""
    Feb 12 11:21:59.662: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/12/23 11:21:59.662
    STEP: verifying the pod e2e-test-httpd-pod was created 02/12/23 11:22:04.713
    Feb 12 11:22:04.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 get pod e2e-test-httpd-pod -o json'
    Feb 12 11:22:04.774: INFO: stderr: ""
    Feb 12 11:22:04.774: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"75024c02ddf9ff9e9c6173ba6a744498313de6ddb11159a58ca467f2fbefcbed\",\n            \"cni.projectcalico.org/podIP\": \"10.233.99.123/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.99.123/32\"\n        },\n        \"creationTimestamp\": \"2023-02-12T11:21:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8958\",\n        \"resourceVersion\": \"6624\",\n        \"uid\": \"9418f145-58b4-4f29-91b7-77d2ad0733e9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-n27gl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"kube-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-n27gl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:21:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:22:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:22:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-12T11:21:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a1e111ad13836e23e822e15f902703bf2df662a4322688a8741e51127a6ae3d8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-12T11:22:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.2.20.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.99.123\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.99.123\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-12T11:21:59Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/12/23 11:22:04.774
    Feb 12 11:22:04.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 replace -f -'
    Feb 12 11:22:06.812: INFO: stderr: ""
    Feb 12 11:22:06.812: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 02/12/23 11:22:06.812
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Feb 12 11:22:08.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8958 delete pods e2e-test-httpd-pod'
    Feb 12 11:22:10.172: INFO: stderr: ""
    Feb 12 11:22:10.172: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:22:10.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8958" for this suite. 02/12/23 11:22:10.176
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:22:10.195
Feb 12 11:22:10.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:22:10.196
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:22:10.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:22:10.3
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-871d3350-6aed-4bbd-a1e5-2728fea8130c 02/12/23 11:22:10.302
STEP: Creating a pod to test consume secrets 02/12/23 11:22:10.315
Feb 12 11:22:10.369: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1" in namespace "projected-3539" to be "Succeeded or Failed"
Feb 12 11:22:10.375: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200297ms
Feb 12 11:22:12.391: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021926951s
Feb 12 11:22:14.389: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01977868s
STEP: Saw pod success 02/12/23 11:22:14.389
Feb 12 11:22:14.389: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1" satisfied condition "Succeeded or Failed"
Feb 12 11:22:14.403: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1 container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 11:22:14.422
Feb 12 11:22:14.438: INFO: Waiting for pod pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1 to disappear
Feb 12 11:22:14.442: INFO: Pod pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 11:22:14.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3539" for this suite. 02/12/23 11:22:14.446
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":53,"skipped":1007,"failed":0}
------------------------------
 [4.261 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:22:10.195
    Feb 12 11:22:10.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:22:10.196
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:22:10.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:22:10.3
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-871d3350-6aed-4bbd-a1e5-2728fea8130c 02/12/23 11:22:10.302
    STEP: Creating a pod to test consume secrets 02/12/23 11:22:10.315
    Feb 12 11:22:10.369: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1" in namespace "projected-3539" to be "Succeeded or Failed"
    Feb 12 11:22:10.375: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200297ms
    Feb 12 11:22:12.391: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021926951s
    Feb 12 11:22:14.389: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01977868s
    STEP: Saw pod success 02/12/23 11:22:14.389
    Feb 12 11:22:14.389: INFO: Pod "pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1" satisfied condition "Succeeded or Failed"
    Feb 12 11:22:14.403: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1 container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 11:22:14.422
    Feb 12 11:22:14.438: INFO: Waiting for pod pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1 to disappear
    Feb 12 11:22:14.442: INFO: Pod pod-projected-secrets-0cd1cb2a-b68f-4e44-9980-7054793414c1 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 11:22:14.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3539" for this suite. 02/12/23 11:22:14.446
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:22:14.456
Feb 12 11:22:14.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename containers 02/12/23 11:22:14.457
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:22:14.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:22:14.479
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Feb 12 11:22:14.491: INFO: Waiting up to 5m0s for pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455" in namespace "containers-7727" to be "running"
Feb 12 11:22:14.495: INFO: Pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.441119ms
Feb 12 11:22:16.507: INFO: Pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455": Phase="Running", Reason="", readiness=true. Elapsed: 2.016484571s
Feb 12 11:22:16.507: INFO: Pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 12 11:22:16.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7727" for this suite. 02/12/23 11:22:16.535
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":54,"skipped":1007,"failed":0}
------------------------------
 [2.092 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:22:14.456
    Feb 12 11:22:14.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename containers 02/12/23 11:22:14.457
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:22:14.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:22:14.479
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Feb 12 11:22:14.491: INFO: Waiting up to 5m0s for pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455" in namespace "containers-7727" to be "running"
    Feb 12 11:22:14.495: INFO: Pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455": Phase="Pending", Reason="", readiness=false. Elapsed: 4.441119ms
    Feb 12 11:22:16.507: INFO: Pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455": Phase="Running", Reason="", readiness=true. Elapsed: 2.016484571s
    Feb 12 11:22:16.507: INFO: Pod "client-containers-d0e68234-4f28-4aff-a9a0-b41397d37455" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 12 11:22:16.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7727" for this suite. 02/12/23 11:22:16.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:22:16.55
Feb 12 11:22:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-preemption 02/12/23 11:22:16.551
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:22:16.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:22:16.59
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 12 11:22:16.605: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 11:23:16.651: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:23:16.656
Feb 12 11:23:16.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-preemption-path 02/12/23 11:23:16.657
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:16.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:16.683
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Feb 12 11:23:16.698: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 12 11:23:16.701: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Feb 12 11:23:16.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-508" for this suite. 02/12/23 11:23:16.724
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:23:16.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7333" for this suite. 02/12/23 11:23:16.746
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":55,"skipped":1025,"failed":0}
------------------------------
 [SLOW TEST] [60.236 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:22:16.55
    Feb 12 11:22:16.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-preemption 02/12/23 11:22:16.551
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:22:16.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:22:16.59
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 12 11:22:16.605: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 12 11:23:16.651: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:23:16.656
    Feb 12 11:23:16.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-preemption-path 02/12/23 11:23:16.657
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:16.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:16.683
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Feb 12 11:23:16.698: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 12 11:23:16.701: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Feb 12 11:23:16.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-508" for this suite. 02/12/23 11:23:16.724
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:23:16.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7333" for this suite. 02/12/23 11:23:16.746
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:23:16.794
Feb 12 11:23:16.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename security-context 02/12/23 11:23:16.795
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:16.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:16.819
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/12/23 11:23:16.821
Feb 12 11:23:16.832: INFO: Waiting up to 5m0s for pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710" in namespace "security-context-4385" to be "Succeeded or Failed"
Feb 12 11:23:16.835: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572933ms
Feb 12 11:23:18.848: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016348949s
Feb 12 11:23:20.847: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015562485s
STEP: Saw pod success 02/12/23 11:23:20.847
Feb 12 11:23:20.848: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710" satisfied condition "Succeeded or Failed"
Feb 12 11:23:20.861: INFO: Trying to get logs from node kube-3 pod security-context-7af87d59-8881-4df4-97ad-abadb25ff710 container test-container: <nil>
STEP: delete the pod 02/12/23 11:23:20.888
Feb 12 11:23:20.918: INFO: Waiting for pod security-context-7af87d59-8881-4df4-97ad-abadb25ff710 to disappear
Feb 12 11:23:20.921: INFO: Pod security-context-7af87d59-8881-4df4-97ad-abadb25ff710 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 12 11:23:20.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4385" for this suite. 02/12/23 11:23:20.924
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":56,"skipped":1074,"failed":0}
------------------------------
 [4.139 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:23:16.794
    Feb 12 11:23:16.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename security-context 02/12/23 11:23:16.795
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:16.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:16.819
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/12/23 11:23:16.821
    Feb 12 11:23:16.832: INFO: Waiting up to 5m0s for pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710" in namespace "security-context-4385" to be "Succeeded or Failed"
    Feb 12 11:23:16.835: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572933ms
    Feb 12 11:23:18.848: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016348949s
    Feb 12 11:23:20.847: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015562485s
    STEP: Saw pod success 02/12/23 11:23:20.847
    Feb 12 11:23:20.848: INFO: Pod "security-context-7af87d59-8881-4df4-97ad-abadb25ff710" satisfied condition "Succeeded or Failed"
    Feb 12 11:23:20.861: INFO: Trying to get logs from node kube-3 pod security-context-7af87d59-8881-4df4-97ad-abadb25ff710 container test-container: <nil>
    STEP: delete the pod 02/12/23 11:23:20.888
    Feb 12 11:23:20.918: INFO: Waiting for pod security-context-7af87d59-8881-4df4-97ad-abadb25ff710 to disappear
    Feb 12 11:23:20.921: INFO: Pod security-context-7af87d59-8881-4df4-97ad-abadb25ff710 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 12 11:23:20.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4385" for this suite. 02/12/23 11:23:20.924
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:23:20.934
Feb 12 11:23:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 11:23:20.935
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:20.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:20.957
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 02/12/23 11:23:20.976
STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:23:20.983
Feb 12 11:23:20.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:23:20.994: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:22.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:23:22.005: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:23.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:23.003: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:24.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:24.016: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:25.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:25.001: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:26.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:26.005: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:27.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:27.196: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:28.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:28.002: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:29.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:29.390: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:31.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:31.484: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:32.044: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 11:23:32.044: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:23:33.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 11:23:33.015: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/12/23 11:23:33.042
Feb 12 11:23:33.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 11:23:33.113: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/12/23 11:23:33.113
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:23:34.15
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-975, will wait for the garbage collector to delete the pods 02/12/23 11:23:34.151
Feb 12 11:23:34.224: INFO: Deleting DaemonSet.extensions daemon-set took: 12.561463ms
Feb 12 11:23:34.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.053476ms
Feb 12 11:23:56.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:23:56.329: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 12 11:23:56.332: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7115"},"items":null}

Feb 12 11:23:56.334: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7115"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:23:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-975" for this suite. 02/12/23 11:23:56.347
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":57,"skipped":1075,"failed":0}
------------------------------
 [SLOW TEST] [35.419 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:23:20.934
    Feb 12 11:23:20.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 11:23:20.935
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:20.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:20.957
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 02/12/23 11:23:20.976
    STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:23:20.983
    Feb 12 11:23:20.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:23:20.994: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:22.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:23:22.005: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:23.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:23.003: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:24.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:24.016: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:25.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:25.001: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:26.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:26.005: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:27.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:27.196: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:28.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:28.002: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:29.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:29.390: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:31.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:31.484: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:32.044: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 11:23:32.044: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:23:33.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 11:23:33.015: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/12/23 11:23:33.042
    Feb 12 11:23:33.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 11:23:33.113: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/12/23 11:23:33.113
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:23:34.15
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-975, will wait for the garbage collector to delete the pods 02/12/23 11:23:34.151
    Feb 12 11:23:34.224: INFO: Deleting DaemonSet.extensions daemon-set took: 12.561463ms
    Feb 12 11:23:34.325: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.053476ms
    Feb 12 11:23:56.329: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:23:56.329: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 12 11:23:56.332: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7115"},"items":null}

    Feb 12 11:23:56.334: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7115"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:23:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-975" for this suite. 02/12/23 11:23:56.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:23:56.355
Feb 12 11:23:56.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:23:56.356
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:56.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:56.378
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:23:56.398
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:23:56.773
STEP: Deploying the webhook pod 02/12/23 11:23:56.784
STEP: Wait for the deployment to be ready 02/12/23 11:23:56.795
Feb 12 11:23:56.804: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 11:23:58.815
STEP: Verifying the service has paired with the endpoint 02/12/23 11:23:58.834
Feb 12 11:23:59.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/12/23 11:23:59.85
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/12/23 11:23:59.889
STEP: Creating a dummy validating-webhook-configuration object 02/12/23 11:23:59.903
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/12/23 11:23:59.911
STEP: Creating a dummy mutating-webhook-configuration object 02/12/23 11:23:59.917
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/12/23 11:23:59.928
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:23:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5136" for this suite. 02/12/23 11:23:59.948
STEP: Destroying namespace "webhook-5136-markers" for this suite. 02/12/23 11:23:59.956
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":58,"skipped":1083,"failed":0}
------------------------------
 [3.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:23:56.355
    Feb 12 11:23:56.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:23:56.356
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:23:56.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:23:56.378
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:23:56.398
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:23:56.773
    STEP: Deploying the webhook pod 02/12/23 11:23:56.784
    STEP: Wait for the deployment to be ready 02/12/23 11:23:56.795
    Feb 12 11:23:56.804: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 11:23:58.815
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:23:58.834
    Feb 12 11:23:59.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/12/23 11:23:59.85
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/12/23 11:23:59.889
    STEP: Creating a dummy validating-webhook-configuration object 02/12/23 11:23:59.903
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/12/23 11:23:59.911
    STEP: Creating a dummy mutating-webhook-configuration object 02/12/23 11:23:59.917
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/12/23 11:23:59.928
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:23:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5136" for this suite. 02/12/23 11:23:59.948
    STEP: Destroying namespace "webhook-5136-markers" for this suite. 02/12/23 11:23:59.956
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:00.071
Feb 12 11:24:00.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:24:00.073
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:00.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:00.12
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/12/23 11:24:00.122
Feb 12 11:24:00.140: INFO: Waiting up to 5m0s for pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563" in namespace "emptydir-2051" to be "Succeeded or Failed"
Feb 12 11:24:00.151: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563": Phase="Pending", Reason="", readiness=false. Elapsed: 11.669683ms
Feb 12 11:24:02.156: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016389367s
Feb 12 11:24:04.189: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04962413s
STEP: Saw pod success 02/12/23 11:24:04.189
Feb 12 11:24:04.190: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563" satisfied condition "Succeeded or Failed"
Feb 12 11:24:04.209: INFO: Trying to get logs from node kube-3 pod pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563 container test-container: <nil>
STEP: delete the pod 02/12/23 11:24:04.222
Feb 12 11:24:04.333: INFO: Waiting for pod pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563 to disappear
Feb 12 11:24:04.362: INFO: Pod pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:24:04.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2051" for this suite. 02/12/23 11:24:04.368
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":59,"skipped":1087,"failed":0}
------------------------------
 [4.353 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:00.071
    Feb 12 11:24:00.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:24:00.073
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:00.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:00.12
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/12/23 11:24:00.122
    Feb 12 11:24:00.140: INFO: Waiting up to 5m0s for pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563" in namespace "emptydir-2051" to be "Succeeded or Failed"
    Feb 12 11:24:00.151: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563": Phase="Pending", Reason="", readiness=false. Elapsed: 11.669683ms
    Feb 12 11:24:02.156: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016389367s
    Feb 12 11:24:04.189: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04962413s
    STEP: Saw pod success 02/12/23 11:24:04.189
    Feb 12 11:24:04.190: INFO: Pod "pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563" satisfied condition "Succeeded or Failed"
    Feb 12 11:24:04.209: INFO: Trying to get logs from node kube-3 pod pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563 container test-container: <nil>
    STEP: delete the pod 02/12/23 11:24:04.222
    Feb 12 11:24:04.333: INFO: Waiting for pod pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563 to disappear
    Feb 12 11:24:04.362: INFO: Pod pod-cdaa40ed-ff8c-4a16-9b40-4b853ef42563 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:24:04.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2051" for this suite. 02/12/23 11:24:04.368
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:04.434
Feb 12 11:24:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename proxy 02/12/23 11:24:04.437
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:04.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:04.561
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 12 11:24:04.569: INFO: Creating pod...
Feb 12 11:24:04.630: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3509" to be "running"
Feb 12 11:24:04.667: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 36.226251ms
Feb 12 11:24:06.679: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.048233971s
Feb 12 11:24:06.679: INFO: Pod "agnhost" satisfied condition "running"
Feb 12 11:24:06.679: INFO: Creating service...
Feb 12 11:24:06.707: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/DELETE
Feb 12 11:24:06.717: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 12 11:24:06.717: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/GET
Feb 12 11:24:06.722: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 12 11:24:06.722: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/HEAD
Feb 12 11:24:06.730: INFO: http.Client request:HEAD | StatusCode:200
Feb 12 11:24:06.730: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 12 11:24:06.734: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 12 11:24:06.734: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/PATCH
Feb 12 11:24:06.739: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 12 11:24:06.739: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/POST
Feb 12 11:24:06.743: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 12 11:24:06.743: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/PUT
Feb 12 11:24:06.748: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 12 11:24:06.748: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/DELETE
Feb 12 11:24:06.754: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 12 11:24:06.754: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/GET
Feb 12 11:24:06.761: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 12 11:24:06.761: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/HEAD
Feb 12 11:24:06.768: INFO: http.Client request:HEAD | StatusCode:200
Feb 12 11:24:06.768: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/OPTIONS
Feb 12 11:24:06.774: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 12 11:24:06.774: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/PATCH
Feb 12 11:24:06.780: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 12 11:24:06.780: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/POST
Feb 12 11:24:06.789: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 12 11:24:06.789: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/PUT
Feb 12 11:24:06.795: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 12 11:24:06.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3509" for this suite. 02/12/23 11:24:06.8
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":60,"skipped":1091,"failed":0}
------------------------------
 [2.377 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:04.434
    Feb 12 11:24:04.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename proxy 02/12/23 11:24:04.437
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:04.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:04.561
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 12 11:24:04.569: INFO: Creating pod...
    Feb 12 11:24:04.630: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3509" to be "running"
    Feb 12 11:24:04.667: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 36.226251ms
    Feb 12 11:24:06.679: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.048233971s
    Feb 12 11:24:06.679: INFO: Pod "agnhost" satisfied condition "running"
    Feb 12 11:24:06.679: INFO: Creating service...
    Feb 12 11:24:06.707: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/DELETE
    Feb 12 11:24:06.717: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 12 11:24:06.717: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/GET
    Feb 12 11:24:06.722: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 12 11:24:06.722: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/HEAD
    Feb 12 11:24:06.730: INFO: http.Client request:HEAD | StatusCode:200
    Feb 12 11:24:06.730: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 12 11:24:06.734: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 12 11:24:06.734: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/PATCH
    Feb 12 11:24:06.739: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 12 11:24:06.739: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/POST
    Feb 12 11:24:06.743: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 12 11:24:06.743: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/pods/agnhost/proxy/some/path/with/PUT
    Feb 12 11:24:06.748: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 12 11:24:06.748: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/DELETE
    Feb 12 11:24:06.754: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 12 11:24:06.754: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/GET
    Feb 12 11:24:06.761: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 12 11:24:06.761: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/HEAD
    Feb 12 11:24:06.768: INFO: http.Client request:HEAD | StatusCode:200
    Feb 12 11:24:06.768: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/OPTIONS
    Feb 12 11:24:06.774: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 12 11:24:06.774: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/PATCH
    Feb 12 11:24:06.780: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 12 11:24:06.780: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/POST
    Feb 12 11:24:06.789: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 12 11:24:06.789: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3509/services/test-service/proxy/some/path/with/PUT
    Feb 12 11:24:06.795: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 12 11:24:06.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3509" for this suite. 02/12/23 11:24:06.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:06.812
Feb 12 11:24:06.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 11:24:06.813
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:06.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:06.846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8202 02/12/23 11:24:06.849
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-8202 02/12/23 11:24:06.864
Feb 12 11:24:06.893: INFO: Found 0 stateful pods, waiting for 1
Feb 12 11:24:16.910: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/12/23 11:24:16.931
STEP: Getting /status 02/12/23 11:24:16.949
Feb 12 11:24:16.958: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/12/23 11:24:16.958
Feb 12 11:24:16.970: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/12/23 11:24:16.97
Feb 12 11:24:16.972: INFO: Observed &StatefulSet event: ADDED
Feb 12 11:24:16.975: INFO: Found Statefulset ss in namespace statefulset-8202 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 12 11:24:16.975: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/12/23 11:24:16.975
Feb 12 11:24:16.975: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 12 11:24:16.986: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/12/23 11:24:16.986
Feb 12 11:24:16.988: INFO: Observed &StatefulSet event: ADDED
Feb 12 11:24:16.988: INFO: Observed Statefulset ss in namespace statefulset-8202 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 12 11:24:16.988: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 11:24:16.988: INFO: Deleting all statefulset in ns statefulset-8202
Feb 12 11:24:16.991: INFO: Scaling statefulset ss to 0
Feb 12 11:24:27.105: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:24:27.116: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 11:24:27.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8202" for this suite. 02/12/23 11:24:27.151
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":61,"skipped":1099,"failed":0}
------------------------------
 [SLOW TEST] [20.350 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:06.812
    Feb 12 11:24:06.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 11:24:06.813
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:06.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:06.846
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8202 02/12/23 11:24:06.849
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-8202 02/12/23 11:24:06.864
    Feb 12 11:24:06.893: INFO: Found 0 stateful pods, waiting for 1
    Feb 12 11:24:16.910: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/12/23 11:24:16.931
    STEP: Getting /status 02/12/23 11:24:16.949
    Feb 12 11:24:16.958: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/12/23 11:24:16.958
    Feb 12 11:24:16.970: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/12/23 11:24:16.97
    Feb 12 11:24:16.972: INFO: Observed &StatefulSet event: ADDED
    Feb 12 11:24:16.975: INFO: Found Statefulset ss in namespace statefulset-8202 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 12 11:24:16.975: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/12/23 11:24:16.975
    Feb 12 11:24:16.975: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 12 11:24:16.986: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/12/23 11:24:16.986
    Feb 12 11:24:16.988: INFO: Observed &StatefulSet event: ADDED
    Feb 12 11:24:16.988: INFO: Observed Statefulset ss in namespace statefulset-8202 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 12 11:24:16.988: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 11:24:16.988: INFO: Deleting all statefulset in ns statefulset-8202
    Feb 12 11:24:16.991: INFO: Scaling statefulset ss to 0
    Feb 12 11:24:27.105: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:24:27.116: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 11:24:27.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8202" for this suite. 02/12/23 11:24:27.151
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:27.162
Feb 12 11:24:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:24:27.163
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:27.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:27.185
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:24:27.186
Feb 12 11:24:27.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb" in namespace "downward-api-8175" to be "Succeeded or Failed"
Feb 12 11:24:27.205: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.562065ms
Feb 12 11:24:29.219: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022537924s
Feb 12 11:24:31.210: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013728102s
STEP: Saw pod success 02/12/23 11:24:31.21
Feb 12 11:24:31.210: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb" satisfied condition "Succeeded or Failed"
Feb 12 11:24:31.222: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb container client-container: <nil>
STEP: delete the pod 02/12/23 11:24:31.244
Feb 12 11:24:31.268: INFO: Waiting for pod downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb to disappear
Feb 12 11:24:31.272: INFO: Pod downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:24:31.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8175" for this suite. 02/12/23 11:24:31.275
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":62,"skipped":1102,"failed":0}
------------------------------
 [4.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:27.162
    Feb 12 11:24:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:24:27.163
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:27.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:27.185
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:24:27.186
    Feb 12 11:24:27.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb" in namespace "downward-api-8175" to be "Succeeded or Failed"
    Feb 12 11:24:27.205: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.562065ms
    Feb 12 11:24:29.219: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022537924s
    Feb 12 11:24:31.210: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013728102s
    STEP: Saw pod success 02/12/23 11:24:31.21
    Feb 12 11:24:31.210: INFO: Pod "downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb" satisfied condition "Succeeded or Failed"
    Feb 12 11:24:31.222: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb container client-container: <nil>
    STEP: delete the pod 02/12/23 11:24:31.244
    Feb 12 11:24:31.268: INFO: Waiting for pod downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb to disappear
    Feb 12 11:24:31.272: INFO: Pod downwardapi-volume-441d5637-dd1f-4c2d-9c54-d5e5a1c574fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:24:31.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8175" for this suite. 02/12/23 11:24:31.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:31.284
Feb 12 11:24:31.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:24:31.285
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:31.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:31.308
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:24:31.31
Feb 12 11:24:31.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946" in namespace "downward-api-988" to be "Succeeded or Failed"
Feb 12 11:24:31.330: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148998ms
Feb 12 11:24:33.346: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025914992s
Feb 12 11:24:35.342: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022474818s
STEP: Saw pod success 02/12/23 11:24:35.342
Feb 12 11:24:35.343: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946" satisfied condition "Succeeded or Failed"
Feb 12 11:24:35.353: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946 container client-container: <nil>
STEP: delete the pod 02/12/23 11:24:35.363
Feb 12 11:24:35.392: INFO: Waiting for pod downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946 to disappear
Feb 12 11:24:35.396: INFO: Pod downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:24:35.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-988" for this suite. 02/12/23 11:24:35.4
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":63,"skipped":1126,"failed":0}
------------------------------
 [4.122 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:31.284
    Feb 12 11:24:31.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:24:31.285
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:31.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:31.308
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:24:31.31
    Feb 12 11:24:31.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946" in namespace "downward-api-988" to be "Succeeded or Failed"
    Feb 12 11:24:31.330: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946": Phase="Pending", Reason="", readiness=false. Elapsed: 10.148998ms
    Feb 12 11:24:33.346: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025914992s
    Feb 12 11:24:35.342: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022474818s
    STEP: Saw pod success 02/12/23 11:24:35.342
    Feb 12 11:24:35.343: INFO: Pod "downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946" satisfied condition "Succeeded or Failed"
    Feb 12 11:24:35.353: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946 container client-container: <nil>
    STEP: delete the pod 02/12/23 11:24:35.363
    Feb 12 11:24:35.392: INFO: Waiting for pod downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946 to disappear
    Feb 12 11:24:35.396: INFO: Pod downwardapi-volume-0ea8160e-33fe-469c-bdcd-3289daf40946 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:24:35.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-988" for this suite. 02/12/23 11:24:35.4
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:35.406
Feb 12 11:24:35.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:24:35.407
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:35.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:35.429
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 12 11:24:35.455: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4929 to be scheduled
Feb 12 11:24:35.466: INFO: 1 pods are not scheduled: [runtimeclass-4929/test-runtimeclass-runtimeclass-4929-preconfigured-handler-ck7vx(86d6fdd3-2c93-4985-8515-90bf17e0910c)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 12 11:24:37.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4929" for this suite. 02/12/23 11:24:37.503
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":64,"skipped":1126,"failed":0}
------------------------------
 [2.120 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:35.406
    Feb 12 11:24:35.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:24:35.407
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:35.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:35.429
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 12 11:24:35.455: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4929 to be scheduled
    Feb 12 11:24:35.466: INFO: 1 pods are not scheduled: [runtimeclass-4929/test-runtimeclass-runtimeclass-4929-preconfigured-handler-ck7vx(86d6fdd3-2c93-4985-8515-90bf17e0910c)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 12 11:24:37.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4929" for this suite. 02/12/23 11:24:37.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:37.534
Feb 12 11:24:37.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:24:37.536
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:37.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:37.561
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:24:37.563
Feb 12 11:24:37.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5" in namespace "downward-api-8230" to be "Succeeded or Failed"
Feb 12 11:24:37.576: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.411165ms
Feb 12 11:24:39.589: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016976253s
Feb 12 11:24:41.592: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020008821s
STEP: Saw pod success 02/12/23 11:24:41.593
Feb 12 11:24:41.593: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5" satisfied condition "Succeeded or Failed"
Feb 12 11:24:41.609: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5 container client-container: <nil>
STEP: delete the pod 02/12/23 11:24:41.633
Feb 12 11:24:41.661: INFO: Waiting for pod downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5 to disappear
Feb 12 11:24:41.666: INFO: Pod downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:24:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8230" for this suite. 02/12/23 11:24:41.67
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":65,"skipped":1142,"failed":0}
------------------------------
 [4.144 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:37.534
    Feb 12 11:24:37.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:24:37.536
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:37.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:37.561
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:24:37.563
    Feb 12 11:24:37.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5" in namespace "downward-api-8230" to be "Succeeded or Failed"
    Feb 12 11:24:37.576: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.411165ms
    Feb 12 11:24:39.589: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016976253s
    Feb 12 11:24:41.592: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020008821s
    STEP: Saw pod success 02/12/23 11:24:41.593
    Feb 12 11:24:41.593: INFO: Pod "downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5" satisfied condition "Succeeded or Failed"
    Feb 12 11:24:41.609: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5 container client-container: <nil>
    STEP: delete the pod 02/12/23 11:24:41.633
    Feb 12 11:24:41.661: INFO: Waiting for pod downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5 to disappear
    Feb 12 11:24:41.666: INFO: Pod downwardapi-volume-444defb6-b138-4824-bdcd-e376c127a0e5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:24:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8230" for this suite. 02/12/23 11:24:41.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:41.679
Feb 12 11:24:41.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename events 02/12/23 11:24:41.68
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:41.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:41.703
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/12/23 11:24:41.705
STEP: listing events in all namespaces 02/12/23 11:24:41.71
STEP: listing events in test namespace 02/12/23 11:24:41.717
STEP: listing events with field selection filtering on source 02/12/23 11:24:41.72
STEP: listing events with field selection filtering on reportingController 02/12/23 11:24:41.723
STEP: getting the test event 02/12/23 11:24:41.726
STEP: patching the test event 02/12/23 11:24:41.728
STEP: getting the test event 02/12/23 11:24:41.736
STEP: updating the test event 02/12/23 11:24:41.739
STEP: getting the test event 02/12/23 11:24:41.745
STEP: deleting the test event 02/12/23 11:24:41.748
STEP: listing events in all namespaces 02/12/23 11:24:41.754
STEP: listing events in test namespace 02/12/23 11:24:41.761
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 12 11:24:41.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9872" for this suite. 02/12/23 11:24:41.766
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":66,"skipped":1156,"failed":0}
------------------------------
 [0.095 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:41.679
    Feb 12 11:24:41.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename events 02/12/23 11:24:41.68
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:41.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:41.703
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/12/23 11:24:41.705
    STEP: listing events in all namespaces 02/12/23 11:24:41.71
    STEP: listing events in test namespace 02/12/23 11:24:41.717
    STEP: listing events with field selection filtering on source 02/12/23 11:24:41.72
    STEP: listing events with field selection filtering on reportingController 02/12/23 11:24:41.723
    STEP: getting the test event 02/12/23 11:24:41.726
    STEP: patching the test event 02/12/23 11:24:41.728
    STEP: getting the test event 02/12/23 11:24:41.736
    STEP: updating the test event 02/12/23 11:24:41.739
    STEP: getting the test event 02/12/23 11:24:41.745
    STEP: deleting the test event 02/12/23 11:24:41.748
    STEP: listing events in all namespaces 02/12/23 11:24:41.754
    STEP: listing events in test namespace 02/12/23 11:24:41.761
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 12 11:24:41.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9872" for this suite. 02/12/23 11:24:41.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:24:41.778
Feb 12 11:24:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:24:41.778
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:41.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:41.803
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/12/23 11:24:41.805
Feb 12 11:24:41.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/12/23 11:24:57.74
Feb 12 11:24:57.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:25:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:25:20.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2028" for this suite. 02/12/23 11:25:20.166
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":67,"skipped":1220,"failed":0}
------------------------------
 [SLOW TEST] [38.394 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:24:41.778
    Feb 12 11:24:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:24:41.778
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:24:41.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:24:41.803
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/12/23 11:24:41.805
    Feb 12 11:24:41.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/12/23 11:24:57.74
    Feb 12 11:24:57.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:25:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:25:20.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2028" for this suite. 02/12/23 11:25:20.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:25:20.178
Feb 12 11:25:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:25:20.179
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:25:20.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:25:20.206
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Feb 12 11:25:20.223: INFO: created pod
Feb 12 11:25:20.223: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5843" to be "Succeeded or Failed"
Feb 12 11:25:20.227: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192324ms
Feb 12 11:25:22.413: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189928136s
Feb 12 11:25:24.244: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02143736s
STEP: Saw pod success 02/12/23 11:25:24.244
Feb 12 11:25:24.245: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 12 11:25:54.246: INFO: polling logs
Feb 12 11:25:54.274: INFO: Pod logs: 
I0212 11:25:20.938332       1 log.go:195] OK: Got token
I0212 11:25:20.938361       1 log.go:195] validating with in-cluster discovery
I0212 11:25:20.938592       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0212 11:25:20.938615       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676201720, NotBefore:1676201120, IssuedAt:1676201120, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"51dac657-2719-4f87-8f0c-84dc9c3be92e"}}}
I0212 11:25:20.949450       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0212 11:25:20.954251       1 log.go:195] OK: Validated signature on JWT
I0212 11:25:20.954339       1 log.go:195] OK: Got valid claims from token!
I0212 11:25:20.954369       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676201720, NotBefore:1676201120, IssuedAt:1676201120, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"51dac657-2719-4f87-8f0c-84dc9c3be92e"}}}

Feb 12 11:25:54.274: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 12 11:25:54.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5843" for this suite. 02/12/23 11:25:54.293
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":68,"skipped":1245,"failed":0}
------------------------------
 [SLOW TEST] [34.127 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:25:20.178
    Feb 12 11:25:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:25:20.179
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:25:20.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:25:20.206
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Feb 12 11:25:20.223: INFO: created pod
    Feb 12 11:25:20.223: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5843" to be "Succeeded or Failed"
    Feb 12 11:25:20.227: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192324ms
    Feb 12 11:25:22.413: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189928136s
    Feb 12 11:25:24.244: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02143736s
    STEP: Saw pod success 02/12/23 11:25:24.244
    Feb 12 11:25:24.245: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 12 11:25:54.246: INFO: polling logs
    Feb 12 11:25:54.274: INFO: Pod logs: 
    I0212 11:25:20.938332       1 log.go:195] OK: Got token
    I0212 11:25:20.938361       1 log.go:195] validating with in-cluster discovery
    I0212 11:25:20.938592       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0212 11:25:20.938615       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676201720, NotBefore:1676201120, IssuedAt:1676201120, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"51dac657-2719-4f87-8f0c-84dc9c3be92e"}}}
    I0212 11:25:20.949450       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0212 11:25:20.954251       1 log.go:195] OK: Validated signature on JWT
    I0212 11:25:20.954339       1 log.go:195] OK: Got valid claims from token!
    I0212 11:25:20.954369       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1676201720, NotBefore:1676201120, IssuedAt:1676201120, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"51dac657-2719-4f87-8f0c-84dc9c3be92e"}}}

    Feb 12 11:25:54.274: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 12 11:25:54.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5843" for this suite. 02/12/23 11:25:54.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:25:54.307
Feb 12 11:25:54.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-pred 02/12/23 11:25:54.308
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:25:54.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:25:54.33
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 12 11:25:54.333: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 11:25:54.339: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 11:25:54.342: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Feb 12 11:25:54.347: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container calico-node ready: true, restart count 1
Feb 12 11:25:54.347: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container coredns ready: true, restart count 0
Feb 12 11:25:54.347: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container kube-apiserver ready: true, restart count 2
Feb 12 11:25:54.347: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container kube-controller-manager ready: true, restart count 5
Feb 12 11:25:54.347: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:25:54.347: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container kube-scheduler ready: true, restart count 5
Feb 12 11:25:54.347: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:25:54.347: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:25:54.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:25:54.347: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:25:54.347: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Feb 12 11:25:54.352: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 11:25:54.352: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container coredns ready: true, restart count 0
Feb 12 11:25:54.352: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container autoscaler ready: true, restart count 0
Feb 12 11:25:54.352: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 12 11:25:54.352: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container kube-controller-manager ready: true, restart count 6
Feb 12 11:25:54.352: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:25:54.352: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container kube-scheduler ready: true, restart count 5
Feb 12 11:25:54.352: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:25:54.352: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:25:54.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:25:54.352: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:25:54.352: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Feb 12 11:25:54.357: INFO: calico-kube-controllers-75748cc9fd-2rtj6 from kube-system started at 2023-02-12 11:00:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 12 11:25:54.357: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 11:25:54.357: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:25:54.357: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container nginx-proxy ready: true, restart count 0
Feb 12 11:25:54.357: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:25:54.357: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 11:25:54.357: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container e2e ready: true, restart count 0
Feb 12 11:25:54.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:25:54.357: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:25:54.357: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:25:54.357: INFO: oidc-discovery-validator from svcaccounts-5843 started at 2023-02-12 11:25:20 +0000 UTC (1 container statuses recorded)
Feb 12 11:25:54.357: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/12/23 11:25:54.357
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17431011aa78df97], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 02/12/23 11:25:54.383
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:25:55.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2788" for this suite. 02/12/23 11:25:55.385
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":69,"skipped":1284,"failed":0}
------------------------------
 [1.085 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:25:54.307
    Feb 12 11:25:54.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-pred 02/12/23 11:25:54.308
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:25:54.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:25:54.33
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 12 11:25:54.333: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 12 11:25:54.339: INFO: Waiting for terminating namespaces to be deleted...
    Feb 12 11:25:54.342: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Feb 12 11:25:54.347: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container calico-node ready: true, restart count 1
    Feb 12 11:25:54.347: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 11:25:54.347: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container kube-apiserver ready: true, restart count 2
    Feb 12 11:25:54.347: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container kube-controller-manager ready: true, restart count 5
    Feb 12 11:25:54.347: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:25:54.347: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container kube-scheduler ready: true, restart count 5
    Feb 12 11:25:54.347: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:25:54.347: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:25:54.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:25:54.347: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:25:54.347: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Feb 12 11:25:54.352: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container autoscaler ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container kube-apiserver ready: true, restart count 1
    Feb 12 11:25:54.352: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container kube-controller-manager ready: true, restart count 6
    Feb 12 11:25:54.352: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container kube-scheduler ready: true, restart count 5
    Feb 12 11:25:54.352: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:25:54.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:25:54.352: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Feb 12 11:25:54.357: INFO: calico-kube-controllers-75748cc9fd-2rtj6 from kube-system started at 2023-02-12 11:00:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container nginx-proxy ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container e2e ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:25:54.357: INFO: oidc-discovery-validator from svcaccounts-5843 started at 2023-02-12 11:25:20 +0000 UTC (1 container statuses recorded)
    Feb 12 11:25:54.357: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/12/23 11:25:54.357
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17431011aa78df97], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 02/12/23 11:25:54.383
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:25:55.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2788" for this suite. 02/12/23 11:25:55.385
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:25:55.393
Feb 12 11:25:55.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:25:55.394
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:25:55.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:25:55.419
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:25:55.424
Feb 12 11:25:55.432: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9870" to be "running and ready"
Feb 12 11:25:55.438: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.273233ms
Feb 12 11:25:55.438: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:25:57.451: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018244627s
Feb 12 11:25:57.451: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 12 11:25:57.451: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 02/12/23 11:25:57.464
Feb 12 11:25:57.485: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9870" to be "running and ready"
Feb 12 11:25:57.491: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500112ms
Feb 12 11:25:57.491: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:25:59.495: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010618174s
Feb 12 11:25:59.495: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 12 11:25:59.495: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/12/23 11:25:59.499
STEP: delete the pod with lifecycle hook 02/12/23 11:25:59.506
Feb 12 11:25:59.518: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 12 11:25:59.522: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 12 11:26:01.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 12 11:26:01.537: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 12 11:26:03.522: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 12 11:26:03.535: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 12 11:26:03.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9870" for this suite. 02/12/23 11:26:03.55
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":70,"skipped":1299,"failed":0}
------------------------------
 [SLOW TEST] [8.176 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:25:55.393
    Feb 12 11:25:55.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:25:55.394
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:25:55.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:25:55.419
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:25:55.424
    Feb 12 11:25:55.432: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9870" to be "running and ready"
    Feb 12 11:25:55.438: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.273233ms
    Feb 12 11:25:55.438: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:25:57.451: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018244627s
    Feb 12 11:25:57.451: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 12 11:25:57.451: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 02/12/23 11:25:57.464
    Feb 12 11:25:57.485: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9870" to be "running and ready"
    Feb 12 11:25:57.491: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.500112ms
    Feb 12 11:25:57.491: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:25:59.495: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010618174s
    Feb 12 11:25:59.495: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 12 11:25:59.495: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/12/23 11:25:59.499
    STEP: delete the pod with lifecycle hook 02/12/23 11:25:59.506
    Feb 12 11:25:59.518: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 12 11:25:59.522: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 12 11:26:01.523: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 12 11:26:01.537: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 12 11:26:03.522: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 12 11:26:03.535: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 12 11:26:03.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9870" for this suite. 02/12/23 11:26:03.55
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:26:03.57
Feb 12 11:26:03.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:26:03.571
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:26:03.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:26:03.597
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-76421861-fd43-4b25-af99-381dfc3df6dd 02/12/23 11:26:03.602
STEP: Creating secret with name s-test-opt-upd-eeb6f64a-339d-49e8-aee6-9eff4edcf940 02/12/23 11:26:03.606
STEP: Creating the pod 02/12/23 11:26:03.611
Feb 12 11:26:03.621: INFO: Waiting up to 5m0s for pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247" in namespace "secrets-5030" to be "running and ready"
Feb 12 11:26:03.627: INFO: Pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239614ms
Feb 12 11:26:03.628: INFO: The phase of Pod pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:26:05.642: INFO: Pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247": Phase="Running", Reason="", readiness=true. Elapsed: 2.021198929s
Feb 12 11:26:05.642: INFO: The phase of Pod pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247 is Running (Ready = true)
Feb 12 11:26:05.642: INFO: Pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-76421861-fd43-4b25-af99-381dfc3df6dd 02/12/23 11:26:05.708
STEP: Updating secret s-test-opt-upd-eeb6f64a-339d-49e8-aee6-9eff4edcf940 02/12/23 11:26:05.718
STEP: Creating secret with name s-test-opt-create-9cb024fa-7f58-479f-b4b3-86774a52327e 02/12/23 11:26:05.724
STEP: waiting to observe update in volume 02/12/23 11:26:05.729
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:26:07.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5030" for this suite. 02/12/23 11:26:07.802
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":71,"skipped":1300,"failed":0}
------------------------------
 [4.244 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:26:03.57
    Feb 12 11:26:03.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:26:03.571
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:26:03.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:26:03.597
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-76421861-fd43-4b25-af99-381dfc3df6dd 02/12/23 11:26:03.602
    STEP: Creating secret with name s-test-opt-upd-eeb6f64a-339d-49e8-aee6-9eff4edcf940 02/12/23 11:26:03.606
    STEP: Creating the pod 02/12/23 11:26:03.611
    Feb 12 11:26:03.621: INFO: Waiting up to 5m0s for pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247" in namespace "secrets-5030" to be "running and ready"
    Feb 12 11:26:03.627: INFO: Pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239614ms
    Feb 12 11:26:03.628: INFO: The phase of Pod pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:26:05.642: INFO: Pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247": Phase="Running", Reason="", readiness=true. Elapsed: 2.021198929s
    Feb 12 11:26:05.642: INFO: The phase of Pod pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247 is Running (Ready = true)
    Feb 12 11:26:05.642: INFO: Pod "pod-secrets-4ad78ee7-a1ce-4bcc-bd6a-9ee28874d247" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-76421861-fd43-4b25-af99-381dfc3df6dd 02/12/23 11:26:05.708
    STEP: Updating secret s-test-opt-upd-eeb6f64a-339d-49e8-aee6-9eff4edcf940 02/12/23 11:26:05.718
    STEP: Creating secret with name s-test-opt-create-9cb024fa-7f58-479f-b4b3-86774a52327e 02/12/23 11:26:05.724
    STEP: waiting to observe update in volume 02/12/23 11:26:05.729
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:26:07.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5030" for this suite. 02/12/23 11:26:07.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:26:07.826
Feb 12 11:26:07.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:26:07.827
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:26:07.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:26:07.852
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 02/12/23 11:26:07.854
Feb 12 11:26:07.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 create -f -'
Feb 12 11:26:08.332: INFO: stderr: ""
Feb 12 11:26:08.332: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 11:26:08.332
Feb 12 11:26:08.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:08.417: INFO: stderr: ""
Feb 12 11:26:08.417: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
Feb 12 11:26:08.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:08.473: INFO: stderr: ""
Feb 12 11:26:08.473: INFO: stdout: ""
Feb 12 11:26:08.473: INFO: update-demo-nautilus-bq2t2 is created but not running
Feb 12 11:26:13.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:13.957: INFO: stderr: ""
Feb 12 11:26:13.957: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
Feb 12 11:26:13.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:16.080: INFO: stderr: ""
Feb 12 11:26:16.080: INFO: stdout: ""
Feb 12 11:26:16.080: INFO: update-demo-nautilus-bq2t2 is created but not running
Feb 12 11:26:21.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:22.516: INFO: stderr: ""
Feb 12 11:26:22.516: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
Feb 12 11:26:22.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:22.677: INFO: stderr: ""
Feb 12 11:26:22.677: INFO: stdout: ""
Feb 12 11:26:22.677: INFO: update-demo-nautilus-bq2t2 is created but not running
Feb 12 11:26:27.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:28.656: INFO: stderr: ""
Feb 12 11:26:28.656: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
Feb 12 11:26:28.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:28.717: INFO: stderr: ""
Feb 12 11:26:28.717: INFO: stdout: ""
Feb 12 11:26:28.717: INFO: update-demo-nautilus-bq2t2 is created but not running
Feb 12 11:26:33.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:33.907: INFO: stderr: ""
Feb 12 11:26:33.907: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
Feb 12 11:26:33.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:34.046: INFO: stderr: ""
Feb 12 11:26:34.046: INFO: stdout: "true"
Feb 12 11:26:34.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:26:34.172: INFO: stderr: ""
Feb 12 11:26:34.172: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:26:34.172: INFO: validating pod update-demo-nautilus-bq2t2
Feb 12 11:26:34.178: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:26:34.178: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:26:34.178: INFO: update-demo-nautilus-bq2t2 is verified up and running
Feb 12 11:26:34.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-gznb5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:34.273: INFO: stderr: ""
Feb 12 11:26:34.273: INFO: stdout: "true"
Feb 12 11:26:34.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-gznb5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:26:34.338: INFO: stderr: ""
Feb 12 11:26:34.338: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:26:34.338: INFO: validating pod update-demo-nautilus-gznb5
Feb 12 11:26:34.342: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:26:34.342: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:26:34.342: INFO: update-demo-nautilus-gznb5 is verified up and running
STEP: scaling down the replication controller 02/12/23 11:26:34.342
Feb 12 11:26:34.343: INFO: scanned /root for discovery docs: <nil>
Feb 12 11:26:34.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 12 11:26:37.418: INFO: stderr: ""
Feb 12 11:26:37.418: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 11:26:37.418
Feb 12 11:26:37.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:37.480: INFO: stderr: ""
Feb 12 11:26:37.480: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
STEP: Replicas for name=update-demo: expected=1 actual=2 02/12/23 11:26:37.48
Feb 12 11:26:42.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:42.538: INFO: stderr: ""
Feb 12 11:26:42.538: INFO: stdout: "update-demo-nautilus-bq2t2 "
Feb 12 11:26:42.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:42.606: INFO: stderr: ""
Feb 12 11:26:42.606: INFO: stdout: "true"
Feb 12 11:26:42.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:26:42.664: INFO: stderr: ""
Feb 12 11:26:42.664: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:26:42.664: INFO: validating pod update-demo-nautilus-bq2t2
Feb 12 11:26:42.669: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:26:42.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:26:42.669: INFO: update-demo-nautilus-bq2t2 is verified up and running
STEP: scaling up the replication controller 02/12/23 11:26:42.669
Feb 12 11:26:42.670: INFO: scanned /root for discovery docs: <nil>
Feb 12 11:26:42.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 12 11:26:43.798: INFO: stderr: ""
Feb 12 11:26:43.798: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 11:26:43.798
Feb 12 11:26:43.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:43.971: INFO: stderr: ""
Feb 12 11:26:43.971: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
Feb 12 11:26:43.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:44.083: INFO: stderr: ""
Feb 12 11:26:44.083: INFO: stdout: "true"
Feb 12 11:26:44.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:26:44.194: INFO: stderr: ""
Feb 12 11:26:44.194: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:26:44.194: INFO: validating pod update-demo-nautilus-bq2t2
Feb 12 11:26:44.201: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:26:44.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:26:44.201: INFO: update-demo-nautilus-bq2t2 is verified up and running
Feb 12 11:26:44.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:26:44.308: INFO: stderr: ""
Feb 12 11:26:44.308: INFO: stdout: ""
Feb 12 11:26:44.308: INFO: update-demo-nautilus-z9vn7 is created but not running
Feb 12 11:26:49.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:26:59.924: INFO: stderr: ""
Feb 12 11:26:59.924: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
Feb 12 11:26:59.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:27:03.559: INFO: stderr: ""
Feb 12 11:27:03.559: INFO: stdout: "true"
Feb 12 11:27:03.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:27:03.705: INFO: stderr: ""
Feb 12 11:27:03.705: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:27:03.705: INFO: validating pod update-demo-nautilus-bq2t2
Feb 12 11:27:03.792: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:27:03.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:27:03.793: INFO: update-demo-nautilus-bq2t2 is verified up and running
Feb 12 11:27:03.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:27:03.917: INFO: stderr: ""
Feb 12 11:27:03.917: INFO: stdout: ""
Feb 12 11:27:03.917: INFO: update-demo-nautilus-z9vn7 is created but not running
Feb 12 11:27:08.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:27:09.039: INFO: stderr: ""
Feb 12 11:27:09.039: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
Feb 12 11:27:09.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:27:09.110: INFO: stderr: ""
Feb 12 11:27:09.110: INFO: stdout: "true"
Feb 12 11:27:09.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:27:09.157: INFO: stderr: ""
Feb 12 11:27:09.157: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:27:09.157: INFO: validating pod update-demo-nautilus-bq2t2
Feb 12 11:27:09.161: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:27:09.161: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:27:09.161: INFO: update-demo-nautilus-bq2t2 is verified up and running
Feb 12 11:27:09.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:27:09.211: INFO: stderr: ""
Feb 12 11:27:09.211: INFO: stdout: ""
Feb 12 11:27:09.211: INFO: update-demo-nautilus-z9vn7 is created but not running
Feb 12 11:27:14.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 11:27:14.410: INFO: stderr: ""
Feb 12 11:27:14.410: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
Feb 12 11:27:14.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:27:14.497: INFO: stderr: ""
Feb 12 11:27:14.497: INFO: stdout: "true"
Feb 12 11:27:14.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:27:14.575: INFO: stderr: ""
Feb 12 11:27:14.575: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:27:14.575: INFO: validating pod update-demo-nautilus-bq2t2
Feb 12 11:27:14.579: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:27:14.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:27:14.579: INFO: update-demo-nautilus-bq2t2 is verified up and running
Feb 12 11:27:14.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 11:27:14.628: INFO: stderr: ""
Feb 12 11:27:14.628: INFO: stdout: "true"
Feb 12 11:27:14.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 11:27:14.679: INFO: stderr: ""
Feb 12 11:27:14.679: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 11:27:14.679: INFO: validating pod update-demo-nautilus-z9vn7
Feb 12 11:27:14.683: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 11:27:14.683: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 11:27:14.683: INFO: update-demo-nautilus-z9vn7 is verified up and running
STEP: using delete to clean up resources 02/12/23 11:27:14.683
Feb 12 11:27:14.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 delete --grace-period=0 --force -f -'
Feb 12 11:27:14.745: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 11:27:14.745: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 12 11:27:14.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:14.799: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:14.799: INFO: stdout: ""
Feb 12 11:27:14.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:14.856: INFO: stderr: ""
Feb 12 11:27:14.856: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:15.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:15.474: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:15.474: INFO: stdout: ""
Feb 12 11:27:15.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:15.532: INFO: stderr: ""
Feb 12 11:27:15.532: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:15.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:15.953: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:15.953: INFO: stdout: ""
Feb 12 11:27:15.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:16.022: INFO: stderr: ""
Feb 12 11:27:16.022: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:16.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:16.410: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:16.410: INFO: stdout: ""
Feb 12 11:27:16.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:16.468: INFO: stderr: ""
Feb 12 11:27:16.469: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:16.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:17.044: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:17.044: INFO: stdout: ""
Feb 12 11:27:17.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:17.146: INFO: stderr: ""
Feb 12 11:27:17.146: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:17.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:17.531: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:17.531: INFO: stdout: ""
Feb 12 11:27:17.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:17.622: INFO: stderr: ""
Feb 12 11:27:17.622: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:17.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:18.071: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:18.071: INFO: stdout: ""
Feb 12 11:27:18.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:18.182: INFO: stderr: ""
Feb 12 11:27:18.182: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:18.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:18.471: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:18.471: INFO: stdout: ""
Feb 12 11:27:18.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:18.537: INFO: stderr: ""
Feb 12 11:27:18.537: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:19.053: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:19.053: INFO: stdout: ""
Feb 12 11:27:19.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:19.163: INFO: stderr: ""
Feb 12 11:27:19.163: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:19.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:19.529: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:19.529: INFO: stdout: ""
Feb 12 11:27:19.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:19.607: INFO: stderr: ""
Feb 12 11:27:19.607: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:19.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:20.024: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:20.024: INFO: stdout: ""
Feb 12 11:27:20.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:20.077: INFO: stderr: ""
Feb 12 11:27:20.077: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:20.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:20.472: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:20.472: INFO: stdout: ""
Feb 12 11:27:20.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:20.597: INFO: stderr: ""
Feb 12 11:27:20.597: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:20.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:20.921: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:20.921: INFO: stdout: ""
Feb 12 11:27:20.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:20.973: INFO: stderr: ""
Feb 12 11:27:20.973: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:21.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:21.542: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:21.542: INFO: stdout: ""
Feb 12 11:27:21.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:21.672: INFO: stderr: ""
Feb 12 11:27:21.672: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:22.033: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:22.033: INFO: stdout: ""
Feb 12 11:27:22.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:22.092: INFO: stderr: ""
Feb 12 11:27:22.092: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:22.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:22.570: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:22.570: INFO: stdout: ""
Feb 12 11:27:22.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:22.761: INFO: stderr: ""
Feb 12 11:27:22.761: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:22.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:22.985: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:22.985: INFO: stdout: ""
Feb 12 11:27:22.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:23.036: INFO: stderr: ""
Feb 12 11:27:23.036: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:23.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:23.517: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:23.517: INFO: stdout: ""
Feb 12 11:27:23.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:23.584: INFO: stderr: ""
Feb 12 11:27:23.584: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:23.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:24.002: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:24.002: INFO: stdout: ""
Feb 12 11:27:24.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:24.079: INFO: stderr: ""
Feb 12 11:27:24.079: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:24.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:24.499: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:24.499: INFO: stdout: ""
Feb 12 11:27:24.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:24.587: INFO: stderr: ""
Feb 12 11:27:24.587: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:24.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:24.984: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:24.984: INFO: stdout: ""
Feb 12 11:27:24.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:25.062: INFO: stderr: ""
Feb 12 11:27:25.062: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:25.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:25.415: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:25.415: INFO: stdout: ""
Feb 12 11:27:25.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:25.466: INFO: stderr: ""
Feb 12 11:27:25.466: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:25.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:25.922: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:25.922: INFO: stdout: ""
Feb 12 11:27:25.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:25.974: INFO: stderr: ""
Feb 12 11:27:25.974: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
Feb 12 11:27:26.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
Feb 12 11:27:26.551: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
Feb 12 11:27:26.551: INFO: stdout: ""
Feb 12 11:27:26.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 11:27:26.633: INFO: stderr: ""
Feb 12 11:27:26.633: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:27:26.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7420" for this suite. 02/12/23 11:27:26.639
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":72,"skipped":1323,"failed":0}
------------------------------
 [SLOW TEST] [78.826 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:26:07.826
    Feb 12 11:26:07.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:26:07.827
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:26:07.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:26:07.852
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 02/12/23 11:26:07.854
    Feb 12 11:26:07.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 create -f -'
    Feb 12 11:26:08.332: INFO: stderr: ""
    Feb 12 11:26:08.332: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 11:26:08.332
    Feb 12 11:26:08.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:08.417: INFO: stderr: ""
    Feb 12 11:26:08.417: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
    Feb 12 11:26:08.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:08.473: INFO: stderr: ""
    Feb 12 11:26:08.473: INFO: stdout: ""
    Feb 12 11:26:08.473: INFO: update-demo-nautilus-bq2t2 is created but not running
    Feb 12 11:26:13.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:13.957: INFO: stderr: ""
    Feb 12 11:26:13.957: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
    Feb 12 11:26:13.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:16.080: INFO: stderr: ""
    Feb 12 11:26:16.080: INFO: stdout: ""
    Feb 12 11:26:16.080: INFO: update-demo-nautilus-bq2t2 is created but not running
    Feb 12 11:26:21.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:22.516: INFO: stderr: ""
    Feb 12 11:26:22.516: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
    Feb 12 11:26:22.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:22.677: INFO: stderr: ""
    Feb 12 11:26:22.677: INFO: stdout: ""
    Feb 12 11:26:22.677: INFO: update-demo-nautilus-bq2t2 is created but not running
    Feb 12 11:26:27.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:28.656: INFO: stderr: ""
    Feb 12 11:26:28.656: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
    Feb 12 11:26:28.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:28.717: INFO: stderr: ""
    Feb 12 11:26:28.717: INFO: stdout: ""
    Feb 12 11:26:28.717: INFO: update-demo-nautilus-bq2t2 is created but not running
    Feb 12 11:26:33.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:33.907: INFO: stderr: ""
    Feb 12 11:26:33.907: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
    Feb 12 11:26:33.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:34.046: INFO: stderr: ""
    Feb 12 11:26:34.046: INFO: stdout: "true"
    Feb 12 11:26:34.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:26:34.172: INFO: stderr: ""
    Feb 12 11:26:34.172: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:26:34.172: INFO: validating pod update-demo-nautilus-bq2t2
    Feb 12 11:26:34.178: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:26:34.178: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:26:34.178: INFO: update-demo-nautilus-bq2t2 is verified up and running
    Feb 12 11:26:34.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-gznb5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:34.273: INFO: stderr: ""
    Feb 12 11:26:34.273: INFO: stdout: "true"
    Feb 12 11:26:34.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-gznb5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:26:34.338: INFO: stderr: ""
    Feb 12 11:26:34.338: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:26:34.338: INFO: validating pod update-demo-nautilus-gznb5
    Feb 12 11:26:34.342: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:26:34.342: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:26:34.342: INFO: update-demo-nautilus-gznb5 is verified up and running
    STEP: scaling down the replication controller 02/12/23 11:26:34.342
    Feb 12 11:26:34.343: INFO: scanned /root for discovery docs: <nil>
    Feb 12 11:26:34.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 12 11:26:37.418: INFO: stderr: ""
    Feb 12 11:26:37.418: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 11:26:37.418
    Feb 12 11:26:37.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:37.480: INFO: stderr: ""
    Feb 12 11:26:37.480: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-gznb5 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 02/12/23 11:26:37.48
    Feb 12 11:26:42.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:42.538: INFO: stderr: ""
    Feb 12 11:26:42.538: INFO: stdout: "update-demo-nautilus-bq2t2 "
    Feb 12 11:26:42.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:42.606: INFO: stderr: ""
    Feb 12 11:26:42.606: INFO: stdout: "true"
    Feb 12 11:26:42.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:26:42.664: INFO: stderr: ""
    Feb 12 11:26:42.664: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:26:42.664: INFO: validating pod update-demo-nautilus-bq2t2
    Feb 12 11:26:42.669: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:26:42.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:26:42.669: INFO: update-demo-nautilus-bq2t2 is verified up and running
    STEP: scaling up the replication controller 02/12/23 11:26:42.669
    Feb 12 11:26:42.670: INFO: scanned /root for discovery docs: <nil>
    Feb 12 11:26:42.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 12 11:26:43.798: INFO: stderr: ""
    Feb 12 11:26:43.798: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 11:26:43.798
    Feb 12 11:26:43.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:43.971: INFO: stderr: ""
    Feb 12 11:26:43.971: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
    Feb 12 11:26:43.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:44.083: INFO: stderr: ""
    Feb 12 11:26:44.083: INFO: stdout: "true"
    Feb 12 11:26:44.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:26:44.194: INFO: stderr: ""
    Feb 12 11:26:44.194: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:26:44.194: INFO: validating pod update-demo-nautilus-bq2t2
    Feb 12 11:26:44.201: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:26:44.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:26:44.201: INFO: update-demo-nautilus-bq2t2 is verified up and running
    Feb 12 11:26:44.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:26:44.308: INFO: stderr: ""
    Feb 12 11:26:44.308: INFO: stdout: ""
    Feb 12 11:26:44.308: INFO: update-demo-nautilus-z9vn7 is created but not running
    Feb 12 11:26:49.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:26:59.924: INFO: stderr: ""
    Feb 12 11:26:59.924: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
    Feb 12 11:26:59.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:27:03.559: INFO: stderr: ""
    Feb 12 11:27:03.559: INFO: stdout: "true"
    Feb 12 11:27:03.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:27:03.705: INFO: stderr: ""
    Feb 12 11:27:03.705: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:27:03.705: INFO: validating pod update-demo-nautilus-bq2t2
    Feb 12 11:27:03.792: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:27:03.793: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:27:03.793: INFO: update-demo-nautilus-bq2t2 is verified up and running
    Feb 12 11:27:03.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:27:03.917: INFO: stderr: ""
    Feb 12 11:27:03.917: INFO: stdout: ""
    Feb 12 11:27:03.917: INFO: update-demo-nautilus-z9vn7 is created but not running
    Feb 12 11:27:08.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:27:09.039: INFO: stderr: ""
    Feb 12 11:27:09.039: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
    Feb 12 11:27:09.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:27:09.110: INFO: stderr: ""
    Feb 12 11:27:09.110: INFO: stdout: "true"
    Feb 12 11:27:09.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:27:09.157: INFO: stderr: ""
    Feb 12 11:27:09.157: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:27:09.157: INFO: validating pod update-demo-nautilus-bq2t2
    Feb 12 11:27:09.161: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:27:09.161: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:27:09.161: INFO: update-demo-nautilus-bq2t2 is verified up and running
    Feb 12 11:27:09.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:27:09.211: INFO: stderr: ""
    Feb 12 11:27:09.211: INFO: stdout: ""
    Feb 12 11:27:09.211: INFO: update-demo-nautilus-z9vn7 is created but not running
    Feb 12 11:27:14.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 11:27:14.410: INFO: stderr: ""
    Feb 12 11:27:14.410: INFO: stdout: "update-demo-nautilus-bq2t2 update-demo-nautilus-z9vn7 "
    Feb 12 11:27:14.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:27:14.497: INFO: stderr: ""
    Feb 12 11:27:14.497: INFO: stdout: "true"
    Feb 12 11:27:14.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-bq2t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:27:14.575: INFO: stderr: ""
    Feb 12 11:27:14.575: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:27:14.575: INFO: validating pod update-demo-nautilus-bq2t2
    Feb 12 11:27:14.579: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:27:14.579: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:27:14.579: INFO: update-demo-nautilus-bq2t2 is verified up and running
    Feb 12 11:27:14.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 11:27:14.628: INFO: stderr: ""
    Feb 12 11:27:14.628: INFO: stdout: "true"
    Feb 12 11:27:14.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods update-demo-nautilus-z9vn7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 11:27:14.679: INFO: stderr: ""
    Feb 12 11:27:14.679: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 11:27:14.679: INFO: validating pod update-demo-nautilus-z9vn7
    Feb 12 11:27:14.683: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 11:27:14.683: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 11:27:14.683: INFO: update-demo-nautilus-z9vn7 is verified up and running
    STEP: using delete to clean up resources 02/12/23 11:27:14.683
    Feb 12 11:27:14.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 delete --grace-period=0 --force -f -'
    Feb 12 11:27:14.745: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 11:27:14.745: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 12 11:27:14.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:14.799: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:14.799: INFO: stdout: ""
    Feb 12 11:27:14.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:14.856: INFO: stderr: ""
    Feb 12 11:27:14.856: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:15.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:15.474: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:15.474: INFO: stdout: ""
    Feb 12 11:27:15.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:15.532: INFO: stderr: ""
    Feb 12 11:27:15.532: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:15.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:15.953: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:15.953: INFO: stdout: ""
    Feb 12 11:27:15.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:16.022: INFO: stderr: ""
    Feb 12 11:27:16.022: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:16.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:16.410: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:16.410: INFO: stdout: ""
    Feb 12 11:27:16.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:16.468: INFO: stderr: ""
    Feb 12 11:27:16.469: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:16.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:17.044: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:17.044: INFO: stdout: ""
    Feb 12 11:27:17.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:17.146: INFO: stderr: ""
    Feb 12 11:27:17.146: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:17.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:17.531: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:17.531: INFO: stdout: ""
    Feb 12 11:27:17.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:17.622: INFO: stderr: ""
    Feb 12 11:27:17.622: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:17.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:18.071: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:18.071: INFO: stdout: ""
    Feb 12 11:27:18.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:18.182: INFO: stderr: ""
    Feb 12 11:27:18.182: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:18.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:18.471: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:18.471: INFO: stdout: ""
    Feb 12 11:27:18.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:18.537: INFO: stderr: ""
    Feb 12 11:27:18.537: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:19.053: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:19.053: INFO: stdout: ""
    Feb 12 11:27:19.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:19.163: INFO: stderr: ""
    Feb 12 11:27:19.163: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:19.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:19.529: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:19.529: INFO: stdout: ""
    Feb 12 11:27:19.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:19.607: INFO: stderr: ""
    Feb 12 11:27:19.607: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:19.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:20.024: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:20.024: INFO: stdout: ""
    Feb 12 11:27:20.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:20.077: INFO: stderr: ""
    Feb 12 11:27:20.077: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:20.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:20.472: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:20.472: INFO: stdout: ""
    Feb 12 11:27:20.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:20.597: INFO: stderr: ""
    Feb 12 11:27:20.597: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:20.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:20.921: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:20.921: INFO: stdout: ""
    Feb 12 11:27:20.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:20.973: INFO: stderr: ""
    Feb 12 11:27:20.973: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:21.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:21.542: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:21.542: INFO: stdout: ""
    Feb 12 11:27:21.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:21.672: INFO: stderr: ""
    Feb 12 11:27:21.672: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:21.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:22.033: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:22.033: INFO: stdout: ""
    Feb 12 11:27:22.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:22.092: INFO: stderr: ""
    Feb 12 11:27:22.092: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:22.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:22.570: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:22.570: INFO: stdout: ""
    Feb 12 11:27:22.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:22.761: INFO: stderr: ""
    Feb 12 11:27:22.761: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:22.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:22.985: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:22.985: INFO: stdout: ""
    Feb 12 11:27:22.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:23.036: INFO: stderr: ""
    Feb 12 11:27:23.036: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:23.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:23.517: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:23.517: INFO: stdout: ""
    Feb 12 11:27:23.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:23.584: INFO: stderr: ""
    Feb 12 11:27:23.584: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:23.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:24.002: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:24.002: INFO: stdout: ""
    Feb 12 11:27:24.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:24.079: INFO: stderr: ""
    Feb 12 11:27:24.079: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:24.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:24.499: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:24.499: INFO: stdout: ""
    Feb 12 11:27:24.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:24.587: INFO: stderr: ""
    Feb 12 11:27:24.587: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:24.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:24.984: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:24.984: INFO: stdout: ""
    Feb 12 11:27:24.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:25.062: INFO: stderr: ""
    Feb 12 11:27:25.062: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:25.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:25.415: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:25.415: INFO: stdout: ""
    Feb 12 11:27:25.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:25.466: INFO: stderr: ""
    Feb 12 11:27:25.466: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:25.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:25.922: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:25.922: INFO: stdout: ""
    Feb 12 11:27:25.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:25.974: INFO: stderr: ""
    Feb 12 11:27:25.974: INFO: stdout: "update-demo-nautilus-bq2t2\nupdate-demo-nautilus-z9vn7\n"
    Feb 12 11:27:26.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get rc,svc -l name=update-demo --no-headers'
    Feb 12 11:27:26.551: INFO: stderr: "No resources found in kubectl-7420 namespace.\n"
    Feb 12 11:27:26.551: INFO: stdout: ""
    Feb 12 11:27:26.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-7420 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 11:27:26.633: INFO: stderr: ""
    Feb 12 11:27:26.633: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:27:26.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7420" for this suite. 02/12/23 11:27:26.639
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:27:26.652
Feb 12 11:27:26.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubelet-test 02/12/23 11:27:26.653
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:31.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:31.934
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 12 11:27:31.948: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215" in namespace "kubelet-test-6386" to be "running and ready"
Feb 12 11:27:31.958: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16977ms
Feb 12 11:27:31.958: INFO: The phase of Pod busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:27:33.970: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02249145s
Feb 12 11:27:33.970: INFO: The phase of Pod busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:27:35.971: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215": Phase="Running", Reason="", readiness=true. Elapsed: 4.022685659s
Feb 12 11:27:35.971: INFO: The phase of Pod busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215 is Running (Ready = true)
Feb 12 11:27:35.971: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 12 11:27:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6386" for this suite. 02/12/23 11:27:36.018
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":73,"skipped":1324,"failed":0}
------------------------------
 [SLOW TEST] [9.382 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:27:26.652
    Feb 12 11:27:26.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubelet-test 02/12/23 11:27:26.653
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:31.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:31.934
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 12 11:27:31.948: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215" in namespace "kubelet-test-6386" to be "running and ready"
    Feb 12 11:27:31.958: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16977ms
    Feb 12 11:27:31.958: INFO: The phase of Pod busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:27:33.970: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02249145s
    Feb 12 11:27:33.970: INFO: The phase of Pod busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:27:35.971: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215": Phase="Running", Reason="", readiness=true. Elapsed: 4.022685659s
    Feb 12 11:27:35.971: INFO: The phase of Pod busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215 is Running (Ready = true)
    Feb 12 11:27:35.971: INFO: Pod "busybox-readonly-fs10c27d5e-e585-4696-add5-5097e578c215" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 12 11:27:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6386" for this suite. 02/12/23 11:27:36.018
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:27:36.038
Feb 12 11:27:36.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename security-context-test 02/12/23 11:27:36.039
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:36.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:36.132
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Feb 12 11:27:36.198: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd" in namespace "security-context-test-9916" to be "Succeeded or Failed"
Feb 12 11:27:36.239: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd": Phase="Pending", Reason="", readiness=false. Elapsed: 41.028092ms
Feb 12 11:27:38.244: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045612895s
Feb 12 11:27:40.248: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049372846s
Feb 12 11:27:40.248: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 12 11:27:40.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9916" for this suite. 02/12/23 11:27:40.255
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":74,"skipped":1324,"failed":0}
------------------------------
 [4.225 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:27:36.038
    Feb 12 11:27:36.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename security-context-test 02/12/23 11:27:36.039
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:36.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:36.132
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Feb 12 11:27:36.198: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd" in namespace "security-context-test-9916" to be "Succeeded or Failed"
    Feb 12 11:27:36.239: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd": Phase="Pending", Reason="", readiness=false. Elapsed: 41.028092ms
    Feb 12 11:27:38.244: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045612895s
    Feb 12 11:27:40.248: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049372846s
    Feb 12 11:27:40.248: INFO: Pod "busybox-user-65534-d490cb3a-a75f-490f-bf47-ae8db01764bd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 12 11:27:40.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9916" for this suite. 02/12/23 11:27:40.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:27:40.266
Feb 12 11:27:40.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svc-latency 02/12/23 11:27:40.267
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:40.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:40.291
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 12 11:27:40.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5527 02/12/23 11:27:40.294
I0212 11:27:40.300391      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5527, replica count: 1
I0212 11:27:41.351779      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 11:27:42.352250      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 11:27:42.486: INFO: Created: latency-svc-2mjcp
Feb 12 11:27:42.516: INFO: Got endpoints: latency-svc-2mjcp [63.662449ms]
Feb 12 11:27:42.544: INFO: Created: latency-svc-7kr5x
Feb 12 11:27:42.556: INFO: Got endpoints: latency-svc-7kr5x [38.988729ms]
Feb 12 11:27:42.556: INFO: Created: latency-svc-474nj
Feb 12 11:27:42.569: INFO: Got endpoints: latency-svc-474nj [52.079447ms]
Feb 12 11:27:42.575: INFO: Created: latency-svc-xcz77
Feb 12 11:27:42.588: INFO: Got endpoints: latency-svc-xcz77 [71.834508ms]
Feb 12 11:27:42.597: INFO: Created: latency-svc-zz9lr
Feb 12 11:27:42.607: INFO: Got endpoints: latency-svc-zz9lr [89.474911ms]
Feb 12 11:27:42.618: INFO: Created: latency-svc-nr9rn
Feb 12 11:27:42.634: INFO: Got endpoints: latency-svc-nr9rn [117.149596ms]
Feb 12 11:27:42.636: INFO: Created: latency-svc-49qdz
Feb 12 11:27:42.649: INFO: Got endpoints: latency-svc-49qdz [132.225482ms]
Feb 12 11:27:42.652: INFO: Created: latency-svc-lmhhn
Feb 12 11:27:42.662: INFO: Got endpoints: latency-svc-lmhhn [144.219321ms]
Feb 12 11:27:42.673: INFO: Created: latency-svc-7hkx8
Feb 12 11:27:42.684: INFO: Got endpoints: latency-svc-7hkx8 [168.111507ms]
Feb 12 11:27:42.695: INFO: Created: latency-svc-7dzt9
Feb 12 11:27:42.714: INFO: Got endpoints: latency-svc-7dzt9 [197.029729ms]
Feb 12 11:27:42.715: INFO: Created: latency-svc-8qzz9
Feb 12 11:27:42.724: INFO: Got endpoints: latency-svc-8qzz9 [206.433983ms]
Feb 12 11:27:42.734: INFO: Created: latency-svc-98qvp
Feb 12 11:27:42.759: INFO: Got endpoints: latency-svc-98qvp [241.411417ms]
Feb 12 11:27:42.769: INFO: Created: latency-svc-5pf85
Feb 12 11:27:42.784: INFO: Got endpoints: latency-svc-5pf85 [266.534177ms]
Feb 12 11:27:42.800: INFO: Created: latency-svc-6tvmb
Feb 12 11:27:42.816: INFO: Created: latency-svc-mbdkm
Feb 12 11:27:42.826: INFO: Got endpoints: latency-svc-6tvmb [308.662674ms]
Feb 12 11:27:42.840: INFO: Got endpoints: latency-svc-mbdkm [322.326357ms]
Feb 12 11:27:42.846: INFO: Created: latency-svc-jwcg9
Feb 12 11:27:42.866: INFO: Got endpoints: latency-svc-jwcg9 [349.044112ms]
Feb 12 11:27:42.885: INFO: Created: latency-svc-cr6z6
Feb 12 11:27:42.899: INFO: Got endpoints: latency-svc-cr6z6 [329.974134ms]
Feb 12 11:27:42.911: INFO: Created: latency-svc-nm8zc
Feb 12 11:27:42.925: INFO: Got endpoints: latency-svc-nm8zc [337.028762ms]
Feb 12 11:27:42.939: INFO: Created: latency-svc-q6htk
Feb 12 11:27:42.949: INFO: Got endpoints: latency-svc-q6htk [342.774002ms]
Feb 12 11:27:42.957: INFO: Created: latency-svc-lzh8r
Feb 12 11:27:42.967: INFO: Got endpoints: latency-svc-lzh8r [333.089094ms]
Feb 12 11:27:42.985: INFO: Created: latency-svc-g26nq
Feb 12 11:27:43.000: INFO: Got endpoints: latency-svc-g26nq [350.79008ms]
Feb 12 11:27:43.008: INFO: Created: latency-svc-qgllx
Feb 12 11:27:43.028: INFO: Got endpoints: latency-svc-qgllx [366.24685ms]
Feb 12 11:27:43.034: INFO: Created: latency-svc-hc858
Feb 12 11:27:43.047: INFO: Created: latency-svc-wzlsh
Feb 12 11:27:43.054: INFO: Got endpoints: latency-svc-hc858 [369.326482ms]
Feb 12 11:27:43.078: INFO: Got endpoints: latency-svc-wzlsh [363.2159ms]
Feb 12 11:27:43.082: INFO: Created: latency-svc-dxf5m
Feb 12 11:27:43.097: INFO: Got endpoints: latency-svc-dxf5m [372.944678ms]
Feb 12 11:27:43.107: INFO: Created: latency-svc-tw8t5
Feb 12 11:27:43.113: INFO: Created: latency-svc-tg4sx
Feb 12 11:27:43.120: INFO: Got endpoints: latency-svc-tw8t5 [360.766929ms]
Feb 12 11:27:43.129: INFO: Got endpoints: latency-svc-tg4sx [344.598219ms]
Feb 12 11:27:43.141: INFO: Created: latency-svc-6v68f
Feb 12 11:27:43.156: INFO: Got endpoints: latency-svc-6v68f [329.794249ms]
Feb 12 11:27:43.167: INFO: Created: latency-svc-s97d5
Feb 12 11:27:43.184: INFO: Got endpoints: latency-svc-s97d5 [626.734467ms]
Feb 12 11:27:43.184: INFO: Created: latency-svc-v2dlb
Feb 12 11:27:43.195: INFO: Got endpoints: latency-svc-v2dlb [355.410909ms]
Feb 12 11:27:43.197: INFO: Created: latency-svc-hgjt7
Feb 12 11:27:43.224: INFO: Got endpoints: latency-svc-hgjt7 [358.14965ms]
Feb 12 11:27:43.224: INFO: Created: latency-svc-v5rbd
Feb 12 11:27:43.230: INFO: Got endpoints: latency-svc-v5rbd [331.093357ms]
Feb 12 11:27:43.244: INFO: Created: latency-svc-5kmfc
Feb 12 11:27:43.257: INFO: Created: latency-svc-bkkkq
Feb 12 11:27:43.263: INFO: Got endpoints: latency-svc-5kmfc [337.970719ms]
Feb 12 11:27:43.281: INFO: Got endpoints: latency-svc-bkkkq [331.418762ms]
Feb 12 11:27:43.285: INFO: Created: latency-svc-dfgdq
Feb 12 11:27:43.294: INFO: Got endpoints: latency-svc-dfgdq [327.459658ms]
Feb 12 11:27:43.305: INFO: Created: latency-svc-rpkbl
Feb 12 11:27:43.323: INFO: Got endpoints: latency-svc-rpkbl [322.662653ms]
Feb 12 11:27:43.329: INFO: Created: latency-svc-pb8tp
Feb 12 11:27:43.848: INFO: Got endpoints: latency-svc-pb8tp [820.291738ms]
Feb 12 11:27:44.561: INFO: Created: latency-svc-zz2xz
Feb 12 11:27:44.826: INFO: Got endpoints: latency-svc-zz2xz [1.772669703s]
Feb 12 11:27:44.947: INFO: Created: latency-svc-rzksb
Feb 12 11:27:45.040: INFO: Got endpoints: latency-svc-rzksb [1.961826064s]
Feb 12 11:27:45.136: INFO: Created: latency-svc-ttftc
Feb 12 11:27:45.293: INFO: Got endpoints: latency-svc-ttftc [2.195585381s]
Feb 12 11:27:45.309: INFO: Created: latency-svc-hvmc8
Feb 12 11:27:45.459: INFO: Got endpoints: latency-svc-hvmc8 [2.339594555s]
Feb 12 11:27:45.481: INFO: Created: latency-svc-tlm75
Feb 12 11:27:45.625: INFO: Got endpoints: latency-svc-tlm75 [2.496540359s]
Feb 12 11:27:45.659: INFO: Created: latency-svc-s8jj6
Feb 12 11:27:45.704: INFO: Got endpoints: latency-svc-s8jj6 [2.547764212s]
Feb 12 11:27:45.838: INFO: Created: latency-svc-dchkf
Feb 12 11:27:45.899: INFO: Created: latency-svc-t4qqw
Feb 12 11:27:45.939: INFO: Got endpoints: latency-svc-dchkf [2.754068124s]
Feb 12 11:27:46.006: INFO: Got endpoints: latency-svc-t4qqw [2.810575508s]
Feb 12 11:27:46.031: INFO: Created: latency-svc-rghgj
Feb 12 11:27:46.109: INFO: Created: latency-svc-vvr4m
Feb 12 11:27:46.109: INFO: Got endpoints: latency-svc-rghgj [2.885084681s]
Feb 12 11:27:46.210: INFO: Got endpoints: latency-svc-vvr4m [2.980190692s]
Feb 12 11:27:46.256: INFO: Created: latency-svc-vtnbh
Feb 12 11:27:46.267: INFO: Created: latency-svc-xb9z8
Feb 12 11:27:46.279: INFO: Got endpoints: latency-svc-vtnbh [3.015342977s]
Feb 12 11:27:46.285: INFO: Got endpoints: latency-svc-xb9z8 [3.004027339s]
Feb 12 11:27:46.298: INFO: Created: latency-svc-wm8d9
Feb 12 11:27:46.308: INFO: Got endpoints: latency-svc-wm8d9 [3.013036541s]
Feb 12 11:27:46.313: INFO: Created: latency-svc-hvn65
Feb 12 11:27:46.326: INFO: Got endpoints: latency-svc-hvn65 [3.002422459s]
Feb 12 11:27:46.331: INFO: Created: latency-svc-j7gxf
Feb 12 11:27:46.341: INFO: Got endpoints: latency-svc-j7gxf [2.492942339s]
Feb 12 11:27:46.348: INFO: Created: latency-svc-2ljpp
Feb 12 11:27:46.376: INFO: Got endpoints: latency-svc-2ljpp [1.549257634s]
Feb 12 11:27:46.383: INFO: Created: latency-svc-j6cj7
Feb 12 11:27:46.391: INFO: Got endpoints: latency-svc-j6cj7 [1.351256115s]
Feb 12 11:27:46.398: INFO: Created: latency-svc-wkzhp
Feb 12 11:27:46.409: INFO: Got endpoints: latency-svc-wkzhp [1.116256867s]
Feb 12 11:27:46.420: INFO: Created: latency-svc-2ftrh
Feb 12 11:27:46.434: INFO: Got endpoints: latency-svc-2ftrh [974.235973ms]
Feb 12 11:27:46.440: INFO: Created: latency-svc-6rxtt
Feb 12 11:27:46.452: INFO: Got endpoints: latency-svc-6rxtt [826.487279ms]
Feb 12 11:27:46.459: INFO: Created: latency-svc-z6n6p
Feb 12 11:27:46.475: INFO: Created: latency-svc-c8tg4
Feb 12 11:27:46.478: INFO: Got endpoints: latency-svc-z6n6p [774.108835ms]
Feb 12 11:27:46.489: INFO: Got endpoints: latency-svc-c8tg4 [550.350786ms]
Feb 12 11:27:46.501: INFO: Created: latency-svc-n2zm9
Feb 12 11:27:46.511: INFO: Got endpoints: latency-svc-n2zm9 [504.975839ms]
Feb 12 11:27:46.526: INFO: Created: latency-svc-prfrg
Feb 12 11:27:46.542: INFO: Got endpoints: latency-svc-prfrg [432.662027ms]
Feb 12 11:27:46.550: INFO: Created: latency-svc-mt2b5
Feb 12 11:27:46.560: INFO: Got endpoints: latency-svc-mt2b5 [349.181006ms]
Feb 12 11:27:46.578: INFO: Created: latency-svc-xqzrw
Feb 12 11:27:46.600: INFO: Got endpoints: latency-svc-xqzrw [321.321216ms]
Feb 12 11:27:46.602: INFO: Created: latency-svc-zqfcg
Feb 12 11:27:46.625: INFO: Got endpoints: latency-svc-zqfcg [338.950954ms]
Feb 12 11:27:46.629: INFO: Created: latency-svc-8tp4k
Feb 12 11:27:46.637: INFO: Got endpoints: latency-svc-8tp4k [329.45072ms]
Feb 12 11:27:46.648: INFO: Created: latency-svc-fx26l
Feb 12 11:27:46.659: INFO: Got endpoints: latency-svc-fx26l [333.333534ms]
Feb 12 11:27:46.665: INFO: Created: latency-svc-7vf57
Feb 12 11:27:46.687: INFO: Got endpoints: latency-svc-7vf57 [345.288414ms]
Feb 12 11:27:46.687: INFO: Created: latency-svc-8628n
Feb 12 11:27:46.700: INFO: Got endpoints: latency-svc-8628n [323.896979ms]
Feb 12 11:27:46.704: INFO: Created: latency-svc-5m7gg
Feb 12 11:27:46.720: INFO: Got endpoints: latency-svc-5m7gg [328.750261ms]
Feb 12 11:27:46.730: INFO: Created: latency-svc-vt88g
Feb 12 11:27:46.750: INFO: Created: latency-svc-nztws
Feb 12 11:27:46.751: INFO: Got endpoints: latency-svc-vt88g [341.454442ms]
Feb 12 11:27:46.760: INFO: Created: latency-svc-s6mdv
Feb 12 11:27:46.766: INFO: Got endpoints: latency-svc-nztws [332.40158ms]
Feb 12 11:27:46.779: INFO: Created: latency-svc-bbbwp
Feb 12 11:27:46.784: INFO: Got endpoints: latency-svc-s6mdv [331.702604ms]
Feb 12 11:27:46.802: INFO: Created: latency-svc-4v9x6
Feb 12 11:27:46.804: INFO: Got endpoints: latency-svc-bbbwp [325.806098ms]
Feb 12 11:27:46.818: INFO: Got endpoints: latency-svc-4v9x6 [328.318468ms]
Feb 12 11:27:46.823: INFO: Created: latency-svc-4x7b9
Feb 12 11:27:46.835: INFO: Got endpoints: latency-svc-4x7b9 [324.11056ms]
Feb 12 11:27:46.846: INFO: Created: latency-svc-6whqs
Feb 12 11:27:46.859: INFO: Got endpoints: latency-svc-6whqs [316.256315ms]
Feb 12 11:27:46.873: INFO: Created: latency-svc-xlm5z
Feb 12 11:27:46.889: INFO: Got endpoints: latency-svc-xlm5z [329.240707ms]
Feb 12 11:27:46.905: INFO: Created: latency-svc-4ppj7
Feb 12 11:27:46.914: INFO: Created: latency-svc-hlfkq
Feb 12 11:27:46.945: INFO: Got endpoints: latency-svc-hlfkq [320.06446ms]
Feb 12 11:27:46.945: INFO: Got endpoints: latency-svc-4ppj7 [344.518686ms]
Feb 12 11:27:46.948: INFO: Created: latency-svc-8bbgz
Feb 12 11:27:46.966: INFO: Got endpoints: latency-svc-8bbgz [328.955746ms]
Feb 12 11:27:46.984: INFO: Created: latency-svc-4lwhr
Feb 12 11:27:46.997: INFO: Got endpoints: latency-svc-4lwhr [337.464104ms]
Feb 12 11:27:47.000: INFO: Created: latency-svc-6cctc
Feb 12 11:27:47.010: INFO: Got endpoints: latency-svc-6cctc [322.940366ms]
Feb 12 11:27:47.031: INFO: Created: latency-svc-86g8f
Feb 12 11:27:47.056: INFO: Got endpoints: latency-svc-86g8f [355.601666ms]
Feb 12 11:27:47.056: INFO: Created: latency-svc-r6flg
Feb 12 11:27:47.059: INFO: Created: latency-svc-ptgd2
Feb 12 11:27:47.072: INFO: Got endpoints: latency-svc-r6flg [352.098612ms]
Feb 12 11:27:47.087: INFO: Created: latency-svc-hqpn9
Feb 12 11:27:47.090: INFO: Got endpoints: latency-svc-ptgd2 [339.899762ms]
Feb 12 11:27:47.112: INFO: Got endpoints: latency-svc-hqpn9 [345.633437ms]
Feb 12 11:27:47.120: INFO: Created: latency-svc-vgnv9
Feb 12 11:27:47.140: INFO: Got endpoints: latency-svc-vgnv9 [356.658087ms]
Feb 12 11:27:47.141: INFO: Created: latency-svc-6jx4c
Feb 12 11:27:47.165: INFO: Created: latency-svc-k2dzh
Feb 12 11:27:47.167: INFO: Got endpoints: latency-svc-6jx4c [362.37643ms]
Feb 12 11:27:47.186: INFO: Got endpoints: latency-svc-k2dzh [367.974178ms]
Feb 12 11:27:47.194: INFO: Created: latency-svc-7kf6q
Feb 12 11:27:47.203: INFO: Got endpoints: latency-svc-7kf6q [367.343121ms]
Feb 12 11:27:47.212: INFO: Created: latency-svc-tskj9
Feb 12 11:27:47.224: INFO: Got endpoints: latency-svc-tskj9 [364.890478ms]
Feb 12 11:27:47.240: INFO: Created: latency-svc-mdfnx
Feb 12 11:27:47.259: INFO: Got endpoints: latency-svc-mdfnx [369.936217ms]
Feb 12 11:27:47.275: INFO: Created: latency-svc-29j8b
Feb 12 11:27:47.280: INFO: Created: latency-svc-w4drd
Feb 12 11:27:47.280: INFO: Got endpoints: latency-svc-29j8b [335.503998ms]
Feb 12 11:27:47.291: INFO: Got endpoints: latency-svc-w4drd [346.410829ms]
Feb 12 11:27:47.300: INFO: Created: latency-svc-xk74v
Feb 12 11:27:47.313: INFO: Got endpoints: latency-svc-xk74v [346.505741ms]
Feb 12 11:27:47.328: INFO: Created: latency-svc-6r4mh
Feb 12 11:27:47.333: INFO: Created: latency-svc-mpv4s
Feb 12 11:27:47.345: INFO: Got endpoints: latency-svc-6r4mh [348.611007ms]
Feb 12 11:27:47.352: INFO: Created: latency-svc-qswb6
Feb 12 11:27:47.356: INFO: Got endpoints: latency-svc-mpv4s [345.929813ms]
Feb 12 11:27:47.369: INFO: Got endpoints: latency-svc-qswb6 [313.003222ms]
Feb 12 11:27:47.384: INFO: Created: latency-svc-c9v62
Feb 12 11:27:47.392: INFO: Created: latency-svc-5jc92
Feb 12 11:27:47.400: INFO: Got endpoints: latency-svc-c9v62 [327.62053ms]
Feb 12 11:27:47.412: INFO: Created: latency-svc-dnrbw
Feb 12 11:27:47.425: INFO: Got endpoints: latency-svc-5jc92 [334.01887ms]
Feb 12 11:27:47.431: INFO: Created: latency-svc-8ncpc
Feb 12 11:27:47.437: INFO: Created: latency-svc-mctmb
Feb 12 11:27:47.458: INFO: Created: latency-svc-6mb9t
Feb 12 11:27:47.476: INFO: Got endpoints: latency-svc-dnrbw [364.262711ms]
Feb 12 11:27:47.488: INFO: Created: latency-svc-lgnmn
Feb 12 11:27:47.491: INFO: Created: latency-svc-bxpnp
Feb 12 11:27:47.513: INFO: Created: latency-svc-bj5tn
Feb 12 11:27:47.519: INFO: Got endpoints: latency-svc-8ncpc [377.939715ms]
Feb 12 11:27:47.532: INFO: Created: latency-svc-r82g6
Feb 12 11:27:47.540: INFO: Created: latency-svc-rtsh6
Feb 12 11:27:47.562: INFO: Created: latency-svc-z59vr
Feb 12 11:27:47.571: INFO: Got endpoints: latency-svc-mctmb [404.234142ms]
Feb 12 11:27:47.584: INFO: Created: latency-svc-cnbhh
Feb 12 11:27:47.600: INFO: Created: latency-svc-9kfcx
Feb 12 11:27:47.613: INFO: Created: latency-svc-gd24d
Feb 12 11:27:47.632: INFO: Got endpoints: latency-svc-6mb9t [445.874564ms]
Feb 12 11:27:47.645: INFO: Created: latency-svc-ll96q
Feb 12 11:27:47.651: INFO: Created: latency-svc-tzwd4
Feb 12 11:27:47.672: INFO: Got endpoints: latency-svc-lgnmn [468.91603ms]
Feb 12 11:27:47.673: INFO: Created: latency-svc-dlxsl
Feb 12 11:27:47.685: INFO: Created: latency-svc-bmsxl
Feb 12 11:27:47.700: INFO: Created: latency-svc-xd6r2
Feb 12 11:27:47.722: INFO: Created: latency-svc-qvrf6
Feb 12 11:27:47.726: INFO: Got endpoints: latency-svc-bxpnp [502.943911ms]
Feb 12 11:27:47.735: INFO: Created: latency-svc-gj84x
Feb 12 11:27:47.751: INFO: Created: latency-svc-jvncz
Feb 12 11:27:47.787: INFO: Got endpoints: latency-svc-bj5tn [527.421177ms]
Feb 12 11:27:47.814: INFO: Created: latency-svc-jlpzg
Feb 12 11:27:47.829: INFO: Got endpoints: latency-svc-r82g6 [548.787771ms]
Feb 12 11:27:47.856: INFO: Created: latency-svc-hl724
Feb 12 11:27:47.869: INFO: Got endpoints: latency-svc-rtsh6 [577.594009ms]
Feb 12 11:27:47.896: INFO: Created: latency-svc-s47qp
Feb 12 11:27:47.929: INFO: Got endpoints: latency-svc-z59vr [615.700969ms]
Feb 12 11:27:47.951: INFO: Created: latency-svc-fpv6l
Feb 12 11:27:47.968: INFO: Got endpoints: latency-svc-cnbhh [622.65361ms]
Feb 12 11:27:47.987: INFO: Created: latency-svc-2cr5g
Feb 12 11:27:48.014: INFO: Got endpoints: latency-svc-9kfcx [658.57832ms]
Feb 12 11:27:48.036: INFO: Created: latency-svc-ff9sf
Feb 12 11:27:48.073: INFO: Got endpoints: latency-svc-gd24d [704.251347ms]
Feb 12 11:27:48.097: INFO: Created: latency-svc-w868m
Feb 12 11:27:48.116: INFO: Got endpoints: latency-svc-ll96q [714.847679ms]
Feb 12 11:27:48.143: INFO: Created: latency-svc-p7rfw
Feb 12 11:27:48.178: INFO: Got endpoints: latency-svc-tzwd4 [753.053527ms]
Feb 12 11:27:48.199: INFO: Created: latency-svc-ztpgw
Feb 12 11:27:48.219: INFO: Got endpoints: latency-svc-dlxsl [742.103242ms]
Feb 12 11:27:48.237: INFO: Created: latency-svc-8rwsn
Feb 12 11:27:48.266: INFO: Got endpoints: latency-svc-bmsxl [747.585017ms]
Feb 12 11:27:48.292: INFO: Created: latency-svc-6mbgh
Feb 12 11:27:48.316: INFO: Got endpoints: latency-svc-xd6r2 [744.642409ms]
Feb 12 11:27:48.335: INFO: Created: latency-svc-kz8j8
Feb 12 11:27:48.373: INFO: Got endpoints: latency-svc-qvrf6 [741.266613ms]
Feb 12 11:27:48.392: INFO: Created: latency-svc-27fd9
Feb 12 11:27:48.418: INFO: Got endpoints: latency-svc-gj84x [745.787134ms]
Feb 12 11:27:48.441: INFO: Created: latency-svc-w8mt7
Feb 12 11:27:48.472: INFO: Got endpoints: latency-svc-jvncz [745.910002ms]
Feb 12 11:27:48.492: INFO: Created: latency-svc-kph66
Feb 12 11:27:48.518: INFO: Got endpoints: latency-svc-jlpzg [731.742073ms]
Feb 12 11:27:48.538: INFO: Created: latency-svc-d7hv9
Feb 12 11:27:48.576: INFO: Got endpoints: latency-svc-hl724 [746.413485ms]
Feb 12 11:27:48.599: INFO: Created: latency-svc-jr7k7
Feb 12 11:27:48.618: INFO: Got endpoints: latency-svc-s47qp [748.768536ms]
Feb 12 11:27:48.633: INFO: Created: latency-svc-42fdx
Feb 12 11:27:48.674: INFO: Got endpoints: latency-svc-fpv6l [745.053531ms]
Feb 12 11:27:48.689: INFO: Created: latency-svc-cwm2f
Feb 12 11:27:48.717: INFO: Got endpoints: latency-svc-2cr5g [748.793783ms]
Feb 12 11:27:48.745: INFO: Created: latency-svc-6mrh6
Feb 12 11:27:48.770: INFO: Got endpoints: latency-svc-ff9sf [755.675926ms]
Feb 12 11:27:48.785: INFO: Created: latency-svc-mgtwf
Feb 12 11:27:48.818: INFO: Got endpoints: latency-svc-w868m [743.709972ms]
Feb 12 11:27:48.835: INFO: Created: latency-svc-vcjpn
Feb 12 11:27:48.871: INFO: Got endpoints: latency-svc-p7rfw [755.468415ms]
Feb 12 11:27:48.889: INFO: Created: latency-svc-94z62
Feb 12 11:27:48.919: INFO: Got endpoints: latency-svc-ztpgw [741.095139ms]
Feb 12 11:27:48.940: INFO: Created: latency-svc-h5bw7
Feb 12 11:27:48.968: INFO: Got endpoints: latency-svc-8rwsn [749.028116ms]
Feb 12 11:27:48.984: INFO: Created: latency-svc-82fs8
Feb 12 11:27:49.023: INFO: Got endpoints: latency-svc-6mbgh [756.211683ms]
Feb 12 11:27:49.040: INFO: Created: latency-svc-42cff
Feb 12 11:27:49.922: INFO: Got endpoints: latency-svc-kz8j8 [1.605531143s]
Feb 12 11:27:50.547: INFO: Got endpoints: latency-svc-w8mt7 [2.128871568s]
Feb 12 11:27:51.121: INFO: Got endpoints: latency-svc-kph66 [2.648333517s]
Feb 12 11:27:51.124: INFO: Got endpoints: latency-svc-27fd9 [2.750692032s]
Feb 12 11:27:51.760: INFO: Got endpoints: latency-svc-jr7k7 [3.184418647s]
Feb 12 11:27:51.761: INFO: Got endpoints: latency-svc-d7hv9 [3.242458571s]
Feb 12 11:27:51.913: INFO: Got endpoints: latency-svc-42fdx [3.294749613s]
Feb 12 11:27:51.914: INFO: Got endpoints: latency-svc-cwm2f [3.239414259s]
Feb 12 11:27:51.914: INFO: Got endpoints: latency-svc-6mrh6 [3.196603609s]
Feb 12 11:27:51.945: INFO: Got endpoints: latency-svc-vcjpn [3.127431488s]
Feb 12 11:27:52.071: INFO: Got endpoints: latency-svc-94z62 [3.199664485s]
Feb 12 11:27:52.073: INFO: Got endpoints: latency-svc-mgtwf [3.302614364s]
Feb 12 11:27:52.112: INFO: Got endpoints: latency-svc-42cff [3.089026369s]
Feb 12 11:27:52.178: INFO: Created: latency-svc-nhs65
Feb 12 11:27:52.178: INFO: Got endpoints: latency-svc-h5bw7 [3.259192649s]
Feb 12 11:27:52.179: INFO: Got endpoints: latency-svc-82fs8 [3.210787762s]
Feb 12 11:27:52.308: INFO: Got endpoints: latency-svc-nhs65 [2.386204118s]
Feb 12 11:27:52.402: INFO: Created: latency-svc-n8g5f
Feb 12 11:27:52.574: INFO: Got endpoints: latency-svc-n8g5f [2.026657425s]
Feb 12 11:27:52.575: INFO: Created: latency-svc-gxt6f
Feb 12 11:27:52.650: INFO: Got endpoints: latency-svc-gxt6f [1.528653054s]
Feb 12 11:27:52.709: INFO: Created: latency-svc-5jmpz
Feb 12 11:27:52.874: INFO: Got endpoints: latency-svc-5jmpz [1.749523561s]
Feb 12 11:27:52.880: INFO: Created: latency-svc-s9g7f
Feb 12 11:27:52.952: INFO: Got endpoints: latency-svc-s9g7f [1.191302362s]
Feb 12 11:27:52.974: INFO: Created: latency-svc-rqw9v
Feb 12 11:27:53.027: INFO: Got endpoints: latency-svc-rqw9v [1.266153801s]
Feb 12 11:27:53.045: INFO: Created: latency-svc-nbbbs
Feb 12 11:27:53.119: INFO: Got endpoints: latency-svc-nbbbs [1.20563223s]
Feb 12 11:27:53.148: INFO: Created: latency-svc-q7tx4
Feb 12 11:27:53.330: INFO: Got endpoints: latency-svc-q7tx4 [1.41574443s]
Feb 12 11:27:53.331: INFO: Created: latency-svc-g4pvz
Feb 12 11:27:53.332: INFO: Created: latency-svc-gfvls
Feb 12 11:27:53.347: INFO: Got endpoints: latency-svc-gfvls [1.4015863s]
Feb 12 11:27:53.347: INFO: Got endpoints: latency-svc-g4pvz [1.433506318s]
Feb 12 11:27:53.355: INFO: Created: latency-svc-jbf2r
Feb 12 11:27:53.369: INFO: Got endpoints: latency-svc-jbf2r [1.297976835s]
Feb 12 11:27:53.376: INFO: Created: latency-svc-9h8mk
Feb 12 11:27:53.386: INFO: Created: latency-svc-98ffj
Feb 12 11:27:53.391: INFO: Got endpoints: latency-svc-9h8mk [1.318254177s]
Feb 12 11:27:53.400: INFO: Got endpoints: latency-svc-98ffj [1.288029295s]
Feb 12 11:27:53.414: INFO: Created: latency-svc-wvjsc
Feb 12 11:27:53.423: INFO: Got endpoints: latency-svc-wvjsc [1.244306519s]
Feb 12 11:27:53.430: INFO: Created: latency-svc-x4dqq
Feb 12 11:27:53.439: INFO: Got endpoints: latency-svc-x4dqq [1.26043842s]
Feb 12 11:27:53.450: INFO: Created: latency-svc-9cc5k
Feb 12 11:27:53.460: INFO: Got endpoints: latency-svc-9cc5k [1.151850229s]
Feb 12 11:27:53.465: INFO: Created: latency-svc-vcddb
Feb 12 11:27:53.482: INFO: Got endpoints: latency-svc-vcddb [908.172204ms]
Feb 12 11:27:53.490: INFO: Created: latency-svc-4rd9m
Feb 12 11:27:53.498: INFO: Got endpoints: latency-svc-4rd9m [847.891878ms]
Feb 12 11:27:53.498: INFO: Created: latency-svc-74q2v
Feb 12 11:27:53.513: INFO: Got endpoints: latency-svc-74q2v [638.622948ms]
Feb 12 11:27:53.525: INFO: Created: latency-svc-4sqm8
Feb 12 11:27:53.546: INFO: Got endpoints: latency-svc-4sqm8 [594.096982ms]
Feb 12 11:27:53.546: INFO: Created: latency-svc-ljp6x
Feb 12 11:27:53.561: INFO: Created: latency-svc-nh65q
Feb 12 11:27:53.566: INFO: Got endpoints: latency-svc-ljp6x [538.441935ms]
Feb 12 11:27:53.574: INFO: Got endpoints: latency-svc-nh65q [455.099889ms]
Feb 12 11:27:53.587: INFO: Created: latency-svc-pgn85
Feb 12 11:27:53.604: INFO: Got endpoints: latency-svc-pgn85 [273.899505ms]
Feb 12 11:27:53.618: INFO: Created: latency-svc-vpglf
Feb 12 11:27:53.630: INFO: Got endpoints: latency-svc-vpglf [282.673737ms]
Feb 12 11:27:53.632: INFO: Created: latency-svc-qjj6h
Feb 12 11:27:53.648: INFO: Created: latency-svc-gq9gp
Feb 12 11:27:53.653: INFO: Got endpoints: latency-svc-qjj6h [306.217559ms]
Feb 12 11:27:53.663: INFO: Got endpoints: latency-svc-gq9gp [294.108047ms]
Feb 12 11:27:53.680: INFO: Created: latency-svc-q76wv
Feb 12 11:27:53.687: INFO: Got endpoints: latency-svc-q76wv [296.305355ms]
Feb 12 11:27:53.688: INFO: Created: latency-svc-xmj7h
Feb 12 11:27:53.709: INFO: Got endpoints: latency-svc-xmj7h [309.140526ms]
Feb 12 11:27:53.711: INFO: Created: latency-svc-swdkq
Feb 12 11:27:53.727: INFO: Got endpoints: latency-svc-swdkq [304.245514ms]
Feb 12 11:27:53.734: INFO: Created: latency-svc-9hmmd
Feb 12 11:27:53.745: INFO: Got endpoints: latency-svc-9hmmd [305.826229ms]
Feb 12 11:27:53.751: INFO: Created: latency-svc-sqzzn
Feb 12 11:27:53.770: INFO: Got endpoints: latency-svc-sqzzn [309.954195ms]
Feb 12 11:27:53.780: INFO: Created: latency-svc-45lrg
Feb 12 11:27:53.786: INFO: Got endpoints: latency-svc-45lrg [304.289984ms]
Feb 12 11:27:53.799: INFO: Created: latency-svc-997x8
Feb 12 11:27:53.812: INFO: Got endpoints: latency-svc-997x8 [313.751961ms]
Feb 12 11:27:53.812: INFO: Created: latency-svc-ngqhx
Feb 12 11:27:53.833: INFO: Got endpoints: latency-svc-ngqhx [319.888754ms]
Feb 12 11:27:53.845: INFO: Created: latency-svc-nzqdf
Feb 12 11:27:53.858: INFO: Got endpoints: latency-svc-nzqdf [311.845854ms]
Feb 12 11:27:53.865: INFO: Created: latency-svc-rhvt7
Feb 12 11:27:53.877: INFO: Got endpoints: latency-svc-rhvt7 [310.501748ms]
Feb 12 11:27:53.877: INFO: Created: latency-svc-mcm8d
Feb 12 11:27:53.900: INFO: Got endpoints: latency-svc-mcm8d [325.762067ms]
Feb 12 11:27:53.903: INFO: Created: latency-svc-fp8cj
Feb 12 11:27:53.920: INFO: Got endpoints: latency-svc-fp8cj [315.776982ms]
Feb 12 11:27:53.928: INFO: Created: latency-svc-85r5d
Feb 12 11:27:53.942: INFO: Created: latency-svc-j6kjk
Feb 12 11:27:53.949: INFO: Got endpoints: latency-svc-85r5d [318.841511ms]
Feb 12 11:27:53.959: INFO: Got endpoints: latency-svc-j6kjk [305.859723ms]
Feb 12 11:27:53.967: INFO: Created: latency-svc-s7ggq
Feb 12 11:27:53.981: INFO: Got endpoints: latency-svc-s7ggq [317.896477ms]
Feb 12 11:27:53.989: INFO: Created: latency-svc-drcg5
Feb 12 11:27:54.001: INFO: Got endpoints: latency-svc-drcg5 [313.787835ms]
Feb 12 11:27:54.007: INFO: Created: latency-svc-g6q5r
Feb 12 11:27:54.026: INFO: Created: latency-svc-rqmwn
Feb 12 11:27:54.026: INFO: Got endpoints: latency-svc-g6q5r [316.423534ms]
Feb 12 11:27:54.041: INFO: Got endpoints: latency-svc-rqmwn [313.921897ms]
Feb 12 11:27:54.043: INFO: Created: latency-svc-l6nwt
Feb 12 11:27:54.068: INFO: Created: latency-svc-2dfcd
Feb 12 11:27:54.078: INFO: Got endpoints: latency-svc-l6nwt [333.054314ms]
Feb 12 11:27:54.079: INFO: Got endpoints: latency-svc-2dfcd [308.458632ms]
Feb 12 11:27:54.097: INFO: Created: latency-svc-4j92b
Feb 12 11:27:54.125: INFO: Got endpoints: latency-svc-4j92b [337.955707ms]
Feb 12 11:27:54.136: INFO: Created: latency-svc-8vnnr
Feb 12 11:27:54.136: INFO: Created: latency-svc-892td
Feb 12 11:27:54.143: INFO: Got endpoints: latency-svc-8vnnr [331.544014ms]
Feb 12 11:27:54.154: INFO: Got endpoints: latency-svc-892td [320.632706ms]
Feb 12 11:27:54.166: INFO: Created: latency-svc-7j8lt
Feb 12 11:27:54.186: INFO: Got endpoints: latency-svc-7j8lt [328.338835ms]
Feb 12 11:27:54.196: INFO: Created: latency-svc-hhrs9
Feb 12 11:27:54.212: INFO: Got endpoints: latency-svc-hhrs9 [335.468375ms]
Feb 12 11:27:54.217: INFO: Created: latency-svc-n4xt6
Feb 12 11:27:54.235: INFO: Got endpoints: latency-svc-n4xt6 [334.937228ms]
Feb 12 11:27:54.238: INFO: Created: latency-svc-548sm
Feb 12 11:27:54.253: INFO: Got endpoints: latency-svc-548sm [333.645097ms]
Feb 12 11:27:54.254: INFO: Created: latency-svc-phld8
Feb 12 11:27:54.267: INFO: Got endpoints: latency-svc-phld8 [318.371109ms]
Feb 12 11:27:54.268: INFO: Latencies: [38.988729ms 52.079447ms 71.834508ms 89.474911ms 117.149596ms 132.225482ms 144.219321ms 168.111507ms 197.029729ms 206.433983ms 241.411417ms 266.534177ms 273.899505ms 282.673737ms 294.108047ms 296.305355ms 304.245514ms 304.289984ms 305.826229ms 305.859723ms 306.217559ms 308.458632ms 308.662674ms 309.140526ms 309.954195ms 310.501748ms 311.845854ms 313.003222ms 313.751961ms 313.787835ms 313.921897ms 315.776982ms 316.256315ms 316.423534ms 317.896477ms 318.371109ms 318.841511ms 319.888754ms 320.06446ms 320.632706ms 321.321216ms 322.326357ms 322.662653ms 322.940366ms 323.896979ms 324.11056ms 325.762067ms 325.806098ms 327.459658ms 327.62053ms 328.318468ms 328.338835ms 328.750261ms 328.955746ms 329.240707ms 329.45072ms 329.794249ms 329.974134ms 331.093357ms 331.418762ms 331.544014ms 331.702604ms 332.40158ms 333.054314ms 333.089094ms 333.333534ms 333.645097ms 334.01887ms 334.937228ms 335.468375ms 335.503998ms 337.028762ms 337.464104ms 337.955707ms 337.970719ms 338.950954ms 339.899762ms 341.454442ms 342.774002ms 344.518686ms 344.598219ms 345.288414ms 345.633437ms 345.929813ms 346.410829ms 346.505741ms 348.611007ms 349.044112ms 349.181006ms 350.79008ms 352.098612ms 355.410909ms 355.601666ms 356.658087ms 358.14965ms 360.766929ms 362.37643ms 363.2159ms 364.262711ms 364.890478ms 366.24685ms 367.343121ms 367.974178ms 369.326482ms 369.936217ms 372.944678ms 377.939715ms 404.234142ms 432.662027ms 445.874564ms 455.099889ms 468.91603ms 502.943911ms 504.975839ms 527.421177ms 538.441935ms 548.787771ms 550.350786ms 577.594009ms 594.096982ms 615.700969ms 622.65361ms 626.734467ms 638.622948ms 658.57832ms 704.251347ms 714.847679ms 731.742073ms 741.095139ms 741.266613ms 742.103242ms 743.709972ms 744.642409ms 745.053531ms 745.787134ms 745.910002ms 746.413485ms 747.585017ms 748.768536ms 748.793783ms 749.028116ms 753.053527ms 755.468415ms 755.675926ms 756.211683ms 774.108835ms 820.291738ms 826.487279ms 847.891878ms 908.172204ms 974.235973ms 1.116256867s 1.151850229s 1.191302362s 1.20563223s 1.244306519s 1.26043842s 1.266153801s 1.288029295s 1.297976835s 1.318254177s 1.351256115s 1.4015863s 1.41574443s 1.433506318s 1.528653054s 1.549257634s 1.605531143s 1.749523561s 1.772669703s 1.961826064s 2.026657425s 2.128871568s 2.195585381s 2.339594555s 2.386204118s 2.492942339s 2.496540359s 2.547764212s 2.648333517s 2.750692032s 2.754068124s 2.810575508s 2.885084681s 2.980190692s 3.002422459s 3.004027339s 3.013036541s 3.015342977s 3.089026369s 3.127431488s 3.184418647s 3.196603609s 3.199664485s 3.210787762s 3.239414259s 3.242458571s 3.259192649s 3.294749613s 3.302614364s]
Feb 12 11:27:54.268: INFO: 50 %ile: 366.24685ms
Feb 12 11:27:54.268: INFO: 90 %ile: 2.750692032s
Feb 12 11:27:54.268: INFO: 99 %ile: 3.294749613s
Feb 12 11:27:54.268: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Feb 12 11:27:54.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5527" for this suite. 02/12/23 11:27:54.274
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":75,"skipped":1346,"failed":0}
------------------------------
 [SLOW TEST] [14.019 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:27:40.266
    Feb 12 11:27:40.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svc-latency 02/12/23 11:27:40.267
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:40.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:40.291
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 12 11:27:40.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5527 02/12/23 11:27:40.294
    I0212 11:27:40.300391      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5527, replica count: 1
    I0212 11:27:41.351779      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 11:27:42.352250      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 11:27:42.486: INFO: Created: latency-svc-2mjcp
    Feb 12 11:27:42.516: INFO: Got endpoints: latency-svc-2mjcp [63.662449ms]
    Feb 12 11:27:42.544: INFO: Created: latency-svc-7kr5x
    Feb 12 11:27:42.556: INFO: Got endpoints: latency-svc-7kr5x [38.988729ms]
    Feb 12 11:27:42.556: INFO: Created: latency-svc-474nj
    Feb 12 11:27:42.569: INFO: Got endpoints: latency-svc-474nj [52.079447ms]
    Feb 12 11:27:42.575: INFO: Created: latency-svc-xcz77
    Feb 12 11:27:42.588: INFO: Got endpoints: latency-svc-xcz77 [71.834508ms]
    Feb 12 11:27:42.597: INFO: Created: latency-svc-zz9lr
    Feb 12 11:27:42.607: INFO: Got endpoints: latency-svc-zz9lr [89.474911ms]
    Feb 12 11:27:42.618: INFO: Created: latency-svc-nr9rn
    Feb 12 11:27:42.634: INFO: Got endpoints: latency-svc-nr9rn [117.149596ms]
    Feb 12 11:27:42.636: INFO: Created: latency-svc-49qdz
    Feb 12 11:27:42.649: INFO: Got endpoints: latency-svc-49qdz [132.225482ms]
    Feb 12 11:27:42.652: INFO: Created: latency-svc-lmhhn
    Feb 12 11:27:42.662: INFO: Got endpoints: latency-svc-lmhhn [144.219321ms]
    Feb 12 11:27:42.673: INFO: Created: latency-svc-7hkx8
    Feb 12 11:27:42.684: INFO: Got endpoints: latency-svc-7hkx8 [168.111507ms]
    Feb 12 11:27:42.695: INFO: Created: latency-svc-7dzt9
    Feb 12 11:27:42.714: INFO: Got endpoints: latency-svc-7dzt9 [197.029729ms]
    Feb 12 11:27:42.715: INFO: Created: latency-svc-8qzz9
    Feb 12 11:27:42.724: INFO: Got endpoints: latency-svc-8qzz9 [206.433983ms]
    Feb 12 11:27:42.734: INFO: Created: latency-svc-98qvp
    Feb 12 11:27:42.759: INFO: Got endpoints: latency-svc-98qvp [241.411417ms]
    Feb 12 11:27:42.769: INFO: Created: latency-svc-5pf85
    Feb 12 11:27:42.784: INFO: Got endpoints: latency-svc-5pf85 [266.534177ms]
    Feb 12 11:27:42.800: INFO: Created: latency-svc-6tvmb
    Feb 12 11:27:42.816: INFO: Created: latency-svc-mbdkm
    Feb 12 11:27:42.826: INFO: Got endpoints: latency-svc-6tvmb [308.662674ms]
    Feb 12 11:27:42.840: INFO: Got endpoints: latency-svc-mbdkm [322.326357ms]
    Feb 12 11:27:42.846: INFO: Created: latency-svc-jwcg9
    Feb 12 11:27:42.866: INFO: Got endpoints: latency-svc-jwcg9 [349.044112ms]
    Feb 12 11:27:42.885: INFO: Created: latency-svc-cr6z6
    Feb 12 11:27:42.899: INFO: Got endpoints: latency-svc-cr6z6 [329.974134ms]
    Feb 12 11:27:42.911: INFO: Created: latency-svc-nm8zc
    Feb 12 11:27:42.925: INFO: Got endpoints: latency-svc-nm8zc [337.028762ms]
    Feb 12 11:27:42.939: INFO: Created: latency-svc-q6htk
    Feb 12 11:27:42.949: INFO: Got endpoints: latency-svc-q6htk [342.774002ms]
    Feb 12 11:27:42.957: INFO: Created: latency-svc-lzh8r
    Feb 12 11:27:42.967: INFO: Got endpoints: latency-svc-lzh8r [333.089094ms]
    Feb 12 11:27:42.985: INFO: Created: latency-svc-g26nq
    Feb 12 11:27:43.000: INFO: Got endpoints: latency-svc-g26nq [350.79008ms]
    Feb 12 11:27:43.008: INFO: Created: latency-svc-qgllx
    Feb 12 11:27:43.028: INFO: Got endpoints: latency-svc-qgllx [366.24685ms]
    Feb 12 11:27:43.034: INFO: Created: latency-svc-hc858
    Feb 12 11:27:43.047: INFO: Created: latency-svc-wzlsh
    Feb 12 11:27:43.054: INFO: Got endpoints: latency-svc-hc858 [369.326482ms]
    Feb 12 11:27:43.078: INFO: Got endpoints: latency-svc-wzlsh [363.2159ms]
    Feb 12 11:27:43.082: INFO: Created: latency-svc-dxf5m
    Feb 12 11:27:43.097: INFO: Got endpoints: latency-svc-dxf5m [372.944678ms]
    Feb 12 11:27:43.107: INFO: Created: latency-svc-tw8t5
    Feb 12 11:27:43.113: INFO: Created: latency-svc-tg4sx
    Feb 12 11:27:43.120: INFO: Got endpoints: latency-svc-tw8t5 [360.766929ms]
    Feb 12 11:27:43.129: INFO: Got endpoints: latency-svc-tg4sx [344.598219ms]
    Feb 12 11:27:43.141: INFO: Created: latency-svc-6v68f
    Feb 12 11:27:43.156: INFO: Got endpoints: latency-svc-6v68f [329.794249ms]
    Feb 12 11:27:43.167: INFO: Created: latency-svc-s97d5
    Feb 12 11:27:43.184: INFO: Got endpoints: latency-svc-s97d5 [626.734467ms]
    Feb 12 11:27:43.184: INFO: Created: latency-svc-v2dlb
    Feb 12 11:27:43.195: INFO: Got endpoints: latency-svc-v2dlb [355.410909ms]
    Feb 12 11:27:43.197: INFO: Created: latency-svc-hgjt7
    Feb 12 11:27:43.224: INFO: Got endpoints: latency-svc-hgjt7 [358.14965ms]
    Feb 12 11:27:43.224: INFO: Created: latency-svc-v5rbd
    Feb 12 11:27:43.230: INFO: Got endpoints: latency-svc-v5rbd [331.093357ms]
    Feb 12 11:27:43.244: INFO: Created: latency-svc-5kmfc
    Feb 12 11:27:43.257: INFO: Created: latency-svc-bkkkq
    Feb 12 11:27:43.263: INFO: Got endpoints: latency-svc-5kmfc [337.970719ms]
    Feb 12 11:27:43.281: INFO: Got endpoints: latency-svc-bkkkq [331.418762ms]
    Feb 12 11:27:43.285: INFO: Created: latency-svc-dfgdq
    Feb 12 11:27:43.294: INFO: Got endpoints: latency-svc-dfgdq [327.459658ms]
    Feb 12 11:27:43.305: INFO: Created: latency-svc-rpkbl
    Feb 12 11:27:43.323: INFO: Got endpoints: latency-svc-rpkbl [322.662653ms]
    Feb 12 11:27:43.329: INFO: Created: latency-svc-pb8tp
    Feb 12 11:27:43.848: INFO: Got endpoints: latency-svc-pb8tp [820.291738ms]
    Feb 12 11:27:44.561: INFO: Created: latency-svc-zz2xz
    Feb 12 11:27:44.826: INFO: Got endpoints: latency-svc-zz2xz [1.772669703s]
    Feb 12 11:27:44.947: INFO: Created: latency-svc-rzksb
    Feb 12 11:27:45.040: INFO: Got endpoints: latency-svc-rzksb [1.961826064s]
    Feb 12 11:27:45.136: INFO: Created: latency-svc-ttftc
    Feb 12 11:27:45.293: INFO: Got endpoints: latency-svc-ttftc [2.195585381s]
    Feb 12 11:27:45.309: INFO: Created: latency-svc-hvmc8
    Feb 12 11:27:45.459: INFO: Got endpoints: latency-svc-hvmc8 [2.339594555s]
    Feb 12 11:27:45.481: INFO: Created: latency-svc-tlm75
    Feb 12 11:27:45.625: INFO: Got endpoints: latency-svc-tlm75 [2.496540359s]
    Feb 12 11:27:45.659: INFO: Created: latency-svc-s8jj6
    Feb 12 11:27:45.704: INFO: Got endpoints: latency-svc-s8jj6 [2.547764212s]
    Feb 12 11:27:45.838: INFO: Created: latency-svc-dchkf
    Feb 12 11:27:45.899: INFO: Created: latency-svc-t4qqw
    Feb 12 11:27:45.939: INFO: Got endpoints: latency-svc-dchkf [2.754068124s]
    Feb 12 11:27:46.006: INFO: Got endpoints: latency-svc-t4qqw [2.810575508s]
    Feb 12 11:27:46.031: INFO: Created: latency-svc-rghgj
    Feb 12 11:27:46.109: INFO: Created: latency-svc-vvr4m
    Feb 12 11:27:46.109: INFO: Got endpoints: latency-svc-rghgj [2.885084681s]
    Feb 12 11:27:46.210: INFO: Got endpoints: latency-svc-vvr4m [2.980190692s]
    Feb 12 11:27:46.256: INFO: Created: latency-svc-vtnbh
    Feb 12 11:27:46.267: INFO: Created: latency-svc-xb9z8
    Feb 12 11:27:46.279: INFO: Got endpoints: latency-svc-vtnbh [3.015342977s]
    Feb 12 11:27:46.285: INFO: Got endpoints: latency-svc-xb9z8 [3.004027339s]
    Feb 12 11:27:46.298: INFO: Created: latency-svc-wm8d9
    Feb 12 11:27:46.308: INFO: Got endpoints: latency-svc-wm8d9 [3.013036541s]
    Feb 12 11:27:46.313: INFO: Created: latency-svc-hvn65
    Feb 12 11:27:46.326: INFO: Got endpoints: latency-svc-hvn65 [3.002422459s]
    Feb 12 11:27:46.331: INFO: Created: latency-svc-j7gxf
    Feb 12 11:27:46.341: INFO: Got endpoints: latency-svc-j7gxf [2.492942339s]
    Feb 12 11:27:46.348: INFO: Created: latency-svc-2ljpp
    Feb 12 11:27:46.376: INFO: Got endpoints: latency-svc-2ljpp [1.549257634s]
    Feb 12 11:27:46.383: INFO: Created: latency-svc-j6cj7
    Feb 12 11:27:46.391: INFO: Got endpoints: latency-svc-j6cj7 [1.351256115s]
    Feb 12 11:27:46.398: INFO: Created: latency-svc-wkzhp
    Feb 12 11:27:46.409: INFO: Got endpoints: latency-svc-wkzhp [1.116256867s]
    Feb 12 11:27:46.420: INFO: Created: latency-svc-2ftrh
    Feb 12 11:27:46.434: INFO: Got endpoints: latency-svc-2ftrh [974.235973ms]
    Feb 12 11:27:46.440: INFO: Created: latency-svc-6rxtt
    Feb 12 11:27:46.452: INFO: Got endpoints: latency-svc-6rxtt [826.487279ms]
    Feb 12 11:27:46.459: INFO: Created: latency-svc-z6n6p
    Feb 12 11:27:46.475: INFO: Created: latency-svc-c8tg4
    Feb 12 11:27:46.478: INFO: Got endpoints: latency-svc-z6n6p [774.108835ms]
    Feb 12 11:27:46.489: INFO: Got endpoints: latency-svc-c8tg4 [550.350786ms]
    Feb 12 11:27:46.501: INFO: Created: latency-svc-n2zm9
    Feb 12 11:27:46.511: INFO: Got endpoints: latency-svc-n2zm9 [504.975839ms]
    Feb 12 11:27:46.526: INFO: Created: latency-svc-prfrg
    Feb 12 11:27:46.542: INFO: Got endpoints: latency-svc-prfrg [432.662027ms]
    Feb 12 11:27:46.550: INFO: Created: latency-svc-mt2b5
    Feb 12 11:27:46.560: INFO: Got endpoints: latency-svc-mt2b5 [349.181006ms]
    Feb 12 11:27:46.578: INFO: Created: latency-svc-xqzrw
    Feb 12 11:27:46.600: INFO: Got endpoints: latency-svc-xqzrw [321.321216ms]
    Feb 12 11:27:46.602: INFO: Created: latency-svc-zqfcg
    Feb 12 11:27:46.625: INFO: Got endpoints: latency-svc-zqfcg [338.950954ms]
    Feb 12 11:27:46.629: INFO: Created: latency-svc-8tp4k
    Feb 12 11:27:46.637: INFO: Got endpoints: latency-svc-8tp4k [329.45072ms]
    Feb 12 11:27:46.648: INFO: Created: latency-svc-fx26l
    Feb 12 11:27:46.659: INFO: Got endpoints: latency-svc-fx26l [333.333534ms]
    Feb 12 11:27:46.665: INFO: Created: latency-svc-7vf57
    Feb 12 11:27:46.687: INFO: Got endpoints: latency-svc-7vf57 [345.288414ms]
    Feb 12 11:27:46.687: INFO: Created: latency-svc-8628n
    Feb 12 11:27:46.700: INFO: Got endpoints: latency-svc-8628n [323.896979ms]
    Feb 12 11:27:46.704: INFO: Created: latency-svc-5m7gg
    Feb 12 11:27:46.720: INFO: Got endpoints: latency-svc-5m7gg [328.750261ms]
    Feb 12 11:27:46.730: INFO: Created: latency-svc-vt88g
    Feb 12 11:27:46.750: INFO: Created: latency-svc-nztws
    Feb 12 11:27:46.751: INFO: Got endpoints: latency-svc-vt88g [341.454442ms]
    Feb 12 11:27:46.760: INFO: Created: latency-svc-s6mdv
    Feb 12 11:27:46.766: INFO: Got endpoints: latency-svc-nztws [332.40158ms]
    Feb 12 11:27:46.779: INFO: Created: latency-svc-bbbwp
    Feb 12 11:27:46.784: INFO: Got endpoints: latency-svc-s6mdv [331.702604ms]
    Feb 12 11:27:46.802: INFO: Created: latency-svc-4v9x6
    Feb 12 11:27:46.804: INFO: Got endpoints: latency-svc-bbbwp [325.806098ms]
    Feb 12 11:27:46.818: INFO: Got endpoints: latency-svc-4v9x6 [328.318468ms]
    Feb 12 11:27:46.823: INFO: Created: latency-svc-4x7b9
    Feb 12 11:27:46.835: INFO: Got endpoints: latency-svc-4x7b9 [324.11056ms]
    Feb 12 11:27:46.846: INFO: Created: latency-svc-6whqs
    Feb 12 11:27:46.859: INFO: Got endpoints: latency-svc-6whqs [316.256315ms]
    Feb 12 11:27:46.873: INFO: Created: latency-svc-xlm5z
    Feb 12 11:27:46.889: INFO: Got endpoints: latency-svc-xlm5z [329.240707ms]
    Feb 12 11:27:46.905: INFO: Created: latency-svc-4ppj7
    Feb 12 11:27:46.914: INFO: Created: latency-svc-hlfkq
    Feb 12 11:27:46.945: INFO: Got endpoints: latency-svc-hlfkq [320.06446ms]
    Feb 12 11:27:46.945: INFO: Got endpoints: latency-svc-4ppj7 [344.518686ms]
    Feb 12 11:27:46.948: INFO: Created: latency-svc-8bbgz
    Feb 12 11:27:46.966: INFO: Got endpoints: latency-svc-8bbgz [328.955746ms]
    Feb 12 11:27:46.984: INFO: Created: latency-svc-4lwhr
    Feb 12 11:27:46.997: INFO: Got endpoints: latency-svc-4lwhr [337.464104ms]
    Feb 12 11:27:47.000: INFO: Created: latency-svc-6cctc
    Feb 12 11:27:47.010: INFO: Got endpoints: latency-svc-6cctc [322.940366ms]
    Feb 12 11:27:47.031: INFO: Created: latency-svc-86g8f
    Feb 12 11:27:47.056: INFO: Got endpoints: latency-svc-86g8f [355.601666ms]
    Feb 12 11:27:47.056: INFO: Created: latency-svc-r6flg
    Feb 12 11:27:47.059: INFO: Created: latency-svc-ptgd2
    Feb 12 11:27:47.072: INFO: Got endpoints: latency-svc-r6flg [352.098612ms]
    Feb 12 11:27:47.087: INFO: Created: latency-svc-hqpn9
    Feb 12 11:27:47.090: INFO: Got endpoints: latency-svc-ptgd2 [339.899762ms]
    Feb 12 11:27:47.112: INFO: Got endpoints: latency-svc-hqpn9 [345.633437ms]
    Feb 12 11:27:47.120: INFO: Created: latency-svc-vgnv9
    Feb 12 11:27:47.140: INFO: Got endpoints: latency-svc-vgnv9 [356.658087ms]
    Feb 12 11:27:47.141: INFO: Created: latency-svc-6jx4c
    Feb 12 11:27:47.165: INFO: Created: latency-svc-k2dzh
    Feb 12 11:27:47.167: INFO: Got endpoints: latency-svc-6jx4c [362.37643ms]
    Feb 12 11:27:47.186: INFO: Got endpoints: latency-svc-k2dzh [367.974178ms]
    Feb 12 11:27:47.194: INFO: Created: latency-svc-7kf6q
    Feb 12 11:27:47.203: INFO: Got endpoints: latency-svc-7kf6q [367.343121ms]
    Feb 12 11:27:47.212: INFO: Created: latency-svc-tskj9
    Feb 12 11:27:47.224: INFO: Got endpoints: latency-svc-tskj9 [364.890478ms]
    Feb 12 11:27:47.240: INFO: Created: latency-svc-mdfnx
    Feb 12 11:27:47.259: INFO: Got endpoints: latency-svc-mdfnx [369.936217ms]
    Feb 12 11:27:47.275: INFO: Created: latency-svc-29j8b
    Feb 12 11:27:47.280: INFO: Created: latency-svc-w4drd
    Feb 12 11:27:47.280: INFO: Got endpoints: latency-svc-29j8b [335.503998ms]
    Feb 12 11:27:47.291: INFO: Got endpoints: latency-svc-w4drd [346.410829ms]
    Feb 12 11:27:47.300: INFO: Created: latency-svc-xk74v
    Feb 12 11:27:47.313: INFO: Got endpoints: latency-svc-xk74v [346.505741ms]
    Feb 12 11:27:47.328: INFO: Created: latency-svc-6r4mh
    Feb 12 11:27:47.333: INFO: Created: latency-svc-mpv4s
    Feb 12 11:27:47.345: INFO: Got endpoints: latency-svc-6r4mh [348.611007ms]
    Feb 12 11:27:47.352: INFO: Created: latency-svc-qswb6
    Feb 12 11:27:47.356: INFO: Got endpoints: latency-svc-mpv4s [345.929813ms]
    Feb 12 11:27:47.369: INFO: Got endpoints: latency-svc-qswb6 [313.003222ms]
    Feb 12 11:27:47.384: INFO: Created: latency-svc-c9v62
    Feb 12 11:27:47.392: INFO: Created: latency-svc-5jc92
    Feb 12 11:27:47.400: INFO: Got endpoints: latency-svc-c9v62 [327.62053ms]
    Feb 12 11:27:47.412: INFO: Created: latency-svc-dnrbw
    Feb 12 11:27:47.425: INFO: Got endpoints: latency-svc-5jc92 [334.01887ms]
    Feb 12 11:27:47.431: INFO: Created: latency-svc-8ncpc
    Feb 12 11:27:47.437: INFO: Created: latency-svc-mctmb
    Feb 12 11:27:47.458: INFO: Created: latency-svc-6mb9t
    Feb 12 11:27:47.476: INFO: Got endpoints: latency-svc-dnrbw [364.262711ms]
    Feb 12 11:27:47.488: INFO: Created: latency-svc-lgnmn
    Feb 12 11:27:47.491: INFO: Created: latency-svc-bxpnp
    Feb 12 11:27:47.513: INFO: Created: latency-svc-bj5tn
    Feb 12 11:27:47.519: INFO: Got endpoints: latency-svc-8ncpc [377.939715ms]
    Feb 12 11:27:47.532: INFO: Created: latency-svc-r82g6
    Feb 12 11:27:47.540: INFO: Created: latency-svc-rtsh6
    Feb 12 11:27:47.562: INFO: Created: latency-svc-z59vr
    Feb 12 11:27:47.571: INFO: Got endpoints: latency-svc-mctmb [404.234142ms]
    Feb 12 11:27:47.584: INFO: Created: latency-svc-cnbhh
    Feb 12 11:27:47.600: INFO: Created: latency-svc-9kfcx
    Feb 12 11:27:47.613: INFO: Created: latency-svc-gd24d
    Feb 12 11:27:47.632: INFO: Got endpoints: latency-svc-6mb9t [445.874564ms]
    Feb 12 11:27:47.645: INFO: Created: latency-svc-ll96q
    Feb 12 11:27:47.651: INFO: Created: latency-svc-tzwd4
    Feb 12 11:27:47.672: INFO: Got endpoints: latency-svc-lgnmn [468.91603ms]
    Feb 12 11:27:47.673: INFO: Created: latency-svc-dlxsl
    Feb 12 11:27:47.685: INFO: Created: latency-svc-bmsxl
    Feb 12 11:27:47.700: INFO: Created: latency-svc-xd6r2
    Feb 12 11:27:47.722: INFO: Created: latency-svc-qvrf6
    Feb 12 11:27:47.726: INFO: Got endpoints: latency-svc-bxpnp [502.943911ms]
    Feb 12 11:27:47.735: INFO: Created: latency-svc-gj84x
    Feb 12 11:27:47.751: INFO: Created: latency-svc-jvncz
    Feb 12 11:27:47.787: INFO: Got endpoints: latency-svc-bj5tn [527.421177ms]
    Feb 12 11:27:47.814: INFO: Created: latency-svc-jlpzg
    Feb 12 11:27:47.829: INFO: Got endpoints: latency-svc-r82g6 [548.787771ms]
    Feb 12 11:27:47.856: INFO: Created: latency-svc-hl724
    Feb 12 11:27:47.869: INFO: Got endpoints: latency-svc-rtsh6 [577.594009ms]
    Feb 12 11:27:47.896: INFO: Created: latency-svc-s47qp
    Feb 12 11:27:47.929: INFO: Got endpoints: latency-svc-z59vr [615.700969ms]
    Feb 12 11:27:47.951: INFO: Created: latency-svc-fpv6l
    Feb 12 11:27:47.968: INFO: Got endpoints: latency-svc-cnbhh [622.65361ms]
    Feb 12 11:27:47.987: INFO: Created: latency-svc-2cr5g
    Feb 12 11:27:48.014: INFO: Got endpoints: latency-svc-9kfcx [658.57832ms]
    Feb 12 11:27:48.036: INFO: Created: latency-svc-ff9sf
    Feb 12 11:27:48.073: INFO: Got endpoints: latency-svc-gd24d [704.251347ms]
    Feb 12 11:27:48.097: INFO: Created: latency-svc-w868m
    Feb 12 11:27:48.116: INFO: Got endpoints: latency-svc-ll96q [714.847679ms]
    Feb 12 11:27:48.143: INFO: Created: latency-svc-p7rfw
    Feb 12 11:27:48.178: INFO: Got endpoints: latency-svc-tzwd4 [753.053527ms]
    Feb 12 11:27:48.199: INFO: Created: latency-svc-ztpgw
    Feb 12 11:27:48.219: INFO: Got endpoints: latency-svc-dlxsl [742.103242ms]
    Feb 12 11:27:48.237: INFO: Created: latency-svc-8rwsn
    Feb 12 11:27:48.266: INFO: Got endpoints: latency-svc-bmsxl [747.585017ms]
    Feb 12 11:27:48.292: INFO: Created: latency-svc-6mbgh
    Feb 12 11:27:48.316: INFO: Got endpoints: latency-svc-xd6r2 [744.642409ms]
    Feb 12 11:27:48.335: INFO: Created: latency-svc-kz8j8
    Feb 12 11:27:48.373: INFO: Got endpoints: latency-svc-qvrf6 [741.266613ms]
    Feb 12 11:27:48.392: INFO: Created: latency-svc-27fd9
    Feb 12 11:27:48.418: INFO: Got endpoints: latency-svc-gj84x [745.787134ms]
    Feb 12 11:27:48.441: INFO: Created: latency-svc-w8mt7
    Feb 12 11:27:48.472: INFO: Got endpoints: latency-svc-jvncz [745.910002ms]
    Feb 12 11:27:48.492: INFO: Created: latency-svc-kph66
    Feb 12 11:27:48.518: INFO: Got endpoints: latency-svc-jlpzg [731.742073ms]
    Feb 12 11:27:48.538: INFO: Created: latency-svc-d7hv9
    Feb 12 11:27:48.576: INFO: Got endpoints: latency-svc-hl724 [746.413485ms]
    Feb 12 11:27:48.599: INFO: Created: latency-svc-jr7k7
    Feb 12 11:27:48.618: INFO: Got endpoints: latency-svc-s47qp [748.768536ms]
    Feb 12 11:27:48.633: INFO: Created: latency-svc-42fdx
    Feb 12 11:27:48.674: INFO: Got endpoints: latency-svc-fpv6l [745.053531ms]
    Feb 12 11:27:48.689: INFO: Created: latency-svc-cwm2f
    Feb 12 11:27:48.717: INFO: Got endpoints: latency-svc-2cr5g [748.793783ms]
    Feb 12 11:27:48.745: INFO: Created: latency-svc-6mrh6
    Feb 12 11:27:48.770: INFO: Got endpoints: latency-svc-ff9sf [755.675926ms]
    Feb 12 11:27:48.785: INFO: Created: latency-svc-mgtwf
    Feb 12 11:27:48.818: INFO: Got endpoints: latency-svc-w868m [743.709972ms]
    Feb 12 11:27:48.835: INFO: Created: latency-svc-vcjpn
    Feb 12 11:27:48.871: INFO: Got endpoints: latency-svc-p7rfw [755.468415ms]
    Feb 12 11:27:48.889: INFO: Created: latency-svc-94z62
    Feb 12 11:27:48.919: INFO: Got endpoints: latency-svc-ztpgw [741.095139ms]
    Feb 12 11:27:48.940: INFO: Created: latency-svc-h5bw7
    Feb 12 11:27:48.968: INFO: Got endpoints: latency-svc-8rwsn [749.028116ms]
    Feb 12 11:27:48.984: INFO: Created: latency-svc-82fs8
    Feb 12 11:27:49.023: INFO: Got endpoints: latency-svc-6mbgh [756.211683ms]
    Feb 12 11:27:49.040: INFO: Created: latency-svc-42cff
    Feb 12 11:27:49.922: INFO: Got endpoints: latency-svc-kz8j8 [1.605531143s]
    Feb 12 11:27:50.547: INFO: Got endpoints: latency-svc-w8mt7 [2.128871568s]
    Feb 12 11:27:51.121: INFO: Got endpoints: latency-svc-kph66 [2.648333517s]
    Feb 12 11:27:51.124: INFO: Got endpoints: latency-svc-27fd9 [2.750692032s]
    Feb 12 11:27:51.760: INFO: Got endpoints: latency-svc-jr7k7 [3.184418647s]
    Feb 12 11:27:51.761: INFO: Got endpoints: latency-svc-d7hv9 [3.242458571s]
    Feb 12 11:27:51.913: INFO: Got endpoints: latency-svc-42fdx [3.294749613s]
    Feb 12 11:27:51.914: INFO: Got endpoints: latency-svc-cwm2f [3.239414259s]
    Feb 12 11:27:51.914: INFO: Got endpoints: latency-svc-6mrh6 [3.196603609s]
    Feb 12 11:27:51.945: INFO: Got endpoints: latency-svc-vcjpn [3.127431488s]
    Feb 12 11:27:52.071: INFO: Got endpoints: latency-svc-94z62 [3.199664485s]
    Feb 12 11:27:52.073: INFO: Got endpoints: latency-svc-mgtwf [3.302614364s]
    Feb 12 11:27:52.112: INFO: Got endpoints: latency-svc-42cff [3.089026369s]
    Feb 12 11:27:52.178: INFO: Created: latency-svc-nhs65
    Feb 12 11:27:52.178: INFO: Got endpoints: latency-svc-h5bw7 [3.259192649s]
    Feb 12 11:27:52.179: INFO: Got endpoints: latency-svc-82fs8 [3.210787762s]
    Feb 12 11:27:52.308: INFO: Got endpoints: latency-svc-nhs65 [2.386204118s]
    Feb 12 11:27:52.402: INFO: Created: latency-svc-n8g5f
    Feb 12 11:27:52.574: INFO: Got endpoints: latency-svc-n8g5f [2.026657425s]
    Feb 12 11:27:52.575: INFO: Created: latency-svc-gxt6f
    Feb 12 11:27:52.650: INFO: Got endpoints: latency-svc-gxt6f [1.528653054s]
    Feb 12 11:27:52.709: INFO: Created: latency-svc-5jmpz
    Feb 12 11:27:52.874: INFO: Got endpoints: latency-svc-5jmpz [1.749523561s]
    Feb 12 11:27:52.880: INFO: Created: latency-svc-s9g7f
    Feb 12 11:27:52.952: INFO: Got endpoints: latency-svc-s9g7f [1.191302362s]
    Feb 12 11:27:52.974: INFO: Created: latency-svc-rqw9v
    Feb 12 11:27:53.027: INFO: Got endpoints: latency-svc-rqw9v [1.266153801s]
    Feb 12 11:27:53.045: INFO: Created: latency-svc-nbbbs
    Feb 12 11:27:53.119: INFO: Got endpoints: latency-svc-nbbbs [1.20563223s]
    Feb 12 11:27:53.148: INFO: Created: latency-svc-q7tx4
    Feb 12 11:27:53.330: INFO: Got endpoints: latency-svc-q7tx4 [1.41574443s]
    Feb 12 11:27:53.331: INFO: Created: latency-svc-g4pvz
    Feb 12 11:27:53.332: INFO: Created: latency-svc-gfvls
    Feb 12 11:27:53.347: INFO: Got endpoints: latency-svc-gfvls [1.4015863s]
    Feb 12 11:27:53.347: INFO: Got endpoints: latency-svc-g4pvz [1.433506318s]
    Feb 12 11:27:53.355: INFO: Created: latency-svc-jbf2r
    Feb 12 11:27:53.369: INFO: Got endpoints: latency-svc-jbf2r [1.297976835s]
    Feb 12 11:27:53.376: INFO: Created: latency-svc-9h8mk
    Feb 12 11:27:53.386: INFO: Created: latency-svc-98ffj
    Feb 12 11:27:53.391: INFO: Got endpoints: latency-svc-9h8mk [1.318254177s]
    Feb 12 11:27:53.400: INFO: Got endpoints: latency-svc-98ffj [1.288029295s]
    Feb 12 11:27:53.414: INFO: Created: latency-svc-wvjsc
    Feb 12 11:27:53.423: INFO: Got endpoints: latency-svc-wvjsc [1.244306519s]
    Feb 12 11:27:53.430: INFO: Created: latency-svc-x4dqq
    Feb 12 11:27:53.439: INFO: Got endpoints: latency-svc-x4dqq [1.26043842s]
    Feb 12 11:27:53.450: INFO: Created: latency-svc-9cc5k
    Feb 12 11:27:53.460: INFO: Got endpoints: latency-svc-9cc5k [1.151850229s]
    Feb 12 11:27:53.465: INFO: Created: latency-svc-vcddb
    Feb 12 11:27:53.482: INFO: Got endpoints: latency-svc-vcddb [908.172204ms]
    Feb 12 11:27:53.490: INFO: Created: latency-svc-4rd9m
    Feb 12 11:27:53.498: INFO: Got endpoints: latency-svc-4rd9m [847.891878ms]
    Feb 12 11:27:53.498: INFO: Created: latency-svc-74q2v
    Feb 12 11:27:53.513: INFO: Got endpoints: latency-svc-74q2v [638.622948ms]
    Feb 12 11:27:53.525: INFO: Created: latency-svc-4sqm8
    Feb 12 11:27:53.546: INFO: Got endpoints: latency-svc-4sqm8 [594.096982ms]
    Feb 12 11:27:53.546: INFO: Created: latency-svc-ljp6x
    Feb 12 11:27:53.561: INFO: Created: latency-svc-nh65q
    Feb 12 11:27:53.566: INFO: Got endpoints: latency-svc-ljp6x [538.441935ms]
    Feb 12 11:27:53.574: INFO: Got endpoints: latency-svc-nh65q [455.099889ms]
    Feb 12 11:27:53.587: INFO: Created: latency-svc-pgn85
    Feb 12 11:27:53.604: INFO: Got endpoints: latency-svc-pgn85 [273.899505ms]
    Feb 12 11:27:53.618: INFO: Created: latency-svc-vpglf
    Feb 12 11:27:53.630: INFO: Got endpoints: latency-svc-vpglf [282.673737ms]
    Feb 12 11:27:53.632: INFO: Created: latency-svc-qjj6h
    Feb 12 11:27:53.648: INFO: Created: latency-svc-gq9gp
    Feb 12 11:27:53.653: INFO: Got endpoints: latency-svc-qjj6h [306.217559ms]
    Feb 12 11:27:53.663: INFO: Got endpoints: latency-svc-gq9gp [294.108047ms]
    Feb 12 11:27:53.680: INFO: Created: latency-svc-q76wv
    Feb 12 11:27:53.687: INFO: Got endpoints: latency-svc-q76wv [296.305355ms]
    Feb 12 11:27:53.688: INFO: Created: latency-svc-xmj7h
    Feb 12 11:27:53.709: INFO: Got endpoints: latency-svc-xmj7h [309.140526ms]
    Feb 12 11:27:53.711: INFO: Created: latency-svc-swdkq
    Feb 12 11:27:53.727: INFO: Got endpoints: latency-svc-swdkq [304.245514ms]
    Feb 12 11:27:53.734: INFO: Created: latency-svc-9hmmd
    Feb 12 11:27:53.745: INFO: Got endpoints: latency-svc-9hmmd [305.826229ms]
    Feb 12 11:27:53.751: INFO: Created: latency-svc-sqzzn
    Feb 12 11:27:53.770: INFO: Got endpoints: latency-svc-sqzzn [309.954195ms]
    Feb 12 11:27:53.780: INFO: Created: latency-svc-45lrg
    Feb 12 11:27:53.786: INFO: Got endpoints: latency-svc-45lrg [304.289984ms]
    Feb 12 11:27:53.799: INFO: Created: latency-svc-997x8
    Feb 12 11:27:53.812: INFO: Got endpoints: latency-svc-997x8 [313.751961ms]
    Feb 12 11:27:53.812: INFO: Created: latency-svc-ngqhx
    Feb 12 11:27:53.833: INFO: Got endpoints: latency-svc-ngqhx [319.888754ms]
    Feb 12 11:27:53.845: INFO: Created: latency-svc-nzqdf
    Feb 12 11:27:53.858: INFO: Got endpoints: latency-svc-nzqdf [311.845854ms]
    Feb 12 11:27:53.865: INFO: Created: latency-svc-rhvt7
    Feb 12 11:27:53.877: INFO: Got endpoints: latency-svc-rhvt7 [310.501748ms]
    Feb 12 11:27:53.877: INFO: Created: latency-svc-mcm8d
    Feb 12 11:27:53.900: INFO: Got endpoints: latency-svc-mcm8d [325.762067ms]
    Feb 12 11:27:53.903: INFO: Created: latency-svc-fp8cj
    Feb 12 11:27:53.920: INFO: Got endpoints: latency-svc-fp8cj [315.776982ms]
    Feb 12 11:27:53.928: INFO: Created: latency-svc-85r5d
    Feb 12 11:27:53.942: INFO: Created: latency-svc-j6kjk
    Feb 12 11:27:53.949: INFO: Got endpoints: latency-svc-85r5d [318.841511ms]
    Feb 12 11:27:53.959: INFO: Got endpoints: latency-svc-j6kjk [305.859723ms]
    Feb 12 11:27:53.967: INFO: Created: latency-svc-s7ggq
    Feb 12 11:27:53.981: INFO: Got endpoints: latency-svc-s7ggq [317.896477ms]
    Feb 12 11:27:53.989: INFO: Created: latency-svc-drcg5
    Feb 12 11:27:54.001: INFO: Got endpoints: latency-svc-drcg5 [313.787835ms]
    Feb 12 11:27:54.007: INFO: Created: latency-svc-g6q5r
    Feb 12 11:27:54.026: INFO: Created: latency-svc-rqmwn
    Feb 12 11:27:54.026: INFO: Got endpoints: latency-svc-g6q5r [316.423534ms]
    Feb 12 11:27:54.041: INFO: Got endpoints: latency-svc-rqmwn [313.921897ms]
    Feb 12 11:27:54.043: INFO: Created: latency-svc-l6nwt
    Feb 12 11:27:54.068: INFO: Created: latency-svc-2dfcd
    Feb 12 11:27:54.078: INFO: Got endpoints: latency-svc-l6nwt [333.054314ms]
    Feb 12 11:27:54.079: INFO: Got endpoints: latency-svc-2dfcd [308.458632ms]
    Feb 12 11:27:54.097: INFO: Created: latency-svc-4j92b
    Feb 12 11:27:54.125: INFO: Got endpoints: latency-svc-4j92b [337.955707ms]
    Feb 12 11:27:54.136: INFO: Created: latency-svc-8vnnr
    Feb 12 11:27:54.136: INFO: Created: latency-svc-892td
    Feb 12 11:27:54.143: INFO: Got endpoints: latency-svc-8vnnr [331.544014ms]
    Feb 12 11:27:54.154: INFO: Got endpoints: latency-svc-892td [320.632706ms]
    Feb 12 11:27:54.166: INFO: Created: latency-svc-7j8lt
    Feb 12 11:27:54.186: INFO: Got endpoints: latency-svc-7j8lt [328.338835ms]
    Feb 12 11:27:54.196: INFO: Created: latency-svc-hhrs9
    Feb 12 11:27:54.212: INFO: Got endpoints: latency-svc-hhrs9 [335.468375ms]
    Feb 12 11:27:54.217: INFO: Created: latency-svc-n4xt6
    Feb 12 11:27:54.235: INFO: Got endpoints: latency-svc-n4xt6 [334.937228ms]
    Feb 12 11:27:54.238: INFO: Created: latency-svc-548sm
    Feb 12 11:27:54.253: INFO: Got endpoints: latency-svc-548sm [333.645097ms]
    Feb 12 11:27:54.254: INFO: Created: latency-svc-phld8
    Feb 12 11:27:54.267: INFO: Got endpoints: latency-svc-phld8 [318.371109ms]
    Feb 12 11:27:54.268: INFO: Latencies: [38.988729ms 52.079447ms 71.834508ms 89.474911ms 117.149596ms 132.225482ms 144.219321ms 168.111507ms 197.029729ms 206.433983ms 241.411417ms 266.534177ms 273.899505ms 282.673737ms 294.108047ms 296.305355ms 304.245514ms 304.289984ms 305.826229ms 305.859723ms 306.217559ms 308.458632ms 308.662674ms 309.140526ms 309.954195ms 310.501748ms 311.845854ms 313.003222ms 313.751961ms 313.787835ms 313.921897ms 315.776982ms 316.256315ms 316.423534ms 317.896477ms 318.371109ms 318.841511ms 319.888754ms 320.06446ms 320.632706ms 321.321216ms 322.326357ms 322.662653ms 322.940366ms 323.896979ms 324.11056ms 325.762067ms 325.806098ms 327.459658ms 327.62053ms 328.318468ms 328.338835ms 328.750261ms 328.955746ms 329.240707ms 329.45072ms 329.794249ms 329.974134ms 331.093357ms 331.418762ms 331.544014ms 331.702604ms 332.40158ms 333.054314ms 333.089094ms 333.333534ms 333.645097ms 334.01887ms 334.937228ms 335.468375ms 335.503998ms 337.028762ms 337.464104ms 337.955707ms 337.970719ms 338.950954ms 339.899762ms 341.454442ms 342.774002ms 344.518686ms 344.598219ms 345.288414ms 345.633437ms 345.929813ms 346.410829ms 346.505741ms 348.611007ms 349.044112ms 349.181006ms 350.79008ms 352.098612ms 355.410909ms 355.601666ms 356.658087ms 358.14965ms 360.766929ms 362.37643ms 363.2159ms 364.262711ms 364.890478ms 366.24685ms 367.343121ms 367.974178ms 369.326482ms 369.936217ms 372.944678ms 377.939715ms 404.234142ms 432.662027ms 445.874564ms 455.099889ms 468.91603ms 502.943911ms 504.975839ms 527.421177ms 538.441935ms 548.787771ms 550.350786ms 577.594009ms 594.096982ms 615.700969ms 622.65361ms 626.734467ms 638.622948ms 658.57832ms 704.251347ms 714.847679ms 731.742073ms 741.095139ms 741.266613ms 742.103242ms 743.709972ms 744.642409ms 745.053531ms 745.787134ms 745.910002ms 746.413485ms 747.585017ms 748.768536ms 748.793783ms 749.028116ms 753.053527ms 755.468415ms 755.675926ms 756.211683ms 774.108835ms 820.291738ms 826.487279ms 847.891878ms 908.172204ms 974.235973ms 1.116256867s 1.151850229s 1.191302362s 1.20563223s 1.244306519s 1.26043842s 1.266153801s 1.288029295s 1.297976835s 1.318254177s 1.351256115s 1.4015863s 1.41574443s 1.433506318s 1.528653054s 1.549257634s 1.605531143s 1.749523561s 1.772669703s 1.961826064s 2.026657425s 2.128871568s 2.195585381s 2.339594555s 2.386204118s 2.492942339s 2.496540359s 2.547764212s 2.648333517s 2.750692032s 2.754068124s 2.810575508s 2.885084681s 2.980190692s 3.002422459s 3.004027339s 3.013036541s 3.015342977s 3.089026369s 3.127431488s 3.184418647s 3.196603609s 3.199664485s 3.210787762s 3.239414259s 3.242458571s 3.259192649s 3.294749613s 3.302614364s]
    Feb 12 11:27:54.268: INFO: 50 %ile: 366.24685ms
    Feb 12 11:27:54.268: INFO: 90 %ile: 2.750692032s
    Feb 12 11:27:54.268: INFO: 99 %ile: 3.294749613s
    Feb 12 11:27:54.268: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Feb 12 11:27:54.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5527" for this suite. 02/12/23 11:27:54.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:27:54.288
Feb 12 11:27:54.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:27:54.289
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:54.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:54.321
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 02/12/23 11:27:54.325
Feb 12 11:27:54.325: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4272 proxy --unix-socket=/tmp/kubectl-proxy-unix4232542558/test'
STEP: retrieving proxy /api/ output 02/12/23 11:27:54.381
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:27:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4272" for this suite. 02/12/23 11:27:54.387
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":76,"skipped":1363,"failed":0}
------------------------------
 [0.111 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:27:54.288
    Feb 12 11:27:54.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:27:54.289
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:54.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:54.321
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 02/12/23 11:27:54.325
    Feb 12 11:27:54.325: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4272 proxy --unix-socket=/tmp/kubectl-proxy-unix4232542558/test'
    STEP: retrieving proxy /api/ output 02/12/23 11:27:54.381
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:27:54.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4272" for this suite. 02/12/23 11:27:54.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:27:54.401
Feb 12 11:27:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:27:54.402
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:54.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:54.43
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 02/12/23 11:27:54.434
Feb 12 11:27:54.448: INFO: Waiting up to 5m0s for pod "pod-rkqtf" in namespace "pods-8006" to be "running"
Feb 12 11:27:54.457: INFO: Pod "pod-rkqtf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.166856ms
Feb 12 11:27:56.467: INFO: Pod "pod-rkqtf": Phase="Running", Reason="", readiness=true. Elapsed: 2.019111961s
Feb 12 11:27:56.467: INFO: Pod "pod-rkqtf" satisfied condition "running"
STEP: patching /status 02/12/23 11:27:56.467
Feb 12 11:27:56.484: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:27:56.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8006" for this suite. 02/12/23 11:27:56.488
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":77,"skipped":1391,"failed":0}
------------------------------
 [2.095 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:27:54.401
    Feb 12 11:27:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:27:54.402
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:54.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:54.43
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 02/12/23 11:27:54.434
    Feb 12 11:27:54.448: INFO: Waiting up to 5m0s for pod "pod-rkqtf" in namespace "pods-8006" to be "running"
    Feb 12 11:27:54.457: INFO: Pod "pod-rkqtf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.166856ms
    Feb 12 11:27:56.467: INFO: Pod "pod-rkqtf": Phase="Running", Reason="", readiness=true. Elapsed: 2.019111961s
    Feb 12 11:27:56.467: INFO: Pod "pod-rkqtf" satisfied condition "running"
    STEP: patching /status 02/12/23 11:27:56.467
    Feb 12 11:27:56.484: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:27:56.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8006" for this suite. 02/12/23 11:27:56.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:27:56.497
Feb 12 11:27:56.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:27:56.498
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:56.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:56.517
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 02/12/23 11:27:56.518
Feb 12 11:27:56.526: INFO: Waiting up to 5m0s for pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86" in namespace "downward-api-9457" to be "running and ready"
Feb 12 11:27:56.530: INFO: Pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377599ms
Feb 12 11:27:56.530: INFO: The phase of Pod annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:27:58.543: INFO: Pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86": Phase="Running", Reason="", readiness=true. Elapsed: 2.017103775s
Feb 12 11:27:58.543: INFO: The phase of Pod annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86 is Running (Ready = true)
Feb 12 11:27:58.543: INFO: Pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86" satisfied condition "running and ready"
Feb 12 11:27:59.094: INFO: Successfully updated pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:28:02.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9457" for this suite. 02/12/23 11:28:02.816
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":78,"skipped":1410,"failed":0}
------------------------------
 [SLOW TEST] [6.521 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:27:56.497
    Feb 12 11:27:56.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:27:56.498
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:27:56.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:27:56.517
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 02/12/23 11:27:56.518
    Feb 12 11:27:56.526: INFO: Waiting up to 5m0s for pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86" in namespace "downward-api-9457" to be "running and ready"
    Feb 12 11:27:56.530: INFO: Pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377599ms
    Feb 12 11:27:56.530: INFO: The phase of Pod annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:27:58.543: INFO: Pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86": Phase="Running", Reason="", readiness=true. Elapsed: 2.017103775s
    Feb 12 11:27:58.543: INFO: The phase of Pod annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86 is Running (Ready = true)
    Feb 12 11:27:58.543: INFO: Pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86" satisfied condition "running and ready"
    Feb 12 11:27:59.094: INFO: Successfully updated pod "annotationupdate47d5190a-341b-4391-9efc-9e10618b3d86"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:28:02.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9457" for this suite. 02/12/23 11:28:02.816
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:03.02
Feb 12 11:28:03.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:28:03.021
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:03.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:03.407
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Feb 12 11:28:03.463: INFO: Got root ca configmap in namespace "svcaccounts-3224"
Feb 12 11:28:03.602: INFO: Deleted root ca configmap in namespace "svcaccounts-3224"
STEP: waiting for a new root ca configmap created 02/12/23 11:28:04.103
Feb 12 11:28:04.108: INFO: Recreated root ca configmap in namespace "svcaccounts-3224"
Feb 12 11:28:04.135: INFO: Updated root ca configmap in namespace "svcaccounts-3224"
STEP: waiting for the root ca configmap reconciled 02/12/23 11:28:04.636
Feb 12 11:28:04.648: INFO: Reconciled root ca configmap in namespace "svcaccounts-3224"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 12 11:28:04.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3224" for this suite. 02/12/23 11:28:04.66
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":79,"skipped":1412,"failed":0}
------------------------------
 [2.599 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:03.02
    Feb 12 11:28:03.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:28:03.021
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:03.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:03.407
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Feb 12 11:28:03.463: INFO: Got root ca configmap in namespace "svcaccounts-3224"
    Feb 12 11:28:03.602: INFO: Deleted root ca configmap in namespace "svcaccounts-3224"
    STEP: waiting for a new root ca configmap created 02/12/23 11:28:04.103
    Feb 12 11:28:04.108: INFO: Recreated root ca configmap in namespace "svcaccounts-3224"
    Feb 12 11:28:04.135: INFO: Updated root ca configmap in namespace "svcaccounts-3224"
    STEP: waiting for the root ca configmap reconciled 02/12/23 11:28:04.636
    Feb 12 11:28:04.648: INFO: Reconciled root ca configmap in namespace "svcaccounts-3224"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 12 11:28:04.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3224" for this suite. 02/12/23 11:28:04.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:05.63
Feb 12 11:28:05.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:28:05.631
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:07.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:07.678
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:28:07.862
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:28:08.356
STEP: Deploying the webhook pod 02/12/23 11:28:08.486
STEP: Wait for the deployment to be ready 02/12/23 11:28:08.614
Feb 12 11:28:08.660: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 12 11:28:10.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/12/23 11:28:12.696
STEP: Verifying the service has paired with the endpoint 02/12/23 11:28:12.745
Feb 12 11:28:13.746: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Feb 12 11:28:14.746: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 02/12/23 11:28:14.751
STEP: create a pod that should be denied by the webhook 02/12/23 11:28:14.788
STEP: create a pod that causes the webhook to hang 02/12/23 11:28:14.813
STEP: create a configmap that should be denied by the webhook 02/12/23 11:28:24.834
STEP: create a configmap that should be admitted by the webhook 02/12/23 11:28:24.909
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/12/23 11:28:24.923
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/12/23 11:28:24.931
STEP: create a namespace that bypass the webhook 02/12/23 11:28:24.937
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/12/23 11:28:24.944
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:28:24.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6859" for this suite. 02/12/23 11:28:24.973
STEP: Destroying namespace "webhook-6859-markers" for this suite. 02/12/23 11:28:24.98
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":80,"skipped":1444,"failed":0}
------------------------------
 [SLOW TEST] [19.432 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:05.63
    Feb 12 11:28:05.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:28:05.631
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:07.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:07.678
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:28:07.862
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:28:08.356
    STEP: Deploying the webhook pod 02/12/23 11:28:08.486
    STEP: Wait for the deployment to be ready 02/12/23 11:28:08.614
    Feb 12 11:28:08.660: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Feb 12 11:28:10.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 11, 28, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/12/23 11:28:12.696
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:28:12.745
    Feb 12 11:28:13.746: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Feb 12 11:28:14.746: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 02/12/23 11:28:14.751
    STEP: create a pod that should be denied by the webhook 02/12/23 11:28:14.788
    STEP: create a pod that causes the webhook to hang 02/12/23 11:28:14.813
    STEP: create a configmap that should be denied by the webhook 02/12/23 11:28:24.834
    STEP: create a configmap that should be admitted by the webhook 02/12/23 11:28:24.909
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/12/23 11:28:24.923
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/12/23 11:28:24.931
    STEP: create a namespace that bypass the webhook 02/12/23 11:28:24.937
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/12/23 11:28:24.944
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:28:24.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6859" for this suite. 02/12/23 11:28:24.973
    STEP: Destroying namespace "webhook-6859-markers" for this suite. 02/12/23 11:28:24.98
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:25.064
Feb 12 11:28:25.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename podtemplate 02/12/23 11:28:25.065
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:25.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:25.112
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/12/23 11:28:25.117
STEP: Replace a pod template 02/12/23 11:28:25.127
Feb 12 11:28:25.156: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 12 11:28:25.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3785" for this suite. 02/12/23 11:28:25.163
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":81,"skipped":1457,"failed":0}
------------------------------
 [0.120 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:25.064
    Feb 12 11:28:25.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename podtemplate 02/12/23 11:28:25.065
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:25.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:25.112
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/12/23 11:28:25.117
    STEP: Replace a pod template 02/12/23 11:28:25.127
    Feb 12 11:28:25.156: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 12 11:28:25.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3785" for this suite. 02/12/23 11:28:25.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:25.192
Feb 12 11:28:25.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:28:25.193
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:25.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:25.224
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c681f763-420e-4dc4-8daa-0735255317bd 02/12/23 11:28:25.233
STEP: Creating the pod 02/12/23 11:28:25.241
Feb 12 11:28:25.254: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9" in namespace "projected-9511" to be "running and ready"
Feb 12 11:28:25.264: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.432337ms
Feb 12 11:28:25.266: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:28:29.097: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842362758s
Feb 12 11:28:29.097: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:28:30.073: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818806069s
Feb 12 11:28:30.073: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:28:31.279: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Running", Reason="", readiness=true. Elapsed: 6.02437739s
Feb 12 11:28:31.279: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Running (Ready = true)
Feb 12 11:28:31.279: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-c681f763-420e-4dc4-8daa-0735255317bd 02/12/23 11:28:31.287
STEP: waiting to observe update in volume 02/12/23 11:28:31.306
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 11:28:35.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9511" for this suite. 02/12/23 11:28:35.395
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":82,"skipped":1490,"failed":0}
------------------------------
 [SLOW TEST] [10.222 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:25.192
    Feb 12 11:28:25.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:28:25.193
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:25.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:25.224
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-c681f763-420e-4dc4-8daa-0735255317bd 02/12/23 11:28:25.233
    STEP: Creating the pod 02/12/23 11:28:25.241
    Feb 12 11:28:25.254: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9" in namespace "projected-9511" to be "running and ready"
    Feb 12 11:28:25.264: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.432337ms
    Feb 12 11:28:25.266: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:28:29.097: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.842362758s
    Feb 12 11:28:29.097: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:28:30.073: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818806069s
    Feb 12 11:28:30.073: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:28:31.279: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9": Phase="Running", Reason="", readiness=true. Elapsed: 6.02437739s
    Feb 12 11:28:31.279: INFO: The phase of Pod pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9 is Running (Ready = true)
    Feb 12 11:28:31.279: INFO: Pod "pod-projected-configmaps-b0aa6e71-7a09-4848-aa64-db0c756fffc9" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-c681f763-420e-4dc4-8daa-0735255317bd 02/12/23 11:28:31.287
    STEP: waiting to observe update in volume 02/12/23 11:28:31.306
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 11:28:35.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9511" for this suite. 02/12/23 11:28:35.395
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:35.415
Feb 12 11:28:35.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:28:35.416
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:35.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:35.441
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Feb 12 11:28:35.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: creating the pod 02/12/23 11:28:35.444
STEP: submitting the pod to kubernetes 02/12/23 11:28:35.444
Feb 12 11:28:35.450: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c" in namespace "pods-1634" to be "running and ready"
Feb 12 11:28:35.466: INFO: Pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.465846ms
Feb 12 11:28:35.466: INFO: The phase of Pod pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:28:37.475: INFO: Pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.025172805s
Feb 12 11:28:37.476: INFO: The phase of Pod pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c is Running (Ready = true)
Feb 12 11:28:37.476: INFO: Pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:28:37.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1634" for this suite. 02/12/23 11:28:37.629
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":83,"skipped":1494,"failed":0}
------------------------------
 [2.222 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:35.415
    Feb 12 11:28:35.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:28:35.416
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:35.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:35.441
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Feb 12 11:28:35.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: creating the pod 02/12/23 11:28:35.444
    STEP: submitting the pod to kubernetes 02/12/23 11:28:35.444
    Feb 12 11:28:35.450: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c" in namespace "pods-1634" to be "running and ready"
    Feb 12 11:28:35.466: INFO: Pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.465846ms
    Feb 12 11:28:35.466: INFO: The phase of Pod pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:28:37.475: INFO: Pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.025172805s
    Feb 12 11:28:37.476: INFO: The phase of Pod pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c is Running (Ready = true)
    Feb 12 11:28:37.476: INFO: Pod "pod-exec-websocket-55a40b4d-85ff-42c8-9420-37990db2cd4c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:28:37.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1634" for this suite. 02/12/23 11:28:37.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:37.642
Feb 12 11:28:37.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:28:37.643
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:37.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:37.666
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-1e26d447-4459-4479-85f1-1be59e2685a2 02/12/23 11:28:37.668
STEP: Creating a pod to test consume configMaps 02/12/23 11:28:37.673
Feb 12 11:28:37.685: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75" in namespace "projected-7904" to be "Succeeded or Failed"
Feb 12 11:28:37.694: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75": Phase="Pending", Reason="", readiness=false. Elapsed: 8.972681ms
Feb 12 11:28:39.704: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018791413s
Feb 12 11:28:41.707: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021811204s
STEP: Saw pod success 02/12/23 11:28:41.707
Feb 12 11:28:41.707: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75" satisfied condition "Succeeded or Failed"
Feb 12 11:28:41.711: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:28:41.718
Feb 12 11:28:41.732: INFO: Waiting for pod pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75 to disappear
Feb 12 11:28:41.736: INFO: Pod pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 11:28:41.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7904" for this suite. 02/12/23 11:28:41.739
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1550,"failed":0}
------------------------------
 [4.104 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:37.642
    Feb 12 11:28:37.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:28:37.643
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:37.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:37.666
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-1e26d447-4459-4479-85f1-1be59e2685a2 02/12/23 11:28:37.668
    STEP: Creating a pod to test consume configMaps 02/12/23 11:28:37.673
    Feb 12 11:28:37.685: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75" in namespace "projected-7904" to be "Succeeded or Failed"
    Feb 12 11:28:37.694: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75": Phase="Pending", Reason="", readiness=false. Elapsed: 8.972681ms
    Feb 12 11:28:39.704: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018791413s
    Feb 12 11:28:41.707: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021811204s
    STEP: Saw pod success 02/12/23 11:28:41.707
    Feb 12 11:28:41.707: INFO: Pod "pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75" satisfied condition "Succeeded or Failed"
    Feb 12 11:28:41.711: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:28:41.718
    Feb 12 11:28:41.732: INFO: Waiting for pod pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75 to disappear
    Feb 12 11:28:41.736: INFO: Pod pod-projected-configmaps-af917f12-b83b-4305-ba25-1f43b4b6cc75 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 11:28:41.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7904" for this suite. 02/12/23 11:28:41.739
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:41.747
Feb 12 11:28:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:28:41.748
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:41.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:41.779
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-187e55f2-f754-470b-888b-7ef14cff9a6d 02/12/23 11:28:41.783
STEP: Creating configMap with name cm-test-opt-upd-59893ea7-aa82-4a84-9634-9364d82688d7 02/12/23 11:28:41.788
STEP: Creating the pod 02/12/23 11:28:41.792
Feb 12 11:28:41.807: INFO: Waiting up to 5m0s for pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a" in namespace "configmap-3818" to be "running and ready"
Feb 12 11:28:41.830: INFO: Pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.669722ms
Feb 12 11:28:41.830: INFO: The phase of Pod pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:28:43.844: INFO: Pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a": Phase="Running", Reason="", readiness=true. Elapsed: 2.036861346s
Feb 12 11:28:43.844: INFO: The phase of Pod pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a is Running (Ready = true)
Feb 12 11:28:43.845: INFO: Pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-187e55f2-f754-470b-888b-7ef14cff9a6d 02/12/23 11:28:43.904
STEP: Updating configmap cm-test-opt-upd-59893ea7-aa82-4a84-9634-9364d82688d7 02/12/23 11:28:43.912
STEP: Creating configMap with name cm-test-opt-create-24ba7cc7-170c-46af-b153-041728d3c542 02/12/23 11:28:43.917
STEP: waiting to observe update in volume 02/12/23 11:28:43.922
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:28:45.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3818" for this suite. 02/12/23 11:28:45.948
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":85,"skipped":1550,"failed":0}
------------------------------
 [4.211 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:41.747
    Feb 12 11:28:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:28:41.748
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:41.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:41.779
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-187e55f2-f754-470b-888b-7ef14cff9a6d 02/12/23 11:28:41.783
    STEP: Creating configMap with name cm-test-opt-upd-59893ea7-aa82-4a84-9634-9364d82688d7 02/12/23 11:28:41.788
    STEP: Creating the pod 02/12/23 11:28:41.792
    Feb 12 11:28:41.807: INFO: Waiting up to 5m0s for pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a" in namespace "configmap-3818" to be "running and ready"
    Feb 12 11:28:41.830: INFO: Pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.669722ms
    Feb 12 11:28:41.830: INFO: The phase of Pod pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:28:43.844: INFO: Pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a": Phase="Running", Reason="", readiness=true. Elapsed: 2.036861346s
    Feb 12 11:28:43.844: INFO: The phase of Pod pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a is Running (Ready = true)
    Feb 12 11:28:43.845: INFO: Pod "pod-configmaps-585fed28-78db-4e19-bb60-5fa10879d50a" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-187e55f2-f754-470b-888b-7ef14cff9a6d 02/12/23 11:28:43.904
    STEP: Updating configmap cm-test-opt-upd-59893ea7-aa82-4a84-9634-9364d82688d7 02/12/23 11:28:43.912
    STEP: Creating configMap with name cm-test-opt-create-24ba7cc7-170c-46af-b153-041728d3c542 02/12/23 11:28:43.917
    STEP: waiting to observe update in volume 02/12/23 11:28:43.922
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:28:45.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3818" for this suite. 02/12/23 11:28:45.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:28:45.96
Feb 12 11:28:45.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 11:28:45.962
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:45.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:45.984
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/12/23 11:28:45.988
STEP: waiting for Deployment to be created 02/12/23 11:28:45.994
STEP: waiting for all Replicas to be Ready 02/12/23 11:28:45.996
Feb 12 11:28:45.997: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:45.997: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:46.008: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:46.008: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:46.044: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:46.044: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:46.066: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:46.066: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 12 11:28:47.337: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 12 11:28:47.337: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 12 11:28:47.743: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/12/23 11:28:47.743
W0212 11:28:47.755445      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 12 11:28:47.756: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/12/23 11:28:47.756
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.759: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.771: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.772: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.796: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.796: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:47.810: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:28:47.810: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:28:47.844: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:28:47.844: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:28:49.336: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:49.336: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:28:49.376: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
STEP: listing Deployments 02/12/23 11:28:49.376
Feb 12 11:28:49.381: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/12/23 11:28:49.381
Feb 12 11:28:56.953: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/12/23 11:28:56.953
Feb 12 11:28:59.503: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:28:59.503: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:28:59.581: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:28:59.614: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:28:59.633: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:29:04.368: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:29:04.840: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:29:04.906: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:29:05.574: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 12 11:29:09.521: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/12/23 11:29:09.58
STEP: fetching the DeploymentStatus 02/12/23 11:29:09.599
Feb 12 11:29:09.606: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3
STEP: deleting the Deployment 02/12/23 11:29:09.611
Feb 12 11:29:09.623: INFO: observed event type MODIFIED
Feb 12 11:29:09.624: INFO: observed event type MODIFIED
Feb 12 11:29:09.624: INFO: observed event type MODIFIED
Feb 12 11:29:09.624: INFO: observed event type MODIFIED
Feb 12 11:29:09.624: INFO: observed event type MODIFIED
Feb 12 11:29:09.624: INFO: observed event type MODIFIED
Feb 12 11:29:09.624: INFO: observed event type MODIFIED
Feb 12 11:29:09.625: INFO: observed event type MODIFIED
Feb 12 11:29:09.625: INFO: observed event type MODIFIED
Feb 12 11:29:09.625: INFO: observed event type MODIFIED
Feb 12 11:29:09.625: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 11:29:09.633: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 12 11:29:09.638: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-537  88f7eb35-1309-47ea-858c-14cb8ea00728 10534 4 2023-02-12 11:28:47 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 39dea732-6b1b-46b4-8b59-b315f14bd703 0xc002a4a767 0xc002a4a768}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39dea732-6b1b-46b4-8b59-b315f14bd703\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4a7f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 12 11:29:09.646: INFO: pod: "test-deployment-54cc775c4b-292ng":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-292ng test-deployment-54cc775c4b- deployment-537  af98d233-099a-41e5-b0ae-0c6268f135ff 10530 0 2023-02-12 11:28:47 +0000 UTC 2023-02-12 11:29:10 +0000 UTC 0xc002a4b3f8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:5a18037a6bcbbc48720a3e353ac63d8eeaf7a32df9d4f7c9aa1d968cfa4498be cni.projectcalico.org/podIP:10.233.120.74/32 cni.projectcalico.org/podIPs:10.233.120.74/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 88f7eb35-1309-47ea-858c-14cb8ea00728 0xc002a4b447 0xc002a4b448}] [] [{kube-controller-manager Update v1 2023-02-12 11:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88f7eb35-1309-47ea-858c-14cb8ea00728\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 11:28:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 11:28:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9h86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9h86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.74,StartTime:2023-02-12 11:28:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:28:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:sha256:4873874c08efc72e9729683a83ffbb7502ee729e9a5ac097723806ea7fa13517,ContainerID:containerd://3a8426c088899c7e467992b1778440cd57066eaf16c25ffc8ff9cf5ee64c9e55,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 12 11:29:09.646: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-537  a4086b3e-447e-4080-8c52-20f555c41102 10526 2 2023-02-12 11:28:59 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 39dea732-6b1b-46b4-8b59-b315f14bd703 0xc002a4a857 0xc002a4a858}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:29:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39dea732-6b1b-46b4-8b59-b315f14bd703\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4a8e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 12 11:29:09.652: INFO: pod: "test-deployment-7c7d8d58c8-2vpjh":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2vpjh test-deployment-7c7d8d58c8- deployment-537  fb2c24b0-e29c-4b36-aa1e-faa297c4d49d 10483 0 2023-02-12 11:28:59 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b23e8061e9cdb5b54b32bf32ee0e8a4aaffed3dfa8ba1db111b51ab7653b42b cni.projectcalico.org/podIP:10.233.120.204/32 cni.projectcalico.org/podIPs:10.233.120.204/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a4086b3e-447e-4080-8c52-20f555c41102 0xc002ac2737 0xc002ac2738}] [] [{kube-controller-manager Update v1 2023-02-12 11:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4086b3e-447e-4080-8c52-20f555c41102\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 11:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 11:29:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk4td,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk4td,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.204,StartTime:2023-02-12 11:28:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:29:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cdfd93b24dd109726cef08ebc6e5d298da234def34a69e002ea9bce1e8396478,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 12 11:29:09.652: INFO: pod: "test-deployment-7c7d8d58c8-csv2s":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-csv2s test-deployment-7c7d8d58c8- deployment-537  fcca86ac-5b75-4b88-bfaa-9dcb98eef704 10525 0 2023-02-12 11:29:04 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:50df625cb59e52f3ed64d296ab8e87e7d3a0a06e18ee95eb319e893d0e5f905e cni.projectcalico.org/podIP:10.233.99.91/32 cni.projectcalico.org/podIPs:10.233.99.91/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a4086b3e-447e-4080-8c52-20f555c41102 0xc002ac2967 0xc002ac2968}] [] [{kube-controller-manager Update v1 2023-02-12 11:29:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4086b3e-447e-4080-8c52-20f555c41102\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 11:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbt5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbt5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.91,StartTime:2023-02-12 11:29:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9fcf436add9476555fe2df3d6adaaf4cf6edb0db0be12d71d47282f37eced377,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 12 11:29:09.652: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-537  2bc53e6f-491f-421e-9c8a-512c1d699328 10390 3 2023-02-12 11:28:45 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 39dea732-6b1b-46b4-8b59-b315f14bd703 0xc002a4a947 0xc002a4a948}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:28:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39dea732-6b1b-46b4-8b59-b315f14bd703\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:28:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4a9d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 11:29:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-537" for this suite. 02/12/23 11:29:09.709
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":86,"skipped":1561,"failed":0}
------------------------------
 [SLOW TEST] [23.774 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:28:45.96
    Feb 12 11:28:45.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 11:28:45.962
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:28:45.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:28:45.984
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/12/23 11:28:45.988
    STEP: waiting for Deployment to be created 02/12/23 11:28:45.994
    STEP: waiting for all Replicas to be Ready 02/12/23 11:28:45.996
    Feb 12 11:28:45.997: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:45.997: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:46.008: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:46.008: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:46.044: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:46.044: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:46.066: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:46.066: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 12 11:28:47.337: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 12 11:28:47.337: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 12 11:28:47.743: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/12/23 11:28:47.743
    W0212 11:28:47.755445      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 12 11:28:47.756: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/12/23 11:28:47.756
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 0
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.758: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.759: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.771: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.772: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.796: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.796: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:47.810: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:28:47.810: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:28:47.844: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:28:47.844: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:28:49.336: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:49.336: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:28:49.376: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    STEP: listing Deployments 02/12/23 11:28:49.376
    Feb 12 11:28:49.381: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/12/23 11:28:49.381
    Feb 12 11:28:56.953: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/12/23 11:28:56.953
    Feb 12 11:28:59.503: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:28:59.503: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:28:59.581: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:28:59.614: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:28:59.633: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:29:04.368: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:29:04.840: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:29:04.906: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:29:05.574: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 12 11:29:09.521: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/12/23 11:29:09.58
    STEP: fetching the DeploymentStatus 02/12/23 11:29:09.599
    Feb 12 11:29:09.606: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 1
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 2
    Feb 12 11:29:09.610: INFO: observed Deployment test-deployment in namespace deployment-537 with ReadyReplicas 3
    STEP: deleting the Deployment 02/12/23 11:29:09.611
    Feb 12 11:29:09.623: INFO: observed event type MODIFIED
    Feb 12 11:29:09.624: INFO: observed event type MODIFIED
    Feb 12 11:29:09.624: INFO: observed event type MODIFIED
    Feb 12 11:29:09.624: INFO: observed event type MODIFIED
    Feb 12 11:29:09.624: INFO: observed event type MODIFIED
    Feb 12 11:29:09.624: INFO: observed event type MODIFIED
    Feb 12 11:29:09.624: INFO: observed event type MODIFIED
    Feb 12 11:29:09.625: INFO: observed event type MODIFIED
    Feb 12 11:29:09.625: INFO: observed event type MODIFIED
    Feb 12 11:29:09.625: INFO: observed event type MODIFIED
    Feb 12 11:29:09.625: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 11:29:09.633: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb 12 11:29:09.638: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-537  88f7eb35-1309-47ea-858c-14cb8ea00728 10534 4 2023-02-12 11:28:47 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 39dea732-6b1b-46b4-8b59-b315f14bd703 0xc002a4a767 0xc002a4a768}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39dea732-6b1b-46b4-8b59-b315f14bd703\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4a7f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Feb 12 11:29:09.646: INFO: pod: "test-deployment-54cc775c4b-292ng":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-292ng test-deployment-54cc775c4b- deployment-537  af98d233-099a-41e5-b0ae-0c6268f135ff 10530 0 2023-02-12 11:28:47 +0000 UTC 2023-02-12 11:29:10 +0000 UTC 0xc002a4b3f8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:5a18037a6bcbbc48720a3e353ac63d8eeaf7a32df9d4f7c9aa1d968cfa4498be cni.projectcalico.org/podIP:10.233.120.74/32 cni.projectcalico.org/podIPs:10.233.120.74/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 88f7eb35-1309-47ea-858c-14cb8ea00728 0xc002a4b447 0xc002a4b448}] [] [{kube-controller-manager Update v1 2023-02-12 11:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88f7eb35-1309-47ea-858c-14cb8ea00728\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 11:28:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 11:28:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9h86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9h86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.74,StartTime:2023-02-12 11:28:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:28:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:sha256:4873874c08efc72e9729683a83ffbb7502ee729e9a5ac097723806ea7fa13517,ContainerID:containerd://3a8426c088899c7e467992b1778440cd57066eaf16c25ffc8ff9cf5ee64c9e55,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 12 11:29:09.646: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-537  a4086b3e-447e-4080-8c52-20f555c41102 10526 2 2023-02-12 11:28:59 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 39dea732-6b1b-46b4-8b59-b315f14bd703 0xc002a4a857 0xc002a4a858}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:29:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39dea732-6b1b-46b4-8b59-b315f14bd703\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4a8e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Feb 12 11:29:09.652: INFO: pod: "test-deployment-7c7d8d58c8-2vpjh":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2vpjh test-deployment-7c7d8d58c8- deployment-537  fb2c24b0-e29c-4b36-aa1e-faa297c4d49d 10483 0 2023-02-12 11:28:59 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:8b23e8061e9cdb5b54b32bf32ee0e8a4aaffed3dfa8ba1db111b51ab7653b42b cni.projectcalico.org/podIP:10.233.120.204/32 cni.projectcalico.org/podIPs:10.233.120.204/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a4086b3e-447e-4080-8c52-20f555c41102 0xc002ac2737 0xc002ac2738}] [] [{kube-controller-manager Update v1 2023-02-12 11:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4086b3e-447e-4080-8c52-20f555c41102\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 11:29:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 11:29:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk4td,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk4td,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:28:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.204,StartTime:2023-02-12 11:28:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:29:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cdfd93b24dd109726cef08ebc6e5d298da234def34a69e002ea9bce1e8396478,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 12 11:29:09.652: INFO: pod: "test-deployment-7c7d8d58c8-csv2s":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-csv2s test-deployment-7c7d8d58c8- deployment-537  fcca86ac-5b75-4b88-bfaa-9dcb98eef704 10525 0 2023-02-12 11:29:04 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:50df625cb59e52f3ed64d296ab8e87e7d3a0a06e18ee95eb319e893d0e5f905e cni.projectcalico.org/podIP:10.233.99.91/32 cni.projectcalico.org/podIPs:10.233.99.91/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 a4086b3e-447e-4080-8c52-20f555c41102 0xc002ac2967 0xc002ac2968}] [] [{kube-controller-manager Update v1 2023-02-12 11:29:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4086b3e-447e-4080-8c52-20f555c41102\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 11:29:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 11:29:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbt5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbt5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:29:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.91,StartTime:2023-02-12 11:29:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:29:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9fcf436add9476555fe2df3d6adaaf4cf6edb0db0be12d71d47282f37eced377,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 12 11:29:09.652: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-537  2bc53e6f-491f-421e-9c8a-512c1d699328 10390 3 2023-02-12 11:28:45 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 39dea732-6b1b-46b4-8b59-b315f14bd703 0xc002a4a947 0xc002a4a948}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:28:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39dea732-6b1b-46b4-8b59-b315f14bd703\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:28:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a4a9d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 11:29:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-537" for this suite. 02/12/23 11:29:09.709
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:09.735
Feb 12 11:29:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:29:09.736
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:09.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:09.783
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 02/12/23 11:29:09.787
STEP: fetching the ConfigMap 02/12/23 11:29:09.803
STEP: patching the ConfigMap 02/12/23 11:29:09.821
STEP: listing all ConfigMaps in all namespaces with a label selector 02/12/23 11:29:09.831
STEP: deleting the ConfigMap by collection with a label selector 02/12/23 11:29:09.84
STEP: listing all ConfigMaps in test namespace 02/12/23 11:29:09.855
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:29:09.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2727" for this suite. 02/12/23 11:29:09.876
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":87,"skipped":1561,"failed":0}
------------------------------
 [0.172 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:09.735
    Feb 12 11:29:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:29:09.736
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:09.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:09.783
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 02/12/23 11:29:09.787
    STEP: fetching the ConfigMap 02/12/23 11:29:09.803
    STEP: patching the ConfigMap 02/12/23 11:29:09.821
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/12/23 11:29:09.831
    STEP: deleting the ConfigMap by collection with a label selector 02/12/23 11:29:09.84
    STEP: listing all ConfigMaps in test namespace 02/12/23 11:29:09.855
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:29:09.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2727" for this suite. 02/12/23 11:29:09.876
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:09.907
Feb 12 11:29:09.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:29:09.91
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:09.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:09.937
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/12/23 11:29:09.939
STEP: submitting the pod to kubernetes 02/12/23 11:29:09.94
STEP: verifying QOS class is set on the pod 02/12/23 11:29:09.95
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Feb 12 11:29:09.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4365" for this suite. 02/12/23 11:29:09.977
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":88,"skipped":1561,"failed":0}
------------------------------
 [0.085 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:09.907
    Feb 12 11:29:09.908: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:29:09.91
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:09.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:09.937
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/12/23 11:29:09.939
    STEP: submitting the pod to kubernetes 02/12/23 11:29:09.94
    STEP: verifying QOS class is set on the pod 02/12/23 11:29:09.95
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Feb 12 11:29:09.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4365" for this suite. 02/12/23 11:29:09.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:09.998
Feb 12 11:29:09.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 11:29:09.999
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:10.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:10.026
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/12/23 11:29:10.029
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9966;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9966;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +notcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_udp@PTR;check="$$(dig +tcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_tcp@PTR;sleep 1; done
 02/12/23 11:29:10.062
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9966;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9966;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +notcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_udp@PTR;check="$$(dig +tcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_tcp@PTR;sleep 1; done
 02/12/23 11:29:10.062
STEP: creating a pod to probe DNS 02/12/23 11:29:10.062
STEP: submitting the pod to kubernetes 02/12/23 11:29:10.062
Feb 12 11:29:10.087: INFO: Waiting up to 15m0s for pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75" in namespace "dns-9966" to be "running"
Feb 12 11:29:10.110: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 22.427823ms
Feb 12 11:29:12.121: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03435093s
Feb 12 11:29:16.091: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003483169s
Feb 12 11:29:16.646: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558804873s
Feb 12 11:29:21.323: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.236031788s
Feb 12 11:29:22.114: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026518452s
Feb 12 11:29:24.122: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 14.03514357s
Feb 12 11:29:26.423: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 16.336314197s
Feb 12 11:29:28.113: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025877227s
Feb 12 11:29:31.101: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Running", Reason="", readiness=true. Elapsed: 21.013519156s
Feb 12 11:29:31.101: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:29:31.101
STEP: looking for the results for each expected name from probers 02/12/23 11:29:31.218
Feb 12 11:29:31.257: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.267: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.282: INFO: Unable to read wheezy_udp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.287: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.292: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.298: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.414: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.417: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.422: INFO: Unable to read jessie_udp@dns-test-service.dns-9966 from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.426: INFO: Unable to read jessie_tcp@dns-test-service.dns-9966 from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.513: INFO: Unable to read jessie_udp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.519: INFO: Unable to read jessie_tcp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.545: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.550: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
Feb 12 11:29:31.563: INFO: Lookups using dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9966.svc wheezy_tcp@dns-test-service.dns-9966.svc wheezy_udp@_http._tcp.dns-test-service.dns-9966.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9966.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9966 jessie_tcp@dns-test-service.dns-9966 jessie_udp@dns-test-service.dns-9966.svc jessie_tcp@dns-test-service.dns-9966.svc jessie_udp@_http._tcp.dns-test-service.dns-9966.svc jessie_tcp@_http._tcp.dns-test-service.dns-9966.svc]

Feb 12 11:29:36.649: INFO: DNS probes using dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75 succeeded

STEP: deleting the pod 02/12/23 11:29:36.649
STEP: deleting the test service 02/12/23 11:29:36.667
STEP: deleting the test headless service 02/12/23 11:29:36.724
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 11:29:36.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9966" for this suite. 02/12/23 11:29:36.761
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":89,"skipped":1610,"failed":0}
------------------------------
 [SLOW TEST] [27.166 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:09.998
    Feb 12 11:29:09.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 11:29:09.999
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:10.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:10.026
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/12/23 11:29:10.029
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9966;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9966;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +notcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_udp@PTR;check="$$(dig +tcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_tcp@PTR;sleep 1; done
     02/12/23 11:29:10.062
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9966;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9966;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9966.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9966.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9966.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9966.svc;check="$$(dig +notcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_udp@PTR;check="$$(dig +tcp +noall +answer +search 193.62.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.62.193_tcp@PTR;sleep 1; done
     02/12/23 11:29:10.062
    STEP: creating a pod to probe DNS 02/12/23 11:29:10.062
    STEP: submitting the pod to kubernetes 02/12/23 11:29:10.062
    Feb 12 11:29:10.087: INFO: Waiting up to 15m0s for pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75" in namespace "dns-9966" to be "running"
    Feb 12 11:29:10.110: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 22.427823ms
    Feb 12 11:29:12.121: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03435093s
    Feb 12 11:29:16.091: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003483169s
    Feb 12 11:29:16.646: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.558804873s
    Feb 12 11:29:21.323: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 11.236031788s
    Feb 12 11:29:22.114: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026518452s
    Feb 12 11:29:24.122: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 14.03514357s
    Feb 12 11:29:26.423: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 16.336314197s
    Feb 12 11:29:28.113: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025877227s
    Feb 12 11:29:31.101: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75": Phase="Running", Reason="", readiness=true. Elapsed: 21.013519156s
    Feb 12 11:29:31.101: INFO: Pod "dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:29:31.101
    STEP: looking for the results for each expected name from probers 02/12/23 11:29:31.218
    Feb 12 11:29:31.257: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.267: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.282: INFO: Unable to read wheezy_udp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.287: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.292: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.298: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.414: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.417: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.422: INFO: Unable to read jessie_udp@dns-test-service.dns-9966 from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.426: INFO: Unable to read jessie_tcp@dns-test-service.dns-9966 from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.513: INFO: Unable to read jessie_udp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.519: INFO: Unable to read jessie_tcp@dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.545: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.550: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9966.svc from pod dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75: the server could not find the requested resource (get pods dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75)
    Feb 12 11:29:31.563: INFO: Lookups using dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9966.svc wheezy_tcp@dns-test-service.dns-9966.svc wheezy_udp@_http._tcp.dns-test-service.dns-9966.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9966.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9966 jessie_tcp@dns-test-service.dns-9966 jessie_udp@dns-test-service.dns-9966.svc jessie_tcp@dns-test-service.dns-9966.svc jessie_udp@_http._tcp.dns-test-service.dns-9966.svc jessie_tcp@_http._tcp.dns-test-service.dns-9966.svc]

    Feb 12 11:29:36.649: INFO: DNS probes using dns-9966/dns-test-e89d66a9-0a34-4c24-bdfc-51b519016e75 succeeded

    STEP: deleting the pod 02/12/23 11:29:36.649
    STEP: deleting the test service 02/12/23 11:29:36.667
    STEP: deleting the test headless service 02/12/23 11:29:36.724
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 11:29:36.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9966" for this suite. 02/12/23 11:29:36.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:37.166
Feb 12 11:29:37.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:29:37.167
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:41.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:41.98
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Feb 12 11:29:44.520: INFO: Waiting up to 5m0s for pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a" in namespace "pods-2886" to be "running and ready"
Feb 12 11:29:44.632: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 112.046943ms
Feb 12 11:29:44.632: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:29:46.927: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407044835s
Feb 12 11:29:46.927: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:29:49.077: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.556710095s
Feb 12 11:29:49.077: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:29:50.636: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116435064s
Feb 12 11:29:50.636: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:29:52.637: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Running", Reason="", readiness=true. Elapsed: 8.117025001s
Feb 12 11:29:52.637: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Running (Ready = true)
Feb 12 11:29:52.637: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a" satisfied condition "running and ready"
Feb 12 11:29:52.678: INFO: Waiting up to 5m0s for pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09" in namespace "pods-2886" to be "Succeeded or Failed"
Feb 12 11:29:52.687: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Pending", Reason="", readiness=false. Elapsed: 8.455297ms
Feb 12 11:29:54.703: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024253237s
Feb 12 11:29:56.702: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023574805s
Feb 12 11:29:58.690: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011835091s
STEP: Saw pod success 02/12/23 11:29:58.69
Feb 12 11:29:58.690: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09" satisfied condition "Succeeded or Failed"
Feb 12 11:29:58.693: INFO: Trying to get logs from node kube-3 pod client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09 container env3cont: <nil>
STEP: delete the pod 02/12/23 11:29:58.699
Feb 12 11:29:58.717: INFO: Waiting for pod client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09 to disappear
Feb 12 11:29:58.720: INFO: Pod client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:29:58.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2886" for this suite. 02/12/23 11:29:58.725
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":90,"skipped":1644,"failed":0}
------------------------------
 [SLOW TEST] [21.566 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:37.166
    Feb 12 11:29:37.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:29:37.167
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:41.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:41.98
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Feb 12 11:29:44.520: INFO: Waiting up to 5m0s for pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a" in namespace "pods-2886" to be "running and ready"
    Feb 12 11:29:44.632: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 112.046943ms
    Feb 12 11:29:44.632: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:29:46.927: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407044835s
    Feb 12 11:29:46.927: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:29:49.077: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.556710095s
    Feb 12 11:29:49.077: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:29:50.636: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116435064s
    Feb 12 11:29:50.636: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:29:52.637: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a": Phase="Running", Reason="", readiness=true. Elapsed: 8.117025001s
    Feb 12 11:29:52.637: INFO: The phase of Pod server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a is Running (Ready = true)
    Feb 12 11:29:52.637: INFO: Pod "server-envvars-16055aba-6121-4ab4-b69b-32e96e86178a" satisfied condition "running and ready"
    Feb 12 11:29:52.678: INFO: Waiting up to 5m0s for pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09" in namespace "pods-2886" to be "Succeeded or Failed"
    Feb 12 11:29:52.687: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Pending", Reason="", readiness=false. Elapsed: 8.455297ms
    Feb 12 11:29:54.703: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024253237s
    Feb 12 11:29:56.702: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023574805s
    Feb 12 11:29:58.690: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011835091s
    STEP: Saw pod success 02/12/23 11:29:58.69
    Feb 12 11:29:58.690: INFO: Pod "client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09" satisfied condition "Succeeded or Failed"
    Feb 12 11:29:58.693: INFO: Trying to get logs from node kube-3 pod client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09 container env3cont: <nil>
    STEP: delete the pod 02/12/23 11:29:58.699
    Feb 12 11:29:58.717: INFO: Waiting for pod client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09 to disappear
    Feb 12 11:29:58.720: INFO: Pod client-envvars-172ae9b8-dbf5-4406-a546-506e95d19b09 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:29:58.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2886" for this suite. 02/12/23 11:29:58.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:58.734
Feb 12 11:29:58.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename ingressclass 02/12/23 11:29:58.734
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:58.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:58.756
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/12/23 11:29:58.759
STEP: getting /apis/networking.k8s.io 02/12/23 11:29:58.76
STEP: getting /apis/networking.k8s.iov1 02/12/23 11:29:58.761
STEP: creating 02/12/23 11:29:58.762
STEP: getting 02/12/23 11:29:58.781
STEP: listing 02/12/23 11:29:58.784
STEP: watching 02/12/23 11:29:58.786
Feb 12 11:29:58.786: INFO: starting watch
STEP: patching 02/12/23 11:29:58.787
STEP: updating 02/12/23 11:29:58.793
Feb 12 11:29:58.799: INFO: waiting for watch events with expected annotations
Feb 12 11:29:58.799: INFO: saw patched and updated annotations
STEP: deleting 02/12/23 11:29:58.799
STEP: deleting a collection 02/12/23 11:29:58.81
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Feb 12 11:29:58.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8908" for this suite. 02/12/23 11:29:58.826
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":91,"skipped":1699,"failed":0}
------------------------------
 [0.099 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:58.734
    Feb 12 11:29:58.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename ingressclass 02/12/23 11:29:58.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:58.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:58.756
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/12/23 11:29:58.759
    STEP: getting /apis/networking.k8s.io 02/12/23 11:29:58.76
    STEP: getting /apis/networking.k8s.iov1 02/12/23 11:29:58.761
    STEP: creating 02/12/23 11:29:58.762
    STEP: getting 02/12/23 11:29:58.781
    STEP: listing 02/12/23 11:29:58.784
    STEP: watching 02/12/23 11:29:58.786
    Feb 12 11:29:58.786: INFO: starting watch
    STEP: patching 02/12/23 11:29:58.787
    STEP: updating 02/12/23 11:29:58.793
    Feb 12 11:29:58.799: INFO: waiting for watch events with expected annotations
    Feb 12 11:29:58.799: INFO: saw patched and updated annotations
    STEP: deleting 02/12/23 11:29:58.799
    STEP: deleting a collection 02/12/23 11:29:58.81
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Feb 12 11:29:58.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-8908" for this suite. 02/12/23 11:29:58.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:58.834
Feb 12 11:29:58.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename conformance-tests 02/12/23 11:29:58.834
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:58.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:58.857
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/12/23 11:29:58.858
Feb 12 11:29:58.858: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Feb 12 11:29:58.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-9800" for this suite. 02/12/23 11:29:58.865
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":92,"skipped":1708,"failed":0}
------------------------------
 [0.039 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:58.834
    Feb 12 11:29:58.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename conformance-tests 02/12/23 11:29:58.834
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:58.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:58.857
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/12/23 11:29:58.858
    Feb 12 11:29:58.858: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Feb 12 11:29:58.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-9800" for this suite. 02/12/23 11:29:58.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:29:58.874
Feb 12 11:29:58.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:29:58.875
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:58.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:58.897
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Feb 12 11:29:58.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/12/23 11:30:06.011
Feb 12 11:30:06.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 create -f -'
Feb 12 11:30:06.615: INFO: stderr: ""
Feb 12 11:30:06.615: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 12 11:30:06.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 delete e2e-test-crd-publish-openapi-8620-crds test-cr'
Feb 12 11:30:06.691: INFO: stderr: ""
Feb 12 11:30:06.691: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 12 11:30:06.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 apply -f -'
Feb 12 11:30:06.885: INFO: stderr: ""
Feb 12 11:30:06.885: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 12 11:30:06.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 delete e2e-test-crd-publish-openapi-8620-crds test-cr'
Feb 12 11:30:08.388: INFO: stderr: ""
Feb 12 11:30:08.388: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/12/23 11:30:08.388
Feb 12 11:30:08.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 explain e2e-test-crd-publish-openapi-8620-crds'
Feb 12 11:30:08.548: INFO: stderr: ""
Feb 12 11:30:08.548: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8620-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:30:10.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4941" for this suite. 02/12/23 11:30:11
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":93,"skipped":1716,"failed":0}
------------------------------
 [SLOW TEST] [12.138 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:29:58.874
    Feb 12 11:29:58.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:29:58.875
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:29:58.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:29:58.897
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Feb 12 11:29:58.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/12/23 11:30:06.011
    Feb 12 11:30:06.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 create -f -'
    Feb 12 11:30:06.615: INFO: stderr: ""
    Feb 12 11:30:06.615: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 12 11:30:06.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 delete e2e-test-crd-publish-openapi-8620-crds test-cr'
    Feb 12 11:30:06.691: INFO: stderr: ""
    Feb 12 11:30:06.691: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 12 11:30:06.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 apply -f -'
    Feb 12 11:30:06.885: INFO: stderr: ""
    Feb 12 11:30:06.885: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 12 11:30:06.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 --namespace=crd-publish-openapi-4941 delete e2e-test-crd-publish-openapi-8620-crds test-cr'
    Feb 12 11:30:08.388: INFO: stderr: ""
    Feb 12 11:30:08.388: INFO: stdout: "e2e-test-crd-publish-openapi-8620-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/12/23 11:30:08.388
    Feb 12 11:30:08.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-4941 explain e2e-test-crd-publish-openapi-8620-crds'
    Feb 12 11:30:08.548: INFO: stderr: ""
    Feb 12 11:30:08.548: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8620-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:30:10.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4941" for this suite. 02/12/23 11:30:11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:30:11.016
Feb 12 11:30:11.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:30:11.017
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:11.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:11.084
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 02/12/23 11:30:11.086
STEP: Counting existing ResourceQuota 02/12/23 11:30:16.091
STEP: Creating a ResourceQuota 02/12/23 11:30:21.106
STEP: Ensuring resource quota status is calculated 02/12/23 11:30:21.124
STEP: Creating a Secret 02/12/23 11:30:23.129
STEP: Ensuring resource quota status captures secret creation 02/12/23 11:30:23.143
STEP: Deleting a secret 02/12/23 11:30:25.158
STEP: Ensuring resource quota status released usage 02/12/23 11:30:25.177
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:30:27.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4982" for this suite. 02/12/23 11:30:27.185
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":94,"skipped":1746,"failed":0}
------------------------------
 [SLOW TEST] [16.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:30:11.016
    Feb 12 11:30:11.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:30:11.017
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:11.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:11.084
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 02/12/23 11:30:11.086
    STEP: Counting existing ResourceQuota 02/12/23 11:30:16.091
    STEP: Creating a ResourceQuota 02/12/23 11:30:21.106
    STEP: Ensuring resource quota status is calculated 02/12/23 11:30:21.124
    STEP: Creating a Secret 02/12/23 11:30:23.129
    STEP: Ensuring resource quota status captures secret creation 02/12/23 11:30:23.143
    STEP: Deleting a secret 02/12/23 11:30:25.158
    STEP: Ensuring resource quota status released usage 02/12/23 11:30:25.177
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:30:27.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4982" for this suite. 02/12/23 11:30:27.185
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:30:27.197
Feb 12 11:30:27.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:30:27.198
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:27.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:27.225
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 02/12/23 11:30:27.227
Feb 12 11:30:27.239: INFO: Waiting up to 5m0s for pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10" in namespace "emptydir-5647" to be "Succeeded or Failed"
Feb 12 11:30:27.247: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Pending", Reason="", readiness=false. Elapsed: 8.720638ms
Feb 12 11:30:29.259: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Running", Reason="", readiness=true. Elapsed: 2.020378225s
Feb 12 11:30:31.255: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Running", Reason="", readiness=false. Elapsed: 4.016531408s
Feb 12 11:30:33.259: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020736264s
STEP: Saw pod success 02/12/23 11:30:33.26
Feb 12 11:30:33.260: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10" satisfied condition "Succeeded or Failed"
Feb 12 11:30:33.270: INFO: Trying to get logs from node kube-3 pod pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10 container test-container: <nil>
STEP: delete the pod 02/12/23 11:30:33.295
Feb 12 11:30:33.312: INFO: Waiting for pod pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10 to disappear
Feb 12 11:30:33.315: INFO: Pod pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:30:33.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5647" for this suite. 02/12/23 11:30:33.32
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":95,"skipped":1749,"failed":0}
------------------------------
 [SLOW TEST] [6.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:30:27.197
    Feb 12 11:30:27.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:30:27.198
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:27.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:27.225
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 02/12/23 11:30:27.227
    Feb 12 11:30:27.239: INFO: Waiting up to 5m0s for pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10" in namespace "emptydir-5647" to be "Succeeded or Failed"
    Feb 12 11:30:27.247: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Pending", Reason="", readiness=false. Elapsed: 8.720638ms
    Feb 12 11:30:29.259: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Running", Reason="", readiness=true. Elapsed: 2.020378225s
    Feb 12 11:30:31.255: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Running", Reason="", readiness=false. Elapsed: 4.016531408s
    Feb 12 11:30:33.259: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020736264s
    STEP: Saw pod success 02/12/23 11:30:33.26
    Feb 12 11:30:33.260: INFO: Pod "pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10" satisfied condition "Succeeded or Failed"
    Feb 12 11:30:33.270: INFO: Trying to get logs from node kube-3 pod pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10 container test-container: <nil>
    STEP: delete the pod 02/12/23 11:30:33.295
    Feb 12 11:30:33.312: INFO: Waiting for pod pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10 to disappear
    Feb 12 11:30:33.315: INFO: Pod pod-6fd72b4d-8d3a-403b-adc3-7fa432a0ca10 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:30:33.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5647" for this suite. 02/12/23 11:30:33.32
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:30:33.332
Feb 12 11:30:33.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 11:30:33.333
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:33.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:33.356
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/12/23 11:30:33.358
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
 02/12/23 11:30:33.369
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
 02/12/23 11:30:33.369
STEP: creating a pod to probe DNS 02/12/23 11:30:33.369
STEP: submitting the pod to kubernetes 02/12/23 11:30:33.37
Feb 12 11:30:33.383: INFO: Waiting up to 15m0s for pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b" in namespace "dns-3780" to be "running"
Feb 12 11:30:33.391: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.455431ms
Feb 12 11:30:35.406: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023398492s
Feb 12 11:30:37.396: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.012764483s
Feb 12 11:30:37.396: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:30:37.396
STEP: looking for the results for each expected name from probers 02/12/23 11:30:37.399
Feb 12 11:30:37.411: INFO: DNS probes using dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b succeeded

STEP: deleting the pod 02/12/23 11:30:37.411
STEP: changing the externalName to bar.example.com 02/12/23 11:30:37.447
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
 02/12/23 11:30:37.468
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
 02/12/23 11:30:37.468
STEP: creating a second pod to probe DNS 02/12/23 11:30:37.468
STEP: submitting the pod to kubernetes 02/12/23 11:30:37.468
Feb 12 11:30:37.483: INFO: Waiting up to 15m0s for pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94" in namespace "dns-3780" to be "running"
Feb 12 11:30:37.492: INFO: Pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94": Phase="Pending", Reason="", readiness=false. Elapsed: 9.165921ms
Feb 12 11:30:39.501: INFO: Pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94": Phase="Running", Reason="", readiness=true. Elapsed: 2.018869807s
Feb 12 11:30:39.502: INFO: Pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:30:39.502
STEP: looking for the results for each expected name from probers 02/12/23 11:30:39.505
Feb 12 11:30:39.515: INFO: File jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local from pod  dns-3780/dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 12 11:30:39.515: INFO: Lookups using dns-3780/dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94 failed for: [jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local]

Feb 12 11:30:44.547: INFO: DNS probes using dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94 succeeded

STEP: deleting the pod 02/12/23 11:30:44.547
STEP: changing the service to type=ClusterIP 02/12/23 11:30:44.593
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
 02/12/23 11:30:44.621
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
 02/12/23 11:30:44.621
STEP: creating a third pod to probe DNS 02/12/23 11:30:44.621
STEP: submitting the pod to kubernetes 02/12/23 11:30:44.626
Feb 12 11:30:44.660: INFO: Waiting up to 15m0s for pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e" in namespace "dns-3780" to be "running"
Feb 12 11:30:44.678: INFO: Pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.528049ms
Feb 12 11:30:46.686: INFO: Pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e": Phase="Running", Reason="", readiness=true. Elapsed: 2.025898278s
Feb 12 11:30:46.686: INFO: Pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:30:46.686
STEP: looking for the results for each expected name from probers 02/12/23 11:30:46.69
Feb 12 11:30:46.704: INFO: DNS probes using dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e succeeded

STEP: deleting the pod 02/12/23 11:30:46.704
STEP: deleting the test externalName service 02/12/23 11:30:46.721
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 11:30:46.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3780" for this suite. 02/12/23 11:30:46.744
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":96,"skipped":1751,"failed":0}
------------------------------
 [SLOW TEST] [13.421 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:30:33.332
    Feb 12 11:30:33.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 11:30:33.333
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:33.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:33.356
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/12/23 11:30:33.358
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
     02/12/23 11:30:33.369
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
     02/12/23 11:30:33.369
    STEP: creating a pod to probe DNS 02/12/23 11:30:33.369
    STEP: submitting the pod to kubernetes 02/12/23 11:30:33.37
    Feb 12 11:30:33.383: INFO: Waiting up to 15m0s for pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b" in namespace "dns-3780" to be "running"
    Feb 12 11:30:33.391: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.455431ms
    Feb 12 11:30:35.406: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023398492s
    Feb 12 11:30:37.396: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b": Phase="Running", Reason="", readiness=true. Elapsed: 4.012764483s
    Feb 12 11:30:37.396: INFO: Pod "dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:30:37.396
    STEP: looking for the results for each expected name from probers 02/12/23 11:30:37.399
    Feb 12 11:30:37.411: INFO: DNS probes using dns-test-a46bde55-fff9-4e77-86ea-4b66a14e1a7b succeeded

    STEP: deleting the pod 02/12/23 11:30:37.411
    STEP: changing the externalName to bar.example.com 02/12/23 11:30:37.447
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
     02/12/23 11:30:37.468
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
     02/12/23 11:30:37.468
    STEP: creating a second pod to probe DNS 02/12/23 11:30:37.468
    STEP: submitting the pod to kubernetes 02/12/23 11:30:37.468
    Feb 12 11:30:37.483: INFO: Waiting up to 15m0s for pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94" in namespace "dns-3780" to be "running"
    Feb 12 11:30:37.492: INFO: Pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94": Phase="Pending", Reason="", readiness=false. Elapsed: 9.165921ms
    Feb 12 11:30:39.501: INFO: Pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94": Phase="Running", Reason="", readiness=true. Elapsed: 2.018869807s
    Feb 12 11:30:39.502: INFO: Pod "dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:30:39.502
    STEP: looking for the results for each expected name from probers 02/12/23 11:30:39.505
    Feb 12 11:30:39.515: INFO: File jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local from pod  dns-3780/dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 12 11:30:39.515: INFO: Lookups using dns-3780/dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94 failed for: [jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local]

    Feb 12 11:30:44.547: INFO: DNS probes using dns-test-731ea9da-b026-4aa6-a798-0aa139c67d94 succeeded

    STEP: deleting the pod 02/12/23 11:30:44.547
    STEP: changing the service to type=ClusterIP 02/12/23 11:30:44.593
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
     02/12/23 11:30:44.621
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3780.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3780.svc.cluster.local; sleep 1; done
     02/12/23 11:30:44.621
    STEP: creating a third pod to probe DNS 02/12/23 11:30:44.621
    STEP: submitting the pod to kubernetes 02/12/23 11:30:44.626
    Feb 12 11:30:44.660: INFO: Waiting up to 15m0s for pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e" in namespace "dns-3780" to be "running"
    Feb 12 11:30:44.678: INFO: Pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.528049ms
    Feb 12 11:30:46.686: INFO: Pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e": Phase="Running", Reason="", readiness=true. Elapsed: 2.025898278s
    Feb 12 11:30:46.686: INFO: Pod "dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:30:46.686
    STEP: looking for the results for each expected name from probers 02/12/23 11:30:46.69
    Feb 12 11:30:46.704: INFO: DNS probes using dns-test-7c45ea95-8077-4f58-a719-5b81d9ca890e succeeded

    STEP: deleting the pod 02/12/23 11:30:46.704
    STEP: deleting the test externalName service 02/12/23 11:30:46.721
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 11:30:46.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3780" for this suite. 02/12/23 11:30:46.744
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:30:46.754
Feb 12 11:30:46.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 11:30:46.755
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:46.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:46.789
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9068 02/12/23 11:30:46.793
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 02/12/23 11:30:46.801
STEP: Creating stateful set ss in namespace statefulset-9068 02/12/23 11:30:46.809
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9068 02/12/23 11:30:46.823
Feb 12 11:30:46.828: INFO: Found 0 stateful pods, waiting for 1
Feb 12 11:30:56.833: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/12/23 11:30:56.833
Feb 12 11:30:56.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 11:30:56.960: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 11:30:56.960: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 11:30:56.960: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 11:30:56.964: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 12 11:31:06.983: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 11:31:06.983: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:31:07.017: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999661s
Feb 12 11:31:08.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995786091s
Feb 12 11:31:09.038: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980891542s
Feb 12 11:31:10.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.975215234s
Feb 12 11:31:11.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961093502s
Feb 12 11:31:12.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955177803s
Feb 12 11:31:13.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.93993536s
Feb 12 11:31:14.100: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.932938637s
Feb 12 11:31:15.112: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912566668s
Feb 12 11:31:16.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 900.81922ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9068 02/12/23 11:31:17.139
Feb 12 11:31:17.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 11:31:18.114: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 11:31:18.114: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 11:31:18.114: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 11:31:18.131: INFO: Found 1 stateful pods, waiting for 3
Feb 12 11:31:28.139: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 11:31:28.140: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 11:31:28.140: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/12/23 11:31:28.14
STEP: Scale down will halt with unhealthy stateful pod 02/12/23 11:31:28.14
Feb 12 11:31:28.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 11:31:28.290: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 11:31:28.290: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 11:31:28.290: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 11:31:28.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 11:31:28.546: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 11:31:28.546: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 11:31:28.546: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 11:31:28.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 11:31:28.693: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 11:31:28.693: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 11:31:28.693: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 11:31:28.693: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:31:28.696: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 12 11:31:38.720: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 11:31:38.720: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 11:31:38.720: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 11:31:38.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999913s
Feb 12 11:31:39.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988553418s
Feb 12 11:31:40.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974991938s
Feb 12 11:31:41.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962705738s
Feb 12 11:31:42.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958779812s
Feb 12 11:31:43.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948677637s
Feb 12 11:31:44.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938763304s
Feb 12 11:31:45.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.92199279s
Feb 12 11:31:46.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.915194102s
Feb 12 11:31:47.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 900.945697ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9068 02/12/23 11:31:48.869
Feb 12 11:31:48.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 11:31:49.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 11:31:49.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 11:31:49.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 11:31:49.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 11:31:49.204: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 11:31:49.204: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 11:31:49.204: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 11:31:49.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 11:31:49.325: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 11:31:49.325: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 11:31:49.325: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 11:31:49.325: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/12/23 11:31:59.348
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 11:31:59.349: INFO: Deleting all statefulset in ns statefulset-9068
Feb 12 11:31:59.361: INFO: Scaling statefulset ss to 0
Feb 12 11:31:59.395: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:31:59.402: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 11:31:59.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9068" for this suite. 02/12/23 11:31:59.426
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":97,"skipped":1754,"failed":0}
------------------------------
 [SLOW TEST] [72.680 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:30:46.754
    Feb 12 11:30:46.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 11:30:46.755
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:30:46.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:30:46.789
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9068 02/12/23 11:30:46.793
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/12/23 11:30:46.801
    STEP: Creating stateful set ss in namespace statefulset-9068 02/12/23 11:30:46.809
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9068 02/12/23 11:30:46.823
    Feb 12 11:30:46.828: INFO: Found 0 stateful pods, waiting for 1
    Feb 12 11:30:56.833: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/12/23 11:30:56.833
    Feb 12 11:30:56.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 11:30:56.960: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 11:30:56.960: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 11:30:56.960: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 11:30:56.964: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 12 11:31:06.983: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 11:31:06.983: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:31:07.017: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999661s
    Feb 12 11:31:08.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995786091s
    Feb 12 11:31:09.038: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980891542s
    Feb 12 11:31:10.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.975215234s
    Feb 12 11:31:11.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961093502s
    Feb 12 11:31:12.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955177803s
    Feb 12 11:31:13.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.93993536s
    Feb 12 11:31:14.100: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.932938637s
    Feb 12 11:31:15.112: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912566668s
    Feb 12 11:31:16.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 900.81922ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9068 02/12/23 11:31:17.139
    Feb 12 11:31:17.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 11:31:18.114: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 11:31:18.114: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 11:31:18.114: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 11:31:18.131: INFO: Found 1 stateful pods, waiting for 3
    Feb 12 11:31:28.139: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 11:31:28.140: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 11:31:28.140: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/12/23 11:31:28.14
    STEP: Scale down will halt with unhealthy stateful pod 02/12/23 11:31:28.14
    Feb 12 11:31:28.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 11:31:28.290: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 11:31:28.290: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 11:31:28.290: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 11:31:28.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 11:31:28.546: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 11:31:28.546: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 11:31:28.546: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 11:31:28.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 11:31:28.693: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 11:31:28.693: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 11:31:28.693: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 11:31:28.693: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:31:28.696: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 12 11:31:38.720: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 11:31:38.720: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 11:31:38.720: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 11:31:38.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999913s
    Feb 12 11:31:39.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988553418s
    Feb 12 11:31:40.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974991938s
    Feb 12 11:31:41.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962705738s
    Feb 12 11:31:42.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958779812s
    Feb 12 11:31:43.816: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948677637s
    Feb 12 11:31:44.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938763304s
    Feb 12 11:31:45.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.92199279s
    Feb 12 11:31:46.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.915194102s
    Feb 12 11:31:47.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 900.945697ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9068 02/12/23 11:31:48.869
    Feb 12 11:31:48.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 11:31:49.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 11:31:49.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 11:31:49.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 11:31:49.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 11:31:49.204: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 11:31:49.204: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 11:31:49.204: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 11:31:49.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9068 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 11:31:49.325: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 11:31:49.325: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 11:31:49.325: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 11:31:49.325: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/12/23 11:31:59.348
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 11:31:59.349: INFO: Deleting all statefulset in ns statefulset-9068
    Feb 12 11:31:59.361: INFO: Scaling statefulset ss to 0
    Feb 12 11:31:59.395: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:31:59.402: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 11:31:59.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9068" for this suite. 02/12/23 11:31:59.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:31:59.435
Feb 12 11:31:59.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:31:59.436
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:31:59.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:31:59.46
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 02/12/23 11:31:59.462
Feb 12 11:31:59.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4656 create -f -'
Feb 12 11:31:59.985: INFO: stderr: ""
Feb 12 11:31:59.985: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/12/23 11:31:59.985
Feb 12 11:31:59.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4656 diff -f -'
Feb 12 11:32:00.439: INFO: rc: 1
Feb 12 11:32:00.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4656 delete -f -'
Feb 12 11:32:00.536: INFO: stderr: ""
Feb 12 11:32:00.536: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:32:00.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4656" for this suite. 02/12/23 11:32:00.543
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":98,"skipped":1772,"failed":0}
------------------------------
 [1.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:31:59.435
    Feb 12 11:31:59.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:31:59.436
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:31:59.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:31:59.46
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 02/12/23 11:31:59.462
    Feb 12 11:31:59.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4656 create -f -'
    Feb 12 11:31:59.985: INFO: stderr: ""
    Feb 12 11:31:59.985: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/12/23 11:31:59.985
    Feb 12 11:31:59.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4656 diff -f -'
    Feb 12 11:32:00.439: INFO: rc: 1
    Feb 12 11:32:00.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-4656 delete -f -'
    Feb 12 11:32:00.536: INFO: stderr: ""
    Feb 12 11:32:00.536: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:32:00.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4656" for this suite. 02/12/23 11:32:00.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:32:00.578
Feb 12 11:32:00.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:32:00.579
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:00.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:00.614
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:32:00.618
Feb 12 11:32:00.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5" in namespace "projected-3731" to be "Succeeded or Failed"
Feb 12 11:32:00.637: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093675ms
Feb 12 11:32:02.665: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032045121s
Feb 12 11:32:04.641: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007561091s
STEP: Saw pod success 02/12/23 11:32:04.641
Feb 12 11:32:04.641: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5" satisfied condition "Succeeded or Failed"
Feb 12 11:32:04.648: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5 container client-container: <nil>
STEP: delete the pod 02/12/23 11:32:04.663
Feb 12 11:32:04.681: INFO: Waiting for pod downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5 to disappear
Feb 12 11:32:04.684: INFO: Pod downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:32:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3731" for this suite. 02/12/23 11:32:04.689
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":99,"skipped":1785,"failed":0}
------------------------------
 [4.121 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:32:00.578
    Feb 12 11:32:00.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:32:00.579
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:00.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:00.614
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:32:00.618
    Feb 12 11:32:00.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5" in namespace "projected-3731" to be "Succeeded or Failed"
    Feb 12 11:32:00.637: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093675ms
    Feb 12 11:32:02.665: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032045121s
    Feb 12 11:32:04.641: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007561091s
    STEP: Saw pod success 02/12/23 11:32:04.641
    Feb 12 11:32:04.641: INFO: Pod "downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5" satisfied condition "Succeeded or Failed"
    Feb 12 11:32:04.648: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5 container client-container: <nil>
    STEP: delete the pod 02/12/23 11:32:04.663
    Feb 12 11:32:04.681: INFO: Waiting for pod downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5 to disappear
    Feb 12 11:32:04.684: INFO: Pod downwardapi-volume-9f310048-34de-4188-be52-9fdfbca045e5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:32:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3731" for this suite. 02/12/23 11:32:04.689
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:32:04.699
Feb 12 11:32:04.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:32:04.701
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:04.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:04.727
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 02/12/23 11:32:04.731
Feb 12 11:32:04.739: INFO: Waiting up to 5m0s for pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4" in namespace "emptydir-9979" to be "Succeeded or Failed"
Feb 12 11:32:04.754: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.600031ms
Feb 12 11:32:06.778: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039086965s
Feb 12 11:32:08.775: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035963309s
Feb 12 11:32:10.772: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033233399s
STEP: Saw pod success 02/12/23 11:32:10.772
Feb 12 11:32:10.773: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4" satisfied condition "Succeeded or Failed"
Feb 12 11:32:10.785: INFO: Trying to get logs from node kube-3 pod pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4 container test-container: <nil>
STEP: delete the pod 02/12/23 11:32:10.821
Feb 12 11:32:10.846: INFO: Waiting for pod pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4 to disappear
Feb 12 11:32:10.849: INFO: Pod pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:32:10.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9979" for this suite. 02/12/23 11:32:10.852
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":100,"skipped":1786,"failed":0}
------------------------------
 [SLOW TEST] [6.159 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:32:04.699
    Feb 12 11:32:04.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:32:04.701
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:04.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:04.727
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/12/23 11:32:04.731
    Feb 12 11:32:04.739: INFO: Waiting up to 5m0s for pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4" in namespace "emptydir-9979" to be "Succeeded or Failed"
    Feb 12 11:32:04.754: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.600031ms
    Feb 12 11:32:06.778: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039086965s
    Feb 12 11:32:08.775: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035963309s
    Feb 12 11:32:10.772: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033233399s
    STEP: Saw pod success 02/12/23 11:32:10.772
    Feb 12 11:32:10.773: INFO: Pod "pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4" satisfied condition "Succeeded or Failed"
    Feb 12 11:32:10.785: INFO: Trying to get logs from node kube-3 pod pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4 container test-container: <nil>
    STEP: delete the pod 02/12/23 11:32:10.821
    Feb 12 11:32:10.846: INFO: Waiting for pod pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4 to disappear
    Feb 12 11:32:10.849: INFO: Pod pod-da9acddd-7d11-44ef-90f8-8f4120cb94a4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:32:10.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9979" for this suite. 02/12/23 11:32:10.852
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:32:10.858
Feb 12 11:32:10.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 11:32:10.859
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:10.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:10.889
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-a19e0327-72ef-4505-b5a4-b908d09b811f in namespace container-probe-1119 02/12/23 11:32:10.892
Feb 12 11:32:10.899: INFO: Waiting up to 5m0s for pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f" in namespace "container-probe-1119" to be "not pending"
Feb 12 11:32:10.904: INFO: Pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609919ms
Feb 12 11:32:12.909: INFO: Pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010362122s
Feb 12 11:32:12.909: INFO: Pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f" satisfied condition "not pending"
Feb 12 11:32:12.909: INFO: Started pod liveness-a19e0327-72ef-4505-b5a4-b908d09b811f in namespace container-probe-1119
STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:32:12.909
Feb 12 11:32:12.915: INFO: Initial restart count of pod liveness-a19e0327-72ef-4505-b5a4-b908d09b811f is 0
Feb 12 11:32:33.030: INFO: Restart count of pod container-probe-1119/liveness-a19e0327-72ef-4505-b5a4-b908d09b811f is now 1 (20.115318447s elapsed)
STEP: deleting the pod 02/12/23 11:32:33.031
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 11:32:33.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1119" for this suite. 02/12/23 11:32:33.066
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":101,"skipped":1787,"failed":0}
------------------------------
 [SLOW TEST] [22.228 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:32:10.858
    Feb 12 11:32:10.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 11:32:10.859
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:10.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:10.889
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-a19e0327-72ef-4505-b5a4-b908d09b811f in namespace container-probe-1119 02/12/23 11:32:10.892
    Feb 12 11:32:10.899: INFO: Waiting up to 5m0s for pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f" in namespace "container-probe-1119" to be "not pending"
    Feb 12 11:32:10.904: INFO: Pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.609919ms
    Feb 12 11:32:12.909: INFO: Pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010362122s
    Feb 12 11:32:12.909: INFO: Pod "liveness-a19e0327-72ef-4505-b5a4-b908d09b811f" satisfied condition "not pending"
    Feb 12 11:32:12.909: INFO: Started pod liveness-a19e0327-72ef-4505-b5a4-b908d09b811f in namespace container-probe-1119
    STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:32:12.909
    Feb 12 11:32:12.915: INFO: Initial restart count of pod liveness-a19e0327-72ef-4505-b5a4-b908d09b811f is 0
    Feb 12 11:32:33.030: INFO: Restart count of pod container-probe-1119/liveness-a19e0327-72ef-4505-b5a4-b908d09b811f is now 1 (20.115318447s elapsed)
    STEP: deleting the pod 02/12/23 11:32:33.031
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 11:32:33.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1119" for this suite. 02/12/23 11:32:33.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:32:33.096
Feb 12 11:32:33.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename hostport 02/12/23 11:32:33.096
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:33.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:33.123
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/12/23 11:32:33.128
Feb 12 11:32:33.135: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6539" to be "running and ready"
Feb 12 11:32:33.140: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.736279ms
Feb 12 11:32:33.140: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:32:35.146: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.009882953s
Feb 12 11:32:35.146: INFO: The phase of Pod pod1 is Running (Ready = false)
Feb 12 11:32:37.154: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.018351022s
Feb 12 11:32:37.154: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 12 11:32:37.154: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.2.20.102 on the node which pod1 resides and expect scheduled 02/12/23 11:32:37.155
Feb 12 11:32:37.175: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6539" to be "running and ready"
Feb 12 11:32:37.181: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.35535ms
Feb 12 11:32:37.181: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:32:39.195: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020281376s
Feb 12 11:32:39.195: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 12 11:32:39.195: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.2.20.102 but use UDP protocol on the node which pod2 resides 02/12/23 11:32:39.196
Feb 12 11:32:39.215: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6539" to be "running and ready"
Feb 12 11:32:39.220: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323738ms
Feb 12 11:32:39.220: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:32:41.259: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.043349409s
Feb 12 11:32:41.259: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 12 11:32:41.259: INFO: Pod "pod3" satisfied condition "running and ready"
Feb 12 11:32:41.270: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6539" to be "running and ready"
Feb 12 11:32:41.274: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.707486ms
Feb 12 11:32:41.274: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:32:43.290: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.019910568s
Feb 12 11:32:43.290: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 12 11:32:43.290: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/12/23 11:32:43.295
Feb 12 11:32:43.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.2.20.102 http://127.0.0.1:54323/hostname] Namespace:hostport-6539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:32:43.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:32:43.295: INFO: ExecWithOptions: Clientset creation
Feb 12 11:32:43.295: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.2.20.102+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.102, port: 54323 02/12/23 11:32:43.38
Feb 12 11:32:43.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.2.20.102:54323/hostname] Namespace:hostport-6539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:32:43.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:32:43.381: INFO: ExecWithOptions: Clientset creation
Feb 12 11:32:43.381: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.2.20.102%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.102, port: 54323 UDP 02/12/23 11:32:43.449
Feb 12 11:32:43.449: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.2.20.102 54323] Namespace:hostport-6539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:32:43.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:32:43.450: INFO: ExecWithOptions: Clientset creation
Feb 12 11:32:43.451: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.2.20.102+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Feb 12 11:32:48.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6539" for this suite. 02/12/23 11:32:48.532
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":102,"skipped":1850,"failed":0}
------------------------------
 [SLOW TEST] [15.447 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:32:33.096
    Feb 12 11:32:33.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename hostport 02/12/23 11:32:33.096
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:33.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:33.123
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/12/23 11:32:33.128
    Feb 12 11:32:33.135: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6539" to be "running and ready"
    Feb 12 11:32:33.140: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.736279ms
    Feb 12 11:32:33.140: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:32:35.146: INFO: Pod "pod1": Phase="Running", Reason="", readiness=false. Elapsed: 2.009882953s
    Feb 12 11:32:35.146: INFO: The phase of Pod pod1 is Running (Ready = false)
    Feb 12 11:32:37.154: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.018351022s
    Feb 12 11:32:37.154: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 12 11:32:37.154: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.2.20.102 on the node which pod1 resides and expect scheduled 02/12/23 11:32:37.155
    Feb 12 11:32:37.175: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6539" to be "running and ready"
    Feb 12 11:32:37.181: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.35535ms
    Feb 12 11:32:37.181: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:32:39.195: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020281376s
    Feb 12 11:32:39.195: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 12 11:32:39.195: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.2.20.102 but use UDP protocol on the node which pod2 resides 02/12/23 11:32:39.196
    Feb 12 11:32:39.215: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6539" to be "running and ready"
    Feb 12 11:32:39.220: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.323738ms
    Feb 12 11:32:39.220: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:32:41.259: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.043349409s
    Feb 12 11:32:41.259: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 12 11:32:41.259: INFO: Pod "pod3" satisfied condition "running and ready"
    Feb 12 11:32:41.270: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6539" to be "running and ready"
    Feb 12 11:32:41.274: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.707486ms
    Feb 12 11:32:41.274: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:32:43.290: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.019910568s
    Feb 12 11:32:43.290: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 12 11:32:43.290: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/12/23 11:32:43.295
    Feb 12 11:32:43.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.2.20.102 http://127.0.0.1:54323/hostname] Namespace:hostport-6539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:32:43.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:32:43.295: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:32:43.295: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.2.20.102+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.102, port: 54323 02/12/23 11:32:43.38
    Feb 12 11:32:43.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.2.20.102:54323/hostname] Namespace:hostport-6539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:32:43.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:32:43.381: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:32:43.381: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.2.20.102%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.2.20.102, port: 54323 UDP 02/12/23 11:32:43.449
    Feb 12 11:32:43.449: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.2.20.102 54323] Namespace:hostport-6539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:32:43.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:32:43.450: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:32:43.451: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.2.20.102+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Feb 12 11:32:48.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-6539" for this suite. 02/12/23 11:32:48.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:32:48.57
Feb 12 11:32:48.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:32:48.571
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:48.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:48.598
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:32:48.603
Feb 12 11:32:48.610: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4359" to be "running and ready"
Feb 12 11:32:48.614: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.422456ms
Feb 12 11:32:48.614: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:32:50.629: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018536408s
Feb 12 11:32:50.629: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 12 11:32:50.629: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 02/12/23 11:32:50.638
Feb 12 11:32:50.655: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4359" to be "running and ready"
Feb 12 11:32:50.660: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.965286ms
Feb 12 11:32:50.660: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:32:52.678: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.02256685s
Feb 12 11:32:52.678: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 12 11:32:52.678: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/12/23 11:32:52.69
Feb 12 11:32:52.710: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 11:32:52.714: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 11:32:54.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 11:32:55.461: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 11:32:56.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 11:32:56.811: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 12 11:32:58.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 12 11:32:58.728: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/12/23 11:32:58.728
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 12 11:32:58.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4359" for this suite. 02/12/23 11:32:58.754
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":103,"skipped":1889,"failed":0}
------------------------------
 [SLOW TEST] [10.195 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:32:48.57
    Feb 12 11:32:48.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:32:48.571
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:48.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:48.598
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:32:48.603
    Feb 12 11:32:48.610: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4359" to be "running and ready"
    Feb 12 11:32:48.614: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.422456ms
    Feb 12 11:32:48.614: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:32:50.629: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.018536408s
    Feb 12 11:32:50.629: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 12 11:32:50.629: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 02/12/23 11:32:50.638
    Feb 12 11:32:50.655: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4359" to be "running and ready"
    Feb 12 11:32:50.660: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.965286ms
    Feb 12 11:32:50.660: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:32:52.678: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.02256685s
    Feb 12 11:32:52.678: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 12 11:32:52.678: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/12/23 11:32:52.69
    Feb 12 11:32:52.710: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 12 11:32:52.714: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 12 11:32:54.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 12 11:32:55.461: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 12 11:32:56.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 12 11:32:56.811: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 12 11:32:58.715: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 12 11:32:58.728: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/12/23 11:32:58.728
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 12 11:32:58.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4359" for this suite. 02/12/23 11:32:58.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:32:58.768
Feb 12 11:32:58.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:32:58.769
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:58.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:58.798
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 02/12/23 11:32:58.8
STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:32:58.808
STEP: Creating a ResourceQuota with not terminating scope 02/12/23 11:33:00.817
STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:33:00.893
STEP: Creating a long running pod 02/12/23 11:33:02.898
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/12/23 11:33:03.072
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/12/23 11:33:05.077
STEP: Deleting the pod 02/12/23 11:33:07.09
STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:07.12
STEP: Creating a terminating pod 02/12/23 11:33:09.124
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/12/23 11:33:09.141
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/12/23 11:33:11.158
STEP: Deleting the pod 02/12/23 11:33:13.184
STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:13.21
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:33:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6224" for this suite. 02/12/23 11:33:15.242
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":104,"skipped":1901,"failed":0}
------------------------------
 [SLOW TEST] [16.494 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:32:58.768
    Feb 12 11:32:58.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:32:58.769
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:32:58.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:32:58.798
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 02/12/23 11:32:58.8
    STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:32:58.808
    STEP: Creating a ResourceQuota with not terminating scope 02/12/23 11:33:00.817
    STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:33:00.893
    STEP: Creating a long running pod 02/12/23 11:33:02.898
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/12/23 11:33:03.072
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/12/23 11:33:05.077
    STEP: Deleting the pod 02/12/23 11:33:07.09
    STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:07.12
    STEP: Creating a terminating pod 02/12/23 11:33:09.124
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/12/23 11:33:09.141
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/12/23 11:33:11.158
    STEP: Deleting the pod 02/12/23 11:33:13.184
    STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:13.21
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:33:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6224" for this suite. 02/12/23 11:33:15.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:15.264
Feb 12 11:33:15.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 11:33:15.265
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:15.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:15.285
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 02/12/23 11:33:15.307
STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:33:15.314
Feb 12 11:33:15.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:33:15.320: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:33:16.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 12 11:33:16.333: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:33:17.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 11:33:17.332: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 02/12/23 11:33:17.336
STEP: DeleteCollection of the DaemonSets 02/12/23 11:33:17.34
STEP: Verify that ReplicaSets have been deleted 02/12/23 11:33:17.35
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Feb 12 11:33:17.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12064"},"items":null}

Feb 12 11:33:17.369: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12064"},"items":[{"metadata":{"name":"daemon-set-7t99x","generateName":"daemon-set-","namespace":"daemonsets-2707","uid":"a6b242ec-56d4-477e-8cbd-bb97cf14593d","resourceVersion":"12053","creationTimestamp":"2023-02-12T11:33:15Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cbe40517e03af5ad8ad53f2552390c90cd698a30c423fa53b14ad72efbc32f1c","cni.projectcalico.org/podIP":"10.233.99.107/32","cni.projectcalico.org/podIPs":"10.233.99.107/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"56ff0c9a-f34f-4974-9637-8545ac33165e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ff0c9a-f34f-4974-9637-8545ac33165e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bhzl9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bhzl9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"}],"hostIP":"10.2.20.103","podIP":"10.233.99.107","podIPs":[{"ip":"10.233.99.107"}],"startTime":"2023-02-12T11:33:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-12T11:33:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://160a773161017ea975e3a1a4104c3cbcf96cee3657e61664ec0c7aecf674d92a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bqqw7","generateName":"daemon-set-","namespace":"daemonsets-2707","uid":"03c38549-98d4-4abd-8a19-e3f478100357","resourceVersion":"12058","creationTimestamp":"2023-02-12T11:33:15Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bf8fbbda6ba9ced5eda9d080a6f60ee9c763545ab3ff8fbda490ef65dfdbf9a5","cni.projectcalico.org/podIP":"10.233.120.76/32","cni.projectcalico.org/podIPs":"10.233.120.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"56ff0c9a-f34f-4974-9637-8545ac33165e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ff0c9a-f34f-4974-9637-8545ac33165e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pb8sw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pb8sw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"}],"hostIP":"10.2.20.101","podIP":"10.233.120.76","podIPs":[{"ip":"10.233.120.76"}],"startTime":"2023-02-12T11:33:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-12T11:33:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f11a88c5d3c6ed34ff07f9976d31a925ddb4aff6031d759f7d43418e7a973df4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xj85s","generateName":"daemon-set-","namespace":"daemonsets-2707","uid":"56883481-3c02-4f12-8c39-a87800286d3c","resourceVersion":"12061","creationTimestamp":"2023-02-12T11:33:15Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04ea3b7ef6a45a14fef0a053c1b5ed6396ce776e72629e1a8db33a45396ede19","cni.projectcalico.org/podIP":"10.233.120.209/32","cni.projectcalico.org/podIPs":"10.233.120.209/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"56ff0c9a-f34f-4974-9637-8545ac33165e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ff0c9a-f34f-4974-9637-8545ac33165e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xqsnj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xqsnj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"}],"hostIP":"10.2.20.102","podIP":"10.233.120.209","podIPs":[{"ip":"10.233.120.209"}],"startTime":"2023-02-12T11:33:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-12T11:33:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://10f3f0e2f74c41032bb12d67edfee6ee934194d5122c1eb55c4cacad4a8bafd4","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:33:17.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2707" for this suite. 02/12/23 11:33:17.408
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":105,"skipped":1942,"failed":0}
------------------------------
 [2.151 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:15.264
    Feb 12 11:33:15.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 11:33:15.265
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:15.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:15.285
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 02/12/23 11:33:15.307
    STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:33:15.314
    Feb 12 11:33:15.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:33:15.320: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:33:16.333: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 12 11:33:16.333: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:33:17.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 11:33:17.332: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 02/12/23 11:33:17.336
    STEP: DeleteCollection of the DaemonSets 02/12/23 11:33:17.34
    STEP: Verify that ReplicaSets have been deleted 02/12/23 11:33:17.35
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Feb 12 11:33:17.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12064"},"items":null}

    Feb 12 11:33:17.369: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12064"},"items":[{"metadata":{"name":"daemon-set-7t99x","generateName":"daemon-set-","namespace":"daemonsets-2707","uid":"a6b242ec-56d4-477e-8cbd-bb97cf14593d","resourceVersion":"12053","creationTimestamp":"2023-02-12T11:33:15Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"cbe40517e03af5ad8ad53f2552390c90cd698a30c423fa53b14ad72efbc32f1c","cni.projectcalico.org/podIP":"10.233.99.107/32","cni.projectcalico.org/podIPs":"10.233.99.107/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"56ff0c9a-f34f-4974-9637-8545ac33165e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ff0c9a-f34f-4974-9637-8545ac33165e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-bhzl9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-bhzl9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"}],"hostIP":"10.2.20.103","podIP":"10.233.99.107","podIPs":[{"ip":"10.233.99.107"}],"startTime":"2023-02-12T11:33:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-12T11:33:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://160a773161017ea975e3a1a4104c3cbcf96cee3657e61664ec0c7aecf674d92a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-bqqw7","generateName":"daemon-set-","namespace":"daemonsets-2707","uid":"03c38549-98d4-4abd-8a19-e3f478100357","resourceVersion":"12058","creationTimestamp":"2023-02-12T11:33:15Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"bf8fbbda6ba9ced5eda9d080a6f60ee9c763545ab3ff8fbda490ef65dfdbf9a5","cni.projectcalico.org/podIP":"10.233.120.76/32","cni.projectcalico.org/podIPs":"10.233.120.76/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"56ff0c9a-f34f-4974-9637-8545ac33165e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ff0c9a-f34f-4974-9637-8545ac33165e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pb8sw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pb8sw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"}],"hostIP":"10.2.20.101","podIP":"10.233.120.76","podIPs":[{"ip":"10.233.120.76"}],"startTime":"2023-02-12T11:33:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-12T11:33:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f11a88c5d3c6ed34ff07f9976d31a925ddb4aff6031d759f7d43418e7a973df4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xj85s","generateName":"daemon-set-","namespace":"daemonsets-2707","uid":"56883481-3c02-4f12-8c39-a87800286d3c","resourceVersion":"12061","creationTimestamp":"2023-02-12T11:33:15Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04ea3b7ef6a45a14fef0a053c1b5ed6396ce776e72629e1a8db33a45396ede19","cni.projectcalico.org/podIP":"10.233.120.209/32","cni.projectcalico.org/podIPs":"10.233.120.209/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"56ff0c9a-f34f-4974-9637-8545ac33165e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ff0c9a-f34f-4974-9637-8545ac33165e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-12T11:33:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xqsnj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xqsnj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"kube-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["kube-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-12T11:33:15Z"}],"hostIP":"10.2.20.102","podIP":"10.233.120.209","podIPs":[{"ip":"10.233.120.209"}],"startTime":"2023-02-12T11:33:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-12T11:33:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://10f3f0e2f74c41032bb12d67edfee6ee934194d5122c1eb55c4cacad4a8bafd4","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:33:17.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2707" for this suite. 02/12/23 11:33:17.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:17.417
Feb 12 11:33:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replication-controller 02/12/23 11:33:17.418
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:17.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:17.44
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Feb 12 11:33:17.442: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/12/23 11:33:18.456
STEP: Checking rc "condition-test" has the desired failure condition set 02/12/23 11:33:18.47
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/12/23 11:33:19.48
Feb 12 11:33:19.498: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/12/23 11:33:19.498
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 12 11:33:20.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8179" for this suite. 02/12/23 11:33:20.516
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":106,"skipped":1961,"failed":0}
------------------------------
 [3.107 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:17.417
    Feb 12 11:33:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replication-controller 02/12/23 11:33:17.418
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:17.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:17.44
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Feb 12 11:33:17.442: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/12/23 11:33:18.456
    STEP: Checking rc "condition-test" has the desired failure condition set 02/12/23 11:33:18.47
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/12/23 11:33:19.48
    Feb 12 11:33:19.498: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/12/23 11:33:19.498
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 12 11:33:20.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8179" for this suite. 02/12/23 11:33:20.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:20.525
Feb 12 11:33:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:33:20.526
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:20.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:20.553
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 02/12/23 11:33:20.558
STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:33:20.568
STEP: Creating a ResourceQuota with not best effort scope 02/12/23 11:33:22.572
STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:33:22.577
STEP: Creating a best-effort pod 02/12/23 11:33:24.598
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/12/23 11:33:24.622
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/12/23 11:33:26.634
STEP: Deleting the pod 02/12/23 11:33:28.646
STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:28.67
STEP: Creating a not best-effort pod 02/12/23 11:33:30.682
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/12/23 11:33:30.714
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/12/23 11:33:32.719
STEP: Deleting the pod 02/12/23 11:33:34.738
STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:34.777
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:33:36.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-480" for this suite. 02/12/23 11:33:36.783
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":107,"skipped":1973,"failed":0}
------------------------------
 [SLOW TEST] [16.265 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:20.525
    Feb 12 11:33:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:33:20.526
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:20.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:20.553
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 02/12/23 11:33:20.558
    STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:33:20.568
    STEP: Creating a ResourceQuota with not best effort scope 02/12/23 11:33:22.572
    STEP: Ensuring ResourceQuota status is calculated 02/12/23 11:33:22.577
    STEP: Creating a best-effort pod 02/12/23 11:33:24.598
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/12/23 11:33:24.622
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/12/23 11:33:26.634
    STEP: Deleting the pod 02/12/23 11:33:28.646
    STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:28.67
    STEP: Creating a not best-effort pod 02/12/23 11:33:30.682
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/12/23 11:33:30.714
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/12/23 11:33:32.719
    STEP: Deleting the pod 02/12/23 11:33:34.738
    STEP: Ensuring resource quota status released the pod usage 02/12/23 11:33:34.777
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:33:36.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-480" for this suite. 02/12/23 11:33:36.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:36.79
Feb 12 11:33:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:33:36.791
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:36.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:36.813
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 02/12/23 11:33:36.816
STEP: Creating a ResourceQuota 02/12/23 11:33:41.833
STEP: Ensuring resource quota status is calculated 02/12/23 11:33:41.845
STEP: Creating a ReplicationController 02/12/23 11:33:43.858
STEP: Ensuring resource quota status captures replication controller creation 02/12/23 11:33:43.887
STEP: Deleting a ReplicationController 02/12/23 11:33:45.909
STEP: Ensuring resource quota status released usage 02/12/23 11:33:45.921
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:33:47.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-585" for this suite. 02/12/23 11:33:47.933
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":108,"skipped":1980,"failed":0}
------------------------------
 [SLOW TEST] [11.153 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:36.79
    Feb 12 11:33:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:33:36.791
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:36.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:36.813
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 02/12/23 11:33:36.816
    STEP: Creating a ResourceQuota 02/12/23 11:33:41.833
    STEP: Ensuring resource quota status is calculated 02/12/23 11:33:41.845
    STEP: Creating a ReplicationController 02/12/23 11:33:43.858
    STEP: Ensuring resource quota status captures replication controller creation 02/12/23 11:33:43.887
    STEP: Deleting a ReplicationController 02/12/23 11:33:45.909
    STEP: Ensuring resource quota status released usage 02/12/23 11:33:45.921
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:33:47.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-585" for this suite. 02/12/23 11:33:47.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:47.949
Feb 12 11:33:47.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename server-version 02/12/23 11:33:47.95
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:47.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:47.97
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/12/23 11:33:47.972
STEP: Confirm major version 02/12/23 11:33:47.973
Feb 12 11:33:47.973: INFO: Major version: 1
STEP: Confirm minor version 02/12/23 11:33:47.973
Feb 12 11:33:47.973: INFO: cleanMinorVersion: 25
Feb 12 11:33:47.973: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Feb 12 11:33:47.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8095" for this suite. 02/12/23 11:33:47.976
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":109,"skipped":2015,"failed":0}
------------------------------
 [0.999 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:47.949
    Feb 12 11:33:47.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename server-version 02/12/23 11:33:47.95
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:47.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:47.97
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/12/23 11:33:47.972
    STEP: Confirm major version 02/12/23 11:33:47.973
    Feb 12 11:33:47.973: INFO: Major version: 1
    STEP: Confirm minor version 02/12/23 11:33:47.973
    Feb 12 11:33:47.973: INFO: cleanMinorVersion: 25
    Feb 12 11:33:47.973: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Feb 12 11:33:47.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8095" for this suite. 02/12/23 11:33:47.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:48.957
Feb 12 11:33:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replicaset 02/12/23 11:33:48.958
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:50.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:50.323
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/12/23 11:33:50.96
STEP: Verify that the required pods have come up 02/12/23 11:33:51.069
Feb 12 11:33:51.079: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 12 11:33:56.242: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/12/23 11:33:56.242
Feb 12 11:33:56.242: INFO: Waiting up to 5m0s for pod "test-rs-r54hz" in namespace "replicaset-2553" to be "running"
Feb 12 11:33:56.348: INFO: Pod "test-rs-r54hz": Phase="Running", Reason="", readiness=true. Elapsed: 106.117497ms
Feb 12 11:33:56.348: INFO: Pod "test-rs-r54hz" satisfied condition "running"
Feb 12 11:33:56.395: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:2 AvailableReplicas:2 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/12/23 11:33:56.395
STEP: DeleteCollection of the ReplicaSets 02/12/23 11:33:56.443
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/12/23 11:33:56.465
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 12 11:33:56.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2553" for this suite. 02/12/23 11:33:56.473
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":110,"skipped":2043,"failed":0}
------------------------------
 [SLOW TEST] [7.580 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:48.957
    Feb 12 11:33:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replicaset 02/12/23 11:33:48.958
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:50.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:50.323
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/12/23 11:33:50.96
    STEP: Verify that the required pods have come up 02/12/23 11:33:51.069
    Feb 12 11:33:51.079: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 12 11:33:56.242: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/12/23 11:33:56.242
    Feb 12 11:33:56.242: INFO: Waiting up to 5m0s for pod "test-rs-r54hz" in namespace "replicaset-2553" to be "running"
    Feb 12 11:33:56.348: INFO: Pod "test-rs-r54hz": Phase="Running", Reason="", readiness=true. Elapsed: 106.117497ms
    Feb 12 11:33:56.348: INFO: Pod "test-rs-r54hz" satisfied condition "running"
    Feb 12 11:33:56.395: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:2 AvailableReplicas:2 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/12/23 11:33:56.395
    STEP: DeleteCollection of the ReplicaSets 02/12/23 11:33:56.443
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/12/23 11:33:56.465
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 12 11:33:56.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2553" for this suite. 02/12/23 11:33:56.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:33:56.537
Feb 12 11:33:56.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-preemption 02/12/23 11:33:56.538
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:56.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:56.894
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 12 11:33:56.984: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 11:34:57.064: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 02/12/23 11:34:57.071
Feb 12 11:34:57.106: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 12 11:34:57.121: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 12 11:34:57.143: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 12 11:34:57.156: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 12 11:34:57.182: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 12 11:34:57.191: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/12/23 11:34:57.191
Feb 12 11:34:57.192: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:34:57.197: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293982ms
Feb 12 11:34:59.205: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013250429s
Feb 12 11:35:01.209: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.017059352s
Feb 12 11:35:01.209: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 12 11:35:01.209: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:35:01.221: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.689118ms
Feb 12 11:35:01.221: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 11:35:01.221: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:35:01.235: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.554678ms
Feb 12 11:35:01.235: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 11:35:01.236: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:35:01.243: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.258416ms
Feb 12 11:35:01.243: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 11:35:01.243: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:35:01.251: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.518834ms
Feb 12 11:35:03.271: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027212875s
Feb 12 11:35:05.264: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021129687s
Feb 12 11:35:07.254: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010948493s
Feb 12 11:35:09.261: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.017682845s
Feb 12 11:35:09.261: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 11:35:09.261: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:35:09.270: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.751973ms
Feb 12 11:35:09.270: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/12/23 11:35:09.27
Feb 12 11:35:09.290: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7625" to be "running"
Feb 12 11:35:09.297: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.528509ms
Feb 12 11:35:11.546: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256280718s
Feb 12 11:35:13.386: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095746819s
Feb 12 11:35:15.313: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022943499s
Feb 12 11:35:17.307: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.017025829s
Feb 12 11:35:17.307: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:35:17.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7625" for this suite. 02/12/23 11:35:17.343
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":111,"skipped":2057,"failed":0}
------------------------------
 [SLOW TEST] [80.852 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:33:56.537
    Feb 12 11:33:56.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-preemption 02/12/23 11:33:56.538
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:33:56.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:33:56.894
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 12 11:33:56.984: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 12 11:34:57.064: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 02/12/23 11:34:57.071
    Feb 12 11:34:57.106: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 12 11:34:57.121: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 12 11:34:57.143: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 12 11:34:57.156: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 12 11:34:57.182: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 12 11:34:57.191: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/12/23 11:34:57.191
    Feb 12 11:34:57.192: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:34:57.197: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.293982ms
    Feb 12 11:34:59.205: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013250429s
    Feb 12 11:35:01.209: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.017059352s
    Feb 12 11:35:01.209: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 12 11:35:01.209: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:35:01.221: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.689118ms
    Feb 12 11:35:01.221: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 11:35:01.221: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:35:01.235: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.554678ms
    Feb 12 11:35:01.235: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 11:35:01.236: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:35:01.243: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.258416ms
    Feb 12 11:35:01.243: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 11:35:01.243: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:35:01.251: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.518834ms
    Feb 12 11:35:03.271: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027212875s
    Feb 12 11:35:05.264: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021129687s
    Feb 12 11:35:07.254: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010948493s
    Feb 12 11:35:09.261: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.017682845s
    Feb 12 11:35:09.261: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 11:35:09.261: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:35:09.270: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.751973ms
    Feb 12 11:35:09.270: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/12/23 11:35:09.27
    Feb 12 11:35:09.290: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7625" to be "running"
    Feb 12 11:35:09.297: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.528509ms
    Feb 12 11:35:11.546: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.256280718s
    Feb 12 11:35:13.386: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095746819s
    Feb 12 11:35:15.313: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022943499s
    Feb 12 11:35:17.307: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.017025829s
    Feb 12 11:35:17.307: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:35:17.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7625" for this suite. 02/12/23 11:35:17.343
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:35:17.392
Feb 12 11:35:17.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 11:35:17.393
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:35:17.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:35:17.414
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Feb 12 11:35:17.436: INFO: Create a RollingUpdate DaemonSet
Feb 12 11:35:17.442: INFO: Check that daemon pods launch on every node of the cluster
Feb 12 11:35:17.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:35:17.450: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:35:18.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:35:18.480: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:35:19.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:35:19.463: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:35:20.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:35:20.510: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:35:21.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 12 11:35:21.464: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:35:22.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 11:35:22.544: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Feb 12 11:35:22.544: INFO: Update the DaemonSet to trigger a rollout
Feb 12 11:35:22.594: INFO: Updating DaemonSet daemon-set
Feb 12 11:35:24.650: INFO: Roll back the DaemonSet before rollout is complete
Feb 12 11:35:24.662: INFO: Updating DaemonSet daemon-set
Feb 12 11:35:24.662: INFO: Make sure DaemonSet rollback is complete
Feb 12 11:35:24.666: INFO: Wrong image for pod: daemon-set-h4lz8. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Feb 12 11:35:24.666: INFO: Pod daemon-set-h4lz8 is not available
Feb 12 11:35:28.676: INFO: Pod daemon-set-df8g9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:35:28.687
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3926, will wait for the garbage collector to delete the pods 02/12/23 11:35:28.687
Feb 12 11:35:28.752: INFO: Deleting DaemonSet.extensions daemon-set took: 10.960933ms
Feb 12 11:35:28.853: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.998711ms
Feb 12 11:35:30.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:35:30.658: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 12 11:35:30.661: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12915"},"items":null}

Feb 12 11:35:30.664: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12915"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:35:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3926" for this suite. 02/12/23 11:35:30.677
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":112,"skipped":2071,"failed":0}
------------------------------
 [SLOW TEST] [13.290 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:35:17.392
    Feb 12 11:35:17.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 11:35:17.393
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:35:17.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:35:17.414
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Feb 12 11:35:17.436: INFO: Create a RollingUpdate DaemonSet
    Feb 12 11:35:17.442: INFO: Check that daemon pods launch on every node of the cluster
    Feb 12 11:35:17.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:35:17.450: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:35:18.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:35:18.480: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:35:19.463: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:35:19.463: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:35:20.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:35:20.510: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:35:21.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 12 11:35:21.464: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:35:22.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 11:35:22.544: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Feb 12 11:35:22.544: INFO: Update the DaemonSet to trigger a rollout
    Feb 12 11:35:22.594: INFO: Updating DaemonSet daemon-set
    Feb 12 11:35:24.650: INFO: Roll back the DaemonSet before rollout is complete
    Feb 12 11:35:24.662: INFO: Updating DaemonSet daemon-set
    Feb 12 11:35:24.662: INFO: Make sure DaemonSet rollback is complete
    Feb 12 11:35:24.666: INFO: Wrong image for pod: daemon-set-h4lz8. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Feb 12 11:35:24.666: INFO: Pod daemon-set-h4lz8 is not available
    Feb 12 11:35:28.676: INFO: Pod daemon-set-df8g9 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:35:28.687
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3926, will wait for the garbage collector to delete the pods 02/12/23 11:35:28.687
    Feb 12 11:35:28.752: INFO: Deleting DaemonSet.extensions daemon-set took: 10.960933ms
    Feb 12 11:35:28.853: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.998711ms
    Feb 12 11:35:30.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:35:30.658: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 12 11:35:30.661: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12915"},"items":null}

    Feb 12 11:35:30.664: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12915"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:35:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3926" for this suite. 02/12/23 11:35:30.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:35:30.686
Feb 12 11:35:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:35:30.687
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:35:30.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:35:30.706
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:35:30.721
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:35:31.246
STEP: Deploying the webhook pod 02/12/23 11:35:31.256
STEP: Wait for the deployment to be ready 02/12/23 11:35:31.273
Feb 12 11:35:31.284: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 11:35:33.295
STEP: Verifying the service has paired with the endpoint 02/12/23 11:35:33.306
Feb 12 11:35:34.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Feb 12 11:35:34.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2322-crds.webhook.example.com via the AdmissionRegistration API 02/12/23 11:35:39.851
STEP: Creating a custom resource that should be mutated by the webhook 02/12/23 11:35:41.974
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:35:42.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2206" for this suite. 02/12/23 11:35:42.808
STEP: Destroying namespace "webhook-2206-markers" for this suite. 02/12/23 11:35:42.844
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":113,"skipped":2121,"failed":0}
------------------------------
 [SLOW TEST] [12.618 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:35:30.686
    Feb 12 11:35:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:35:30.687
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:35:30.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:35:30.706
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:35:30.721
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:35:31.246
    STEP: Deploying the webhook pod 02/12/23 11:35:31.256
    STEP: Wait for the deployment to be ready 02/12/23 11:35:31.273
    Feb 12 11:35:31.284: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 11:35:33.295
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:35:33.306
    Feb 12 11:35:34.307: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Feb 12 11:35:34.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2322-crds.webhook.example.com via the AdmissionRegistration API 02/12/23 11:35:39.851
    STEP: Creating a custom resource that should be mutated by the webhook 02/12/23 11:35:41.974
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:35:42.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2206" for this suite. 02/12/23 11:35:42.808
    STEP: Destroying namespace "webhook-2206-markers" for this suite. 02/12/23 11:35:42.844
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:35:43.308
Feb 12 11:35:43.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 11:35:43.308
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:35:43.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:35:43.436
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7996 02/12/23 11:35:43.438
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-7996 02/12/23 11:35:43.492
Feb 12 11:35:43.635: INFO: Found 0 stateful pods, waiting for 1
Feb 12 11:35:53.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/12/23 11:35:53.645
STEP: updating a scale subresource 02/12/23 11:35:53.648
STEP: verifying the statefulset Spec.Replicas was modified 02/12/23 11:35:53.657
STEP: Patch a scale subresource 02/12/23 11:35:53.659
STEP: verifying the statefulset Spec.Replicas was modified 02/12/23 11:35:53.675
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 11:35:53.683: INFO: Deleting all statefulset in ns statefulset-7996
Feb 12 11:35:53.686: INFO: Scaling statefulset ss to 0
Feb 12 11:36:03.735: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:36:03.745: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 11:36:03.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7996" for this suite. 02/12/23 11:36:03.794
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":114,"skipped":2144,"failed":0}
------------------------------
 [SLOW TEST] [20.497 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:35:43.308
    Feb 12 11:35:43.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 11:35:43.308
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:35:43.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:35:43.436
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7996 02/12/23 11:35:43.438
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-7996 02/12/23 11:35:43.492
    Feb 12 11:35:43.635: INFO: Found 0 stateful pods, waiting for 1
    Feb 12 11:35:53.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/12/23 11:35:53.645
    STEP: updating a scale subresource 02/12/23 11:35:53.648
    STEP: verifying the statefulset Spec.Replicas was modified 02/12/23 11:35:53.657
    STEP: Patch a scale subresource 02/12/23 11:35:53.659
    STEP: verifying the statefulset Spec.Replicas was modified 02/12/23 11:35:53.675
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 11:35:53.683: INFO: Deleting all statefulset in ns statefulset-7996
    Feb 12 11:35:53.686: INFO: Scaling statefulset ss to 0
    Feb 12 11:36:03.735: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:36:03.745: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 11:36:03.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7996" for this suite. 02/12/23 11:36:03.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:36:03.806
Feb 12 11:36:03.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replicaset 02/12/23 11:36:03.807
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:03.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:03.828
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/12/23 11:36:03.833
STEP: Verify that the required pods have come up. 02/12/23 11:36:03.84
Feb 12 11:36:03.843: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 12 11:36:08.857: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/12/23 11:36:08.857
STEP: Getting /status 02/12/23 11:36:08.858
Feb 12 11:36:08.869: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/12/23 11:36:08.869
Feb 12 11:36:08.917: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/12/23 11:36:08.917
Feb 12 11:36:08.918: INFO: Observed &ReplicaSet event: ADDED
Feb 12 11:36:08.919: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.919: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.919: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.919: INFO: Found replicaset test-rs in namespace replicaset-2654 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 12 11:36:08.919: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/12/23 11:36:08.919
Feb 12 11:36:08.919: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 12 11:36:08.927: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/12/23 11:36:08.927
Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: ADDED
Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.930: INFO: Observed replicaset test-rs in namespace replicaset-2654 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
Feb 12 11:36:08.930: INFO: Found replicaset test-rs in namespace replicaset-2654 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 12 11:36:08.930: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 12 11:36:08.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2654" for this suite. 02/12/23 11:36:08.934
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":115,"skipped":2162,"failed":0}
------------------------------
 [SLOW TEST] [5.144 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:36:03.806
    Feb 12 11:36:03.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replicaset 02/12/23 11:36:03.807
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:03.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:03.828
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/12/23 11:36:03.833
    STEP: Verify that the required pods have come up. 02/12/23 11:36:03.84
    Feb 12 11:36:03.843: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 12 11:36:08.857: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/12/23 11:36:08.857
    STEP: Getting /status 02/12/23 11:36:08.858
    Feb 12 11:36:08.869: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/12/23 11:36:08.869
    Feb 12 11:36:08.917: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/12/23 11:36:08.917
    Feb 12 11:36:08.918: INFO: Observed &ReplicaSet event: ADDED
    Feb 12 11:36:08.919: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.919: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.919: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.919: INFO: Found replicaset test-rs in namespace replicaset-2654 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 12 11:36:08.919: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/12/23 11:36:08.919
    Feb 12 11:36:08.919: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 12 11:36:08.927: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/12/23 11:36:08.927
    Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: ADDED
    Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.930: INFO: Observed replicaset test-rs in namespace replicaset-2654 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 12 11:36:08.930: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 12 11:36:08.930: INFO: Found replicaset test-rs in namespace replicaset-2654 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 12 11:36:08.930: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 12 11:36:08.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2654" for this suite. 02/12/23 11:36:08.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:36:08.96
Feb 12 11:36:08.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:36:08.961
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:08.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:08.992
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:36:08.996
Feb 12 11:36:09.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb" in namespace "projected-6348" to be "Succeeded or Failed"
Feb 12 11:36:09.010: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.082356ms
Feb 12 11:36:11.018: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012742166s
Feb 12 11:36:13.018: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012407759s
STEP: Saw pod success 02/12/23 11:36:13.018
Feb 12 11:36:13.018: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb" satisfied condition "Succeeded or Failed"
Feb 12 11:36:13.022: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb container client-container: <nil>
STEP: delete the pod 02/12/23 11:36:13.039
Feb 12 11:36:13.057: INFO: Waiting for pod downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb to disappear
Feb 12 11:36:13.063: INFO: Pod downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:36:13.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6348" for this suite. 02/12/23 11:36:13.067
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":116,"skipped":2194,"failed":0}
------------------------------
 [4.118 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:36:08.96
    Feb 12 11:36:08.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:36:08.961
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:08.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:08.992
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:36:08.996
    Feb 12 11:36:09.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb" in namespace "projected-6348" to be "Succeeded or Failed"
    Feb 12 11:36:09.010: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.082356ms
    Feb 12 11:36:11.018: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012742166s
    Feb 12 11:36:13.018: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012407759s
    STEP: Saw pod success 02/12/23 11:36:13.018
    Feb 12 11:36:13.018: INFO: Pod "downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb" satisfied condition "Succeeded or Failed"
    Feb 12 11:36:13.022: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb container client-container: <nil>
    STEP: delete the pod 02/12/23 11:36:13.039
    Feb 12 11:36:13.057: INFO: Waiting for pod downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb to disappear
    Feb 12 11:36:13.063: INFO: Pod downwardapi-volume-fa69ce83-eb25-4484-a5cd-95bfabf5c9eb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:36:13.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6348" for this suite. 02/12/23 11:36:13.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:36:13.081
Feb 12 11:36:13.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:36:13.082
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:13.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:13.108
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-bc0438cb-2478-4db0-b450-2df98819b9d6 02/12/23 11:36:13.11
STEP: Creating a pod to test consume secrets 02/12/23 11:36:13.115
Feb 12 11:36:13.125: INFO: Waiting up to 5m0s for pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d" in namespace "secrets-964" to be "Succeeded or Failed"
Feb 12 11:36:13.136: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.03849ms
Feb 12 11:36:15.152: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026342509s
Feb 12 11:36:17.155: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029731999s
STEP: Saw pod success 02/12/23 11:36:17.155
Feb 12 11:36:17.155: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d" satisfied condition "Succeeded or Failed"
Feb 12 11:36:17.167: INFO: Trying to get logs from node kube-3 pod pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d container secret-env-test: <nil>
STEP: delete the pod 02/12/23 11:36:17.18
Feb 12 11:36:17.201: INFO: Waiting for pod pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d to disappear
Feb 12 11:36:17.205: INFO: Pod pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:36:17.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-964" for this suite. 02/12/23 11:36:17.208
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":117,"skipped":2207,"failed":0}
------------------------------
 [4.133 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:36:13.081
    Feb 12 11:36:13.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:36:13.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:13.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:13.108
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-bc0438cb-2478-4db0-b450-2df98819b9d6 02/12/23 11:36:13.11
    STEP: Creating a pod to test consume secrets 02/12/23 11:36:13.115
    Feb 12 11:36:13.125: INFO: Waiting up to 5m0s for pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d" in namespace "secrets-964" to be "Succeeded or Failed"
    Feb 12 11:36:13.136: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.03849ms
    Feb 12 11:36:15.152: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026342509s
    Feb 12 11:36:17.155: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029731999s
    STEP: Saw pod success 02/12/23 11:36:17.155
    Feb 12 11:36:17.155: INFO: Pod "pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d" satisfied condition "Succeeded or Failed"
    Feb 12 11:36:17.167: INFO: Trying to get logs from node kube-3 pod pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d container secret-env-test: <nil>
    STEP: delete the pod 02/12/23 11:36:17.18
    Feb 12 11:36:17.201: INFO: Waiting for pod pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d to disappear
    Feb 12 11:36:17.205: INFO: Pod pod-secrets-849fbc30-b2bd-40cd-83e8-ced5ef7dac5d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:36:17.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-964" for this suite. 02/12/23 11:36:17.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:36:17.215
Feb 12 11:36:17.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:36:17.216
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:17.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:17.236
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Feb 12 11:36:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/12/23 11:36:24.378
Feb 12 11:36:24.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 create -f -'
Feb 12 11:36:24.926: INFO: stderr: ""
Feb 12 11:36:24.926: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 12 11:36:24.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 delete e2e-test-crd-publish-openapi-3910-crds test-cr'
Feb 12 11:36:25.294: INFO: stderr: ""
Feb 12 11:36:25.294: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 12 11:36:25.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 apply -f -'
Feb 12 11:36:25.890: INFO: stderr: ""
Feb 12 11:36:25.890: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 12 11:36:25.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 delete e2e-test-crd-publish-openapi-3910-crds test-cr'
Feb 12 11:36:26.052: INFO: stderr: ""
Feb 12 11:36:26.052: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/12/23 11:36:26.052
Feb 12 11:36:26.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 explain e2e-test-crd-publish-openapi-3910-crds'
Feb 12 11:36:26.260: INFO: stderr: ""
Feb 12 11:36:26.260: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3910-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:36:31.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8341" for this suite. 02/12/23 11:36:31.063
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":118,"skipped":2214,"failed":0}
------------------------------
 [SLOW TEST] [13.873 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:36:17.215
    Feb 12 11:36:17.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:36:17.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:17.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:17.236
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Feb 12 11:36:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/12/23 11:36:24.378
    Feb 12 11:36:24.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 create -f -'
    Feb 12 11:36:24.926: INFO: stderr: ""
    Feb 12 11:36:24.926: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 12 11:36:24.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 delete e2e-test-crd-publish-openapi-3910-crds test-cr'
    Feb 12 11:36:25.294: INFO: stderr: ""
    Feb 12 11:36:25.294: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 12 11:36:25.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 apply -f -'
    Feb 12 11:36:25.890: INFO: stderr: ""
    Feb 12 11:36:25.890: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 12 11:36:25.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 --namespace=crd-publish-openapi-8341 delete e2e-test-crd-publish-openapi-3910-crds test-cr'
    Feb 12 11:36:26.052: INFO: stderr: ""
    Feb 12 11:36:26.052: INFO: stdout: "e2e-test-crd-publish-openapi-3910-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/12/23 11:36:26.052
    Feb 12 11:36:26.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-8341 explain e2e-test-crd-publish-openapi-3910-crds'
    Feb 12 11:36:26.260: INFO: stderr: ""
    Feb 12 11:36:26.260: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3910-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:36:31.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8341" for this suite. 02/12/23 11:36:31.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:36:31.088
Feb 12 11:36:31.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-watch 02/12/23 11:36:31.089
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:31.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:31.176
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 12 11:36:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Creating first CR  02/12/23 11:36:38.822
Feb 12 11:36:38.827: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:38Z]] name:name1 resourceVersion:13388 uid:8dbfca29-3da8-4629-820c-ff884d5dc123] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/12/23 11:36:48.827
Feb 12 11:36:48.844: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:48Z]] name:name2 resourceVersion:13407 uid:2d71e814-ded4-429f-9707-5ae719fa4b73] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/12/23 11:36:58.844
Feb 12 11:36:58.851: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:58Z]] name:name1 resourceVersion:13428 uid:8dbfca29-3da8-4629-820c-ff884d5dc123] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/12/23 11:37:08.852
Feb 12 11:37:08.880: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:37:08Z]] name:name2 resourceVersion:13447 uid:2d71e814-ded4-429f-9707-5ae719fa4b73] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/12/23 11:37:18.881
Feb 12 11:37:18.895: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:58Z]] name:name1 resourceVersion:13469 uid:8dbfca29-3da8-4629-820c-ff884d5dc123] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/12/23 11:37:28.896
Feb 12 11:37:28.908: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:37:08Z]] name:name2 resourceVersion:13488 uid:2d71e814-ded4-429f-9707-5ae719fa4b73] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:37:39.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8763" for this suite. 02/12/23 11:37:39.444
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":119,"skipped":2229,"failed":0}
------------------------------
 [SLOW TEST] [68.366 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:36:31.088
    Feb 12 11:36:31.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-watch 02/12/23 11:36:31.089
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:36:31.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:36:31.176
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 12 11:36:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Creating first CR  02/12/23 11:36:38.822
    Feb 12 11:36:38.827: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:38Z]] name:name1 resourceVersion:13388 uid:8dbfca29-3da8-4629-820c-ff884d5dc123] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/12/23 11:36:48.827
    Feb 12 11:36:48.844: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:48Z]] name:name2 resourceVersion:13407 uid:2d71e814-ded4-429f-9707-5ae719fa4b73] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/12/23 11:36:58.844
    Feb 12 11:36:58.851: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:58Z]] name:name1 resourceVersion:13428 uid:8dbfca29-3da8-4629-820c-ff884d5dc123] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/12/23 11:37:08.852
    Feb 12 11:37:08.880: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:37:08Z]] name:name2 resourceVersion:13447 uid:2d71e814-ded4-429f-9707-5ae719fa4b73] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/12/23 11:37:18.881
    Feb 12 11:37:18.895: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:36:58Z]] name:name1 resourceVersion:13469 uid:8dbfca29-3da8-4629-820c-ff884d5dc123] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/12/23 11:37:28.896
    Feb 12 11:37:28.908: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-12T11:36:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-12T11:37:08Z]] name:name2 resourceVersion:13488 uid:2d71e814-ded4-429f-9707-5ae719fa4b73] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:37:39.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-8763" for this suite. 02/12/23 11:37:39.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:37:39.457
Feb 12 11:37:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-pred 02/12/23 11:37:39.458
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:37:39.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:37:39.483
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 12 11:37:39.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 11:37:39.491: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 11:37:39.494: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Feb 12 11:37:39.500: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container calico-node ready: true, restart count 1
Feb 12 11:37:39.500: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container coredns ready: true, restart count 0
Feb 12 11:37:39.500: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container kube-apiserver ready: true, restart count 2
Feb 12 11:37:39.500: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container kube-controller-manager ready: true, restart count 6
Feb 12 11:37:39.500: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:37:39.500: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container kube-scheduler ready: true, restart count 5
Feb 12 11:37:39.500: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:37:39.500: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:37:39.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:37:39.500: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:37:39.500: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Feb 12 11:37:39.507: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 11:37:39.507: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container coredns ready: true, restart count 0
Feb 12 11:37:39.507: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container autoscaler ready: true, restart count 0
Feb 12 11:37:39.507: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 12 11:37:39.507: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container kube-controller-manager ready: true, restart count 6
Feb 12 11:37:39.507: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:37:39.507: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container kube-scheduler ready: true, restart count 6
Feb 12 11:37:39.507: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:37:39.507: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:37:39.507: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:37:39.507: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 11:37:39.507: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Feb 12 11:37:39.513: INFO: calico-kube-controllers-75748cc9fd-2rtj6 from kube-system started at 2023-02-12 11:00:53 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.513: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 12 11:37:39.513: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.513: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 11:37:39.513: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.513: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 11:37:39.513: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.513: INFO: 	Container nginx-proxy ready: true, restart count 0
Feb 12 11:37:39.513: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.513: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 11:37:39.513: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
Feb 12 11:37:39.513: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 11:37:39.513: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:37:39.514: INFO: 	Container e2e ready: true, restart count 0
Feb 12 11:37:39.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:37:39.514: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 11:37:39.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 11:37:39.514: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/12/23 11:37:39.514
Feb 12 11:37:39.523: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5432" to be "running"
Feb 12 11:37:39.533: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.968415ms
Feb 12 11:37:41.545: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.021916206s
Feb 12 11:37:41.546: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/12/23 11:37:41.559
STEP: Trying to apply a random label on the found node. 02/12/23 11:37:41.59
STEP: verifying the node has the label kubernetes.io/e2e-f7ad62d0-fc30-406c-bf31-09a3a48de482 95 02/12/23 11:37:41.605
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/12/23 11:37:41.61
Feb 12 11:37:41.617: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5432" to be "not pending"
Feb 12 11:37:41.620: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.114832ms
Feb 12 11:37:43.629: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011843634s
Feb 12 11:37:43.629: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.2.20.103 on the node which pod4 resides and expect not scheduled 02/12/23 11:37:43.629
Feb 12 11:37:43.643: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5432" to be "not pending"
Feb 12 11:37:43.656: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.209118ms
Feb 12 11:37:45.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026468486s
Feb 12 11:37:47.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031849529s
Feb 12 11:37:49.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028351311s
Feb 12 11:37:51.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027969058s
Feb 12 11:37:53.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029127503s
Feb 12 11:37:55.663: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019081492s
Feb 12 11:37:57.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017887661s
Feb 12 11:37:59.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023981434s
Feb 12 11:38:01.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.030281948s
Feb 12 11:38:03.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029948218s
Feb 12 11:38:05.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025501255s
Feb 12 11:38:07.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.028530322s
Feb 12 11:38:09.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026835354s
Feb 12 11:38:11.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.024688327s
Feb 12 11:38:14.268: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.624158186s
Feb 12 11:38:16.298: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.65451364s
Feb 12 11:38:17.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023132959s
Feb 12 11:38:19.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.028951302s
Feb 12 11:38:21.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026967072s
Feb 12 11:38:23.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.028798474s
Feb 12 11:38:25.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026860669s
Feb 12 11:38:27.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.024009756s
Feb 12 11:38:29.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025475651s
Feb 12 11:38:31.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.027147331s
Feb 12 11:38:33.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017694415s
Feb 12 11:38:35.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017728552s
Feb 12 11:38:37.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.121746352s
Feb 12 11:38:39.735: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.091700821s
Feb 12 11:38:41.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.016734576s
Feb 12 11:38:43.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.02681494s
Feb 12 11:38:45.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.027203366s
Feb 12 11:38:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028197599s
Feb 12 11:38:49.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026627728s
Feb 12 11:38:51.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017064024s
Feb 12 11:38:53.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.02570259s
Feb 12 11:38:55.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.025241454s
Feb 12 11:38:57.662: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.018663433s
Feb 12 11:38:59.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.017623542s
Feb 12 11:39:01.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.028664849s
Feb 12 11:39:03.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.028297996s
Feb 12 11:39:05.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.02088057s
Feb 12 11:39:07.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.029198817s
Feb 12 11:39:09.659: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015886742s
Feb 12 11:39:11.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.027185999s
Feb 12 11:39:13.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.027251692s
Feb 12 11:39:15.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027914339s
Feb 12 11:39:17.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.026211646s
Feb 12 11:39:19.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.026920252s
Feb 12 11:39:21.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025540755s
Feb 12 11:39:23.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023935875s
Feb 12 11:39:25.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025940417s
Feb 12 11:39:27.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017507523s
Feb 12 11:39:29.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026687873s
Feb 12 11:39:31.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.027710133s
Feb 12 11:39:33.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017702946s
Feb 12 11:39:35.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.024503762s
Feb 12 11:39:37.659: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015855766s
Feb 12 11:39:39.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029876206s
Feb 12 11:39:41.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02528862s
Feb 12 11:39:43.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024329689s
Feb 12 11:39:45.659: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.015341367s
Feb 12 11:39:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.028634072s
Feb 12 11:39:49.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025530849s
Feb 12 11:39:51.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.025985677s
Feb 12 11:39:53.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.025590289s
Feb 12 11:39:55.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.02661535s
Feb 12 11:39:57.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.028509018s
Feb 12 11:39:59.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.024947269s
Feb 12 11:40:01.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.026313972s
Feb 12 11:40:03.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.016531674s
Feb 12 11:40:05.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.025101443s
Feb 12 11:40:07.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.029011434s
Feb 12 11:40:09.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.023263919s
Feb 12 11:40:11.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.027980275s
Feb 12 11:40:13.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.022143717s
Feb 12 11:40:15.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.027805271s
Feb 12 11:40:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.036337826s
Feb 12 11:40:19.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.025884733s
Feb 12 11:40:21.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.0162765s
Feb 12 11:40:23.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.029534952s
Feb 12 11:40:25.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.028742893s
Feb 12 11:40:27.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.025534043s
Feb 12 11:40:29.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020118875s
Feb 12 11:40:31.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.027568149s
Feb 12 11:40:33.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.031812585s
Feb 12 11:40:35.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.026274138s
Feb 12 11:40:37.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.026822003s
Feb 12 11:40:39.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.016526584s
Feb 12 11:40:41.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.026448027s
Feb 12 11:40:43.665: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.021479712s
Feb 12 11:40:45.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.027277014s
Feb 12 11:40:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.028369648s
Feb 12 11:40:49.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.027075258s
Feb 12 11:40:51.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.023807209s
Feb 12 11:40:53.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.026707911s
Feb 12 11:40:55.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.026286966s
Feb 12 11:40:57.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016136295s
Feb 12 11:40:59.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.022507048s
Feb 12 11:41:01.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.028309822s
Feb 12 11:41:03.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.024455699s
Feb 12 11:41:05.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.023162981s
Feb 12 11:41:07.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.029435728s
Feb 12 11:41:09.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.025738318s
Feb 12 11:41:11.665: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.021258236s
Feb 12 11:41:13.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.022464675s
Feb 12 11:41:15.662: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018060679s
Feb 12 11:41:17.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.030306989s
Feb 12 11:41:19.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.02249708s
Feb 12 11:41:21.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.02563268s
Feb 12 11:41:23.663: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019863852s
Feb 12 11:41:25.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.017223659s
Feb 12 11:41:27.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.031418203s
Feb 12 11:41:29.662: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.018955034s
Feb 12 11:41:31.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.026931669s
Feb 12 11:41:33.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.017121332s
Feb 12 11:41:35.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.020688436s
Feb 12 11:41:37.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.017552206s
Feb 12 11:41:39.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.025575882s
Feb 12 11:41:41.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.028444406s
Feb 12 11:41:43.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.028467488s
Feb 12 11:41:45.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026388545s
Feb 12 11:41:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.028108332s
Feb 12 11:41:49.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.025941663s
Feb 12 11:41:51.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.017421132s
Feb 12 11:41:53.931: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.287667538s
Feb 12 11:41:55.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.026760104s
Feb 12 11:41:57.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.023956736s
Feb 12 11:41:59.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.027573676s
Feb 12 11:42:01.663: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.019062314s
Feb 12 11:42:03.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.031127217s
Feb 12 11:42:05.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.027222892s
Feb 12 11:42:07.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.028261929s
Feb 12 11:42:09.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.016427811s
Feb 12 11:42:11.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.025379082s
Feb 12 11:42:13.682: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.038777281s
Feb 12 11:42:16.049: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.40600062s
Feb 12 11:42:17.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.030615138s
Feb 12 11:42:19.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.023327543s
Feb 12 11:42:21.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.025222449s
Feb 12 11:42:23.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.02542776s
Feb 12 11:42:25.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.025349837s
Feb 12 11:42:27.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016795266s
Feb 12 11:42:29.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.022372482s
Feb 12 11:42:31.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.025954348s
Feb 12 11:42:33.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.028898327s
Feb 12 11:42:35.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.025338703s
Feb 12 11:42:37.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016311309s
Feb 12 11:42:39.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.02561682s
Feb 12 11:42:41.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.027282425s
Feb 12 11:42:43.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.024989888s
Feb 12 11:42:43.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.036259884s
STEP: removing the label kubernetes.io/e2e-f7ad62d0-fc30-406c-bf31-09a3a48de482 off the node kube-3 02/12/23 11:42:43.68
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f7ad62d0-fc30-406c-bf31-09a3a48de482 02/12/23 11:42:43.72
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:42:43.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5432" for this suite. 02/12/23 11:42:43.735
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":120,"skipped":2270,"failed":0}
------------------------------
 [SLOW TEST] [304.287 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:37:39.457
    Feb 12 11:37:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-pred 02/12/23 11:37:39.458
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:37:39.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:37:39.483
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 12 11:37:39.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 12 11:37:39.491: INFO: Waiting for terminating namespaces to be deleted...
    Feb 12 11:37:39.494: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Feb 12 11:37:39.500: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container calico-node ready: true, restart count 1
    Feb 12 11:37:39.500: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 11:37:39.500: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container kube-apiserver ready: true, restart count 2
    Feb 12 11:37:39.500: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container kube-controller-manager ready: true, restart count 6
    Feb 12 11:37:39.500: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:37:39.500: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container kube-scheduler ready: true, restart count 5
    Feb 12 11:37:39.500: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:37:39.500: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:37:39.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:37:39.500: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:37:39.500: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Feb 12 11:37:39.507: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container autoscaler ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container kube-apiserver ready: true, restart count 1
    Feb 12 11:37:39.507: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container kube-controller-manager ready: true, restart count 6
    Feb 12 11:37:39.507: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container kube-scheduler ready: true, restart count 6
    Feb 12 11:37:39.507: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:37:39.507: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 11:37:39.507: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Feb 12 11:37:39.513: INFO: calico-kube-controllers-75748cc9fd-2rtj6 from kube-system started at 2023-02-12 11:00:53 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.513: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 12 11:37:39.513: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.513: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 11:37:39.513: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.513: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 11:37:39.513: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.513: INFO: 	Container nginx-proxy ready: true, restart count 0
    Feb 12 11:37:39.513: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.513: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 11:37:39.513: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
    Feb 12 11:37:39.513: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 12 11:37:39.513: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:37:39.514: INFO: 	Container e2e ready: true, restart count 0
    Feb 12 11:37:39.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:37:39.514: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 11:37:39.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 11:37:39.514: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/12/23 11:37:39.514
    Feb 12 11:37:39.523: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5432" to be "running"
    Feb 12 11:37:39.533: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.968415ms
    Feb 12 11:37:41.545: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.021916206s
    Feb 12 11:37:41.546: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/12/23 11:37:41.559
    STEP: Trying to apply a random label on the found node. 02/12/23 11:37:41.59
    STEP: verifying the node has the label kubernetes.io/e2e-f7ad62d0-fc30-406c-bf31-09a3a48de482 95 02/12/23 11:37:41.605
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/12/23 11:37:41.61
    Feb 12 11:37:41.617: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5432" to be "not pending"
    Feb 12 11:37:41.620: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.114832ms
    Feb 12 11:37:43.629: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011843634s
    Feb 12 11:37:43.629: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.2.20.103 on the node which pod4 resides and expect not scheduled 02/12/23 11:37:43.629
    Feb 12 11:37:43.643: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5432" to be "not pending"
    Feb 12 11:37:43.656: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.209118ms
    Feb 12 11:37:45.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026468486s
    Feb 12 11:37:47.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031849529s
    Feb 12 11:37:49.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028351311s
    Feb 12 11:37:51.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027969058s
    Feb 12 11:37:53.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029127503s
    Feb 12 11:37:55.663: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019081492s
    Feb 12 11:37:57.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.017887661s
    Feb 12 11:37:59.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.023981434s
    Feb 12 11:38:01.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.030281948s
    Feb 12 11:38:03.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029948218s
    Feb 12 11:38:05.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025501255s
    Feb 12 11:38:07.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.028530322s
    Feb 12 11:38:09.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026835354s
    Feb 12 11:38:11.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.024688327s
    Feb 12 11:38:14.268: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.624158186s
    Feb 12 11:38:16.298: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.65451364s
    Feb 12 11:38:17.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.023132959s
    Feb 12 11:38:19.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.028951302s
    Feb 12 11:38:21.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026967072s
    Feb 12 11:38:23.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.028798474s
    Feb 12 11:38:25.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026860669s
    Feb 12 11:38:27.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.024009756s
    Feb 12 11:38:29.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025475651s
    Feb 12 11:38:31.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.027147331s
    Feb 12 11:38:33.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.017694415s
    Feb 12 11:38:35.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017728552s
    Feb 12 11:38:37.765: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.121746352s
    Feb 12 11:38:39.735: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.091700821s
    Feb 12 11:38:41.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.016734576s
    Feb 12 11:38:43.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.02681494s
    Feb 12 11:38:45.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.027203366s
    Feb 12 11:38:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.028197599s
    Feb 12 11:38:49.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026627728s
    Feb 12 11:38:51.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.017064024s
    Feb 12 11:38:53.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.02570259s
    Feb 12 11:38:55.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.025241454s
    Feb 12 11:38:57.662: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.018663433s
    Feb 12 11:38:59.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.017623542s
    Feb 12 11:39:01.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.028664849s
    Feb 12 11:39:03.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.028297996s
    Feb 12 11:39:05.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.02088057s
    Feb 12 11:39:07.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.029198817s
    Feb 12 11:39:09.659: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015886742s
    Feb 12 11:39:11.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.027185999s
    Feb 12 11:39:13.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.027251692s
    Feb 12 11:39:15.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027914339s
    Feb 12 11:39:17.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.026211646s
    Feb 12 11:39:19.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.026920252s
    Feb 12 11:39:21.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025540755s
    Feb 12 11:39:23.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.023935875s
    Feb 12 11:39:25.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025940417s
    Feb 12 11:39:27.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017507523s
    Feb 12 11:39:29.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026687873s
    Feb 12 11:39:31.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.027710133s
    Feb 12 11:39:33.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017702946s
    Feb 12 11:39:35.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.024503762s
    Feb 12 11:39:37.659: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015855766s
    Feb 12 11:39:39.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029876206s
    Feb 12 11:39:41.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.02528862s
    Feb 12 11:39:43.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.024329689s
    Feb 12 11:39:45.659: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.015341367s
    Feb 12 11:39:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.028634072s
    Feb 12 11:39:49.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.025530849s
    Feb 12 11:39:51.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.025985677s
    Feb 12 11:39:53.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.025590289s
    Feb 12 11:39:55.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.02661535s
    Feb 12 11:39:57.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.028509018s
    Feb 12 11:39:59.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.024947269s
    Feb 12 11:40:01.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.026313972s
    Feb 12 11:40:03.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.016531674s
    Feb 12 11:40:05.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.025101443s
    Feb 12 11:40:07.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.029011434s
    Feb 12 11:40:09.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.023263919s
    Feb 12 11:40:11.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.027980275s
    Feb 12 11:40:13.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.022143717s
    Feb 12 11:40:15.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.027805271s
    Feb 12 11:40:17.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.036337826s
    Feb 12 11:40:19.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.025884733s
    Feb 12 11:40:21.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.0162765s
    Feb 12 11:40:23.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.029534952s
    Feb 12 11:40:25.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.028742893s
    Feb 12 11:40:27.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.025534043s
    Feb 12 11:40:29.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020118875s
    Feb 12 11:40:31.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.027568149s
    Feb 12 11:40:33.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.031812585s
    Feb 12 11:40:35.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.026274138s
    Feb 12 11:40:37.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.026822003s
    Feb 12 11:40:39.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.016526584s
    Feb 12 11:40:41.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.026448027s
    Feb 12 11:40:43.665: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.021479712s
    Feb 12 11:40:45.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.027277014s
    Feb 12 11:40:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.028369648s
    Feb 12 11:40:49.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.027075258s
    Feb 12 11:40:51.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.023807209s
    Feb 12 11:40:53.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.026707911s
    Feb 12 11:40:55.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.026286966s
    Feb 12 11:40:57.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016136295s
    Feb 12 11:40:59.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.022507048s
    Feb 12 11:41:01.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.028309822s
    Feb 12 11:41:03.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.024455699s
    Feb 12 11:41:05.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.023162981s
    Feb 12 11:41:07.673: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.029435728s
    Feb 12 11:41:09.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.025738318s
    Feb 12 11:41:11.665: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.021258236s
    Feb 12 11:41:13.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.022464675s
    Feb 12 11:41:15.662: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018060679s
    Feb 12 11:41:17.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.030306989s
    Feb 12 11:41:19.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.02249708s
    Feb 12 11:41:21.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.02563268s
    Feb 12 11:41:23.663: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019863852s
    Feb 12 11:41:25.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.017223659s
    Feb 12 11:41:27.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.031418203s
    Feb 12 11:41:29.662: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.018955034s
    Feb 12 11:41:31.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.026931669s
    Feb 12 11:41:33.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.017121332s
    Feb 12 11:41:35.664: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.020688436s
    Feb 12 11:41:37.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.017552206s
    Feb 12 11:41:39.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.025575882s
    Feb 12 11:41:41.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.028444406s
    Feb 12 11:41:43.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.028467488s
    Feb 12 11:41:45.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.026388545s
    Feb 12 11:41:47.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.028108332s
    Feb 12 11:41:49.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.025941663s
    Feb 12 11:41:51.661: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.017421132s
    Feb 12 11:41:53.931: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.287667538s
    Feb 12 11:41:55.670: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.026760104s
    Feb 12 11:41:57.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.023956736s
    Feb 12 11:41:59.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.027573676s
    Feb 12 11:42:01.663: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.019062314s
    Feb 12 11:42:03.675: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.031127217s
    Feb 12 11:42:05.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.027222892s
    Feb 12 11:42:07.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.028261929s
    Feb 12 11:42:09.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.016427811s
    Feb 12 11:42:11.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.025379082s
    Feb 12 11:42:13.682: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.038777281s
    Feb 12 11:42:16.049: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.40600062s
    Feb 12 11:42:17.674: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.030615138s
    Feb 12 11:42:19.667: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.023327543s
    Feb 12 11:42:21.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.025222449s
    Feb 12 11:42:23.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.02542776s
    Feb 12 11:42:25.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.025349837s
    Feb 12 11:42:27.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016795266s
    Feb 12 11:42:29.666: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.022372482s
    Feb 12 11:42:31.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.025954348s
    Feb 12 11:42:33.672: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.028898327s
    Feb 12 11:42:35.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.025338703s
    Feb 12 11:42:37.660: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016311309s
    Feb 12 11:42:39.669: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.02561682s
    Feb 12 11:42:41.671: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.027282425s
    Feb 12 11:42:43.668: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.024989888s
    Feb 12 11:42:43.680: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.036259884s
    STEP: removing the label kubernetes.io/e2e-f7ad62d0-fc30-406c-bf31-09a3a48de482 off the node kube-3 02/12/23 11:42:43.68
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f7ad62d0-fc30-406c-bf31-09a3a48de482 02/12/23 11:42:43.72
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:42:43.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5432" for this suite. 02/12/23 11:42:43.735
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:42:43.746
Feb 12 11:42:43.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:42:43.747
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:42:43.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:42:43.773
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 02/12/23 11:42:43.775
Feb 12 11:42:43.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: mark a version not serverd 02/12/23 11:42:55.077
STEP: check the unserved version gets removed 02/12/23 11:42:55.093
STEP: check the other version is not changed 02/12/23 11:42:56.772
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:43:01.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-912" for this suite. 02/12/23 11:43:01.672
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":121,"skipped":2277,"failed":0}
------------------------------
 [SLOW TEST] [17.932 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:42:43.746
    Feb 12 11:42:43.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:42:43.747
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:42:43.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:42:43.773
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 02/12/23 11:42:43.775
    Feb 12 11:42:43.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: mark a version not serverd 02/12/23 11:42:55.077
    STEP: check the unserved version gets removed 02/12/23 11:42:55.093
    STEP: check the other version is not changed 02/12/23 11:42:56.772
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:43:01.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-912" for this suite. 02/12/23 11:43:01.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:43:01.678
Feb 12 11:43:01.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:43:01.68
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:01.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:01.7
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Feb 12 11:43:01.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/12/23 11:43:09.845
Feb 12 11:43:09.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 create -f -'
Feb 12 11:43:10.342: INFO: stderr: ""
Feb 12 11:43:10.343: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 12 11:43:10.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 delete e2e-test-crd-publish-openapi-6516-crds test-cr'
Feb 12 11:43:10.421: INFO: stderr: ""
Feb 12 11:43:10.421: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 12 11:43:10.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 apply -f -'
Feb 12 11:43:10.637: INFO: stderr: ""
Feb 12 11:43:10.637: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 12 11:43:10.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 delete e2e-test-crd-publish-openapi-6516-crds test-cr'
Feb 12 11:43:10.742: INFO: stderr: ""
Feb 12 11:43:10.742: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/12/23 11:43:10.742
Feb 12 11:43:10.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 explain e2e-test-crd-publish-openapi-6516-crds'
Feb 12 11:43:10.953: INFO: stderr: ""
Feb 12 11:43:10.953: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6516-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:43:14.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6577" for this suite. 02/12/23 11:43:14.158
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":122,"skipped":2285,"failed":0}
------------------------------
 [SLOW TEST] [12.486 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:43:01.678
    Feb 12 11:43:01.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:43:01.68
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:01.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:01.7
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Feb 12 11:43:01.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/12/23 11:43:09.845
    Feb 12 11:43:09.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 create -f -'
    Feb 12 11:43:10.342: INFO: stderr: ""
    Feb 12 11:43:10.343: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 12 11:43:10.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 delete e2e-test-crd-publish-openapi-6516-crds test-cr'
    Feb 12 11:43:10.421: INFO: stderr: ""
    Feb 12 11:43:10.421: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 12 11:43:10.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 apply -f -'
    Feb 12 11:43:10.637: INFO: stderr: ""
    Feb 12 11:43:10.637: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 12 11:43:10.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 --namespace=crd-publish-openapi-6577 delete e2e-test-crd-publish-openapi-6516-crds test-cr'
    Feb 12 11:43:10.742: INFO: stderr: ""
    Feb 12 11:43:10.742: INFO: stdout: "e2e-test-crd-publish-openapi-6516-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/12/23 11:43:10.742
    Feb 12 11:43:10.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6577 explain e2e-test-crd-publish-openapi-6516-crds'
    Feb 12 11:43:10.953: INFO: stderr: ""
    Feb 12 11:43:10.953: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6516-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:43:14.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6577" for this suite. 02/12/23 11:43:14.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:43:14.173
Feb 12 11:43:14.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:43:14.174
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:14.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:14.193
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 02/12/23 11:43:14.195
Feb 12 11:43:14.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: rename a version 02/12/23 11:43:26.513
STEP: check the new version name is served 02/12/23 11:43:26.758
STEP: check the old version name is removed 02/12/23 11:43:29.382
STEP: check the other version is not changed 02/12/23 11:43:30.71
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:43:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8417" for this suite. 02/12/23 11:43:35.594
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":123,"skipped":2314,"failed":0}
------------------------------
 [SLOW TEST] [21.433 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:43:14.173
    Feb 12 11:43:14.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:43:14.174
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:14.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:14.193
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 02/12/23 11:43:14.195
    Feb 12 11:43:14.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: rename a version 02/12/23 11:43:26.513
    STEP: check the new version name is served 02/12/23 11:43:26.758
    STEP: check the old version name is removed 02/12/23 11:43:29.382
    STEP: check the other version is not changed 02/12/23 11:43:30.71
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:43:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8417" for this suite. 02/12/23 11:43:35.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:43:35.609
Feb 12 11:43:35.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:43:35.61
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:35.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:35.691
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/12/23 11:43:35.693
Feb 12 11:43:35.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:43:42.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:43:58.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7889" for this suite. 02/12/23 11:43:58.597
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":124,"skipped":2331,"failed":0}
------------------------------
 [SLOW TEST] [22.995 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:43:35.609
    Feb 12 11:43:35.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:43:35.61
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:35.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:35.691
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/12/23 11:43:35.693
    Feb 12 11:43:35.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:43:42.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:43:58.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7889" for this suite. 02/12/23 11:43:58.597
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:43:58.605
Feb 12 11:43:58.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:43:58.607
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:58.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:58.628
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:43:58.651
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:43:58.998
STEP: Deploying the webhook pod 02/12/23 11:43:59.006
STEP: Wait for the deployment to be ready 02/12/23 11:43:59.023
Feb 12 11:43:59.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 11:44:01.08
STEP: Verifying the service has paired with the endpoint 02/12/23 11:44:01.109
Feb 12 11:44:02.109: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 02/12/23 11:44:02.189
STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 11:44:02.218
STEP: Deleting the collection of validation webhooks 02/12/23 11:44:02.245
STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 11:44:02.294
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:44:02.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6298" for this suite. 02/12/23 11:44:02.311
STEP: Destroying namespace "webhook-6298-markers" for this suite. 02/12/23 11:44:02.318
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":125,"skipped":2331,"failed":0}
------------------------------
 [3.785 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:43:58.605
    Feb 12 11:43:58.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:43:58.607
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:43:58.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:43:58.628
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:43:58.651
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:43:58.998
    STEP: Deploying the webhook pod 02/12/23 11:43:59.006
    STEP: Wait for the deployment to be ready 02/12/23 11:43:59.023
    Feb 12 11:43:59.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 11:44:01.08
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:44:01.109
    Feb 12 11:44:02.109: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 02/12/23 11:44:02.189
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 11:44:02.218
    STEP: Deleting the collection of validation webhooks 02/12/23 11:44:02.245
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 11:44:02.294
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:44:02.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6298" for this suite. 02/12/23 11:44:02.311
    STEP: Destroying namespace "webhook-6298-markers" for this suite. 02/12/23 11:44:02.318
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:02.392
Feb 12 11:44:02.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replicaset 02/12/23 11:44:02.393
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:02.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:02.446
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 12 11:44:02.451: INFO: Creating ReplicaSet my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e
Feb 12 11:44:02.466: INFO: Pod name my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e: Found 0 pods out of 1
Feb 12 11:44:07.470: INFO: Pod name my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e: Found 1 pods out of 1
Feb 12 11:44:07.470: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e" is running
Feb 12 11:44:07.470: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p" in namespace "replicaset-6094" to be "running"
Feb 12 11:44:07.475: INFO: Pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.052879ms
Feb 12 11:44:07.475: INFO: Pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p" satisfied condition "running"
Feb 12 11:44:07.475: INFO: Pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:02 +0000 UTC Reason: Message:}])
Feb 12 11:44:07.475: INFO: Trying to dial the pod
Feb 12 11:44:12.511: INFO: Controller my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e: Got expected result from replica 1 [my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p]: "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 12 11:44:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6094" for this suite. 02/12/23 11:44:12.524
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":126,"skipped":2360,"failed":0}
------------------------------
 [SLOW TEST] [10.148 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:02.392
    Feb 12 11:44:02.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replicaset 02/12/23 11:44:02.393
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:02.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:02.446
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 12 11:44:02.451: INFO: Creating ReplicaSet my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e
    Feb 12 11:44:02.466: INFO: Pod name my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e: Found 0 pods out of 1
    Feb 12 11:44:07.470: INFO: Pod name my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e: Found 1 pods out of 1
    Feb 12 11:44:07.470: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e" is running
    Feb 12 11:44:07.470: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p" in namespace "replicaset-6094" to be "running"
    Feb 12 11:44:07.475: INFO: Pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.052879ms
    Feb 12 11:44:07.475: INFO: Pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p" satisfied condition "running"
    Feb 12 11:44:07.475: INFO: Pod "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-12 11:44:02 +0000 UTC Reason: Message:}])
    Feb 12 11:44:07.475: INFO: Trying to dial the pod
    Feb 12 11:44:12.511: INFO: Controller my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e: Got expected result from replica 1 [my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p]: "my-hostname-basic-f5d77db4-ddd6-49fa-983a-969c4371c75e-qcb2p", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 12 11:44:12.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6094" for this suite. 02/12/23 11:44:12.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:12.541
Feb 12 11:44:12.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename subpath 02/12/23 11:44:12.543
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:12.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:12.567
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/12/23 11:44:12.569
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-pjdq 02/12/23 11:44:12.581
STEP: Creating a pod to test atomic-volume-subpath 02/12/23 11:44:12.581
Feb 12 11:44:12.590: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjdq" in namespace "subpath-5401" to be "Succeeded or Failed"
Feb 12 11:44:12.597: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.929309ms
Feb 12 11:44:14.601: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011143109s
Feb 12 11:44:16.606: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 4.015472961s
Feb 12 11:44:18.610: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 6.019725515s
Feb 12 11:44:20.603: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 8.012189084s
Feb 12 11:44:22.618: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027505218s
Feb 12 11:44:24.602: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 12.011230925s
Feb 12 11:44:26.608: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 14.017876664s
Feb 12 11:44:28.602: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 16.011439603s
Feb 12 11:44:30.609: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 18.018836584s
Feb 12 11:44:32.605: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 20.014505554s
Feb 12 11:44:34.602: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=false. Elapsed: 22.011616413s
Feb 12 11:44:36.601: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010746039s
STEP: Saw pod success 02/12/23 11:44:36.601
Feb 12 11:44:36.601: INFO: Pod "pod-subpath-test-projected-pjdq" satisfied condition "Succeeded or Failed"
Feb 12 11:44:36.605: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-projected-pjdq container test-container-subpath-projected-pjdq: <nil>
STEP: delete the pod 02/12/23 11:44:36.621
Feb 12 11:44:36.647: INFO: Waiting for pod pod-subpath-test-projected-pjdq to disappear
Feb 12 11:44:36.651: INFO: Pod pod-subpath-test-projected-pjdq no longer exists
STEP: Deleting pod pod-subpath-test-projected-pjdq 02/12/23 11:44:36.651
Feb 12 11:44:36.651: INFO: Deleting pod "pod-subpath-test-projected-pjdq" in namespace "subpath-5401"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 12 11:44:36.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5401" for this suite. 02/12/23 11:44:36.657
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":127,"skipped":2373,"failed":0}
------------------------------
 [SLOW TEST] [24.122 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:12.541
    Feb 12 11:44:12.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename subpath 02/12/23 11:44:12.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:12.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:12.567
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/12/23 11:44:12.569
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-pjdq 02/12/23 11:44:12.581
    STEP: Creating a pod to test atomic-volume-subpath 02/12/23 11:44:12.581
    Feb 12 11:44:12.590: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjdq" in namespace "subpath-5401" to be "Succeeded or Failed"
    Feb 12 11:44:12.597: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.929309ms
    Feb 12 11:44:14.601: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011143109s
    Feb 12 11:44:16.606: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 4.015472961s
    Feb 12 11:44:18.610: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 6.019725515s
    Feb 12 11:44:20.603: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 8.012189084s
    Feb 12 11:44:22.618: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027505218s
    Feb 12 11:44:24.602: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 12.011230925s
    Feb 12 11:44:26.608: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 14.017876664s
    Feb 12 11:44:28.602: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 16.011439603s
    Feb 12 11:44:30.609: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 18.018836584s
    Feb 12 11:44:32.605: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=true. Elapsed: 20.014505554s
    Feb 12 11:44:34.602: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Running", Reason="", readiness=false. Elapsed: 22.011616413s
    Feb 12 11:44:36.601: INFO: Pod "pod-subpath-test-projected-pjdq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010746039s
    STEP: Saw pod success 02/12/23 11:44:36.601
    Feb 12 11:44:36.601: INFO: Pod "pod-subpath-test-projected-pjdq" satisfied condition "Succeeded or Failed"
    Feb 12 11:44:36.605: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-projected-pjdq container test-container-subpath-projected-pjdq: <nil>
    STEP: delete the pod 02/12/23 11:44:36.621
    Feb 12 11:44:36.647: INFO: Waiting for pod pod-subpath-test-projected-pjdq to disappear
    Feb 12 11:44:36.651: INFO: Pod pod-subpath-test-projected-pjdq no longer exists
    STEP: Deleting pod pod-subpath-test-projected-pjdq 02/12/23 11:44:36.651
    Feb 12 11:44:36.651: INFO: Deleting pod "pod-subpath-test-projected-pjdq" in namespace "subpath-5401"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 12 11:44:36.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5401" for this suite. 02/12/23 11:44:36.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:36.664
Feb 12 11:44:36.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:44:36.665
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:36.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:36.686
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 02/12/23 11:44:36.688
STEP: Getting a ResourceQuota 02/12/23 11:44:36.692
STEP: Updating a ResourceQuota 02/12/23 11:44:36.695
STEP: Verifying a ResourceQuota was modified 02/12/23 11:44:36.707
STEP: Deleting a ResourceQuota 02/12/23 11:44:36.71
STEP: Verifying the deleted ResourceQuota 02/12/23 11:44:36.716
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:44:36.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8226" for this suite. 02/12/23 11:44:36.721
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":128,"skipped":2378,"failed":0}
------------------------------
 [0.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:36.664
    Feb 12 11:44:36.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:44:36.665
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:36.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:36.686
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 02/12/23 11:44:36.688
    STEP: Getting a ResourceQuota 02/12/23 11:44:36.692
    STEP: Updating a ResourceQuota 02/12/23 11:44:36.695
    STEP: Verifying a ResourceQuota was modified 02/12/23 11:44:36.707
    STEP: Deleting a ResourceQuota 02/12/23 11:44:36.71
    STEP: Verifying the deleted ResourceQuota 02/12/23 11:44:36.716
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:44:36.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8226" for this suite. 02/12/23 11:44:36.721
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:36.727
Feb 12 11:44:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 11:44:36.728
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:36.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:36.749
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 12 11:44:36.802: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"60508fd7-8358-4910-b13a-5d1179cf8490", Controller:(*bool)(0xc004131366), BlockOwnerDeletion:(*bool)(0xc004131367)}}
Feb 12 11:44:36.836: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"039a3eb7-00d0-49ac-9964-c1bf91a1cf36", Controller:(*bool)(0xc003faad6e), BlockOwnerDeletion:(*bool)(0xc003faad6f)}}
Feb 12 11:44:36.843: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b187f7db-dff3-446a-ba50-d28040fa3bc3", Controller:(*bool)(0xc0040e3e36), BlockOwnerDeletion:(*bool)(0xc0040e3e37)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 11:44:41.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9573" for this suite. 02/12/23 11:44:41.864
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":129,"skipped":2380,"failed":0}
------------------------------
 [SLOW TEST] [5.145 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:36.727
    Feb 12 11:44:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 11:44:36.728
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:36.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:36.749
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 12 11:44:36.802: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"60508fd7-8358-4910-b13a-5d1179cf8490", Controller:(*bool)(0xc004131366), BlockOwnerDeletion:(*bool)(0xc004131367)}}
    Feb 12 11:44:36.836: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"039a3eb7-00d0-49ac-9964-c1bf91a1cf36", Controller:(*bool)(0xc003faad6e), BlockOwnerDeletion:(*bool)(0xc003faad6f)}}
    Feb 12 11:44:36.843: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b187f7db-dff3-446a-ba50-d28040fa3bc3", Controller:(*bool)(0xc0040e3e36), BlockOwnerDeletion:(*bool)(0xc0040e3e37)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 11:44:41.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9573" for this suite. 02/12/23 11:44:41.864
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:41.873
Feb 12 11:44:41.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename job 02/12/23 11:44:41.874
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:41.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:41.902
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 02/12/23 11:44:41.907
STEP: Ensuring job reaches completions 02/12/23 11:44:41.915
STEP: Ensuring pods with index for job exist 02/12/23 11:44:55.922
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 12 11:44:55.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1039" for this suite. 02/12/23 11:44:55.931
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":130,"skipped":2381,"failed":0}
------------------------------
 [SLOW TEST] [14.065 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:41.873
    Feb 12 11:44:41.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename job 02/12/23 11:44:41.874
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:41.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:41.902
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 02/12/23 11:44:41.907
    STEP: Ensuring job reaches completions 02/12/23 11:44:41.915
    STEP: Ensuring pods with index for job exist 02/12/23 11:44:55.922
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 12 11:44:55.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1039" for this suite. 02/12/23 11:44:55.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:55.938
Feb 12 11:44:55.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:44:55.939
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:55.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:55.965
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-96245af2-1212-4b6a-98f0-514cda0b316b 02/12/23 11:44:55.968
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:44:55.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4298" for this suite. 02/12/23 11:44:55.972
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":131,"skipped":2387,"failed":0}
------------------------------
 [0.041 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:55.938
    Feb 12 11:44:55.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:44:55.939
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:55.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:55.965
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-96245af2-1212-4b6a-98f0-514cda0b316b 02/12/23 11:44:55.968
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:44:55.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4298" for this suite. 02/12/23 11:44:55.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:44:55.981
Feb 12 11:44:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename disruption 02/12/23 11:44:55.981
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:55.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:55.999
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 02/12/23 11:44:56.001
STEP: Waiting for the pdb to be processed 02/12/23 11:44:56.007
STEP: First trying to evict a pod which shouldn't be evictable 02/12/23 11:44:58.046
STEP: Waiting for all pods to be running 02/12/23 11:44:58.047
Feb 12 11:44:58.052: INFO: pods: 0 < 3
STEP: locating a running pod 02/12/23 11:45:00.057
STEP: Updating the pdb to allow a pod to be evicted 02/12/23 11:45:00.066
STEP: Waiting for the pdb to be processed 02/12/23 11:45:00.077
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/12/23 11:45:02.088
STEP: Waiting for all pods to be running 02/12/23 11:45:02.088
STEP: Waiting for the pdb to observed all healthy pods 02/12/23 11:45:02.093
STEP: Patching the pdb to disallow a pod to be evicted 02/12/23 11:45:02.119
STEP: Waiting for the pdb to be processed 02/12/23 11:45:02.157
STEP: Waiting for all pods to be running 02/12/23 11:45:04.181
STEP: locating a running pod 02/12/23 11:45:04.193
STEP: Deleting the pdb to allow a pod to be evicted 02/12/23 11:45:04.227
STEP: Waiting for the pdb to be deleted 02/12/23 11:45:04.244
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/12/23 11:45:04.247
STEP: Waiting for all pods to be running 02/12/23 11:45:04.247
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 12 11:45:04.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-703" for this suite. 02/12/23 11:45:04.277
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":132,"skipped":2420,"failed":0}
------------------------------
 [SLOW TEST] [8.314 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:44:55.981
    Feb 12 11:44:55.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename disruption 02/12/23 11:44:55.981
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:44:55.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:44:55.999
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 02/12/23 11:44:56.001
    STEP: Waiting for the pdb to be processed 02/12/23 11:44:56.007
    STEP: First trying to evict a pod which shouldn't be evictable 02/12/23 11:44:58.046
    STEP: Waiting for all pods to be running 02/12/23 11:44:58.047
    Feb 12 11:44:58.052: INFO: pods: 0 < 3
    STEP: locating a running pod 02/12/23 11:45:00.057
    STEP: Updating the pdb to allow a pod to be evicted 02/12/23 11:45:00.066
    STEP: Waiting for the pdb to be processed 02/12/23 11:45:00.077
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/12/23 11:45:02.088
    STEP: Waiting for all pods to be running 02/12/23 11:45:02.088
    STEP: Waiting for the pdb to observed all healthy pods 02/12/23 11:45:02.093
    STEP: Patching the pdb to disallow a pod to be evicted 02/12/23 11:45:02.119
    STEP: Waiting for the pdb to be processed 02/12/23 11:45:02.157
    STEP: Waiting for all pods to be running 02/12/23 11:45:04.181
    STEP: locating a running pod 02/12/23 11:45:04.193
    STEP: Deleting the pdb to allow a pod to be evicted 02/12/23 11:45:04.227
    STEP: Waiting for the pdb to be deleted 02/12/23 11:45:04.244
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/12/23 11:45:04.247
    STEP: Waiting for all pods to be running 02/12/23 11:45:04.247
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 12 11:45:04.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-703" for this suite. 02/12/23 11:45:04.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:45:04.298
Feb 12 11:45:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 11:45:04.299
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:04.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:04.322
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 02/12/23 11:45:04.345
STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:45:04.353
Feb 12 11:45:04.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:45:04.362: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:45:05.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:45:05.372: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:45:06.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 11:45:06.369: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 02/12/23 11:45:06.372
Feb 12 11:45:06.375: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/12/23 11:45:06.375
Feb 12 11:45:06.383: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/12/23 11:45:06.383
Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: ADDED
Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.386: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.386: INFO: Found daemon set daemon-set in namespace daemonsets-697 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 12 11:45:06.386: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/12/23 11:45:06.386
STEP: watching for the daemon set status to be patched 02/12/23 11:45:06.395
Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: ADDED
Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.398: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.398: INFO: Observed daemon set daemon-set in namespace daemonsets-697 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 12 11:45:06.398: INFO: Observed &DaemonSet event: MODIFIED
Feb 12 11:45:06.398: INFO: Found daemon set daemon-set in namespace daemonsets-697 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 12 11:45:06.398: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:45:06.403
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-697, will wait for the garbage collector to delete the pods 02/12/23 11:45:06.403
Feb 12 11:45:06.466: INFO: Deleting DaemonSet.extensions daemon-set took: 9.494006ms
Feb 12 11:45:06.567: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.048381ms
Feb 12 11:45:09.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 11:45:09.371: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 12 11:45:09.375: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15010"},"items":null}

Feb 12 11:45:09.378: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15010"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:45:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-697" for this suite. 02/12/23 11:45:09.398
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":133,"skipped":2430,"failed":0}
------------------------------
 [SLOW TEST] [5.108 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:45:04.298
    Feb 12 11:45:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 11:45:04.299
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:04.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:04.322
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 02/12/23 11:45:04.345
    STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:45:04.353
    Feb 12 11:45:04.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:45:04.362: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:45:05.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:45:05.372: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:45:06.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 11:45:06.369: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 02/12/23 11:45:06.372
    Feb 12 11:45:06.375: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/12/23 11:45:06.375
    Feb 12 11:45:06.383: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/12/23 11:45:06.383
    Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: ADDED
    Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.385: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.386: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.386: INFO: Found daemon set daemon-set in namespace daemonsets-697 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 12 11:45:06.386: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/12/23 11:45:06.386
    STEP: watching for the daemon set status to be patched 02/12/23 11:45:06.395
    Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: ADDED
    Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.397: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.398: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.398: INFO: Observed daemon set daemon-set in namespace daemonsets-697 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 12 11:45:06.398: INFO: Observed &DaemonSet event: MODIFIED
    Feb 12 11:45:06.398: INFO: Found daemon set daemon-set in namespace daemonsets-697 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 12 11:45:06.398: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/12/23 11:45:06.403
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-697, will wait for the garbage collector to delete the pods 02/12/23 11:45:06.403
    Feb 12 11:45:06.466: INFO: Deleting DaemonSet.extensions daemon-set took: 9.494006ms
    Feb 12 11:45:06.567: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.048381ms
    Feb 12 11:45:09.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 11:45:09.371: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 12 11:45:09.375: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15010"},"items":null}

    Feb 12 11:45:09.378: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15010"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:45:09.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-697" for this suite. 02/12/23 11:45:09.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:45:09.407
Feb 12 11:45:09.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename job 02/12/23 11:45:09.408
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:09.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:09.432
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 02/12/23 11:45:09.439
STEP: Patching the Job 02/12/23 11:45:09.448
STEP: Watching for Job to be patched 02/12/23 11:45:09.467
Feb 12 11:45:09.469: INFO: Event ADDED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 12 11:45:09.469: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 12 11:45:09.469: INFO: Event MODIFIED found for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/12/23 11:45:09.469
STEP: Watching for Job to be updated 02/12/23 11:45:09.483
Feb 12 11:45:09.485: INFO: Event MODIFIED found for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:09.486: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/12/23 11:45:09.486
Feb 12 11:45:09.514: INFO: Job: e2e-nq5n9 as labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched]
STEP: Waiting for job to complete 02/12/23 11:45:09.517
STEP: Delete a job collection with a labelselector 02/12/23 11:45:29.531
STEP: Watching for Job to be deleted 02/12/23 11:45:29.561
Feb 12 11:45:29.566: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.566: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.566: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.568: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 12 11:45:29.568: INFO: Event DELETED found for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/12/23 11:45:29.568
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 12 11:45:29.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1489" for this suite. 02/12/23 11:45:29.585
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":134,"skipped":2454,"failed":0}
------------------------------
 [SLOW TEST] [20.197 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:45:09.407
    Feb 12 11:45:09.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename job 02/12/23 11:45:09.408
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:09.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:09.432
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 02/12/23 11:45:09.439
    STEP: Patching the Job 02/12/23 11:45:09.448
    STEP: Watching for Job to be patched 02/12/23 11:45:09.467
    Feb 12 11:45:09.469: INFO: Event ADDED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 12 11:45:09.469: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 12 11:45:09.469: INFO: Event MODIFIED found for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/12/23 11:45:09.469
    STEP: Watching for Job to be updated 02/12/23 11:45:09.483
    Feb 12 11:45:09.485: INFO: Event MODIFIED found for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:09.486: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/12/23 11:45:09.486
    Feb 12 11:45:09.514: INFO: Job: e2e-nq5n9 as labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched]
    STEP: Waiting for job to complete 02/12/23 11:45:09.517
    STEP: Delete a job collection with a labelselector 02/12/23 11:45:29.531
    STEP: Watching for Job to be deleted 02/12/23 11:45:29.561
    Feb 12 11:45:29.566: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.566: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.566: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.567: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.568: INFO: Event MODIFIED observed for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 12 11:45:29.568: INFO: Event DELETED found for Job e2e-nq5n9 in namespace job-1489 with labels: map[e2e-job-label:e2e-nq5n9 e2e-nq5n9:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/12/23 11:45:29.568
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 12 11:45:29.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1489" for this suite. 02/12/23 11:45:29.585
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:45:29.604
Feb 12 11:45:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:45:29.606
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:29.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:29.66
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1648 02/12/23 11:45:29.661
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/12/23 11:45:29.674
STEP: creating service externalsvc in namespace services-1648 02/12/23 11:45:29.674
STEP: creating replication controller externalsvc in namespace services-1648 02/12/23 11:45:29.696
I0212 11:45:29.710004      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1648, replica count: 2
I0212 11:45:32.761120      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/12/23 11:45:32.774
Feb 12 11:45:32.810: INFO: Creating new exec pod
Feb 12 11:45:32.831: INFO: Waiting up to 5m0s for pod "execpodlbzbj" in namespace "services-1648" to be "running"
Feb 12 11:45:32.842: INFO: Pod "execpodlbzbj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.493141ms
Feb 12 11:45:34.847: INFO: Pod "execpodlbzbj": Phase="Running", Reason="", readiness=true. Elapsed: 2.016026871s
Feb 12 11:45:34.847: INFO: Pod "execpodlbzbj" satisfied condition "running"
Feb 12 11:45:34.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1648 exec execpodlbzbj -- /bin/sh -x -c nslookup clusterip-service.services-1648.svc.cluster.local'
Feb 12 11:45:34.992: INFO: stderr: "+ nslookup clusterip-service.services-1648.svc.cluster.local\n"
Feb 12 11:45:34.992: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-1648.svc.cluster.local\tcanonical name = externalsvc.services-1648.svc.cluster.local.\nName:\texternalsvc.services-1648.svc.cluster.local\nAddress: 10.233.61.110\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1648, will wait for the garbage collector to delete the pods 02/12/23 11:45:34.992
Feb 12 11:45:35.053: INFO: Deleting ReplicationController externalsvc took: 6.553833ms
Feb 12 11:45:35.154: INFO: Terminating ReplicationController externalsvc pods took: 100.492709ms
Feb 12 11:45:36.885: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:45:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1648" for this suite. 02/12/23 11:45:36.924
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":135,"skipped":2456,"failed":0}
------------------------------
 [SLOW TEST] [7.339 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:45:29.604
    Feb 12 11:45:29.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:45:29.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:29.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:29.66
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1648 02/12/23 11:45:29.661
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/12/23 11:45:29.674
    STEP: creating service externalsvc in namespace services-1648 02/12/23 11:45:29.674
    STEP: creating replication controller externalsvc in namespace services-1648 02/12/23 11:45:29.696
    I0212 11:45:29.710004      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1648, replica count: 2
    I0212 11:45:32.761120      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/12/23 11:45:32.774
    Feb 12 11:45:32.810: INFO: Creating new exec pod
    Feb 12 11:45:32.831: INFO: Waiting up to 5m0s for pod "execpodlbzbj" in namespace "services-1648" to be "running"
    Feb 12 11:45:32.842: INFO: Pod "execpodlbzbj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.493141ms
    Feb 12 11:45:34.847: INFO: Pod "execpodlbzbj": Phase="Running", Reason="", readiness=true. Elapsed: 2.016026871s
    Feb 12 11:45:34.847: INFO: Pod "execpodlbzbj" satisfied condition "running"
    Feb 12 11:45:34.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1648 exec execpodlbzbj -- /bin/sh -x -c nslookup clusterip-service.services-1648.svc.cluster.local'
    Feb 12 11:45:34.992: INFO: stderr: "+ nslookup clusterip-service.services-1648.svc.cluster.local\n"
    Feb 12 11:45:34.992: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-1648.svc.cluster.local\tcanonical name = externalsvc.services-1648.svc.cluster.local.\nName:\texternalsvc.services-1648.svc.cluster.local\nAddress: 10.233.61.110\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1648, will wait for the garbage collector to delete the pods 02/12/23 11:45:34.992
    Feb 12 11:45:35.053: INFO: Deleting ReplicationController externalsvc took: 6.553833ms
    Feb 12 11:45:35.154: INFO: Terminating ReplicationController externalsvc pods took: 100.492709ms
    Feb 12 11:45:36.885: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:45:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1648" for this suite. 02/12/23 11:45:36.924
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:45:36.945
Feb 12 11:45:36.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename endpointslice 02/12/23 11:45:36.946
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:36.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:36.982
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 12 11:45:37.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5950" for this suite. 02/12/23 11:45:37.084
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":136,"skipped":2479,"failed":0}
------------------------------
 [0.153 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:45:36.945
    Feb 12 11:45:36.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename endpointslice 02/12/23 11:45:36.946
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:36.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:36.982
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 12 11:45:37.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5950" for this suite. 02/12/23 11:45:37.084
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:45:37.099
Feb 12 11:45:37.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replicaset 02/12/23 11:45:37.101
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:37.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:37.129
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/12/23 11:45:37.134
Feb 12 11:45:37.153: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4948" to be "running and ready"
Feb 12 11:45:37.169: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 15.593198ms
Feb 12 11:45:37.169: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:45:39.181: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.028112392s
Feb 12 11:45:39.181: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 12 11:45:39.181: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/12/23 11:45:39.193
STEP: Then the orphan pod is adopted 02/12/23 11:45:39.206
STEP: When the matched label of one of its pods change 02/12/23 11:45:40.219
Feb 12 11:45:40.223: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/12/23 11:45:40.238
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 12 11:45:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4948" for this suite. 02/12/23 11:45:41.252
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":137,"skipped":2483,"failed":0}
------------------------------
 [4.161 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:45:37.099
    Feb 12 11:45:37.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replicaset 02/12/23 11:45:37.101
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:37.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:37.129
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/12/23 11:45:37.134
    Feb 12 11:45:37.153: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4948" to be "running and ready"
    Feb 12 11:45:37.169: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 15.593198ms
    Feb 12 11:45:37.169: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:45:39.181: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.028112392s
    Feb 12 11:45:39.181: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 12 11:45:39.181: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/12/23 11:45:39.193
    STEP: Then the orphan pod is adopted 02/12/23 11:45:39.206
    STEP: When the matched label of one of its pods change 02/12/23 11:45:40.219
    Feb 12 11:45:40.223: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/12/23 11:45:40.238
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 12 11:45:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4948" for this suite. 02/12/23 11:45:41.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:45:41.265
Feb 12 11:45:41.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:45:41.266
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:41.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:41.289
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 02/12/23 11:45:58.301
STEP: Creating a ResourceQuota 02/12/23 11:46:03.311
STEP: Ensuring resource quota status is calculated 02/12/23 11:46:03.322
STEP: Creating a ConfigMap 02/12/23 11:46:05.326
STEP: Ensuring resource quota status captures configMap creation 02/12/23 11:46:05.345
STEP: Deleting a ConfigMap 02/12/23 11:46:07.35
STEP: Ensuring resource quota status released usage 02/12/23 11:46:07.356
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:46:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8351" for this suite. 02/12/23 11:46:09.387
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":138,"skipped":2513,"failed":0}
------------------------------
 [SLOW TEST] [28.133 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:45:41.265
    Feb 12 11:45:41.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:45:41.266
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:45:41.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:45:41.289
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 02/12/23 11:45:58.301
    STEP: Creating a ResourceQuota 02/12/23 11:46:03.311
    STEP: Ensuring resource quota status is calculated 02/12/23 11:46:03.322
    STEP: Creating a ConfigMap 02/12/23 11:46:05.326
    STEP: Ensuring resource quota status captures configMap creation 02/12/23 11:46:05.345
    STEP: Deleting a ConfigMap 02/12/23 11:46:07.35
    STEP: Ensuring resource quota status released usage 02/12/23 11:46:07.356
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:46:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8351" for this suite. 02/12/23 11:46:09.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:09.399
Feb 12 11:46:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 11:46:09.401
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:09.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:09.424
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 02/12/23 11:46:09.427
Feb 12 11:46:09.435: INFO: Waiting up to 5m0s for pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839" in namespace "var-expansion-6118" to be "Succeeded or Failed"
Feb 12 11:46:09.443: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839": Phase="Pending", Reason="", readiness=false. Elapsed: 8.600952ms
Feb 12 11:46:11.458: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023677437s
Feb 12 11:46:13.459: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024014336s
STEP: Saw pod success 02/12/23 11:46:13.459
Feb 12 11:46:13.459: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839" satisfied condition "Succeeded or Failed"
Feb 12 11:46:13.473: INFO: Trying to get logs from node kube-3 pod var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839 container dapi-container: <nil>
STEP: delete the pod 02/12/23 11:46:13.521
Feb 12 11:46:13.547: INFO: Waiting for pod var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839 to disappear
Feb 12 11:46:13.554: INFO: Pod var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 11:46:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6118" for this suite. 02/12/23 11:46:13.56
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":139,"skipped":2529,"failed":0}
------------------------------
 [4.170 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:09.399
    Feb 12 11:46:09.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 11:46:09.401
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:09.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:09.424
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 02/12/23 11:46:09.427
    Feb 12 11:46:09.435: INFO: Waiting up to 5m0s for pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839" in namespace "var-expansion-6118" to be "Succeeded or Failed"
    Feb 12 11:46:09.443: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839": Phase="Pending", Reason="", readiness=false. Elapsed: 8.600952ms
    Feb 12 11:46:11.458: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023677437s
    Feb 12 11:46:13.459: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024014336s
    STEP: Saw pod success 02/12/23 11:46:13.459
    Feb 12 11:46:13.459: INFO: Pod "var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839" satisfied condition "Succeeded or Failed"
    Feb 12 11:46:13.473: INFO: Trying to get logs from node kube-3 pod var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 11:46:13.521
    Feb 12 11:46:13.547: INFO: Waiting for pod var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839 to disappear
    Feb 12 11:46:13.554: INFO: Pod var-expansion-288c9a95-1b2c-4127-9108-d7f02d8ab839 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 11:46:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6118" for this suite. 02/12/23 11:46:13.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:13.57
Feb 12 11:46:13.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:46:13.571
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:13.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:13.589
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-6c894082-88ec-455e-b47f-a9352f0d6bb3 02/12/23 11:46:13.591
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:46:13.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-292" for this suite. 02/12/23 11:46:13.596
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":140,"skipped":2563,"failed":0}
------------------------------
 [0.033 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:13.57
    Feb 12 11:46:13.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:46:13.571
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:13.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:13.589
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-6c894082-88ec-455e-b47f-a9352f0d6bb3 02/12/23 11:46:13.591
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:46:13.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-292" for this suite. 02/12/23 11:46:13.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:13.605
Feb 12 11:46:13.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 11:46:13.606
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:13.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:13.627
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Feb 12 11:46:13.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 create -f -'
Feb 12 11:46:14.129: INFO: stderr: ""
Feb 12 11:46:14.129: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 12 11:46:14.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 create -f -'
Feb 12 11:46:14.691: INFO: stderr: ""
Feb 12 11:46:14.691: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/12/23 11:46:14.691
Feb 12 11:46:15.705: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:46:15.705: INFO: Found 0 / 1
Feb 12 11:46:16.704: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:46:16.704: INFO: Found 1 / 1
Feb 12 11:46:16.704: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 12 11:46:16.713: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 12 11:46:16.713: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 12 11:46:16.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe pod agnhost-primary-9vkpd'
Feb 12 11:46:16.780: INFO: stderr: ""
Feb 12 11:46:16.780: INFO: stdout: "Name:             agnhost-primary-9vkpd\nNamespace:        kubectl-8585\nPriority:         0\nService Account:  default\nNode:             kube-3/10.2.20.103\nStart Time:       Sun, 12 Feb 2023 11:46:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 92a43406332c4b6ad711cf89ffd12be7a43eabec1b9c1f820ff4307bfd4f16a0\n                  cni.projectcalico.org/podIP: 10.233.99.77/32\n                  cni.projectcalico.org/podIPs: 10.233.99.77/32\nStatus:           Running\nIP:               10.233.99.77\nIPs:\n  IP:           10.233.99.77\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://326c8d2715f1136affef2275d513e2508d96d25613babd264ce4ca3bf74541d6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 12 Feb 2023 11:46:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64rsn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64rsn:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8585/agnhost-primary-9vkpd to kube-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Feb 12 11:46:16.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe rc agnhost-primary'
Feb 12 11:46:16.859: INFO: stderr: ""
Feb 12 11:46:16.859: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8585\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9vkpd\n"
Feb 12 11:46:16.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe service agnhost-primary'
Feb 12 11:46:16.932: INFO: stderr: ""
Feb 12 11:46:16.932: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8585\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.7.76\nIPs:               10.233.7.76\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.99.77:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 12 11:46:16.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe node kube-1'
Feb 12 11:46:17.019: INFO: stderr: ""
Feb 12 11:46:17.019: INFO: stdout: "Name:               kube-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.20.101/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.120.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 12 Feb 2023 10:55:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  kube-1\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 12 Feb 2023 11:46:09 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 12 Feb 2023 11:00:18 +0000   Sun, 12 Feb 2023 11:00:18 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 10:55:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 10:55:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 10:55:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 11:01:43 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.2.20.101\n  Hostname:    kube-1\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      129064092Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 4025576Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      118945466991\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3923176Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 141a8a1238e54076968c1782cd0808a9\n  System UUID:                141a8a12-38e5-4076-968c-1782cd0808a9\n  Boot ID:                    3831c5e8-d1cb-4574-9739-546c2d2b6c8c\n  Kernel Version:             5.4.0-126-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-p72st                                          150m (3%)     300m (7%)   64M (1%)         500M (12%)     48m\n  kube-system                 coredns-588bb58b94-c4894                                   100m (2%)     0 (0%)      70Mi (1%)        300Mi (7%)     45m\n  kube-system                 kube-apiserver-kube-1                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-controller-manager-kube-1                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-proxy-dxsbj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                 kube-scheduler-kube-1                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 nodelocaldns-6sjhv                                         100m (2%)     0 (0%)      70Mi (1%)        200Mi (5%)     44m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests        Limits\n  --------               --------        ------\n  cpu                    900m (22%)      300m (7%)\n  memory                 210800640 (5%)  1024288k (25%)\n  ephemeral-storage      0 (0%)          0 (0%)\n  hugepages-1Gi          0 (0%)          0 (0%)\n  hugepages-2Mi          0 (0%)          0 (0%)\n  scheduling.k8s.io/foo  0               0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 48m                kube-proxy       \n  Normal   Starting                 50m                kube-proxy       \n  Normal   NodeHasSufficientMemory  51m (x7 over 51m)  kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    51m (x6 over 51m)  kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     51m (x6 over 51m)  kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   Starting                 50m                kubelet          Starting kubelet.\n  Normal   NodeAllocatableEnforced  50m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     50m                kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasNoDiskPressure    50m                kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  50m                kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      50m                kubelet          invalid capacity 0 on image filesystem\n  Normal   RegisteredNode           50m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   NodeAllocatableEnforced  49m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  49m                kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID     49m                kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasNoDiskPressure    49m                kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   Starting                 49m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      49m                kubelet          invalid capacity 0 on image filesystem\n  Normal   RegisteredNode           49m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           48m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   NodeReady                46m                kubelet          Node kube-1 status is now: NodeReady\n  Normal   RegisteredNode           45m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           44m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   NodeHasSufficientPID     44m                kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   Starting                 44m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      44m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  44m                kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    44m                kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeNotReady             44m                kubelet          Node kube-1 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  44m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                44m                kubelet          Node kube-1 status is now: NodeReady\n  Normal   RegisteredNode           43m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           38m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           37m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           18m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n"
Feb 12 11:46:17.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe namespace kubectl-8585'
Feb 12 11:46:17.077: INFO: stderr: ""
Feb 12 11:46:17.077: INFO: stdout: "Name:         kubectl-8585\nLabels:       e2e-framework=kubectl\n              e2e-run=2c3b1601-a843-49d3-92af-7f94d0476c26\n              kubernetes.io/metadata.name=kubectl-8585\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 11:46:17.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8585" for this suite. 02/12/23 11:46:17.081
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":141,"skipped":2576,"failed":0}
------------------------------
 [3.485 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:13.605
    Feb 12 11:46:13.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 11:46:13.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:13.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:13.627
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Feb 12 11:46:13.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 create -f -'
    Feb 12 11:46:14.129: INFO: stderr: ""
    Feb 12 11:46:14.129: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 12 11:46:14.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 create -f -'
    Feb 12 11:46:14.691: INFO: stderr: ""
    Feb 12 11:46:14.691: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/12/23 11:46:14.691
    Feb 12 11:46:15.705: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:46:15.705: INFO: Found 0 / 1
    Feb 12 11:46:16.704: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:46:16.704: INFO: Found 1 / 1
    Feb 12 11:46:16.704: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 12 11:46:16.713: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 12 11:46:16.713: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 12 11:46:16.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe pod agnhost-primary-9vkpd'
    Feb 12 11:46:16.780: INFO: stderr: ""
    Feb 12 11:46:16.780: INFO: stdout: "Name:             agnhost-primary-9vkpd\nNamespace:        kubectl-8585\nPriority:         0\nService Account:  default\nNode:             kube-3/10.2.20.103\nStart Time:       Sun, 12 Feb 2023 11:46:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 92a43406332c4b6ad711cf89ffd12be7a43eabec1b9c1f820ff4307bfd4f16a0\n                  cni.projectcalico.org/podIP: 10.233.99.77/32\n                  cni.projectcalico.org/podIPs: 10.233.99.77/32\nStatus:           Running\nIP:               10.233.99.77\nIPs:\n  IP:           10.233.99.77\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://326c8d2715f1136affef2275d513e2508d96d25613babd264ce4ca3bf74541d6\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 12 Feb 2023 11:46:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64rsn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64rsn:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8585/agnhost-primary-9vkpd to kube-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Feb 12 11:46:16.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe rc agnhost-primary'
    Feb 12 11:46:16.859: INFO: stderr: ""
    Feb 12 11:46:16.859: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8585\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-9vkpd\n"
    Feb 12 11:46:16.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe service agnhost-primary'
    Feb 12 11:46:16.932: INFO: stderr: ""
    Feb 12 11:46:16.932: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8585\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.7.76\nIPs:               10.233.7.76\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.99.77:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 12 11:46:16.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe node kube-1'
    Feb 12 11:46:17.019: INFO: stderr: ""
    Feb 12 11:46:17.019: INFO: stdout: "Name:               kube-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=kube-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.2.20.101/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.120.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 12 Feb 2023 10:55:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  kube-1\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 12 Feb 2023 11:46:09 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 12 Feb 2023 11:00:18 +0000   Sun, 12 Feb 2023 11:00:18 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 10:55:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 10:55:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 10:55:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 12 Feb 2023 11:46:09 +0000   Sun, 12 Feb 2023 11:01:43 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.2.20.101\n  Hostname:    kube-1\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      129064092Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 4025576Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      118945466991\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3923176Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 141a8a1238e54076968c1782cd0808a9\n  System UUID:                141a8a12-38e5-4076-968c-1782cd0808a9\n  Boot ID:                    3831c5e8-d1cb-4574-9739-546c2d2b6c8c\n  Kernel Version:             5.4.0-126-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.6\n  Kube-Proxy Version:         v1.25.6\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-p72st                                          150m (3%)     300m (7%)   64M (1%)         500M (12%)     48m\n  kube-system                 coredns-588bb58b94-c4894                                   100m (2%)     0 (0%)      70Mi (1%)        300Mi (7%)     45m\n  kube-system                 kube-apiserver-kube-1                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-controller-manager-kube-1                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 kube-proxy-dxsbj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\n  kube-system                 kube-scheduler-kube-1                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         50m\n  kube-system                 nodelocaldns-6sjhv                                         100m (2%)     0 (0%)      70Mi (1%)        200Mi (5%)     44m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         40m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests        Limits\n  --------               --------        ------\n  cpu                    900m (22%)      300m (7%)\n  memory                 210800640 (5%)  1024288k (25%)\n  ephemeral-storage      0 (0%)          0 (0%)\n  hugepages-1Gi          0 (0%)          0 (0%)\n  hugepages-2Mi          0 (0%)          0 (0%)\n  scheduling.k8s.io/foo  0               0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 48m                kube-proxy       \n  Normal   Starting                 50m                kube-proxy       \n  Normal   NodeHasSufficientMemory  51m (x7 over 51m)  kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    51m (x6 over 51m)  kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     51m (x6 over 51m)  kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   Starting                 50m                kubelet          Starting kubelet.\n  Normal   NodeAllocatableEnforced  50m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientPID     50m                kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasNoDiskPressure    50m                kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory  50m                kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      50m                kubelet          invalid capacity 0 on image filesystem\n  Normal   RegisteredNode           50m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   NodeAllocatableEnforced  49m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  49m                kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID     49m                kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   NodeHasNoDiskPressure    49m                kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   Starting                 49m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      49m                kubelet          invalid capacity 0 on image filesystem\n  Normal   RegisteredNode           49m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           48m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   NodeReady                46m                kubelet          Node kube-1 status is now: NodeReady\n  Normal   RegisteredNode           45m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           44m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   NodeHasSufficientPID     44m                kubelet          Node kube-1 status is now: NodeHasSufficientPID\n  Normal   Starting                 44m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      44m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  44m                kubelet          Node kube-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    44m                kubelet          Node kube-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeNotReady             44m                kubelet          Node kube-1 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  44m                kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                44m                kubelet          Node kube-1 status is now: NodeReady\n  Normal   RegisteredNode           43m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           38m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           37m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n  Normal   RegisteredNode           18m                node-controller  Node kube-1 event: Registered Node kube-1 in Controller\n"
    Feb 12 11:46:17.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8585 describe namespace kubectl-8585'
    Feb 12 11:46:17.077: INFO: stderr: ""
    Feb 12 11:46:17.077: INFO: stdout: "Name:         kubectl-8585\nLabels:       e2e-framework=kubectl\n              e2e-run=2c3b1601-a843-49d3-92af-7f94d0476c26\n              kubernetes.io/metadata.name=kubectl-8585\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 11:46:17.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8585" for this suite. 02/12/23 11:46:17.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:17.091
Feb 12 11:46:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 11:46:17.092
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:17.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:17.117
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-472.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-472.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/12/23 11:46:17.12
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-472.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-472.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/12/23 11:46:17.12
STEP: creating a pod to probe /etc/hosts 02/12/23 11:46:17.12
STEP: submitting the pod to kubernetes 02/12/23 11:46:17.12
Feb 12 11:46:17.129: INFO: Waiting up to 15m0s for pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b" in namespace "dns-472" to be "running"
Feb 12 11:46:17.140: INFO: Pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.118453ms
Feb 12 11:46:19.157: INFO: Pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b": Phase="Running", Reason="", readiness=true. Elapsed: 2.028348708s
Feb 12 11:46:19.158: INFO: Pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:46:19.158
STEP: looking for the results for each expected name from probers 02/12/23 11:46:19.171
Feb 12 11:46:19.217: INFO: DNS probes using dns-472/dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b succeeded

STEP: deleting the pod 02/12/23 11:46:19.217
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 11:46:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-472" for this suite. 02/12/23 11:46:19.257
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":142,"skipped":2591,"failed":0}
------------------------------
 [2.175 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:17.091
    Feb 12 11:46:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 11:46:17.092
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:17.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:17.117
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-472.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-472.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/12/23 11:46:17.12
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-472.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-472.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/12/23 11:46:17.12
    STEP: creating a pod to probe /etc/hosts 02/12/23 11:46:17.12
    STEP: submitting the pod to kubernetes 02/12/23 11:46:17.12
    Feb 12 11:46:17.129: INFO: Waiting up to 15m0s for pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b" in namespace "dns-472" to be "running"
    Feb 12 11:46:17.140: INFO: Pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.118453ms
    Feb 12 11:46:19.157: INFO: Pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b": Phase="Running", Reason="", readiness=true. Elapsed: 2.028348708s
    Feb 12 11:46:19.158: INFO: Pod "dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:46:19.158
    STEP: looking for the results for each expected name from probers 02/12/23 11:46:19.171
    Feb 12 11:46:19.217: INFO: DNS probes using dns-472/dns-test-2ef951b3-4c87-49c1-9fad-016cebaa357b succeeded

    STEP: deleting the pod 02/12/23 11:46:19.217
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 11:46:19.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-472" for this suite. 02/12/23 11:46:19.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:19.278
Feb 12 11:46:19.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:46:19.279
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:19.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:19.298
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 02/12/23 11:46:19.3
STEP: watching for the ServiceAccount to be added 02/12/23 11:46:19.311
STEP: patching the ServiceAccount 02/12/23 11:46:19.312
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/12/23 11:46:19.319
STEP: deleting the ServiceAccount 02/12/23 11:46:19.323
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 12 11:46:19.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5691" for this suite. 02/12/23 11:46:19.343
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":143,"skipped":2618,"failed":0}
------------------------------
 [0.072 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:19.278
    Feb 12 11:46:19.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svcaccounts 02/12/23 11:46:19.279
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:19.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:19.298
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 02/12/23 11:46:19.3
    STEP: watching for the ServiceAccount to be added 02/12/23 11:46:19.311
    STEP: patching the ServiceAccount 02/12/23 11:46:19.312
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/12/23 11:46:19.319
    STEP: deleting the ServiceAccount 02/12/23 11:46:19.323
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 12 11:46:19.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5691" for this suite. 02/12/23 11:46:19.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:19.354
Feb 12 11:46:19.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:46:19.355
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:19.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:19.382
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 02/12/23 11:46:19.384
Feb 12 11:46:19.394: INFO: Waiting up to 5m0s for pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba" in namespace "downward-api-9555" to be "running and ready"
Feb 12 11:46:19.401: INFO: Pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.373272ms
Feb 12 11:46:19.401: INFO: The phase of Pod labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:46:21.414: INFO: Pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba": Phase="Running", Reason="", readiness=true. Elapsed: 2.019805507s
Feb 12 11:46:21.414: INFO: The phase of Pod labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba is Running (Ready = true)
Feb 12 11:46:21.414: INFO: Pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba" satisfied condition "running and ready"
Feb 12 11:46:21.964: INFO: Successfully updated pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:46:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9555" for this suite. 02/12/23 11:46:26.004
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":144,"skipped":2649,"failed":0}
------------------------------
 [SLOW TEST] [6.660 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:19.354
    Feb 12 11:46:19.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:46:19.355
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:19.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:19.382
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 02/12/23 11:46:19.384
    Feb 12 11:46:19.394: INFO: Waiting up to 5m0s for pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba" in namespace "downward-api-9555" to be "running and ready"
    Feb 12 11:46:19.401: INFO: Pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.373272ms
    Feb 12 11:46:19.401: INFO: The phase of Pod labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:46:21.414: INFO: Pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba": Phase="Running", Reason="", readiness=true. Elapsed: 2.019805507s
    Feb 12 11:46:21.414: INFO: The phase of Pod labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba is Running (Ready = true)
    Feb 12 11:46:21.414: INFO: Pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba" satisfied condition "running and ready"
    Feb 12 11:46:21.964: INFO: Successfully updated pod "labelsupdate4e26b769-ca1b-4a47-8291-5cc7d4888dba"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:46:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9555" for this suite. 02/12/23 11:46:26.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:26.016
Feb 12 11:46:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:46:26.017
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:26.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:26.037
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/12/23 11:46:26.039
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/12/23 11:46:26.04
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/12/23 11:46:26.04
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/12/23 11:46:26.04
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/12/23 11:46:26.04
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/12/23 11:46:26.04
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/12/23 11:46:26.041
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:46:26.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9870" for this suite. 02/12/23 11:46:26.044
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":145,"skipped":2673,"failed":0}
------------------------------
 [0.036 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:26.016
    Feb 12 11:46:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:46:26.017
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:26.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:26.037
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/12/23 11:46:26.039
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/12/23 11:46:26.04
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/12/23 11:46:26.04
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/12/23 11:46:26.04
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/12/23 11:46:26.04
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/12/23 11:46:26.04
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/12/23 11:46:26.041
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:46:26.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9870" for this suite. 02/12/23 11:46:26.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:26.053
Feb 12 11:46:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename namespaces 02/12/23 11:46:26.054
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:26.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:26.075
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 02/12/23 11:46:26.078
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:26.097
STEP: Creating a service in the namespace 02/12/23 11:46:26.101
STEP: Deleting the namespace 02/12/23 11:46:26.116
STEP: Waiting for the namespace to be removed. 02/12/23 11:46:26.14
STEP: Recreating the namespace 02/12/23 11:46:38.146
STEP: Verifying there is no service in the namespace 02/12/23 11:46:38.23
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:46:38.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5714" for this suite. 02/12/23 11:46:38.24
STEP: Destroying namespace "nsdeletetest-2812" for this suite. 02/12/23 11:46:38.267
Feb 12 11:46:38.271: INFO: Namespace nsdeletetest-2812 was already deleted
STEP: Destroying namespace "nsdeletetest-174" for this suite. 02/12/23 11:46:38.271
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":146,"skipped":2693,"failed":0}
------------------------------
 [SLOW TEST] [12.238 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:26.053
    Feb 12 11:46:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename namespaces 02/12/23 11:46:26.054
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:26.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:26.075
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 02/12/23 11:46:26.078
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:26.097
    STEP: Creating a service in the namespace 02/12/23 11:46:26.101
    STEP: Deleting the namespace 02/12/23 11:46:26.116
    STEP: Waiting for the namespace to be removed. 02/12/23 11:46:26.14
    STEP: Recreating the namespace 02/12/23 11:46:38.146
    STEP: Verifying there is no service in the namespace 02/12/23 11:46:38.23
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:46:38.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5714" for this suite. 02/12/23 11:46:38.24
    STEP: Destroying namespace "nsdeletetest-2812" for this suite. 02/12/23 11:46:38.267
    Feb 12 11:46:38.271: INFO: Namespace nsdeletetest-2812 was already deleted
    STEP: Destroying namespace "nsdeletetest-174" for this suite. 02/12/23 11:46:38.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:38.292
Feb 12 11:46:38.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:46:38.293
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:38.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:38.313
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:46:38.315
Feb 12 11:46:38.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7" in namespace "projected-9473" to be "Succeeded or Failed"
Feb 12 11:46:38.331: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.203922ms
Feb 12 11:46:40.344: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021133357s
Feb 12 11:46:42.346: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023126514s
STEP: Saw pod success 02/12/23 11:46:42.347
Feb 12 11:46:42.348: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7" satisfied condition "Succeeded or Failed"
Feb 12 11:46:42.351: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7 container client-container: <nil>
STEP: delete the pod 02/12/23 11:46:42.359
Feb 12 11:46:42.379: INFO: Waiting for pod downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7 to disappear
Feb 12 11:46:42.382: INFO: Pod downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:46:42.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9473" for this suite. 02/12/23 11:46:42.385
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":147,"skipped":2721,"failed":0}
------------------------------
 [4.101 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:38.292
    Feb 12 11:46:38.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:46:38.293
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:38.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:38.313
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:46:38.315
    Feb 12 11:46:38.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7" in namespace "projected-9473" to be "Succeeded or Failed"
    Feb 12 11:46:38.331: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.203922ms
    Feb 12 11:46:40.344: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021133357s
    Feb 12 11:46:42.346: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023126514s
    STEP: Saw pod success 02/12/23 11:46:42.347
    Feb 12 11:46:42.348: INFO: Pod "downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7" satisfied condition "Succeeded or Failed"
    Feb 12 11:46:42.351: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7 container client-container: <nil>
    STEP: delete the pod 02/12/23 11:46:42.359
    Feb 12 11:46:42.379: INFO: Waiting for pod downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7 to disappear
    Feb 12 11:46:42.382: INFO: Pod downwardapi-volume-5b908321-d960-4161-9641-c2006e07e8b7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:46:42.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9473" for this suite. 02/12/23 11:46:42.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:46:42.397
Feb 12 11:46:42.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename job 02/12/23 11:46:42.398
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:42.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:42.433
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 02/12/23 11:46:42.437
STEP: Ensuring active pods == parallelism 02/12/23 11:46:42.447
STEP: delete a job 02/12/23 11:46:44.456
STEP: deleting Job.batch foo in namespace job-3295, will wait for the garbage collector to delete the pods 02/12/23 11:46:44.457
Feb 12 11:46:44.541: INFO: Deleting Job.batch foo took: 19.791681ms
Feb 12 11:46:44.642: INFO: Terminating Job.batch foo pods took: 101.244842ms
STEP: Ensuring job was deleted 02/12/23 11:47:17.143
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 12 11:47:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3295" for this suite. 02/12/23 11:47:17.149
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":148,"skipped":2730,"failed":0}
------------------------------
 [SLOW TEST] [34.759 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:46:42.397
    Feb 12 11:46:42.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename job 02/12/23 11:46:42.398
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:46:42.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:46:42.433
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 02/12/23 11:46:42.437
    STEP: Ensuring active pods == parallelism 02/12/23 11:46:42.447
    STEP: delete a job 02/12/23 11:46:44.456
    STEP: deleting Job.batch foo in namespace job-3295, will wait for the garbage collector to delete the pods 02/12/23 11:46:44.457
    Feb 12 11:46:44.541: INFO: Deleting Job.batch foo took: 19.791681ms
    Feb 12 11:46:44.642: INFO: Terminating Job.batch foo pods took: 101.244842ms
    STEP: Ensuring job was deleted 02/12/23 11:47:17.143
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 12 11:47:17.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-3295" for this suite. 02/12/23 11:47:17.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:47:17.157
Feb 12 11:47:17.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:47:17.158
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:17.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:17.179
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 02/12/23 11:47:17.18
Feb 12 11:47:17.189: INFO: Waiting up to 5m0s for pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49" in namespace "pods-7439" to be "running and ready"
Feb 12 11:47:17.197: INFO: Pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.654463ms
Feb 12 11:47:17.197: INFO: The phase of Pod pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:47:19.211: INFO: Pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49": Phase="Running", Reason="", readiness=true. Elapsed: 2.021902884s
Feb 12 11:47:19.211: INFO: The phase of Pod pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49 is Running (Ready = true)
Feb 12 11:47:19.211: INFO: Pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49" satisfied condition "running and ready"
Feb 12 11:47:19.235: INFO: Pod pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49 has hostIP: 10.2.20.103
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:47:19.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7439" for this suite. 02/12/23 11:47:19.248
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":149,"skipped":2738,"failed":0}
------------------------------
 [2.101 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:47:17.157
    Feb 12 11:47:17.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:47:17.158
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:17.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:17.179
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 02/12/23 11:47:17.18
    Feb 12 11:47:17.189: INFO: Waiting up to 5m0s for pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49" in namespace "pods-7439" to be "running and ready"
    Feb 12 11:47:17.197: INFO: Pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49": Phase="Pending", Reason="", readiness=false. Elapsed: 7.654463ms
    Feb 12 11:47:17.197: INFO: The phase of Pod pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:47:19.211: INFO: Pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49": Phase="Running", Reason="", readiness=true. Elapsed: 2.021902884s
    Feb 12 11:47:19.211: INFO: The phase of Pod pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49 is Running (Ready = true)
    Feb 12 11:47:19.211: INFO: Pod "pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49" satisfied condition "running and ready"
    Feb 12 11:47:19.235: INFO: Pod pod-hostip-d3e84230-1ec6-4244-a214-d9daff15be49 has hostIP: 10.2.20.103
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:47:19.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7439" for this suite. 02/12/23 11:47:19.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:47:19.265
Feb 12 11:47:19.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:47:19.266
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:19.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:19.29
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 02/12/23 11:47:19.291
Feb 12 11:47:19.291: INFO: Creating e2e-svc-a-chsw9
Feb 12 11:47:19.301: INFO: Creating e2e-svc-b-6jbqf
Feb 12 11:47:19.318: INFO: Creating e2e-svc-c-dwp2j
STEP: deleting service collection 02/12/23 11:47:19.341
Feb 12 11:47:19.391: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:47:19.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-22" for this suite. 02/12/23 11:47:19.396
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":150,"skipped":2777,"failed":0}
------------------------------
 [0.140 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:47:19.265
    Feb 12 11:47:19.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:47:19.266
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:19.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:19.29
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 02/12/23 11:47:19.291
    Feb 12 11:47:19.291: INFO: Creating e2e-svc-a-chsw9
    Feb 12 11:47:19.301: INFO: Creating e2e-svc-b-6jbqf
    Feb 12 11:47:19.318: INFO: Creating e2e-svc-c-dwp2j
    STEP: deleting service collection 02/12/23 11:47:19.341
    Feb 12 11:47:19.391: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:47:19.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-22" for this suite. 02/12/23 11:47:19.396
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:47:19.405
Feb 12 11:47:19.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename tables 02/12/23 11:47:19.406
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:19.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:19.435
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Feb 12 11:47:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2957" for this suite. 02/12/23 11:47:19.449
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":151,"skipped":2784,"failed":0}
------------------------------
 [0.054 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:47:19.405
    Feb 12 11:47:19.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename tables 02/12/23 11:47:19.406
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:19.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:19.435
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Feb 12 11:47:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-2957" for this suite. 02/12/23 11:47:19.449
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:47:19.459
Feb 12 11:47:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 11:47:19.46
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:19.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:19.496
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a in namespace container-probe-233 02/12/23 11:47:19.5
Feb 12 11:47:19.510: INFO: Waiting up to 5m0s for pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a" in namespace "container-probe-233" to be "not pending"
Feb 12 11:47:19.517: INFO: Pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.194195ms
Feb 12 11:47:21.533: INFO: Pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a": Phase="Running", Reason="", readiness=true. Elapsed: 2.022775407s
Feb 12 11:47:21.533: INFO: Pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a" satisfied condition "not pending"
Feb 12 11:47:21.533: INFO: Started pod busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a in namespace container-probe-233
STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:47:21.533
Feb 12 11:47:21.552: INFO: Initial restart count of pod busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a is 0
Feb 12 11:48:11.911: INFO: Restart count of pod container-probe-233/busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a is now 1 (50.358762927s elapsed)
STEP: deleting the pod 02/12/23 11:48:11.911
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 11:48:11.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-233" for this suite. 02/12/23 11:48:11.932
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":152,"skipped":2786,"failed":0}
------------------------------
 [SLOW TEST] [52.482 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:47:19.459
    Feb 12 11:47:19.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 11:47:19.46
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:47:19.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:47:19.496
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a in namespace container-probe-233 02/12/23 11:47:19.5
    Feb 12 11:47:19.510: INFO: Waiting up to 5m0s for pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a" in namespace "container-probe-233" to be "not pending"
    Feb 12 11:47:19.517: INFO: Pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.194195ms
    Feb 12 11:47:21.533: INFO: Pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a": Phase="Running", Reason="", readiness=true. Elapsed: 2.022775407s
    Feb 12 11:47:21.533: INFO: Pod "busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a" satisfied condition "not pending"
    Feb 12 11:47:21.533: INFO: Started pod busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a in namespace container-probe-233
    STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:47:21.533
    Feb 12 11:47:21.552: INFO: Initial restart count of pod busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a is 0
    Feb 12 11:48:11.911: INFO: Restart count of pod container-probe-233/busybox-fdc832a9-ebd3-4fa2-aeb7-921a670a935a is now 1 (50.358762927s elapsed)
    STEP: deleting the pod 02/12/23 11:48:11.911
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 11:48:11.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-233" for this suite. 02/12/23 11:48:11.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:48:11.942
Feb 12 11:48:11.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 11:48:11.943
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:48:11.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:48:11.964
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/12/23 11:48:11.97
STEP: create the rc2 02/12/23 11:48:11.976
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/12/23 11:48:23.675
STEP: delete the rc simpletest-rc-to-be-deleted 02/12/23 11:48:27.369
STEP: wait for the rc to be deleted 02/12/23 11:48:27.406
Feb 12 11:48:32.941: INFO: 82 pods remaining
Feb 12 11:48:32.941: INFO: 80 pods has nil DeletionTimestamp
Feb 12 11:48:32.941: INFO: 
Feb 12 11:48:38.468: INFO: 56 pods remaining
Feb 12 11:48:38.468: INFO: 55 pods has nil DeletionTimestamp
Feb 12 11:48:38.468: INFO: 
STEP: Gathering metrics 02/12/23 11:48:46.276
Feb 12 11:48:46.941: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Feb 12 11:48:47.223: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 282.773701ms
Feb 12 11:48:47.223: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Feb 12 11:48:47.223: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Feb 12 11:48:47.533: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 12 11:48:47.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-24d4s" in namespace "gc-2041"
Feb 12 11:48:47.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5dm" in namespace "gc-2041"
Feb 12 11:48:47.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fxmh" in namespace "gc-2041"
Feb 12 11:48:47.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j767" in namespace "gc-2041"
Feb 12 11:48:48.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rxpw" in namespace "gc-2041"
Feb 12 11:48:48.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xzk4" in namespace "gc-2041"
Feb 12 11:48:48.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-4k62w" in namespace "gc-2041"
Feb 12 11:48:48.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lvdc" in namespace "gc-2041"
Feb 12 11:48:48.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qrxs" in namespace "gc-2041"
Feb 12 11:48:48.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wr2m" in namespace "gc-2041"
Feb 12 11:48:48.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-52r4r" in namespace "gc-2041"
Feb 12 11:48:48.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-56x8w" in namespace "gc-2041"
Feb 12 11:48:48.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-58ws2" in namespace "gc-2041"
Feb 12 11:48:48.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-5djrg" in namespace "gc-2041"
Feb 12 11:48:49.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-5klj9" in namespace "gc-2041"
Feb 12 11:48:49.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-696zz" in namespace "gc-2041"
Feb 12 11:48:49.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gh6q" in namespace "gc-2041"
Feb 12 11:48:49.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-75fhl" in namespace "gc-2041"
Feb 12 11:48:49.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jwg2" in namespace "gc-2041"
Feb 12 11:48:49.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kc55" in namespace "gc-2041"
Feb 12 11:48:49.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-82vnr" in namespace "gc-2041"
Feb 12 11:48:49.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bmn8" in namespace "gc-2041"
Feb 12 11:48:49.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dh9r" in namespace "gc-2041"
Feb 12 11:48:50.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-94cj7" in namespace "gc-2041"
Feb 12 11:48:50.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b47l" in namespace "gc-2041"
Feb 12 11:48:50.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gx69" in namespace "gc-2041"
Feb 12 11:48:50.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w59m" in namespace "gc-2041"
Feb 12 11:48:50.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf59v" in namespace "gc-2041"
Feb 12 11:48:50.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm59z" in namespace "gc-2041"
Feb 12 11:48:50.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz8pq" in namespace "gc-2041"
Feb 12 11:48:53.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzvgf" in namespace "gc-2041"
Feb 12 11:48:53.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckz8v" in namespace "gc-2041"
Feb 12 11:48:53.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvrs5" in namespace "gc-2041"
Feb 12 11:48:53.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvzjl" in namespace "gc-2041"
Feb 12 11:48:53.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw8nd" in namespace "gc-2041"
Feb 12 11:48:53.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6ght" in namespace "gc-2041"
Feb 12 11:48:53.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfsf7" in namespace "gc-2041"
Feb 12 11:48:53.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-dknmc" in namespace "gc-2041"
Feb 12 11:48:54.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlhzd" in namespace "gc-2041"
Feb 12 11:48:54.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxx8d" in namespace "gc-2041"
Feb 12 11:48:54.187: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2vwf" in namespace "gc-2041"
Feb 12 11:48:54.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-f92c6" in namespace "gc-2041"
Feb 12 11:48:54.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-flnvp" in namespace "gc-2041"
Feb 12 11:48:54.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwhr9" in namespace "gc-2041"
Feb 12 11:48:54.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzrbb" in namespace "gc-2041"
Feb 12 11:48:54.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-g89mm" in namespace "gc-2041"
Feb 12 11:48:54.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9lvd" in namespace "gc-2041"
Feb 12 11:48:55.071: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbmfw" in namespace "gc-2041"
Feb 12 11:49:07.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd56k" in namespace "gc-2041"
Feb 12 11:49:07.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd69w" in namespace "gc-2041"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 11:49:07.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2041" for this suite. 02/12/23 11:49:07.229
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":153,"skipped":2800,"failed":0}
------------------------------
 [SLOW TEST] [55.334 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:48:11.942
    Feb 12 11:48:11.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 11:48:11.943
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:48:11.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:48:11.964
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/12/23 11:48:11.97
    STEP: create the rc2 02/12/23 11:48:11.976
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/12/23 11:48:23.675
    STEP: delete the rc simpletest-rc-to-be-deleted 02/12/23 11:48:27.369
    STEP: wait for the rc to be deleted 02/12/23 11:48:27.406
    Feb 12 11:48:32.941: INFO: 82 pods remaining
    Feb 12 11:48:32.941: INFO: 80 pods has nil DeletionTimestamp
    Feb 12 11:48:32.941: INFO: 
    Feb 12 11:48:38.468: INFO: 56 pods remaining
    Feb 12 11:48:38.468: INFO: 55 pods has nil DeletionTimestamp
    Feb 12 11:48:38.468: INFO: 
    STEP: Gathering metrics 02/12/23 11:48:46.276
    Feb 12 11:48:46.941: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Feb 12 11:48:47.223: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 282.773701ms
    Feb 12 11:48:47.223: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Feb 12 11:48:47.223: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Feb 12 11:48:47.533: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 12 11:48:47.534: INFO: Deleting pod "simpletest-rc-to-be-deleted-24d4s" in namespace "gc-2041"
    Feb 12 11:48:47.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d5dm" in namespace "gc-2041"
    Feb 12 11:48:47.758: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fxmh" in namespace "gc-2041"
    Feb 12 11:48:47.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j767" in namespace "gc-2041"
    Feb 12 11:48:48.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rxpw" in namespace "gc-2041"
    Feb 12 11:48:48.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xzk4" in namespace "gc-2041"
    Feb 12 11:48:48.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-4k62w" in namespace "gc-2041"
    Feb 12 11:48:48.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lvdc" in namespace "gc-2041"
    Feb 12 11:48:48.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qrxs" in namespace "gc-2041"
    Feb 12 11:48:48.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wr2m" in namespace "gc-2041"
    Feb 12 11:48:48.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-52r4r" in namespace "gc-2041"
    Feb 12 11:48:48.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-56x8w" in namespace "gc-2041"
    Feb 12 11:48:48.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-58ws2" in namespace "gc-2041"
    Feb 12 11:48:48.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-5djrg" in namespace "gc-2041"
    Feb 12 11:48:49.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-5klj9" in namespace "gc-2041"
    Feb 12 11:48:49.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-696zz" in namespace "gc-2041"
    Feb 12 11:48:49.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gh6q" in namespace "gc-2041"
    Feb 12 11:48:49.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-75fhl" in namespace "gc-2041"
    Feb 12 11:48:49.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jwg2" in namespace "gc-2041"
    Feb 12 11:48:49.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kc55" in namespace "gc-2041"
    Feb 12 11:48:49.729: INFO: Deleting pod "simpletest-rc-to-be-deleted-82vnr" in namespace "gc-2041"
    Feb 12 11:48:49.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bmn8" in namespace "gc-2041"
    Feb 12 11:48:49.909: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dh9r" in namespace "gc-2041"
    Feb 12 11:48:50.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-94cj7" in namespace "gc-2041"
    Feb 12 11:48:50.237: INFO: Deleting pod "simpletest-rc-to-be-deleted-9b47l" in namespace "gc-2041"
    Feb 12 11:48:50.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gx69" in namespace "gc-2041"
    Feb 12 11:48:50.389: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w59m" in namespace "gc-2041"
    Feb 12 11:48:50.456: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf59v" in namespace "gc-2041"
    Feb 12 11:48:50.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm59z" in namespace "gc-2041"
    Feb 12 11:48:50.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz8pq" in namespace "gc-2041"
    Feb 12 11:48:53.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzvgf" in namespace "gc-2041"
    Feb 12 11:48:53.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckz8v" in namespace "gc-2041"
    Feb 12 11:48:53.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvrs5" in namespace "gc-2041"
    Feb 12 11:48:53.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvzjl" in namespace "gc-2041"
    Feb 12 11:48:53.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw8nd" in namespace "gc-2041"
    Feb 12 11:48:53.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6ght" in namespace "gc-2041"
    Feb 12 11:48:53.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfsf7" in namespace "gc-2041"
    Feb 12 11:48:53.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-dknmc" in namespace "gc-2041"
    Feb 12 11:48:54.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlhzd" in namespace "gc-2041"
    Feb 12 11:48:54.112: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxx8d" in namespace "gc-2041"
    Feb 12 11:48:54.187: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2vwf" in namespace "gc-2041"
    Feb 12 11:48:54.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-f92c6" in namespace "gc-2041"
    Feb 12 11:48:54.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-flnvp" in namespace "gc-2041"
    Feb 12 11:48:54.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwhr9" in namespace "gc-2041"
    Feb 12 11:48:54.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzrbb" in namespace "gc-2041"
    Feb 12 11:48:54.787: INFO: Deleting pod "simpletest-rc-to-be-deleted-g89mm" in namespace "gc-2041"
    Feb 12 11:48:54.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9lvd" in namespace "gc-2041"
    Feb 12 11:48:55.071: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbmfw" in namespace "gc-2041"
    Feb 12 11:49:07.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd56k" in namespace "gc-2041"
    Feb 12 11:49:07.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd69w" in namespace "gc-2041"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 11:49:07.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2041" for this suite. 02/12/23 11:49:07.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:49:07.277
Feb 12 11:49:07.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:49:07.279
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:33.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:33.118
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:49:33.234
Feb 12 11:49:33.242: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-373" to be "running and ready"
Feb 12 11:49:33.246: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.344411ms
Feb 12 11:49:33.246: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:49:35.257: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014984593s
Feb 12 11:49:35.257: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 12 11:49:35.257: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 02/12/23 11:49:35.262
Feb 12 11:49:35.273: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-373" to be "running and ready"
Feb 12 11:49:35.279: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162567ms
Feb 12 11:49:35.279: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:49:37.301: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.028204134s
Feb 12 11:49:37.301: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 12 11:49:37.301: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/12/23 11:49:37.31
STEP: delete the pod with lifecycle hook 02/12/23 11:49:37.32
Feb 12 11:49:37.333: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 11:49:37.339: INFO: Pod pod-with-poststart-http-hook still exists
Feb 12 11:49:39.340: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 11:49:39.344: INFO: Pod pod-with-poststart-http-hook still exists
Feb 12 11:49:41.341: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 11:49:41.356: INFO: Pod pod-with-poststart-http-hook still exists
Feb 12 11:49:43.340: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 11:49:43.357: INFO: Pod pod-with-poststart-http-hook still exists
Feb 12 11:49:45.340: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 12 11:49:45.349: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 12 11:49:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-373" for this suite. 02/12/23 11:49:45.356
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":154,"skipped":2825,"failed":0}
------------------------------
 [SLOW TEST] [38.090 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:49:07.277
    Feb 12 11:49:07.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:49:07.279
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:33.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:33.118
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:49:33.234
    Feb 12 11:49:33.242: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-373" to be "running and ready"
    Feb 12 11:49:33.246: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.344411ms
    Feb 12 11:49:33.246: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:49:35.257: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014984593s
    Feb 12 11:49:35.257: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 12 11:49:35.257: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 02/12/23 11:49:35.262
    Feb 12 11:49:35.273: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-373" to be "running and ready"
    Feb 12 11:49:35.279: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162567ms
    Feb 12 11:49:35.279: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:49:37.301: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.028204134s
    Feb 12 11:49:37.301: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 12 11:49:37.301: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/12/23 11:49:37.31
    STEP: delete the pod with lifecycle hook 02/12/23 11:49:37.32
    Feb 12 11:49:37.333: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 12 11:49:37.339: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 12 11:49:39.340: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 12 11:49:39.344: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 12 11:49:41.341: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 12 11:49:41.356: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 12 11:49:43.340: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 12 11:49:43.357: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 12 11:49:45.340: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 12 11:49:45.349: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 12 11:49:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-373" for this suite. 02/12/23 11:49:45.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:49:45.367
Feb 12 11:49:45.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename containers 02/12/23 11:49:45.37
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:45.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:45.4
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 02/12/23 11:49:45.403
Feb 12 11:49:45.421: INFO: Waiting up to 5m0s for pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b" in namespace "containers-4711" to be "Succeeded or Failed"
Feb 12 11:49:45.433: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.900521ms
Feb 12 11:49:47.436: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015517023s
Feb 12 11:49:49.518: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096946798s
STEP: Saw pod success 02/12/23 11:49:49.518
Feb 12 11:49:49.518: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b" satisfied condition "Succeeded or Failed"
Feb 12 11:49:49.595: INFO: Trying to get logs from node kube-3 pod client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:49:50.757
Feb 12 11:49:52.479: INFO: Waiting for pod client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b to disappear
Feb 12 11:49:53.415: INFO: Pod client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 12 11:49:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4711" for this suite. 02/12/23 11:49:53.435
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":155,"skipped":2846,"failed":0}
------------------------------
 [SLOW TEST] [8.185 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:49:45.367
    Feb 12 11:49:45.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename containers 02/12/23 11:49:45.37
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:45.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:45.4
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 02/12/23 11:49:45.403
    Feb 12 11:49:45.421: INFO: Waiting up to 5m0s for pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b" in namespace "containers-4711" to be "Succeeded or Failed"
    Feb 12 11:49:45.433: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.900521ms
    Feb 12 11:49:47.436: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015517023s
    Feb 12 11:49:49.518: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096946798s
    STEP: Saw pod success 02/12/23 11:49:49.518
    Feb 12 11:49:49.518: INFO: Pod "client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b" satisfied condition "Succeeded or Failed"
    Feb 12 11:49:49.595: INFO: Trying to get logs from node kube-3 pod client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:49:50.757
    Feb 12 11:49:52.479: INFO: Waiting for pod client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b to disappear
    Feb 12 11:49:53.415: INFO: Pod client-containers-6f8bea2a-23c5-473c-95c2-88117ee0549b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 12 11:49:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4711" for this suite. 02/12/23 11:49:53.435
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:49:53.553
Feb 12 11:49:53.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:49:53.554
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:53.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:53.733
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 02/12/23 11:49:53.748
STEP: waiting for available Endpoint 02/12/23 11:49:53.779
STEP: listing all Endpoints 02/12/23 11:49:53.78
STEP: updating the Endpoint 02/12/23 11:49:53.783
STEP: fetching the Endpoint 02/12/23 11:49:53.875
STEP: patching the Endpoint 02/12/23 11:49:53.886
STEP: fetching the Endpoint 02/12/23 11:49:54.023
STEP: deleting the Endpoint by Collection 02/12/23 11:49:54.037
STEP: waiting for Endpoint deletion 02/12/23 11:49:54.061
STEP: fetching the Endpoint 02/12/23 11:49:54.063
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:49:54.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5415" for this suite. 02/12/23 11:49:54.075
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":156,"skipped":2847,"failed":0}
------------------------------
 [0.542 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:49:53.553
    Feb 12 11:49:53.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:49:53.554
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:53.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:53.733
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 02/12/23 11:49:53.748
    STEP: waiting for available Endpoint 02/12/23 11:49:53.779
    STEP: listing all Endpoints 02/12/23 11:49:53.78
    STEP: updating the Endpoint 02/12/23 11:49:53.783
    STEP: fetching the Endpoint 02/12/23 11:49:53.875
    STEP: patching the Endpoint 02/12/23 11:49:53.886
    STEP: fetching the Endpoint 02/12/23 11:49:54.023
    STEP: deleting the Endpoint by Collection 02/12/23 11:49:54.037
    STEP: waiting for Endpoint deletion 02/12/23 11:49:54.061
    STEP: fetching the Endpoint 02/12/23 11:49:54.063
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:49:54.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5415" for this suite. 02/12/23 11:49:54.075
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:49:54.095
Feb 12 11:49:54.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/12/23 11:49:54.096
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:54.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:54.214
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/12/23 11:49:54.275
STEP: Creating hostNetwork=false pod 02/12/23 11:49:54.275
Feb 12 11:49:54.348: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-885" to be "running and ready"
Feb 12 11:49:54.354: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.738652ms
Feb 12 11:49:54.354: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:49:56.359: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010869164s
Feb 12 11:49:56.359: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:49:58.358: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010355387s
Feb 12 11:49:58.358: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 12 11:49:58.358: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/12/23 11:49:58.363
Feb 12 11:49:58.381: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-885" to be "running and ready"
Feb 12 11:49:58.385: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900134ms
Feb 12 11:49:58.385: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:50:00.403: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021381885s
Feb 12 11:50:00.403: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 12 11:50:00.403: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/12/23 11:50:00.415
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/12/23 11:50:00.415
Feb 12 11:50:00.416: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.417: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.417: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 12 11:50:00.529: INFO: Exec stderr: ""
Feb 12 11:50:00.529: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.530: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.530: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 12 11:50:00.593: INFO: Exec stderr: ""
Feb 12 11:50:00.593: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.593: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.593: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 12 11:50:00.648: INFO: Exec stderr: ""
Feb 12 11:50:00.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.649: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.649: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 12 11:50:00.706: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/12/23 11:50:00.706
Feb 12 11:50:00.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.707: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.707: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 12 11:50:00.761: INFO: Exec stderr: ""
Feb 12 11:50:00.761: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.761: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.761: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 12 11:50:00.824: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/12/23 11:50:00.824
Feb 12 11:50:00.824: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.825: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.825: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 12 11:50:00.897: INFO: Exec stderr: ""
Feb 12 11:50:00.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.897: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.897: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 12 11:50:00.958: INFO: Exec stderr: ""
Feb 12 11:50:00.958: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:00.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:00.959: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:00.959: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 12 11:50:01.008: INFO: Exec stderr: ""
Feb 12 11:50:01.008: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 11:50:01.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:50:01.008: INFO: ExecWithOptions: Clientset creation
Feb 12 11:50:01.008: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 12 11:50:01.064: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Feb 12 11:50:01.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-885" for this suite. 02/12/23 11:50:01.067
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":157,"skipped":2849,"failed":0}
------------------------------
 [SLOW TEST] [6.985 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:49:54.095
    Feb 12 11:49:54.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/12/23 11:49:54.096
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:49:54.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:49:54.214
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/12/23 11:49:54.275
    STEP: Creating hostNetwork=false pod 02/12/23 11:49:54.275
    Feb 12 11:49:54.348: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-885" to be "running and ready"
    Feb 12 11:49:54.354: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.738652ms
    Feb 12 11:49:54.354: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:49:56.359: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010869164s
    Feb 12 11:49:56.359: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:49:58.358: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010355387s
    Feb 12 11:49:58.358: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 12 11:49:58.358: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/12/23 11:49:58.363
    Feb 12 11:49:58.381: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-885" to be "running and ready"
    Feb 12 11:49:58.385: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900134ms
    Feb 12 11:49:58.385: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:50:00.403: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021381885s
    Feb 12 11:50:00.403: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 12 11:50:00.403: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/12/23 11:50:00.415
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/12/23 11:50:00.415
    Feb 12 11:50:00.416: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.417: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.417: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 12 11:50:00.529: INFO: Exec stderr: ""
    Feb 12 11:50:00.529: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.530: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.530: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 12 11:50:00.593: INFO: Exec stderr: ""
    Feb 12 11:50:00.593: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.593: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.593: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 12 11:50:00.648: INFO: Exec stderr: ""
    Feb 12 11:50:00.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.649: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.649: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 12 11:50:00.706: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/12/23 11:50:00.706
    Feb 12 11:50:00.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.707: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.707: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 12 11:50:00.761: INFO: Exec stderr: ""
    Feb 12 11:50:00.761: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.761: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.761: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 12 11:50:00.824: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/12/23 11:50:00.824
    Feb 12 11:50:00.824: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.825: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.825: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 12 11:50:00.897: INFO: Exec stderr: ""
    Feb 12 11:50:00.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.897: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.897: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 12 11:50:00.958: INFO: Exec stderr: ""
    Feb 12 11:50:00.958: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:00.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:00.959: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:00.959: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 12 11:50:01.008: INFO: Exec stderr: ""
    Feb 12 11:50:01.008: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-885 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 11:50:01.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:50:01.008: INFO: ExecWithOptions: Clientset creation
    Feb 12 11:50:01.008: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-885/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 12 11:50:01.064: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Feb 12 11:50:01.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-885" for this suite. 02/12/23 11:50:01.067
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:01.081
Feb 12 11:50:01.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename security-context-test 02/12/23 11:50:01.082
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:01.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:01.101
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Feb 12 11:50:01.114: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64" in namespace "security-context-test-7640" to be "Succeeded or Failed"
Feb 12 11:50:01.117: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214622ms
Feb 12 11:50:03.132: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017808095s
Feb 12 11:50:05.131: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017116029s
Feb 12 11:50:07.131: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017361194s
Feb 12 11:50:07.131: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 12 11:50:07.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7640" for this suite. 02/12/23 11:50:07.144
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":158,"skipped":2851,"failed":0}
------------------------------
 [SLOW TEST] [6.082 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:01.081
    Feb 12 11:50:01.081: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename security-context-test 02/12/23 11:50:01.082
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:01.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:01.101
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Feb 12 11:50:01.114: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64" in namespace "security-context-test-7640" to be "Succeeded or Failed"
    Feb 12 11:50:01.117: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214622ms
    Feb 12 11:50:03.132: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017808095s
    Feb 12 11:50:05.131: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017116029s
    Feb 12 11:50:07.131: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017361194s
    Feb 12 11:50:07.131: INFO: Pod "busybox-readonly-false-c8df2ce8-787b-4df1-b55a-9193f8f86b64" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 12 11:50:07.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7640" for this suite. 02/12/23 11:50:07.144
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:07.164
Feb 12 11:50:07.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:50:07.166
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:07.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:07.189
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8945/secret-test-a8648134-41cb-4650-bf09-981c9e8ddd9e 02/12/23 11:50:07.191
STEP: Creating a pod to test consume secrets 02/12/23 11:50:07.195
Feb 12 11:50:07.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95" in namespace "secrets-8945" to be "Succeeded or Failed"
Feb 12 11:50:07.212: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.205761ms
Feb 12 11:50:09.218: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390834s
Feb 12 11:50:11.216: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011874604s
STEP: Saw pod success 02/12/23 11:50:11.216
Feb 12 11:50:11.216: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95" satisfied condition "Succeeded or Failed"
Feb 12 11:50:11.220: INFO: Trying to get logs from node kube-3 pod pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95 container env-test: <nil>
STEP: delete the pod 02/12/23 11:50:11.226
Feb 12 11:50:11.244: INFO: Waiting for pod pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95 to disappear
Feb 12 11:50:11.247: INFO: Pod pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:50:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8945" for this suite. 02/12/23 11:50:11.25
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":159,"skipped":2854,"failed":0}
------------------------------
 [4.093 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:07.164
    Feb 12 11:50:07.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:50:07.166
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:07.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:07.189
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8945/secret-test-a8648134-41cb-4650-bf09-981c9e8ddd9e 02/12/23 11:50:07.191
    STEP: Creating a pod to test consume secrets 02/12/23 11:50:07.195
    Feb 12 11:50:07.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95" in namespace "secrets-8945" to be "Succeeded or Failed"
    Feb 12 11:50:07.212: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.205761ms
    Feb 12 11:50:09.218: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013390834s
    Feb 12 11:50:11.216: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011874604s
    STEP: Saw pod success 02/12/23 11:50:11.216
    Feb 12 11:50:11.216: INFO: Pod "pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95" satisfied condition "Succeeded or Failed"
    Feb 12 11:50:11.220: INFO: Trying to get logs from node kube-3 pod pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95 container env-test: <nil>
    STEP: delete the pod 02/12/23 11:50:11.226
    Feb 12 11:50:11.244: INFO: Waiting for pod pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95 to disappear
    Feb 12 11:50:11.247: INFO: Pod pod-configmaps-e55ae2ad-ac70-4fb1-ad0f-c8a85aa8aa95 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:50:11.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8945" for this suite. 02/12/23 11:50:11.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:11.258
Feb 12 11:50:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:50:11.258
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:11.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:11.28
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4541 02/12/23 11:50:11.281
STEP: changing the ExternalName service to type=ClusterIP 02/12/23 11:50:11.288
STEP: creating replication controller externalname-service in namespace services-4541 02/12/23 11:50:11.315
I0212 11:50:11.325717      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4541, replica count: 2
I0212 11:50:14.378126      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 11:50:17.378858      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 11:50:20.379550      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 11:50:20.380: INFO: Creating new exec pod
Feb 12 11:50:20.394: INFO: Waiting up to 5m0s for pod "execpodzpk8n" in namespace "services-4541" to be "running"
Feb 12 11:50:20.400: INFO: Pod "execpodzpk8n": Phase="Pending", Reason="", readiness=false. Elapsed: 5.423983ms
Feb 12 11:50:22.404: INFO: Pod "execpodzpk8n": Phase="Running", Reason="", readiness=true. Elapsed: 2.00936715s
Feb 12 11:50:22.404: INFO: Pod "execpodzpk8n" satisfied condition "running"
Feb 12 11:50:23.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4541 exec execpodzpk8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 12 11:50:23.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 12 11:50:23.715: INFO: stdout: "externalname-service-ncrst"
Feb 12 11:50:23.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4541 exec execpodzpk8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.161 80'
Feb 12 11:50:23.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.161 80\nConnection to 10.233.2.161 80 port [tcp/http] succeeded!\n"
Feb 12 11:50:23.903: INFO: stdout: "externalname-service-hmrqw"
Feb 12 11:50:23.903: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:50:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4541" for this suite. 02/12/23 11:50:23.946
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":160,"skipped":2862,"failed":0}
------------------------------
 [SLOW TEST] [12.706 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:11.258
    Feb 12 11:50:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:50:11.258
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:11.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:11.28
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4541 02/12/23 11:50:11.281
    STEP: changing the ExternalName service to type=ClusterIP 02/12/23 11:50:11.288
    STEP: creating replication controller externalname-service in namespace services-4541 02/12/23 11:50:11.315
    I0212 11:50:11.325717      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4541, replica count: 2
    I0212 11:50:14.378126      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 11:50:17.378858      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 11:50:20.379550      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 11:50:20.380: INFO: Creating new exec pod
    Feb 12 11:50:20.394: INFO: Waiting up to 5m0s for pod "execpodzpk8n" in namespace "services-4541" to be "running"
    Feb 12 11:50:20.400: INFO: Pod "execpodzpk8n": Phase="Pending", Reason="", readiness=false. Elapsed: 5.423983ms
    Feb 12 11:50:22.404: INFO: Pod "execpodzpk8n": Phase="Running", Reason="", readiness=true. Elapsed: 2.00936715s
    Feb 12 11:50:22.404: INFO: Pod "execpodzpk8n" satisfied condition "running"
    Feb 12 11:50:23.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4541 exec execpodzpk8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 12 11:50:23.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 12 11:50:23.715: INFO: stdout: "externalname-service-ncrst"
    Feb 12 11:50:23.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4541 exec execpodzpk8n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.161 80'
    Feb 12 11:50:23.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.161 80\nConnection to 10.233.2.161 80 port [tcp/http] succeeded!\n"
    Feb 12 11:50:23.903: INFO: stdout: "externalname-service-hmrqw"
    Feb 12 11:50:23.903: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:50:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4541" for this suite. 02/12/23 11:50:23.946
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:23.965
Feb 12 11:50:23.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:50:23.966
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:23.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:24
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:50:24.029
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:50:24.451
STEP: Deploying the webhook pod 02/12/23 11:50:24.46
STEP: Wait for the deployment to be ready 02/12/23 11:50:24.474
Feb 12 11:50:24.479: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/12/23 11:50:26.516
STEP: Verifying the service has paired with the endpoint 02/12/23 11:50:26.546
Feb 12 11:50:27.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 02/12/23 11:50:27.56
STEP: Creating a custom resource definition that should be denied by the webhook 02/12/23 11:50:27.592
Feb 12 11:50:27.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:50:27.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9453" for this suite. 02/12/23 11:50:27.624
STEP: Destroying namespace "webhook-9453-markers" for this suite. 02/12/23 11:50:27.634
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":161,"skipped":2898,"failed":0}
------------------------------
 [3.744 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:23.965
    Feb 12 11:50:23.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:50:23.966
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:23.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:24
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:50:24.029
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:50:24.451
    STEP: Deploying the webhook pod 02/12/23 11:50:24.46
    STEP: Wait for the deployment to be ready 02/12/23 11:50:24.474
    Feb 12 11:50:24.479: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/12/23 11:50:26.516
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:50:26.546
    Feb 12 11:50:27.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/12/23 11:50:27.56
    STEP: Creating a custom resource definition that should be denied by the webhook 02/12/23 11:50:27.592
    Feb 12 11:50:27.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:50:27.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9453" for this suite. 02/12/23 11:50:27.624
    STEP: Destroying namespace "webhook-9453-markers" for this suite. 02/12/23 11:50:27.634
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:27.71
Feb 12 11:50:27.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:50:27.71
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:27.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:27.742
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8412-delete-me 02/12/23 11:50:27.753
STEP: Waiting for the RuntimeClass to disappear 02/12/23 11:50:27.766
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 12 11:50:27.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8412" for this suite. 02/12/23 11:50:27.791
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":162,"skipped":2923,"failed":0}
------------------------------
 [0.092 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:27.71
    Feb 12 11:50:27.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:50:27.71
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:27.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:27.742
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8412-delete-me 02/12/23 11:50:27.753
    STEP: Waiting for the RuntimeClass to disappear 02/12/23 11:50:27.766
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 12 11:50:27.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8412" for this suite. 02/12/23 11:50:27.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:27.807
Feb 12 11:50:27.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename watch 02/12/23 11:50:27.808
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:27.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:27.836
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/12/23 11:50:27.84
STEP: creating a watch on configmaps with label B 02/12/23 11:50:27.843
STEP: creating a watch on configmaps with label A or B 02/12/23 11:50:27.845
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/12/23 11:50:27.847
Feb 12 11:50:27.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18763 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:50:27.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18763 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/12/23 11:50:27.858
Feb 12 11:50:27.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18764 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:50:27.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18764 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/12/23 11:50:27.873
Feb 12 11:50:27.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18765 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:50:27.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18765 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/12/23 11:50:27.891
Feb 12 11:50:27.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18767 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:50:27.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18767 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/12/23 11:50:27.909
Feb 12 11:50:27.915: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18768 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:50:27.915: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18768 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/12/23 11:50:37.916
Feb 12 11:50:37.931: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18850 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:50:37.931: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18850 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 12 11:50:47.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2425" for this suite. 02/12/23 11:50:47.945
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":163,"skipped":2942,"failed":0}
------------------------------
 [SLOW TEST] [20.157 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:27.807
    Feb 12 11:50:27.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename watch 02/12/23 11:50:27.808
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:27.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:27.836
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/12/23 11:50:27.84
    STEP: creating a watch on configmaps with label B 02/12/23 11:50:27.843
    STEP: creating a watch on configmaps with label A or B 02/12/23 11:50:27.845
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/12/23 11:50:27.847
    Feb 12 11:50:27.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18763 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:50:27.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18763 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/12/23 11:50:27.858
    Feb 12 11:50:27.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18764 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:50:27.873: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18764 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/12/23 11:50:27.873
    Feb 12 11:50:27.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18765 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:50:27.891: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18765 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/12/23 11:50:27.891
    Feb 12 11:50:27.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18767 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:50:27.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2425  3456b847-cf84-42a0-a801-c5e108e4c13e 18767 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/12/23 11:50:27.909
    Feb 12 11:50:27.915: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18768 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:50:27.915: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18768 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/12/23 11:50:37.916
    Feb 12 11:50:37.931: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18850 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:50:37.931: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2425  3c117895-2e9d-47a7-b1e3-0d69a91dbbf8 18850 0 2023-02-12 11:50:27 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-12 11:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 12 11:50:47.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2425" for this suite. 02/12/23 11:50:47.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:47.966
Feb 12 11:50:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename cronjob 02/12/23 11:50:47.967
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:47.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:47.988
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/12/23 11:50:47.99
STEP: creating 02/12/23 11:50:47.99
STEP: getting 02/12/23 11:50:47.999
STEP: listing 02/12/23 11:50:48.004
STEP: watching 02/12/23 11:50:48.016
Feb 12 11:50:48.016: INFO: starting watch
STEP: cluster-wide listing 02/12/23 11:50:48.017
STEP: cluster-wide watching 02/12/23 11:50:48.02
Feb 12 11:50:48.021: INFO: starting watch
STEP: patching 02/12/23 11:50:48.022
STEP: updating 02/12/23 11:50:48.031
Feb 12 11:50:48.041: INFO: waiting for watch events with expected annotations
Feb 12 11:50:48.041: INFO: saw patched and updated annotations
STEP: patching /status 02/12/23 11:50:48.041
STEP: updating /status 02/12/23 11:50:48.048
STEP: get /status 02/12/23 11:50:48.056
STEP: deleting 02/12/23 11:50:48.058
STEP: deleting a collection 02/12/23 11:50:48.075
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 12 11:50:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3317" for this suite. 02/12/23 11:50:48.088
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":164,"skipped":3011,"failed":0}
------------------------------
 [0.130 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:47.966
    Feb 12 11:50:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename cronjob 02/12/23 11:50:47.967
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:47.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:47.988
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/12/23 11:50:47.99
    STEP: creating 02/12/23 11:50:47.99
    STEP: getting 02/12/23 11:50:47.999
    STEP: listing 02/12/23 11:50:48.004
    STEP: watching 02/12/23 11:50:48.016
    Feb 12 11:50:48.016: INFO: starting watch
    STEP: cluster-wide listing 02/12/23 11:50:48.017
    STEP: cluster-wide watching 02/12/23 11:50:48.02
    Feb 12 11:50:48.021: INFO: starting watch
    STEP: patching 02/12/23 11:50:48.022
    STEP: updating 02/12/23 11:50:48.031
    Feb 12 11:50:48.041: INFO: waiting for watch events with expected annotations
    Feb 12 11:50:48.041: INFO: saw patched and updated annotations
    STEP: patching /status 02/12/23 11:50:48.041
    STEP: updating /status 02/12/23 11:50:48.048
    STEP: get /status 02/12/23 11:50:48.056
    STEP: deleting 02/12/23 11:50:48.058
    STEP: deleting a collection 02/12/23 11:50:48.075
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 12 11:50:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3317" for this suite. 02/12/23 11:50:48.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:48.098
Feb 12 11:50:48.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:50:48.098
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:48.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:48.117
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 12 11:50:48.132: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1968 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 12 11:50:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1968" for this suite. 02/12/23 11:50:48.159
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":165,"skipped":3076,"failed":0}
------------------------------
 [0.081 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:48.098
    Feb 12 11:50:48.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:50:48.098
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:48.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:48.117
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 12 11:50:48.132: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1968 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 12 11:50:48.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1968" for this suite. 02/12/23 11:50:48.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:48.179
Feb 12 11:50:48.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename csistoragecapacity 02/12/23 11:50:48.18
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:48.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:48.212
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/12/23 11:50:48.214
STEP: getting /apis/storage.k8s.io 02/12/23 11:50:48.215
STEP: getting /apis/storage.k8s.io/v1 02/12/23 11:50:48.216
STEP: creating 02/12/23 11:50:48.217
STEP: watching 02/12/23 11:50:48.238
Feb 12 11:50:48.238: INFO: starting watch
STEP: getting 02/12/23 11:50:48.248
STEP: listing in namespace 02/12/23 11:50:48.252
STEP: listing across namespaces 02/12/23 11:50:48.254
STEP: patching 02/12/23 11:50:48.264
STEP: updating 02/12/23 11:50:48.272
Feb 12 11:50:48.278: INFO: waiting for watch events with expected annotations in namespace
Feb 12 11:50:48.278: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/12/23 11:50:48.278
STEP: deleting a collection 02/12/23 11:50:48.289
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Feb 12 11:50:48.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-3868" for this suite. 02/12/23 11:50:48.307
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":166,"skipped":3081,"failed":0}
------------------------------
 [0.136 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:48.179
    Feb 12 11:50:48.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename csistoragecapacity 02/12/23 11:50:48.18
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:48.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:48.212
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/12/23 11:50:48.214
    STEP: getting /apis/storage.k8s.io 02/12/23 11:50:48.215
    STEP: getting /apis/storage.k8s.io/v1 02/12/23 11:50:48.216
    STEP: creating 02/12/23 11:50:48.217
    STEP: watching 02/12/23 11:50:48.238
    Feb 12 11:50:48.238: INFO: starting watch
    STEP: getting 02/12/23 11:50:48.248
    STEP: listing in namespace 02/12/23 11:50:48.252
    STEP: listing across namespaces 02/12/23 11:50:48.254
    STEP: patching 02/12/23 11:50:48.264
    STEP: updating 02/12/23 11:50:48.272
    Feb 12 11:50:48.278: INFO: waiting for watch events with expected annotations in namespace
    Feb 12 11:50:48.278: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/12/23 11:50:48.278
    STEP: deleting a collection 02/12/23 11:50:48.289
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Feb 12 11:50:48.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-3868" for this suite. 02/12/23 11:50:48.307
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:48.315
Feb 12 11:50:48.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 11:50:48.316
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:48.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:48.332
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 12 11:50:48.334: INFO: Creating simple deployment test-new-deployment
Feb 12 11:50:48.357: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 02/12/23 11:50:50.414
STEP: updating a scale subresource 02/12/23 11:50:50.424
STEP: verifying the deployment Spec.Replicas was modified 02/12/23 11:50:50.434
STEP: Patch a scale subresource 02/12/23 11:50:50.44
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 11:50:50.487: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6120  b8147ef1-aefa-4572-97cf-c3e2849b9a90 18937 3 2023-02-12 11:50:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-12 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:50:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004886278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-12 11:50:49 +0000 UTC,LastTransitionTime:2023-02-12 11:50:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-12 11:50:49 +0000 UTC,LastTransitionTime:2023-02-12 11:50:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 11:50:50.497: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6120  0bd22517-b44f-4996-91c4-549ca2679114 18941 3 2023-02-12 11:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b8147ef1-aefa-4572-97cf-c3e2849b9a90 0xc0019780e7 0xc0019780e8}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:50:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-12 11:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8147ef1-aefa-4572-97cf-c3e2849b9a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001978188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 11:50:50.507: INFO: Pod "test-new-deployment-845c8977d9-84lq2" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-84lq2 test-new-deployment-845c8977d9- deployment-6120  abb3665a-5c78-4426-a874-00e7f6250cc8 18932 0 2023-02-12 11:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3e0d2ab5b7e5bd3b1e45b0b0f506dc091ce5ae59948bbcc1b9ef1cf22c3f6b17 cni.projectcalico.org/podIP:10.233.99.124/32 cni.projectcalico.org/podIPs:10.233.99.124/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0bd22517-b44f-4996-91c4-549ca2679114 0xc0019785b7 0xc0019785b8}] [] [{calico Update v1 2023-02-12 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-12 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22517-b44f-4996-91c4-549ca2679114\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 11:50:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvsfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvsfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.124,StartTime:2023-02-12 11:50:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20eb4bb3e1219bbb87c801e5778991f8e6e78bd50a72e10bab725d168f43fb2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 11:50:50.507: INFO: Pod "test-new-deployment-845c8977d9-bw8vd" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-bw8vd test-new-deployment-845c8977d9- deployment-6120  2ef2f788-336d-4d1e-a02d-3cd386834c4b 18946 0 2023-02-12 11:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0bd22517-b44f-4996-91c4-549ca2679114 0xc0019787b7 0xc0019787b8}] [] [{kube-controller-manager Update v1 2023-02-12 11:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22517-b44f-4996-91c4-549ca2679114\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 11:50:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbnqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbnqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 11:50:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 11:50:50.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6120" for this suite. 02/12/23 11:50:50.512
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":167,"skipped":3081,"failed":0}
------------------------------
 [2.216 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:48.315
    Feb 12 11:50:48.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 11:50:48.316
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:48.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:48.332
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 12 11:50:48.334: INFO: Creating simple deployment test-new-deployment
    Feb 12 11:50:48.357: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 02/12/23 11:50:50.414
    STEP: updating a scale subresource 02/12/23 11:50:50.424
    STEP: verifying the deployment Spec.Replicas was modified 02/12/23 11:50:50.434
    STEP: Patch a scale subresource 02/12/23 11:50:50.44
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 11:50:50.487: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6120  b8147ef1-aefa-4572-97cf-c3e2849b9a90 18937 3 2023-02-12 11:50:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-12 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 11:50:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004886278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-12 11:50:49 +0000 UTC,LastTransitionTime:2023-02-12 11:50:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-02-12 11:50:49 +0000 UTC,LastTransitionTime:2023-02-12 11:50:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 12 11:50:50.497: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6120  0bd22517-b44f-4996-91c4-549ca2679114 18941 3 2023-02-12 11:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b8147ef1-aefa-4572-97cf-c3e2849b9a90 0xc0019780e7 0xc0019780e8}] [] [{kube-controller-manager Update apps/v1 2023-02-12 11:50:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-02-12 11:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8147ef1-aefa-4572-97cf-c3e2849b9a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001978188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 11:50:50.507: INFO: Pod "test-new-deployment-845c8977d9-84lq2" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-84lq2 test-new-deployment-845c8977d9- deployment-6120  abb3665a-5c78-4426-a874-00e7f6250cc8 18932 0 2023-02-12 11:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3e0d2ab5b7e5bd3b1e45b0b0f506dc091ce5ae59948bbcc1b9ef1cf22c3f6b17 cni.projectcalico.org/podIP:10.233.99.124/32 cni.projectcalico.org/podIPs:10.233.99.124/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0bd22517-b44f-4996-91c4-549ca2679114 0xc0019785b7 0xc0019785b8}] [] [{calico Update v1 2023-02-12 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-02-12 11:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22517-b44f-4996-91c4-549ca2679114\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 11:50:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvsfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvsfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.124,StartTime:2023-02-12 11:50:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 11:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://20eb4bb3e1219bbb87c801e5778991f8e6e78bd50a72e10bab725d168f43fb2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 11:50:50.507: INFO: Pod "test-new-deployment-845c8977d9-bw8vd" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-bw8vd test-new-deployment-845c8977d9- deployment-6120  2ef2f788-336d-4d1e-a02d-3cd386834c4b 18946 0 2023-02-12 11:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 0bd22517-b44f-4996-91c4-549ca2679114 0xc0019787b7 0xc0019787b8}] [] [{kube-controller-manager Update v1 2023-02-12 11:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bd22517-b44f-4996-91c4-549ca2679114\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 11:50:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbnqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbnqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 11:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 11:50:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 11:50:50.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6120" for this suite. 02/12/23 11:50:50.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:50.531
Feb 12 11:50:50.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:50:50.535
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:50.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:50.587
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 02/12/23 11:50:50.599
STEP: watching for Pod to be ready 02/12/23 11:50:50.614
Feb 12 11:50:50.616: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 12 11:50:50.618: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
Feb 12 11:50:50.634: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
Feb 12 11:50:51.144: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
Feb 12 11:50:51.907: INFO: Found Pod pod-test in namespace pods-7594 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/12/23 11:50:51.912
STEP: getting the Pod and ensuring that it's patched 02/12/23 11:50:51.927
STEP: replacing the Pod's status Ready condition to False 02/12/23 11:50:51.932
STEP: check the Pod again to ensure its Ready conditions are False 02/12/23 11:50:51.943
STEP: deleting the Pod via a Collection with a LabelSelector 02/12/23 11:50:51.943
STEP: watching for the Pod to be deleted 02/12/23 11:50:51.954
Feb 12 11:50:51.955: INFO: observed event type MODIFIED
Feb 12 11:50:52.948: INFO: observed event type MODIFIED
Feb 12 11:50:54.063: INFO: observed event type MODIFIED
Feb 12 11:50:54.900: INFO: observed event type MODIFIED
Feb 12 11:50:54.912: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:50:54.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7594" for this suite. 02/12/23 11:50:54.929
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":168,"skipped":3088,"failed":0}
------------------------------
 [4.406 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:50.531
    Feb 12 11:50:50.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:50:50.535
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:50.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:50.587
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 02/12/23 11:50:50.599
    STEP: watching for Pod to be ready 02/12/23 11:50:50.614
    Feb 12 11:50:50.616: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 12 11:50:50.618: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
    Feb 12 11:50:50.634: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
    Feb 12 11:50:51.144: INFO: observed Pod pod-test in namespace pods-7594 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
    Feb 12 11:50:51.907: INFO: Found Pod pod-test in namespace pods-7594 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:50:50 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/12/23 11:50:51.912
    STEP: getting the Pod and ensuring that it's patched 02/12/23 11:50:51.927
    STEP: replacing the Pod's status Ready condition to False 02/12/23 11:50:51.932
    STEP: check the Pod again to ensure its Ready conditions are False 02/12/23 11:50:51.943
    STEP: deleting the Pod via a Collection with a LabelSelector 02/12/23 11:50:51.943
    STEP: watching for the Pod to be deleted 02/12/23 11:50:51.954
    Feb 12 11:50:51.955: INFO: observed event type MODIFIED
    Feb 12 11:50:52.948: INFO: observed event type MODIFIED
    Feb 12 11:50:54.063: INFO: observed event type MODIFIED
    Feb 12 11:50:54.900: INFO: observed event type MODIFIED
    Feb 12 11:50:54.912: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:50:54.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7594" for this suite. 02/12/23 11:50:54.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:50:54.944
Feb 12 11:50:54.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename security-context-test 02/12/23 11:50:54.945
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:54.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:54.963
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Feb 12 11:50:54.977: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de" in namespace "security-context-test-3814" to be "Succeeded or Failed"
Feb 12 11:50:54.987: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.226001ms
Feb 12 11:50:57.992: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014017684s
Feb 12 11:50:58.997: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019027924s
Feb 12 11:51:01.018: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040162224s
Feb 12 11:51:02.992: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014854985s
Feb 12 11:51:02.992: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Feb 12 11:51:02.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3814" for this suite. 02/12/23 11:51:03.001
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":169,"skipped":3145,"failed":0}
------------------------------
 [SLOW TEST] [8.066 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:50:54.944
    Feb 12 11:50:54.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename security-context-test 02/12/23 11:50:54.945
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:50:54.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:50:54.963
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Feb 12 11:50:54.977: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de" in namespace "security-context-test-3814" to be "Succeeded or Failed"
    Feb 12 11:50:54.987: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.226001ms
    Feb 12 11:50:57.992: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014017684s
    Feb 12 11:50:58.997: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019027924s
    Feb 12 11:51:01.018: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040162224s
    Feb 12 11:51:02.992: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014854985s
    Feb 12 11:51:02.992: INFO: Pod "alpine-nnp-false-e90b24be-93bc-40ba-99ef-6f77012453de" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Feb 12 11:51:02.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3814" for this suite. 02/12/23 11:51:03.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:03.011
Feb 12 11:51:03.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:51:03.012
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:03.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:03.031
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 02/12/23 11:51:03.033
Feb 12 11:51:03.043: INFO: Waiting up to 5m0s for pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046" in namespace "downward-api-7902" to be "Succeeded or Failed"
Feb 12 11:51:03.047: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341408ms
Feb 12 11:51:05.060: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016377155s
Feb 12 11:51:07.059: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015279148s
STEP: Saw pod success 02/12/23 11:51:07.059
Feb 12 11:51:07.059: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046" satisfied condition "Succeeded or Failed"
Feb 12 11:51:07.072: INFO: Trying to get logs from node kube-3 pod downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046 container dapi-container: <nil>
STEP: delete the pod 02/12/23 11:51:07.098
Feb 12 11:51:07.126: INFO: Waiting for pod downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046 to disappear
Feb 12 11:51:07.129: INFO: Pod downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 12 11:51:07.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7902" for this suite. 02/12/23 11:51:07.134
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":170,"skipped":3153,"failed":0}
------------------------------
 [4.130 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:03.011
    Feb 12 11:51:03.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:51:03.012
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:03.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:03.031
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 02/12/23 11:51:03.033
    Feb 12 11:51:03.043: INFO: Waiting up to 5m0s for pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046" in namespace "downward-api-7902" to be "Succeeded or Failed"
    Feb 12 11:51:03.047: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341408ms
    Feb 12 11:51:05.060: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016377155s
    Feb 12 11:51:07.059: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015279148s
    STEP: Saw pod success 02/12/23 11:51:07.059
    Feb 12 11:51:07.059: INFO: Pod "downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046" satisfied condition "Succeeded or Failed"
    Feb 12 11:51:07.072: INFO: Trying to get logs from node kube-3 pod downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 11:51:07.098
    Feb 12 11:51:07.126: INFO: Waiting for pod downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046 to disappear
    Feb 12 11:51:07.129: INFO: Pod downward-api-556d05ee-8bff-424a-94ff-fbf9aae51046 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 12 11:51:07.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7902" for this suite. 02/12/23 11:51:07.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:07.142
Feb 12 11:51:07.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:51:07.142
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:07.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:07.16
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 12 11:51:07.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2586" for this suite. 02/12/23 11:51:07.173
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":171,"skipped":3166,"failed":0}
------------------------------
 [0.039 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:07.142
    Feb 12 11:51:07.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename runtimeclass 02/12/23 11:51:07.142
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:07.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:07.16
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 12 11:51:07.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2586" for this suite. 02/12/23 11:51:07.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:07.181
Feb 12 11:51:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename events 02/12/23 11:51:07.181
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:07.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:07.198
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/12/23 11:51:07.199
STEP: listing all events in all namespaces 02/12/23 11:51:07.204
STEP: patching the test event 02/12/23 11:51:07.213
STEP: fetching the test event 02/12/23 11:51:07.22
STEP: updating the test event 02/12/23 11:51:07.222
STEP: getting the test event 02/12/23 11:51:07.231
STEP: deleting the test event 02/12/23 11:51:07.233
STEP: listing all events in all namespaces 02/12/23 11:51:07.241
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Feb 12 11:51:07.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9768" for this suite. 02/12/23 11:51:07.254
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":172,"skipped":3173,"failed":0}
------------------------------
 [0.080 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:07.181
    Feb 12 11:51:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename events 02/12/23 11:51:07.181
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:07.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:07.198
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/12/23 11:51:07.199
    STEP: listing all events in all namespaces 02/12/23 11:51:07.204
    STEP: patching the test event 02/12/23 11:51:07.213
    STEP: fetching the test event 02/12/23 11:51:07.22
    STEP: updating the test event 02/12/23 11:51:07.222
    STEP: getting the test event 02/12/23 11:51:07.231
    STEP: deleting the test event 02/12/23 11:51:07.233
    STEP: listing all events in all namespaces 02/12/23 11:51:07.241
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Feb 12 11:51:07.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9768" for this suite. 02/12/23 11:51:07.254
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:07.261
Feb 12 11:51:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:51:07.262
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:07.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:07.283
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 02/12/23 11:51:07.286
STEP: Creating a ResourceQuota 02/12/23 11:51:12.289
STEP: Ensuring resource quota status is calculated 02/12/23 11:51:12.296
STEP: Creating a ReplicaSet 02/12/23 11:51:14.31
STEP: Ensuring resource quota status captures replicaset creation 02/12/23 11:51:14.345
STEP: Deleting a ReplicaSet 02/12/23 11:51:16.348
STEP: Ensuring resource quota status released usage 02/12/23 11:51:16.354
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:51:18.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3245" for this suite. 02/12/23 11:51:18.374
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":173,"skipped":3177,"failed":0}
------------------------------
 [SLOW TEST] [11.123 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:07.261
    Feb 12 11:51:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:51:07.262
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:07.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:07.283
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 02/12/23 11:51:07.286
    STEP: Creating a ResourceQuota 02/12/23 11:51:12.289
    STEP: Ensuring resource quota status is calculated 02/12/23 11:51:12.296
    STEP: Creating a ReplicaSet 02/12/23 11:51:14.31
    STEP: Ensuring resource quota status captures replicaset creation 02/12/23 11:51:14.345
    STEP: Deleting a ReplicaSet 02/12/23 11:51:16.348
    STEP: Ensuring resource quota status released usage 02/12/23 11:51:16.354
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:51:18.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3245" for this suite. 02/12/23 11:51:18.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:18.385
Feb 12 11:51:18.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 11:51:18.386
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:18.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:18.407
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 02/12/23 11:51:18.409
STEP: Creating a ResourceQuota 02/12/23 11:51:23.42
STEP: Ensuring resource quota status is calculated 02/12/23 11:51:23.431
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 11:51:25.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1335" for this suite. 02/12/23 11:51:25.439
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":174,"skipped":3202,"failed":0}
------------------------------
 [SLOW TEST] [7.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:18.385
    Feb 12 11:51:18.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 11:51:18.386
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:18.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:18.407
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 02/12/23 11:51:18.409
    STEP: Creating a ResourceQuota 02/12/23 11:51:23.42
    STEP: Ensuring resource quota status is calculated 02/12/23 11:51:23.431
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 11:51:25.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1335" for this suite. 02/12/23 11:51:25.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:25.45
Feb 12 11:51:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:51:25.451
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:25.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:25.473
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-1955 02/12/23 11:51:25.475
STEP: creating service affinity-clusterip-transition in namespace services-1955 02/12/23 11:51:25.475
STEP: creating replication controller affinity-clusterip-transition in namespace services-1955 02/12/23 11:51:25.489
I0212 11:51:25.504283      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1955, replica count: 3
I0212 11:51:28.555509      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 11:51:28.580: INFO: Creating new exec pod
Feb 12 11:51:28.591: INFO: Waiting up to 5m0s for pod "execpod-affinityjjrvj" in namespace "services-1955" to be "running"
Feb 12 11:51:28.597: INFO: Pod "execpod-affinityjjrvj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459227ms
Feb 12 11:51:30.601: INFO: Pod "execpod-affinityjjrvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009180023s
Feb 12 11:51:30.601: INFO: Pod "execpod-affinityjjrvj" satisfied condition "running"
Feb 12 11:51:31.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Feb 12 11:51:31.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 12 11:51:31.930: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:51:31.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.100 80'
Feb 12 11:51:32.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.100 80\nConnection to 10.233.43.100 80 port [tcp/http] succeeded!\n"
Feb 12 11:51:32.092: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:51:32.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.100:80/ ; done'
Feb 12 11:51:32.294: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n"
Feb 12 11:51:32.294: INFO: stdout: "\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5"
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
Feb 12 11:51:32.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.100:80/ ; done'
Feb 12 11:51:32.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n"
Feb 12 11:51:32.505: INFO: stdout: "\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt"
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
Feb 12 11:51:32.505: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1955, will wait for the garbage collector to delete the pods 02/12/23 11:51:32.523
Feb 12 11:51:32.595: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.705993ms
Feb 12 11:51:32.695: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.415418ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:51:40.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1955" for this suite. 02/12/23 11:51:40.04
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":175,"skipped":3214,"failed":0}
------------------------------
 [SLOW TEST] [14.598 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:25.45
    Feb 12 11:51:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:51:25.451
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:25.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:25.473
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-1955 02/12/23 11:51:25.475
    STEP: creating service affinity-clusterip-transition in namespace services-1955 02/12/23 11:51:25.475
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1955 02/12/23 11:51:25.489
    I0212 11:51:25.504283      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1955, replica count: 3
    I0212 11:51:28.555509      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 11:51:28.580: INFO: Creating new exec pod
    Feb 12 11:51:28.591: INFO: Waiting up to 5m0s for pod "execpod-affinityjjrvj" in namespace "services-1955" to be "running"
    Feb 12 11:51:28.597: INFO: Pod "execpod-affinityjjrvj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459227ms
    Feb 12 11:51:30.601: INFO: Pod "execpod-affinityjjrvj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009180023s
    Feb 12 11:51:30.601: INFO: Pod "execpod-affinityjjrvj" satisfied condition "running"
    Feb 12 11:51:31.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Feb 12 11:51:31.930: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 12 11:51:31.930: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:51:31.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.100 80'
    Feb 12 11:51:32.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.100 80\nConnection to 10.233.43.100 80 port [tcp/http] succeeded!\n"
    Feb 12 11:51:32.092: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:51:32.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.100:80/ ; done'
    Feb 12 11:51:32.294: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n"
    Feb 12 11:51:32.294: INFO: stdout: "\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5\naffinity-clusterip-transition-hdn8f\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-vcsj5"
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-hdn8f
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.294: INFO: Received response from host: affinity-clusterip-transition-vcsj5
    Feb 12 11:51:32.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-1955 exec execpod-affinityjjrvj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.43.100:80/ ; done'
    Feb 12 11:51:32.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.43.100:80/\n"
    Feb 12 11:51:32.505: INFO: stdout: "\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt\naffinity-clusterip-transition-52spt"
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Received response from host: affinity-clusterip-transition-52spt
    Feb 12 11:51:32.505: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1955, will wait for the garbage collector to delete the pods 02/12/23 11:51:32.523
    Feb 12 11:51:32.595: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.705993ms
    Feb 12 11:51:32.695: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.415418ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:51:40.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1955" for this suite. 02/12/23 11:51:40.04
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:40.048
Feb 12 11:51:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename disruption 02/12/23 11:51:40.049
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:40.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:40.085
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 02/12/23 11:51:40.089
STEP: Waiting for the pdb to be processed 02/12/23 11:51:40.098
STEP: updating the pdb 02/12/23 11:51:42.108
STEP: Waiting for the pdb to be processed 02/12/23 11:51:42.126
STEP: patching the pdb 02/12/23 11:51:44.146
STEP: Waiting for the pdb to be processed 02/12/23 11:51:44.172
STEP: Waiting for the pdb to be deleted 02/12/23 11:51:46.193
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 12 11:51:46.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8494" for this suite. 02/12/23 11:51:46.201
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":176,"skipped":3222,"failed":0}
------------------------------
 [SLOW TEST] [6.162 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:40.048
    Feb 12 11:51:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename disruption 02/12/23 11:51:40.049
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:40.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:40.085
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 02/12/23 11:51:40.089
    STEP: Waiting for the pdb to be processed 02/12/23 11:51:40.098
    STEP: updating the pdb 02/12/23 11:51:42.108
    STEP: Waiting for the pdb to be processed 02/12/23 11:51:42.126
    STEP: patching the pdb 02/12/23 11:51:44.146
    STEP: Waiting for the pdb to be processed 02/12/23 11:51:44.172
    STEP: Waiting for the pdb to be deleted 02/12/23 11:51:46.193
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 12 11:51:46.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8494" for this suite. 02/12/23 11:51:46.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:46.212
Feb 12 11:51:46.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:51:46.213
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:46.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:46.237
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-b92a8292-8dda-4b7e-b753-7458fabc2880 02/12/23 11:51:46.239
STEP: Creating a pod to test consume configMaps 02/12/23 11:51:46.249
Feb 12 11:51:46.258: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922" in namespace "configmap-1131" to be "Succeeded or Failed"
Feb 12 11:51:46.263: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812334ms
Feb 12 11:51:48.267: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009012944s
Feb 12 11:51:50.272: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014307544s
STEP: Saw pod success 02/12/23 11:51:50.272
Feb 12 11:51:50.273: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922" satisfied condition "Succeeded or Failed"
Feb 12 11:51:50.279: INFO: Trying to get logs from node kube-3 pod pod-configmaps-ce943065-6663-4311-9ae4-d93973505922 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:51:50.29
Feb 12 11:51:50.308: INFO: Waiting for pod pod-configmaps-ce943065-6663-4311-9ae4-d93973505922 to disappear
Feb 12 11:51:50.315: INFO: Pod pod-configmaps-ce943065-6663-4311-9ae4-d93973505922 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:51:50.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1131" for this suite. 02/12/23 11:51:50.318
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":177,"skipped":3227,"failed":0}
------------------------------
 [4.112 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:46.212
    Feb 12 11:51:46.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:51:46.213
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:46.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:46.237
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-b92a8292-8dda-4b7e-b753-7458fabc2880 02/12/23 11:51:46.239
    STEP: Creating a pod to test consume configMaps 02/12/23 11:51:46.249
    Feb 12 11:51:46.258: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922" in namespace "configmap-1131" to be "Succeeded or Failed"
    Feb 12 11:51:46.263: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922": Phase="Pending", Reason="", readiness=false. Elapsed: 4.812334ms
    Feb 12 11:51:48.267: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009012944s
    Feb 12 11:51:50.272: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014307544s
    STEP: Saw pod success 02/12/23 11:51:50.272
    Feb 12 11:51:50.273: INFO: Pod "pod-configmaps-ce943065-6663-4311-9ae4-d93973505922" satisfied condition "Succeeded or Failed"
    Feb 12 11:51:50.279: INFO: Trying to get logs from node kube-3 pod pod-configmaps-ce943065-6663-4311-9ae4-d93973505922 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:51:50.29
    Feb 12 11:51:50.308: INFO: Waiting for pod pod-configmaps-ce943065-6663-4311-9ae4-d93973505922 to disappear
    Feb 12 11:51:50.315: INFO: Pod pod-configmaps-ce943065-6663-4311-9ae4-d93973505922 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:51:50.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1131" for this suite. 02/12/23 11:51:50.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:50.325
Feb 12 11:51:50.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename lease-test 02/12/23 11:51:50.326
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:50.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:50.346
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Feb 12 11:51:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2751" for this suite. 02/12/23 11:51:50.403
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":178,"skipped":3262,"failed":0}
------------------------------
 [0.084 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:50.325
    Feb 12 11:51:50.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename lease-test 02/12/23 11:51:50.326
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:50.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:50.346
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Feb 12 11:51:50.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-2751" for this suite. 02/12/23 11:51:50.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:50.409
Feb 12 11:51:50.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:51:50.41
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:50.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:50.427
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/12/23 11:51:50.429
Feb 12 11:51:50.439: INFO: Waiting up to 5m0s for pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d" in namespace "emptydir-730" to be "Succeeded or Failed"
Feb 12 11:51:50.444: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.50659ms
Feb 12 11:51:52.459: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019017843s
Feb 12 11:51:54.458: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018459747s
STEP: Saw pod success 02/12/23 11:51:54.458
Feb 12 11:51:54.459: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d" satisfied condition "Succeeded or Failed"
Feb 12 11:51:54.470: INFO: Trying to get logs from node kube-3 pod pod-09472196-1beb-475f-81d3-5ad9cf37734d container test-container: <nil>
STEP: delete the pod 02/12/23 11:51:54.494
Feb 12 11:51:54.520: INFO: Waiting for pod pod-09472196-1beb-475f-81d3-5ad9cf37734d to disappear
Feb 12 11:51:54.524: INFO: Pod pod-09472196-1beb-475f-81d3-5ad9cf37734d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:51:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-730" for this suite. 02/12/23 11:51:54.528
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":179,"skipped":3270,"failed":0}
------------------------------
 [4.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:50.409
    Feb 12 11:51:50.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:51:50.41
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:50.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:50.427
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/12/23 11:51:50.429
    Feb 12 11:51:50.439: INFO: Waiting up to 5m0s for pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d" in namespace "emptydir-730" to be "Succeeded or Failed"
    Feb 12 11:51:50.444: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.50659ms
    Feb 12 11:51:52.459: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019017843s
    Feb 12 11:51:54.458: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018459747s
    STEP: Saw pod success 02/12/23 11:51:54.458
    Feb 12 11:51:54.459: INFO: Pod "pod-09472196-1beb-475f-81d3-5ad9cf37734d" satisfied condition "Succeeded or Failed"
    Feb 12 11:51:54.470: INFO: Trying to get logs from node kube-3 pod pod-09472196-1beb-475f-81d3-5ad9cf37734d container test-container: <nil>
    STEP: delete the pod 02/12/23 11:51:54.494
    Feb 12 11:51:54.520: INFO: Waiting for pod pod-09472196-1beb-475f-81d3-5ad9cf37734d to disappear
    Feb 12 11:51:54.524: INFO: Pod pod-09472196-1beb-475f-81d3-5ad9cf37734d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:51:54.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-730" for this suite. 02/12/23 11:51:54.528
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:51:54.541
Feb 12 11:51:54.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename watch 02/12/23 11:51:54.542
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:54.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:54.566
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/12/23 11:51:54.569
STEP: creating a new configmap 02/12/23 11:51:54.57
STEP: modifying the configmap once 02/12/23 11:51:54.575
STEP: changing the label value of the configmap 02/12/23 11:51:54.584
STEP: Expecting to observe a delete notification for the watched object 02/12/23 11:51:54.597
Feb 12 11:51:54.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19523 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:51:54.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19524 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:51:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:51:54.598: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19525 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:51:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/12/23 11:51:54.598
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/12/23 11:51:54.606
STEP: changing the label value of the configmap back 02/12/23 11:52:04.607
STEP: modifying the configmap a third time 02/12/23 11:52:04.624
STEP: deleting the configmap 02/12/23 11:52:04.631
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/12/23 11:52:04.644
Feb 12 11:52:04.644: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19568 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:52:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:52:04.644: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19569 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:52:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 11:52:04.644: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19570 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:52:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 12 11:52:04.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5621" for this suite. 02/12/23 11:52:04.647
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":180,"skipped":3274,"failed":0}
------------------------------
 [SLOW TEST] [10.113 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:51:54.541
    Feb 12 11:51:54.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename watch 02/12/23 11:51:54.542
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:51:54.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:51:54.566
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/12/23 11:51:54.569
    STEP: creating a new configmap 02/12/23 11:51:54.57
    STEP: modifying the configmap once 02/12/23 11:51:54.575
    STEP: changing the label value of the configmap 02/12/23 11:51:54.584
    STEP: Expecting to observe a delete notification for the watched object 02/12/23 11:51:54.597
    Feb 12 11:51:54.597: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19523 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:51:54.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19524 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:51:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:51:54.598: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19525 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:51:54 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/12/23 11:51:54.598
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/12/23 11:51:54.606
    STEP: changing the label value of the configmap back 02/12/23 11:52:04.607
    STEP: modifying the configmap a third time 02/12/23 11:52:04.624
    STEP: deleting the configmap 02/12/23 11:52:04.631
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/12/23 11:52:04.644
    Feb 12 11:52:04.644: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19568 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:52:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:52:04.644: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19569 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:52:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 11:52:04.644: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5621  c8f920e9-8462-455b-8552-9ec296bce3b3 19570 0 2023-02-12 11:51:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-12 11:52:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 12 11:52:04.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5621" for this suite. 02/12/23 11:52:04.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:04.658
Feb 12 11:52:04.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 11:52:04.658
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:04.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:04.679
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 02/12/23 11:52:04.681
Feb 12 11:52:04.690: INFO: created test-pod-1
Feb 12 11:52:04.701: INFO: created test-pod-2
Feb 12 11:52:04.714: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/12/23 11:52:04.714
Feb 12 11:52:04.714: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3904' to be running and ready
Feb 12 11:52:04.737: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:52:04.737: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:52:04.737: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 12 11:52:04.738: INFO: 0 / 3 pods in namespace 'pods-3904' are running and ready (0 seconds elapsed)
Feb 12 11:52:04.738: INFO: expected 0 pod replicas in namespace 'pods-3904', 0 are Running and Ready.
Feb 12 11:52:04.738: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
Feb 12 11:52:04.738: INFO: test-pod-1  kube-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  }]
Feb 12 11:52:04.738: INFO: test-pod-2  kube-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  }]
Feb 12 11:52:04.738: INFO: test-pod-3  kube-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  }]
Feb 12 11:52:04.738: INFO: 
Feb 12 11:52:06.764: INFO: 3 / 3 pods in namespace 'pods-3904' are running and ready (2 seconds elapsed)
Feb 12 11:52:06.765: INFO: expected 0 pod replicas in namespace 'pods-3904', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/12/23 11:52:06.807
Feb 12 11:52:06.810: INFO: Pod quantity 3 is different from expected quantity 0
Feb 12 11:52:07.822: INFO: Pod quantity 3 is different from expected quantity 0
Feb 12 11:52:08.815: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 11:52:09.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3904" for this suite. 02/12/23 11:52:09.818
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":181,"skipped":3285,"failed":0}
------------------------------
 [SLOW TEST] [5.171 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:04.658
    Feb 12 11:52:04.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 11:52:04.658
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:04.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:04.679
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 02/12/23 11:52:04.681
    Feb 12 11:52:04.690: INFO: created test-pod-1
    Feb 12 11:52:04.701: INFO: created test-pod-2
    Feb 12 11:52:04.714: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/12/23 11:52:04.714
    Feb 12 11:52:04.714: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3904' to be running and ready
    Feb 12 11:52:04.737: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:52:04.737: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:52:04.737: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 12 11:52:04.738: INFO: 0 / 3 pods in namespace 'pods-3904' are running and ready (0 seconds elapsed)
    Feb 12 11:52:04.738: INFO: expected 0 pod replicas in namespace 'pods-3904', 0 are Running and Ready.
    Feb 12 11:52:04.738: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    Feb 12 11:52:04.738: INFO: test-pod-1  kube-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  }]
    Feb 12 11:52:04.738: INFO: test-pod-2  kube-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  }]
    Feb 12 11:52:04.738: INFO: test-pod-3  kube-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 11:52:04 +0000 UTC  }]
    Feb 12 11:52:04.738: INFO: 
    Feb 12 11:52:06.764: INFO: 3 / 3 pods in namespace 'pods-3904' are running and ready (2 seconds elapsed)
    Feb 12 11:52:06.765: INFO: expected 0 pod replicas in namespace 'pods-3904', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/12/23 11:52:06.807
    Feb 12 11:52:06.810: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 12 11:52:07.822: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 12 11:52:08.815: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 11:52:09.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3904" for this suite. 02/12/23 11:52:09.818
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:09.828
Feb 12 11:52:09.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename containers 02/12/23 11:52:09.829
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:09.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:09.848
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 02/12/23 11:52:09.852
Feb 12 11:52:09.867: INFO: Waiting up to 5m0s for pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e" in namespace "containers-5550" to be "Succeeded or Failed"
Feb 12 11:52:09.883: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.050106ms
Feb 12 11:52:11.888: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021387145s
Feb 12 11:52:13.894: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027102744s
STEP: Saw pod success 02/12/23 11:52:13.894
Feb 12 11:52:13.894: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e" satisfied condition "Succeeded or Failed"
Feb 12 11:52:13.897: INFO: Trying to get logs from node kube-3 pod client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:52:13.903
Feb 12 11:52:13.920: INFO: Waiting for pod client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e to disappear
Feb 12 11:52:13.923: INFO: Pod client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 12 11:52:13.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5550" for this suite. 02/12/23 11:52:13.926
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":182,"skipped":3287,"failed":0}
------------------------------
 [4.103 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:09.828
    Feb 12 11:52:09.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename containers 02/12/23 11:52:09.829
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:09.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:09.848
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 02/12/23 11:52:09.852
    Feb 12 11:52:09.867: INFO: Waiting up to 5m0s for pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e" in namespace "containers-5550" to be "Succeeded or Failed"
    Feb 12 11:52:09.883: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.050106ms
    Feb 12 11:52:11.888: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021387145s
    Feb 12 11:52:13.894: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027102744s
    STEP: Saw pod success 02/12/23 11:52:13.894
    Feb 12 11:52:13.894: INFO: Pod "client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e" satisfied condition "Succeeded or Failed"
    Feb 12 11:52:13.897: INFO: Trying to get logs from node kube-3 pod client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:52:13.903
    Feb 12 11:52:13.920: INFO: Waiting for pod client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e to disappear
    Feb 12 11:52:13.923: INFO: Pod client-containers-4fef57aa-3ede-4565-b646-ec99c1706c4e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 12 11:52:13.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5550" for this suite. 02/12/23 11:52:13.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:13.939
Feb 12 11:52:13.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:52:13.939
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:13.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:13.964
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-0892e0df-25c3-4ad3-a0a2-4f565e47059e 02/12/23 11:52:13.966
STEP: Creating a pod to test consume configMaps 02/12/23 11:52:13.971
Feb 12 11:52:13.980: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816" in namespace "configmap-6213" to be "Succeeded or Failed"
Feb 12 11:52:13.988: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Pending", Reason="", readiness=false. Elapsed: 7.397915ms
Feb 12 11:52:17.073: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Running", Reason="", readiness=true. Elapsed: 3.092319029s
Feb 12 11:52:18.058: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Running", Reason="", readiness=false. Elapsed: 4.077636774s
Feb 12 11:52:20.004: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023477494s
STEP: Saw pod success 02/12/23 11:52:20.005
Feb 12 11:52:20.005: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816" satisfied condition "Succeeded or Failed"
Feb 12 11:52:20.020: INFO: Trying to get logs from node kube-3 pod pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 11:52:20.038
Feb 12 11:52:20.064: INFO: Waiting for pod pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816 to disappear
Feb 12 11:52:20.069: INFO: Pod pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:52:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6213" for this suite. 02/12/23 11:52:20.074
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":183,"skipped":3333,"failed":0}
------------------------------
 [SLOW TEST] [6.142 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:13.939
    Feb 12 11:52:13.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:52:13.939
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:13.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:13.964
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-0892e0df-25c3-4ad3-a0a2-4f565e47059e 02/12/23 11:52:13.966
    STEP: Creating a pod to test consume configMaps 02/12/23 11:52:13.971
    Feb 12 11:52:13.980: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816" in namespace "configmap-6213" to be "Succeeded or Failed"
    Feb 12 11:52:13.988: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Pending", Reason="", readiness=false. Elapsed: 7.397915ms
    Feb 12 11:52:17.073: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Running", Reason="", readiness=true. Elapsed: 3.092319029s
    Feb 12 11:52:18.058: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Running", Reason="", readiness=false. Elapsed: 4.077636774s
    Feb 12 11:52:20.004: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023477494s
    STEP: Saw pod success 02/12/23 11:52:20.005
    Feb 12 11:52:20.005: INFO: Pod "pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816" satisfied condition "Succeeded or Failed"
    Feb 12 11:52:20.020: INFO: Trying to get logs from node kube-3 pod pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 11:52:20.038
    Feb 12 11:52:20.064: INFO: Waiting for pod pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816 to disappear
    Feb 12 11:52:20.069: INFO: Pod pod-configmaps-0e74e402-0f77-4060-878a-5279dbb71816 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:52:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6213" for this suite. 02/12/23 11:52:20.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:20.085
Feb 12 11:52:20.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename init-container 02/12/23 11:52:20.085
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:20.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:20.102
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 02/12/23 11:52:20.105
Feb 12 11:52:20.105: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 11:52:25.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4282" for this suite. 02/12/23 11:52:25.126
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":184,"skipped":3364,"failed":0}
------------------------------
 [SLOW TEST] [5.051 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:20.085
    Feb 12 11:52:20.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename init-container 02/12/23 11:52:20.085
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:20.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:20.102
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 02/12/23 11:52:20.105
    Feb 12 11:52:20.105: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 11:52:25.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4282" for this suite. 02/12/23 11:52:25.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:25.137
Feb 12 11:52:25.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:52:25.138
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:25.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:25.158
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 02/12/23 11:52:25.165
STEP: watching for the Service to be added 02/12/23 11:52:25.18
Feb 12 11:52:25.183: INFO: Found Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 12 11:52:25.183: INFO: Service test-service-z85xq created
STEP: Getting /status 02/12/23 11:52:25.183
Feb 12 11:52:25.190: INFO: Service test-service-z85xq has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/12/23 11:52:25.19
STEP: watching for the Service to be patched 02/12/23 11:52:25.201
Feb 12 11:52:25.209: INFO: observed Service test-service-z85xq in namespace services-6831 with annotations: map[] & LoadBalancer: {[]}
Feb 12 11:52:25.209: INFO: Found Service test-service-z85xq in namespace services-6831 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 12 11:52:25.209: INFO: Service test-service-z85xq has service status patched
STEP: updating the ServiceStatus 02/12/23 11:52:25.209
Feb 12 11:52:25.229: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/12/23 11:52:25.229
Feb 12 11:52:25.233: INFO: Observed Service test-service-z85xq in namespace services-6831 with annotations: map[] & Conditions: {[]}
Feb 12 11:52:25.233: INFO: Observed event: &Service{ObjectMeta:{test-service-z85xq  services-6831  56a32040-e44d-479f-822f-b73b25a60cf1 19791 0 2023-02-12 11:52:25 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-12 11:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-12 11:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.39.185,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.39.185],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 12 11:52:25.233: INFO: Found Service test-service-z85xq in namespace services-6831 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 12 11:52:25.233: INFO: Service test-service-z85xq has service status updated
STEP: patching the service 02/12/23 11:52:25.233
STEP: watching for the Service to be patched 02/12/23 11:52:25.247
Feb 12 11:52:25.251: INFO: observed Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true]
Feb 12 11:52:25.251: INFO: observed Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true]
Feb 12 11:52:25.251: INFO: observed Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true]
Feb 12 11:52:25.251: INFO: Found Service test-service-z85xq in namespace services-6831 with labels: map[test-service:patched test-service-static:true]
Feb 12 11:52:25.252: INFO: Service test-service-z85xq patched
STEP: deleting the service 02/12/23 11:52:25.252
STEP: watching for the Service to be deleted 02/12/23 11:52:25.295
Feb 12 11:52:25.301: INFO: Observed event: ADDED
Feb 12 11:52:25.301: INFO: Observed event: MODIFIED
Feb 12 11:52:25.302: INFO: Observed event: MODIFIED
Feb 12 11:52:25.302: INFO: Observed event: MODIFIED
Feb 12 11:52:25.302: INFO: Found Service test-service-z85xq in namespace services-6831 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 12 11:52:25.302: INFO: Service test-service-z85xq deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:52:25.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6831" for this suite. 02/12/23 11:52:25.312
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":185,"skipped":3381,"failed":0}
------------------------------
 [0.189 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:25.137
    Feb 12 11:52:25.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:52:25.138
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:25.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:25.158
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 02/12/23 11:52:25.165
    STEP: watching for the Service to be added 02/12/23 11:52:25.18
    Feb 12 11:52:25.183: INFO: Found Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 12 11:52:25.183: INFO: Service test-service-z85xq created
    STEP: Getting /status 02/12/23 11:52:25.183
    Feb 12 11:52:25.190: INFO: Service test-service-z85xq has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/12/23 11:52:25.19
    STEP: watching for the Service to be patched 02/12/23 11:52:25.201
    Feb 12 11:52:25.209: INFO: observed Service test-service-z85xq in namespace services-6831 with annotations: map[] & LoadBalancer: {[]}
    Feb 12 11:52:25.209: INFO: Found Service test-service-z85xq in namespace services-6831 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 12 11:52:25.209: INFO: Service test-service-z85xq has service status patched
    STEP: updating the ServiceStatus 02/12/23 11:52:25.209
    Feb 12 11:52:25.229: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/12/23 11:52:25.229
    Feb 12 11:52:25.233: INFO: Observed Service test-service-z85xq in namespace services-6831 with annotations: map[] & Conditions: {[]}
    Feb 12 11:52:25.233: INFO: Observed event: &Service{ObjectMeta:{test-service-z85xq  services-6831  56a32040-e44d-479f-822f-b73b25a60cf1 19791 0 2023-02-12 11:52:25 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-12 11:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-12 11:52:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.39.185,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.39.185],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 12 11:52:25.233: INFO: Found Service test-service-z85xq in namespace services-6831 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 12 11:52:25.233: INFO: Service test-service-z85xq has service status updated
    STEP: patching the service 02/12/23 11:52:25.233
    STEP: watching for the Service to be patched 02/12/23 11:52:25.247
    Feb 12 11:52:25.251: INFO: observed Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true]
    Feb 12 11:52:25.251: INFO: observed Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true]
    Feb 12 11:52:25.251: INFO: observed Service test-service-z85xq in namespace services-6831 with labels: map[test-service-static:true]
    Feb 12 11:52:25.251: INFO: Found Service test-service-z85xq in namespace services-6831 with labels: map[test-service:patched test-service-static:true]
    Feb 12 11:52:25.252: INFO: Service test-service-z85xq patched
    STEP: deleting the service 02/12/23 11:52:25.252
    STEP: watching for the Service to be deleted 02/12/23 11:52:25.295
    Feb 12 11:52:25.301: INFO: Observed event: ADDED
    Feb 12 11:52:25.301: INFO: Observed event: MODIFIED
    Feb 12 11:52:25.302: INFO: Observed event: MODIFIED
    Feb 12 11:52:25.302: INFO: Observed event: MODIFIED
    Feb 12 11:52:25.302: INFO: Found Service test-service-z85xq in namespace services-6831 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 12 11:52:25.302: INFO: Service test-service-z85xq deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:52:25.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6831" for this suite. 02/12/23 11:52:25.312
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:25.326
Feb 12 11:52:25.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:52:25.328
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:25.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:25.369
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:52:25.396
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:52:25.766
STEP: Deploying the webhook pod 02/12/23 11:52:25.775
STEP: Wait for the deployment to be ready 02/12/23 11:52:25.799
Feb 12 11:52:25.810: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/12/23 11:52:27.851
STEP: Verifying the service has paired with the endpoint 02/12/23 11:52:27.872
Feb 12 11:52:28.873: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/12/23 11:52:28.892
STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:28.893
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/12/23 11:52:28.924
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/12/23 11:52:29.94
STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:29.941
STEP: Having no error when timeout is longer than webhook latency 02/12/23 11:52:30.977
STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:30.977
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/12/23 11:52:36.025
STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:36.025
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:52:41.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4976" for this suite. 02/12/23 11:52:41.066
STEP: Destroying namespace "webhook-4976-markers" for this suite. 02/12/23 11:52:41.074
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":186,"skipped":3383,"failed":0}
------------------------------
 [SLOW TEST] [15.823 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:25.326
    Feb 12 11:52:25.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:52:25.328
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:25.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:25.369
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:52:25.396
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:52:25.766
    STEP: Deploying the webhook pod 02/12/23 11:52:25.775
    STEP: Wait for the deployment to be ready 02/12/23 11:52:25.799
    Feb 12 11:52:25.810: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/12/23 11:52:27.851
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:52:27.872
    Feb 12 11:52:28.873: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/12/23 11:52:28.892
    STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:28.893
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/12/23 11:52:28.924
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/12/23 11:52:29.94
    STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:29.941
    STEP: Having no error when timeout is longer than webhook latency 02/12/23 11:52:30.977
    STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:30.977
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/12/23 11:52:36.025
    STEP: Registering slow webhook via the AdmissionRegistration API 02/12/23 11:52:36.025
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:52:41.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4976" for this suite. 02/12/23 11:52:41.066
    STEP: Destroying namespace "webhook-4976-markers" for this suite. 02/12/23 11:52:41.074
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:41.15
Feb 12 11:52:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-webhook 02/12/23 11:52:41.151
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:41.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:41.204
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/12/23 11:52:41.213
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/12/23 11:52:41.844
STEP: Deploying the custom resource conversion webhook pod 02/12/23 11:52:41.851
STEP: Wait for the deployment to be ready 02/12/23 11:52:41.866
Feb 12 11:52:41.884: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 11:52:43.915
STEP: Verifying the service has paired with the endpoint 02/12/23 11:52:43.939
Feb 12 11:52:44.940: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 12 11:52:44.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Creating a v1 custom resource 02/12/23 11:52:52.549
STEP: v2 custom resource should be converted 02/12/23 11:52:52.559
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:52:53.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4271" for this suite. 02/12/23 11:52:53.089
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":187,"skipped":3385,"failed":0}
------------------------------
 [SLOW TEST] [12.035 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:41.15
    Feb 12 11:52:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-webhook 02/12/23 11:52:41.151
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:41.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:41.204
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/12/23 11:52:41.213
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/12/23 11:52:41.844
    STEP: Deploying the custom resource conversion webhook pod 02/12/23 11:52:41.851
    STEP: Wait for the deployment to be ready 02/12/23 11:52:41.866
    Feb 12 11:52:41.884: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 11:52:43.915
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:52:43.939
    Feb 12 11:52:44.940: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 12 11:52:44.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Creating a v1 custom resource 02/12/23 11:52:52.549
    STEP: v2 custom resource should be converted 02/12/23 11:52:52.559
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:52:53.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-4271" for this suite. 02/12/23 11:52:53.089
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:53.185
Feb 12 11:52:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:52:53.186
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:53.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:53.23
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-fc9aa5df-d0f0-4b4e-a8ca-b9c828cece26 02/12/23 11:52:53.235
STEP: Creating a pod to test consume secrets 02/12/23 11:52:53.243
Feb 12 11:52:53.258: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd" in namespace "projected-4420" to be "Succeeded or Failed"
Feb 12 11:52:53.285: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.775658ms
Feb 12 11:52:55.297: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039026476s
Feb 12 11:52:57.301: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042352177s
STEP: Saw pod success 02/12/23 11:52:57.301
Feb 12 11:52:57.302: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd" satisfied condition "Succeeded or Failed"
Feb 12 11:52:57.313: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd container projected-secret-volume-test: <nil>
STEP: delete the pod 02/12/23 11:52:57.324
Feb 12 11:52:57.342: INFO: Waiting for pod pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd to disappear
Feb 12 11:52:57.345: INFO: Pod pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 11:52:57.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4420" for this suite. 02/12/23 11:52:57.351
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":188,"skipped":3395,"failed":0}
------------------------------
 [4.173 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:53.185
    Feb 12 11:52:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:52:53.186
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:53.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:53.23
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-fc9aa5df-d0f0-4b4e-a8ca-b9c828cece26 02/12/23 11:52:53.235
    STEP: Creating a pod to test consume secrets 02/12/23 11:52:53.243
    Feb 12 11:52:53.258: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd" in namespace "projected-4420" to be "Succeeded or Failed"
    Feb 12 11:52:53.285: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.775658ms
    Feb 12 11:52:55.297: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039026476s
    Feb 12 11:52:57.301: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042352177s
    STEP: Saw pod success 02/12/23 11:52:57.301
    Feb 12 11:52:57.302: INFO: Pod "pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd" satisfied condition "Succeeded or Failed"
    Feb 12 11:52:57.313: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 11:52:57.324
    Feb 12 11:52:57.342: INFO: Waiting for pod pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd to disappear
    Feb 12 11:52:57.345: INFO: Pod pod-projected-secrets-443bb1bb-7c85-4311-b576-3fd21f2eb8bd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 11:52:57.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4420" for this suite. 02/12/23 11:52:57.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:52:57.362
Feb 12 11:52:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 11:52:57.363
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:57.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:57.38
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/12/23 11:52:57.382
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/12/23 11:52:57.388
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/12/23 11:52:57.388
STEP: creating a pod to probe DNS 02/12/23 11:52:57.388
STEP: submitting the pod to kubernetes 02/12/23 11:52:57.388
Feb 12 11:52:57.403: INFO: Waiting up to 15m0s for pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39" in namespace "dns-323" to be "running"
Feb 12 11:52:57.406: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208454ms
Feb 12 11:52:59.418: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015694487s
Feb 12 11:53:01.419: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39": Phase="Running", Reason="", readiness=true. Elapsed: 4.016304532s
Feb 12 11:53:01.419: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:53:01.419
STEP: looking for the results for each expected name from probers 02/12/23 11:53:01.432
Feb 12 11:53:01.471: INFO: DNS probes using dns-323/dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39 succeeded

STEP: deleting the pod 02/12/23 11:53:01.471
STEP: deleting the test headless service 02/12/23 11:53:01.495
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 11:53:01.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-323" for this suite. 02/12/23 11:53:01.517
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":189,"skipped":3437,"failed":0}
------------------------------
 [4.162 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:52:57.362
    Feb 12 11:52:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 11:52:57.363
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:52:57.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:52:57.38
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/12/23 11:52:57.382
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/12/23 11:52:57.388
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-323.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/12/23 11:52:57.388
    STEP: creating a pod to probe DNS 02/12/23 11:52:57.388
    STEP: submitting the pod to kubernetes 02/12/23 11:52:57.388
    Feb 12 11:52:57.403: INFO: Waiting up to 15m0s for pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39" in namespace "dns-323" to be "running"
    Feb 12 11:52:57.406: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208454ms
    Feb 12 11:52:59.418: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015694487s
    Feb 12 11:53:01.419: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39": Phase="Running", Reason="", readiness=true. Elapsed: 4.016304532s
    Feb 12 11:53:01.419: INFO: Pod "dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:53:01.419
    STEP: looking for the results for each expected name from probers 02/12/23 11:53:01.432
    Feb 12 11:53:01.471: INFO: DNS probes using dns-323/dns-test-2c7e9d59-32f1-4ad8-9bff-39e0e4210f39 succeeded

    STEP: deleting the pod 02/12/23 11:53:01.471
    STEP: deleting the test headless service 02/12/23 11:53:01.495
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 11:53:01.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-323" for this suite. 02/12/23 11:53:01.517
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:53:01.526
Feb 12 11:53:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename cronjob 02/12/23 11:53:01.527
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:53:01.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:53:01.544
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/12/23 11:53:01.546
STEP: Ensuring a job is scheduled 02/12/23 11:53:01.551
STEP: Ensuring exactly one is scheduled 02/12/23 11:54:01.566
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/12/23 11:54:01.577
STEP: Ensuring the job is replaced with a new one 02/12/23 11:54:01.589
STEP: Removing cronjob 02/12/23 11:55:01.602
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 12 11:55:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5709" for this suite. 02/12/23 11:55:01.631
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":190,"skipped":3439,"failed":0}
------------------------------
 [SLOW TEST] [120.114 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:53:01.526
    Feb 12 11:53:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename cronjob 02/12/23 11:53:01.527
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:53:01.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:53:01.544
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/12/23 11:53:01.546
    STEP: Ensuring a job is scheduled 02/12/23 11:53:01.551
    STEP: Ensuring exactly one is scheduled 02/12/23 11:54:01.566
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/12/23 11:54:01.577
    STEP: Ensuring the job is replaced with a new one 02/12/23 11:54:01.589
    STEP: Removing cronjob 02/12/23 11:55:01.602
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 12 11:55:01.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5709" for this suite. 02/12/23 11:55:01.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:01.641
Feb 12 11:55:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 11:55:01.642
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:01.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:01.675
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 02/12/23 11:55:01.677
Feb 12 11:55:01.690: INFO: Waiting up to 5m0s for pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036" in namespace "var-expansion-3555" to be "Succeeded or Failed"
Feb 12 11:55:01.694: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616255ms
Feb 12 11:55:03.699: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009511679s
Feb 12 11:55:05.711: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020824424s
STEP: Saw pod success 02/12/23 11:55:05.711
Feb 12 11:55:05.711: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036" satisfied condition "Succeeded or Failed"
Feb 12 11:55:05.715: INFO: Trying to get logs from node kube-3 pod var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036 container dapi-container: <nil>
STEP: delete the pod 02/12/23 11:55:05.728
Feb 12 11:55:05.746: INFO: Waiting for pod var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036 to disappear
Feb 12 11:55:05.750: INFO: Pod var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 11:55:05.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3555" for this suite. 02/12/23 11:55:05.754
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":191,"skipped":3446,"failed":0}
------------------------------
 [4.121 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:01.641
    Feb 12 11:55:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 11:55:01.642
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:01.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:01.675
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 02/12/23 11:55:01.677
    Feb 12 11:55:01.690: INFO: Waiting up to 5m0s for pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036" in namespace "var-expansion-3555" to be "Succeeded or Failed"
    Feb 12 11:55:01.694: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616255ms
    Feb 12 11:55:03.699: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009511679s
    Feb 12 11:55:05.711: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020824424s
    STEP: Saw pod success 02/12/23 11:55:05.711
    Feb 12 11:55:05.711: INFO: Pod "var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036" satisfied condition "Succeeded or Failed"
    Feb 12 11:55:05.715: INFO: Trying to get logs from node kube-3 pod var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 11:55:05.728
    Feb 12 11:55:05.746: INFO: Waiting for pod var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036 to disappear
    Feb 12 11:55:05.750: INFO: Pod var-expansion-e788abc0-ecb6-4587-9145-7d189dbe8036 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 11:55:05.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3555" for this suite. 02/12/23 11:55:05.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:05.765
Feb 12 11:55:05.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:55:05.766
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:05.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:05.784
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:55:05.787
Feb 12 11:55:05.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d" in namespace "downward-api-9527" to be "Succeeded or Failed"
Feb 12 11:55:05.804: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839123ms
Feb 12 11:55:07.822: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023019276s
Feb 12 11:55:09.813: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014811715s
STEP: Saw pod success 02/12/23 11:55:09.814
Feb 12 11:55:09.814: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d" satisfied condition "Succeeded or Failed"
Feb 12 11:55:09.824: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d container client-container: <nil>
STEP: delete the pod 02/12/23 11:55:09.842
Feb 12 11:55:09.873: INFO: Waiting for pod downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d to disappear
Feb 12 11:55:09.879: INFO: Pod downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:55:09.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9527" for this suite. 02/12/23 11:55:09.886
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":192,"skipped":3457,"failed":0}
------------------------------
 [4.127 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:05.765
    Feb 12 11:55:05.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:55:05.766
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:05.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:05.784
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:55:05.787
    Feb 12 11:55:05.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d" in namespace "downward-api-9527" to be "Succeeded or Failed"
    Feb 12 11:55:05.804: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.839123ms
    Feb 12 11:55:07.822: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023019276s
    Feb 12 11:55:09.813: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014811715s
    STEP: Saw pod success 02/12/23 11:55:09.814
    Feb 12 11:55:09.814: INFO: Pod "downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d" satisfied condition "Succeeded or Failed"
    Feb 12 11:55:09.824: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d container client-container: <nil>
    STEP: delete the pod 02/12/23 11:55:09.842
    Feb 12 11:55:09.873: INFO: Waiting for pod downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d to disappear
    Feb 12 11:55:09.879: INFO: Pod downwardapi-volume-af59d836-a360-4af2-9d03-caa1556cb87d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:55:09.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9527" for this suite. 02/12/23 11:55:09.886
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:09.892
Feb 12 11:55:09.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 11:55:09.893
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:09.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:09.918
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-21b7c4f1-3e85-4f72-abba-4e1b2f427a68 02/12/23 11:55:09.92
STEP: Creating a pod to test consume secrets 02/12/23 11:55:09.925
Feb 12 11:55:09.933: INFO: Waiting up to 5m0s for pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d" in namespace "secrets-6994" to be "Succeeded or Failed"
Feb 12 11:55:09.939: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485242ms
Feb 12 11:55:11.943: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009811561s
Feb 12 11:55:13.947: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01344353s
STEP: Saw pod success 02/12/23 11:55:13.947
Feb 12 11:55:13.947: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d" satisfied condition "Succeeded or Failed"
Feb 12 11:55:13.953: INFO: Trying to get logs from node kube-3 pod pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 11:55:13.963
Feb 12 11:55:13.985: INFO: Waiting for pod pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d to disappear
Feb 12 11:55:13.990: INFO: Pod pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 11:55:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6994" for this suite. 02/12/23 11:55:13.993
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":193,"skipped":3458,"failed":0}
------------------------------
 [4.107 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:09.892
    Feb 12 11:55:09.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 11:55:09.893
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:09.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:09.918
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-21b7c4f1-3e85-4f72-abba-4e1b2f427a68 02/12/23 11:55:09.92
    STEP: Creating a pod to test consume secrets 02/12/23 11:55:09.925
    Feb 12 11:55:09.933: INFO: Waiting up to 5m0s for pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d" in namespace "secrets-6994" to be "Succeeded or Failed"
    Feb 12 11:55:09.939: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485242ms
    Feb 12 11:55:11.943: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009811561s
    Feb 12 11:55:13.947: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01344353s
    STEP: Saw pod success 02/12/23 11:55:13.947
    Feb 12 11:55:13.947: INFO: Pod "pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d" satisfied condition "Succeeded or Failed"
    Feb 12 11:55:13.953: INFO: Trying to get logs from node kube-3 pod pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 11:55:13.963
    Feb 12 11:55:13.985: INFO: Waiting for pod pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d to disappear
    Feb 12 11:55:13.990: INFO: Pod pod-secrets-ea476eaa-31f5-421e-a13f-c5ed0c728b1d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 11:55:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6994" for this suite. 02/12/23 11:55:13.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:14.001
Feb 12 11:55:14.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 11:55:14.002
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:14.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:14.017
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-a1858c0f-7242-4de5-a21a-3669fb15f7bb 02/12/23 11:55:14.022
STEP: Creating the pod 02/12/23 11:55:14.028
Feb 12 11:55:14.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621" in namespace "configmap-5862" to be "running and ready"
Feb 12 11:55:14.042: INFO: Pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621": Phase="Pending", Reason="", readiness=false. Elapsed: 6.901148ms
Feb 12 11:55:14.042: INFO: The phase of Pod pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:55:16.055: INFO: Pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621": Phase="Running", Reason="", readiness=true. Elapsed: 2.01947556s
Feb 12 11:55:16.055: INFO: The phase of Pod pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621 is Running (Ready = true)
Feb 12 11:55:16.055: INFO: Pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-a1858c0f-7242-4de5-a21a-3669fb15f7bb 02/12/23 11:55:16.079
STEP: waiting to observe update in volume 02/12/23 11:55:16.092
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 11:55:18.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5862" for this suite. 02/12/23 11:55:18.117
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":194,"skipped":3503,"failed":0}
------------------------------
 [4.126 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:14.001
    Feb 12 11:55:14.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 11:55:14.002
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:14.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:14.017
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-a1858c0f-7242-4de5-a21a-3669fb15f7bb 02/12/23 11:55:14.022
    STEP: Creating the pod 02/12/23 11:55:14.028
    Feb 12 11:55:14.035: INFO: Waiting up to 5m0s for pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621" in namespace "configmap-5862" to be "running and ready"
    Feb 12 11:55:14.042: INFO: Pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621": Phase="Pending", Reason="", readiness=false. Elapsed: 6.901148ms
    Feb 12 11:55:14.042: INFO: The phase of Pod pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:55:16.055: INFO: Pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621": Phase="Running", Reason="", readiness=true. Elapsed: 2.01947556s
    Feb 12 11:55:16.055: INFO: The phase of Pod pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621 is Running (Ready = true)
    Feb 12 11:55:16.055: INFO: Pod "pod-configmaps-c372dd2e-1965-4bc5-9ed0-fba9da7d9621" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-a1858c0f-7242-4de5-a21a-3669fb15f7bb 02/12/23 11:55:16.079
    STEP: waiting to observe update in volume 02/12/23 11:55:16.092
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 11:55:18.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5862" for this suite. 02/12/23 11:55:18.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:18.13
Feb 12 11:55:18.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 11:55:18.132
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:18.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:18.151
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:55:18.154
Feb 12 11:55:18.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24" in namespace "downward-api-8995" to be "Succeeded or Failed"
Feb 12 11:55:18.175: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.863331ms
Feb 12 11:55:20.188: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020860174s
Feb 12 11:55:22.382: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215168567s
Feb 12 11:55:24.322: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 6.155188574s
Feb 12 11:55:26.807: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.640332525s
STEP: Saw pod success 02/12/23 11:55:26.808
Feb 12 11:55:26.808: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24" satisfied condition "Succeeded or Failed"
Feb 12 11:55:26.908: INFO: Trying to get logs from node kube-1 pod downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24 container client-container: <nil>
STEP: delete the pod 02/12/23 11:55:26.964
Feb 12 11:55:27.238: INFO: Waiting for pod downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24 to disappear
Feb 12 11:55:27.279: INFO: Pod downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 11:55:27.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8995" for this suite. 02/12/23 11:55:27.286
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":195,"skipped":3516,"failed":0}
------------------------------
 [SLOW TEST] [9.180 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:18.13
    Feb 12 11:55:18.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 11:55:18.132
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:18.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:18.151
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:55:18.154
    Feb 12 11:55:18.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24" in namespace "downward-api-8995" to be "Succeeded or Failed"
    Feb 12 11:55:18.175: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.863331ms
    Feb 12 11:55:20.188: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020860174s
    Feb 12 11:55:22.382: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215168567s
    Feb 12 11:55:24.322: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Pending", Reason="", readiness=false. Elapsed: 6.155188574s
    Feb 12 11:55:26.807: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.640332525s
    STEP: Saw pod success 02/12/23 11:55:26.808
    Feb 12 11:55:26.808: INFO: Pod "downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24" satisfied condition "Succeeded or Failed"
    Feb 12 11:55:26.908: INFO: Trying to get logs from node kube-1 pod downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24 container client-container: <nil>
    STEP: delete the pod 02/12/23 11:55:26.964
    Feb 12 11:55:27.238: INFO: Waiting for pod downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24 to disappear
    Feb 12 11:55:27.279: INFO: Pod downwardapi-volume-23767c71-d5f0-418d-bf8a-bb9d4ef2ad24 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 11:55:27.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8995" for this suite. 02/12/23 11:55:27.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:27.311
Feb 12 11:55:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:55:27.312
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:27.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:27.51
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-34233d40-eebe-4022-bab6-88deff1df2b9 02/12/23 11:55:27.533
STEP: Creating a pod to test consume secrets 02/12/23 11:55:27.596
Feb 12 11:55:27.657: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc" in namespace "projected-3321" to be "Succeeded or Failed"
Feb 12 11:55:27.745: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 88.609092ms
Feb 12 11:55:29.758: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1016157s
Feb 12 11:55:31.759: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.102355885s
STEP: Saw pod success 02/12/23 11:55:31.759
Feb 12 11:55:31.759: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc" satisfied condition "Succeeded or Failed"
Feb 12 11:55:31.784: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc container projected-secret-volume-test: <nil>
STEP: delete the pod 02/12/23 11:55:31.8
Feb 12 11:55:31.820: INFO: Waiting for pod pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc to disappear
Feb 12 11:55:31.824: INFO: Pod pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 11:55:31.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3321" for this suite. 02/12/23 11:55:31.827
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":196,"skipped":3522,"failed":0}
------------------------------
 [4.524 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:27.311
    Feb 12 11:55:27.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:55:27.312
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:27.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:27.51
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-34233d40-eebe-4022-bab6-88deff1df2b9 02/12/23 11:55:27.533
    STEP: Creating a pod to test consume secrets 02/12/23 11:55:27.596
    Feb 12 11:55:27.657: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc" in namespace "projected-3321" to be "Succeeded or Failed"
    Feb 12 11:55:27.745: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 88.609092ms
    Feb 12 11:55:29.758: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1016157s
    Feb 12 11:55:31.759: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.102355885s
    STEP: Saw pod success 02/12/23 11:55:31.759
    Feb 12 11:55:31.759: INFO: Pod "pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc" satisfied condition "Succeeded or Failed"
    Feb 12 11:55:31.784: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 11:55:31.8
    Feb 12 11:55:31.820: INFO: Waiting for pod pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc to disappear
    Feb 12 11:55:31.824: INFO: Pod pod-projected-secrets-27b34c7a-d528-47bd-82bf-e6d661b26ffc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 11:55:31.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3321" for this suite. 02/12/23 11:55:31.827
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:31.837
Feb 12 11:55:31.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 11:55:31.838
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:31.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:31.855
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 02/12/23 11:55:31.857
Feb 12 11:55:31.867: INFO: Waiting up to 5m0s for pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371" in namespace "emptydir-8621" to be "Succeeded or Failed"
Feb 12 11:55:31.871: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103949ms
Feb 12 11:55:33.887: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020243788s
Feb 12 11:55:35.878: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011921s
STEP: Saw pod success 02/12/23 11:55:35.879
Feb 12 11:55:35.879: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371" satisfied condition "Succeeded or Failed"
Feb 12 11:55:35.884: INFO: Trying to get logs from node kube-3 pod pod-120abb6d-3791-4ee0-917f-6ad2cb71c371 container test-container: <nil>
STEP: delete the pod 02/12/23 11:55:35.891
Feb 12 11:55:35.906: INFO: Waiting for pod pod-120abb6d-3791-4ee0-917f-6ad2cb71c371 to disappear
Feb 12 11:55:35.909: INFO: Pod pod-120abb6d-3791-4ee0-917f-6ad2cb71c371 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 11:55:35.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8621" for this suite. 02/12/23 11:55:35.912
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3524,"failed":0}
------------------------------
 [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:31.837
    Feb 12 11:55:31.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 11:55:31.838
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:31.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:31.855
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/12/23 11:55:31.857
    Feb 12 11:55:31.867: INFO: Waiting up to 5m0s for pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371" in namespace "emptydir-8621" to be "Succeeded or Failed"
    Feb 12 11:55:31.871: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103949ms
    Feb 12 11:55:33.887: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020243788s
    Feb 12 11:55:35.878: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011921s
    STEP: Saw pod success 02/12/23 11:55:35.879
    Feb 12 11:55:35.879: INFO: Pod "pod-120abb6d-3791-4ee0-917f-6ad2cb71c371" satisfied condition "Succeeded or Failed"
    Feb 12 11:55:35.884: INFO: Trying to get logs from node kube-3 pod pod-120abb6d-3791-4ee0-917f-6ad2cb71c371 container test-container: <nil>
    STEP: delete the pod 02/12/23 11:55:35.891
    Feb 12 11:55:35.906: INFO: Waiting for pod pod-120abb6d-3791-4ee0-917f-6ad2cb71c371 to disappear
    Feb 12 11:55:35.909: INFO: Pod pod-120abb6d-3791-4ee0-917f-6ad2cb71c371 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 11:55:35.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8621" for this suite. 02/12/23 11:55:35.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:35.918
Feb 12 11:55:35.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sysctl 02/12/23 11:55:35.919
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:35.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:35.94
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/12/23 11:55:35.941
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 11:55:35.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3747" for this suite. 02/12/23 11:55:35.948
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":198,"skipped":3530,"failed":0}
------------------------------
 [0.036 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:35.918
    Feb 12 11:55:35.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sysctl 02/12/23 11:55:35.919
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:35.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:35.94
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/12/23 11:55:35.941
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 11:55:35.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3747" for this suite. 02/12/23 11:55:35.948
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:35.955
Feb 12 11:55:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:55:35.956
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:35.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:35.974
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-2fe08055-8bab-4166-9462-00193273e86c 02/12/23 11:55:35.979
STEP: Creating configMap with name cm-test-opt-upd-30a497fa-cde3-4eb2-b7f5-98a6f3d0b0c7 02/12/23 11:55:35.983
STEP: Creating the pod 02/12/23 11:55:35.987
Feb 12 11:55:36.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020" in namespace "projected-3409" to be "running and ready"
Feb 12 11:55:36.007: INFO: Pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793753ms
Feb 12 11:55:36.007: INFO: The phase of Pod pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:55:38.022: INFO: Pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020": Phase="Running", Reason="", readiness=true. Elapsed: 2.020248195s
Feb 12 11:55:38.022: INFO: The phase of Pod pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020 is Running (Ready = true)
Feb 12 11:55:38.022: INFO: Pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2fe08055-8bab-4166-9462-00193273e86c 02/12/23 11:55:38.071
STEP: Updating configmap cm-test-opt-upd-30a497fa-cde3-4eb2-b7f5-98a6f3d0b0c7 02/12/23 11:55:38.08
STEP: Creating configMap with name cm-test-opt-create-7dc7d5fb-7016-4ebb-bfc1-d2f2835266a1 02/12/23 11:55:38.086
STEP: waiting to observe update in volume 02/12/23 11:55:38.091
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 11:55:40.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3409" for this suite. 02/12/23 11:55:40.177
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":199,"skipped":3533,"failed":0}
------------------------------
 [4.233 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:35.955
    Feb 12 11:55:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:55:35.956
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:35.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:35.974
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-2fe08055-8bab-4166-9462-00193273e86c 02/12/23 11:55:35.979
    STEP: Creating configMap with name cm-test-opt-upd-30a497fa-cde3-4eb2-b7f5-98a6f3d0b0c7 02/12/23 11:55:35.983
    STEP: Creating the pod 02/12/23 11:55:35.987
    Feb 12 11:55:36.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020" in namespace "projected-3409" to be "running and ready"
    Feb 12 11:55:36.007: INFO: Pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793753ms
    Feb 12 11:55:36.007: INFO: The phase of Pod pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:55:38.022: INFO: Pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020": Phase="Running", Reason="", readiness=true. Elapsed: 2.020248195s
    Feb 12 11:55:38.022: INFO: The phase of Pod pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020 is Running (Ready = true)
    Feb 12 11:55:38.022: INFO: Pod "pod-projected-configmaps-b130d233-5e19-465f-8f27-7078b8dbb020" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2fe08055-8bab-4166-9462-00193273e86c 02/12/23 11:55:38.071
    STEP: Updating configmap cm-test-opt-upd-30a497fa-cde3-4eb2-b7f5-98a6f3d0b0c7 02/12/23 11:55:38.08
    STEP: Creating configMap with name cm-test-opt-create-7dc7d5fb-7016-4ebb-bfc1-d2f2835266a1 02/12/23 11:55:38.086
    STEP: waiting to observe update in volume 02/12/23 11:55:38.091
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 11:55:40.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3409" for this suite. 02/12/23 11:55:40.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:55:40.191
Feb 12 11:55:40.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:55:40.192
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:40.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:40.213
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/12/23 11:55:40.215
Feb 12 11:55:40.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 11:55:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:56:02.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2780" for this suite. 02/12/23 11:56:02.583
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":200,"skipped":3550,"failed":0}
------------------------------
 [SLOW TEST] [22.398 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:55:40.191
    Feb 12 11:55:40.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 11:55:40.192
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:55:40.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:55:40.213
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/12/23 11:55:40.215
    Feb 12 11:55:40.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 11:55:47.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:56:02.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2780" for this suite. 02/12/23 11:56:02.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:02.593
Feb 12 11:56:02.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 11:56:02.594
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:02.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:02.616
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 11:56:02.641
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:56:03.36
STEP: Deploying the webhook pod 02/12/23 11:56:03.366
STEP: Wait for the deployment to be ready 02/12/23 11:56:03.384
Feb 12 11:56:03.395: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 11:56:05.407
STEP: Verifying the service has paired with the endpoint 02/12/23 11:56:05.417
Feb 12 11:56:06.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 02/12/23 11:56:06.501
STEP: Creating a configMap that should be mutated 02/12/23 11:56:06.509
STEP: Deleting the collection of validation webhooks 02/12/23 11:56:06.53
STEP: Creating a configMap that should not be mutated 02/12/23 11:56:06.602
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:56:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5030" for this suite. 02/12/23 11:56:06.617
STEP: Destroying namespace "webhook-5030-markers" for this suite. 02/12/23 11:56:06.623
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":201,"skipped":3575,"failed":0}
------------------------------
 [4.121 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:02.593
    Feb 12 11:56:02.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 11:56:02.594
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:02.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:02.616
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 11:56:02.641
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 11:56:03.36
    STEP: Deploying the webhook pod 02/12/23 11:56:03.366
    STEP: Wait for the deployment to be ready 02/12/23 11:56:03.384
    Feb 12 11:56:03.395: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 11:56:05.407
    STEP: Verifying the service has paired with the endpoint 02/12/23 11:56:05.417
    Feb 12 11:56:06.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 02/12/23 11:56:06.501
    STEP: Creating a configMap that should be mutated 02/12/23 11:56:06.509
    STEP: Deleting the collection of validation webhooks 02/12/23 11:56:06.53
    STEP: Creating a configMap that should not be mutated 02/12/23 11:56:06.602
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:56:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5030" for this suite. 02/12/23 11:56:06.617
    STEP: Destroying namespace "webhook-5030-markers" for this suite. 02/12/23 11:56:06.623
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:06.715
Feb 12 11:56:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename controllerrevisions 02/12/23 11:56:06.718
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:06.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:06.763
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-6jbxn-daemon-set" 02/12/23 11:56:06.794
STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:56:06.805
Feb 12 11:56:06.816: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 0
Feb 12 11:56:06.816: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:56:07.825: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 0
Feb 12 11:56:07.825: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 11:56:08.823: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 3
Feb 12 11:56:08.823: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-6jbxn-daemon-set
STEP: Confirm DaemonSet "e2e-6jbxn-daemon-set" successfully created with "daemonset-name=e2e-6jbxn-daemon-set" label 02/12/23 11:56:08.826
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-6jbxn-daemon-set" 02/12/23 11:56:08.833
Feb 12 11:56:08.837: INFO: Located ControllerRevision: "e2e-6jbxn-daemon-set-76f84d7759"
STEP: Patching ControllerRevision "e2e-6jbxn-daemon-set-76f84d7759" 02/12/23 11:56:08.841
Feb 12 11:56:08.850: INFO: e2e-6jbxn-daemon-set-76f84d7759 has been patched
STEP: Create a new ControllerRevision 02/12/23 11:56:08.85
Feb 12 11:56:08.860: INFO: Created ControllerRevision: e2e-6jbxn-daemon-set-578c7555c5
STEP: Confirm that there are two ControllerRevisions 02/12/23 11:56:08.86
Feb 12 11:56:08.860: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 12 11:56:08.863: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-6jbxn-daemon-set-76f84d7759" 02/12/23 11:56:08.863
STEP: Confirm that there is only one ControllerRevision 02/12/23 11:56:08.871
Feb 12 11:56:08.871: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 12 11:56:08.874: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-6jbxn-daemon-set-578c7555c5" 02/12/23 11:56:08.878
Feb 12 11:56:08.889: INFO: e2e-6jbxn-daemon-set-578c7555c5 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/12/23 11:56:08.889
W0212 11:56:08.898051      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/12/23 11:56:08.898
Feb 12 11:56:08.898: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 12 11:56:09.908: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 12 11:56:09.924: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-6jbxn-daemon-set-578c7555c5=updated" 02/12/23 11:56:09.924
STEP: Confirm that there is only one ControllerRevision 02/12/23 11:56:09.941
Feb 12 11:56:09.941: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 12 11:56:09.948: INFO: Found 1 ControllerRevisions
Feb 12 11:56:09.951: INFO: ControllerRevision "e2e-6jbxn-daemon-set-7986b658b" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-6jbxn-daemon-set" 02/12/23 11:56:09.954
STEP: deleting DaemonSet.extensions e2e-6jbxn-daemon-set in namespace controllerrevisions-1238, will wait for the garbage collector to delete the pods 02/12/23 11:56:09.954
Feb 12 11:56:10.019: INFO: Deleting DaemonSet.extensions e2e-6jbxn-daemon-set took: 10.820408ms
Feb 12 11:56:10.120: INFO: Terminating DaemonSet.extensions e2e-6jbxn-daemon-set pods took: 101.036065ms
Feb 12 11:56:15.225: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 0
Feb 12 11:56:15.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-6jbxn-daemon-set
Feb 12 11:56:15.227: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21037"},"items":null}

Feb 12 11:56:15.229: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21037"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Feb 12 11:56:15.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-1238" for this suite. 02/12/23 11:56:15.243
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":202,"skipped":3592,"failed":0}
------------------------------
 [SLOW TEST] [8.535 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:06.715
    Feb 12 11:56:06.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename controllerrevisions 02/12/23 11:56:06.718
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:06.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:06.763
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-6jbxn-daemon-set" 02/12/23 11:56:06.794
    STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 11:56:06.805
    Feb 12 11:56:06.816: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 0
    Feb 12 11:56:06.816: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:56:07.825: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 0
    Feb 12 11:56:07.825: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 11:56:08.823: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 3
    Feb 12 11:56:08.823: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-6jbxn-daemon-set
    STEP: Confirm DaemonSet "e2e-6jbxn-daemon-set" successfully created with "daemonset-name=e2e-6jbxn-daemon-set" label 02/12/23 11:56:08.826
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-6jbxn-daemon-set" 02/12/23 11:56:08.833
    Feb 12 11:56:08.837: INFO: Located ControllerRevision: "e2e-6jbxn-daemon-set-76f84d7759"
    STEP: Patching ControllerRevision "e2e-6jbxn-daemon-set-76f84d7759" 02/12/23 11:56:08.841
    Feb 12 11:56:08.850: INFO: e2e-6jbxn-daemon-set-76f84d7759 has been patched
    STEP: Create a new ControllerRevision 02/12/23 11:56:08.85
    Feb 12 11:56:08.860: INFO: Created ControllerRevision: e2e-6jbxn-daemon-set-578c7555c5
    STEP: Confirm that there are two ControllerRevisions 02/12/23 11:56:08.86
    Feb 12 11:56:08.860: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 12 11:56:08.863: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-6jbxn-daemon-set-76f84d7759" 02/12/23 11:56:08.863
    STEP: Confirm that there is only one ControllerRevision 02/12/23 11:56:08.871
    Feb 12 11:56:08.871: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 12 11:56:08.874: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-6jbxn-daemon-set-578c7555c5" 02/12/23 11:56:08.878
    Feb 12 11:56:08.889: INFO: e2e-6jbxn-daemon-set-578c7555c5 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/12/23 11:56:08.889
    W0212 11:56:08.898051      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/12/23 11:56:08.898
    Feb 12 11:56:08.898: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 12 11:56:09.908: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 12 11:56:09.924: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-6jbxn-daemon-set-578c7555c5=updated" 02/12/23 11:56:09.924
    STEP: Confirm that there is only one ControllerRevision 02/12/23 11:56:09.941
    Feb 12 11:56:09.941: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 12 11:56:09.948: INFO: Found 1 ControllerRevisions
    Feb 12 11:56:09.951: INFO: ControllerRevision "e2e-6jbxn-daemon-set-7986b658b" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-6jbxn-daemon-set" 02/12/23 11:56:09.954
    STEP: deleting DaemonSet.extensions e2e-6jbxn-daemon-set in namespace controllerrevisions-1238, will wait for the garbage collector to delete the pods 02/12/23 11:56:09.954
    Feb 12 11:56:10.019: INFO: Deleting DaemonSet.extensions e2e-6jbxn-daemon-set took: 10.820408ms
    Feb 12 11:56:10.120: INFO: Terminating DaemonSet.extensions e2e-6jbxn-daemon-set pods took: 101.036065ms
    Feb 12 11:56:15.225: INFO: Number of nodes with available pods controlled by daemonset e2e-6jbxn-daemon-set: 0
    Feb 12 11:56:15.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-6jbxn-daemon-set
    Feb 12 11:56:15.227: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21037"},"items":null}

    Feb 12 11:56:15.229: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21037"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 11:56:15.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-1238" for this suite. 02/12/23 11:56:15.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:15.251
Feb 12 11:56:15.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename disruption 02/12/23 11:56:15.252
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:15.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:15.272
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:15.274
Feb 12 11:56:15.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename disruption-2 02/12/23 11:56:15.275
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:15.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:15.298
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 02/12/23 11:56:15.306
STEP: Waiting for the pdb to be processed 02/12/23 11:56:17.335
STEP: Waiting for the pdb to be processed 02/12/23 11:56:17.354
STEP: listing a collection of PDBs across all namespaces 02/12/23 11:56:17.362
STEP: listing a collection of PDBs in namespace disruption-8529 02/12/23 11:56:17.368
STEP: deleting a collection of PDBs 02/12/23 11:56:17.371
STEP: Waiting for the PDB collection to be deleted 02/12/23 11:56:17.392
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Feb 12 11:56:17.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4436" for this suite. 02/12/23 11:56:17.4
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 12 11:56:17.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8529" for this suite. 02/12/23 11:56:17.415
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":203,"skipped":3605,"failed":0}
------------------------------
 [2.173 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:15.251
    Feb 12 11:56:15.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename disruption 02/12/23 11:56:15.252
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:15.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:15.272
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:15.274
    Feb 12 11:56:15.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename disruption-2 02/12/23 11:56:15.275
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:15.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:15.298
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 02/12/23 11:56:15.306
    STEP: Waiting for the pdb to be processed 02/12/23 11:56:17.335
    STEP: Waiting for the pdb to be processed 02/12/23 11:56:17.354
    STEP: listing a collection of PDBs across all namespaces 02/12/23 11:56:17.362
    STEP: listing a collection of PDBs in namespace disruption-8529 02/12/23 11:56:17.368
    STEP: deleting a collection of PDBs 02/12/23 11:56:17.371
    STEP: Waiting for the PDB collection to be deleted 02/12/23 11:56:17.392
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Feb 12 11:56:17.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-4436" for this suite. 02/12/23 11:56:17.4
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 12 11:56:17.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8529" for this suite. 02/12/23 11:56:17.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:17.425
Feb 12 11:56:17.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:56:17.426
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:17.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:17.452
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-8689 02/12/23 11:56:17.454
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[] 02/12/23 11:56:17.473
Feb 12 11:56:17.479: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 12 11:56:18.507: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8689 02/12/23 11:56:18.507
Feb 12 11:56:18.528: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8689" to be "running and ready"
Feb 12 11:56:18.537: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.154166ms
Feb 12 11:56:18.538: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:56:20.543: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014281622s
Feb 12 11:56:20.543: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 12 11:56:20.543: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[pod1:[100]] 02/12/23 11:56:20.546
Feb 12 11:56:20.563: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8689 02/12/23 11:56:20.563
Feb 12 11:56:20.570: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8689" to be "running and ready"
Feb 12 11:56:20.583: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.153001ms
Feb 12 11:56:20.583: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:56:22.589: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019191358s
Feb 12 11:56:22.589: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 12 11:56:22.589: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[pod1:[100] pod2:[101]] 02/12/23 11:56:22.593
Feb 12 11:56:22.616: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/12/23 11:56:22.616
Feb 12 11:56:22.616: INFO: Creating new exec pod
Feb 12 11:56:22.627: INFO: Waiting up to 5m0s for pod "execpodbx44h" in namespace "services-8689" to be "running"
Feb 12 11:56:22.632: INFO: Pod "execpodbx44h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.012481ms
Feb 12 11:56:24.637: INFO: Pod "execpodbx44h": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530464s
Feb 12 11:56:24.637: INFO: Pod "execpodbx44h" satisfied condition "running"
Feb 12 11:56:25.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Feb 12 11:56:25.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 12 11:56:25.952: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:56:25.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.216 80'
Feb 12 11:56:26.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.216 80\nConnection to 10.233.52.216 80 port [tcp/http] succeeded!\n"
Feb 12 11:56:26.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:56:26.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Feb 12 11:56:26.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 12 11:56:26.343: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:56:26.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.216 81'
Feb 12 11:56:26.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.216 81\nConnection to 10.233.52.216 81 port [tcp/*] succeeded!\n"
Feb 12 11:56:26.475: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8689 02/12/23 11:56:26.475
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[pod2:[101]] 02/12/23 11:56:26.502
Feb 12 11:56:26.530: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8689 02/12/23 11:56:26.53
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[] 02/12/23 11:56:26.586
Feb 12 11:56:26.617: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:56:26.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8689" for this suite. 02/12/23 11:56:26.695
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":204,"skipped":3611,"failed":0}
------------------------------
 [SLOW TEST] [9.284 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:17.425
    Feb 12 11:56:17.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:56:17.426
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:17.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:17.452
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-8689 02/12/23 11:56:17.454
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[] 02/12/23 11:56:17.473
    Feb 12 11:56:17.479: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Feb 12 11:56:18.507: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8689 02/12/23 11:56:18.507
    Feb 12 11:56:18.528: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8689" to be "running and ready"
    Feb 12 11:56:18.537: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.154166ms
    Feb 12 11:56:18.538: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:56:20.543: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014281622s
    Feb 12 11:56:20.543: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 12 11:56:20.543: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[pod1:[100]] 02/12/23 11:56:20.546
    Feb 12 11:56:20.563: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8689 02/12/23 11:56:20.563
    Feb 12 11:56:20.570: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8689" to be "running and ready"
    Feb 12 11:56:20.583: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.153001ms
    Feb 12 11:56:20.583: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:56:22.589: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019191358s
    Feb 12 11:56:22.589: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 12 11:56:22.589: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[pod1:[100] pod2:[101]] 02/12/23 11:56:22.593
    Feb 12 11:56:22.616: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/12/23 11:56:22.616
    Feb 12 11:56:22.616: INFO: Creating new exec pod
    Feb 12 11:56:22.627: INFO: Waiting up to 5m0s for pod "execpodbx44h" in namespace "services-8689" to be "running"
    Feb 12 11:56:22.632: INFO: Pod "execpodbx44h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.012481ms
    Feb 12 11:56:24.637: INFO: Pod "execpodbx44h": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530464s
    Feb 12 11:56:24.637: INFO: Pod "execpodbx44h" satisfied condition "running"
    Feb 12 11:56:25.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Feb 12 11:56:25.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 12 11:56:25.952: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:56:25.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.216 80'
    Feb 12 11:56:26.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.216 80\nConnection to 10.233.52.216 80 port [tcp/http] succeeded!\n"
    Feb 12 11:56:26.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:56:26.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Feb 12 11:56:26.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 12 11:56:26.343: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:56:26.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-8689 exec execpodbx44h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.216 81'
    Feb 12 11:56:26.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.216 81\nConnection to 10.233.52.216 81 port [tcp/*] succeeded!\n"
    Feb 12 11:56:26.475: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8689 02/12/23 11:56:26.475
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[pod2:[101]] 02/12/23 11:56:26.502
    Feb 12 11:56:26.530: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8689 02/12/23 11:56:26.53
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8689 to expose endpoints map[] 02/12/23 11:56:26.586
    Feb 12 11:56:26.617: INFO: successfully validated that service multi-endpoint-test in namespace services-8689 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:56:26.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8689" for this suite. 02/12/23 11:56:26.695
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:26.711
Feb 12 11:56:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:56:26.712
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:26.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:26.752
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:56:26.765
Feb 12 11:56:26.784: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5271" to be "running and ready"
Feb 12 11:56:26.791: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.651802ms
Feb 12 11:56:26.791: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:56:28.821: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.036711773s
Feb 12 11:56:28.821: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 12 11:56:28.821: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 02/12/23 11:56:28.834
Feb 12 11:56:28.851: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5271" to be "running and ready"
Feb 12 11:56:28.860: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.665346ms
Feb 12 11:56:28.860: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 12 11:56:30.876: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024533747s
Feb 12 11:56:30.876: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 12 11:56:30.876: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/12/23 11:56:30.891
Feb 12 11:56:30.914: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 11:56:30.919: INFO: Pod pod-with-prestop-http-hook still exists
Feb 12 11:56:32.920: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 11:56:32.932: INFO: Pod pod-with-prestop-http-hook still exists
Feb 12 11:56:34.920: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 12 11:56:34.935: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/12/23 11:56:34.936
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Feb 12 11:56:34.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5271" for this suite. 02/12/23 11:56:34.963
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":205,"skipped":3642,"failed":0}
------------------------------
 [SLOW TEST] [8.261 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:26.711
    Feb 12 11:56:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/12/23 11:56:26.712
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:26.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:26.752
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 02/12/23 11:56:26.765
    Feb 12 11:56:26.784: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5271" to be "running and ready"
    Feb 12 11:56:26.791: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.651802ms
    Feb 12 11:56:26.791: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:56:28.821: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.036711773s
    Feb 12 11:56:28.821: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 12 11:56:28.821: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 02/12/23 11:56:28.834
    Feb 12 11:56:28.851: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5271" to be "running and ready"
    Feb 12 11:56:28.860: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 8.665346ms
    Feb 12 11:56:28.860: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 11:56:30.876: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024533747s
    Feb 12 11:56:30.876: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 12 11:56:30.876: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/12/23 11:56:30.891
    Feb 12 11:56:30.914: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 12 11:56:30.919: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 12 11:56:32.920: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 12 11:56:32.932: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 12 11:56:34.920: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 12 11:56:34.935: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/12/23 11:56:34.936
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Feb 12 11:56:34.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5271" for this suite. 02/12/23 11:56:34.963
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:34.974
Feb 12 11:56:34.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 11:56:34.976
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:35.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:35.056
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3917 02/12/23 11:56:35.058
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 02/12/23 11:56:35.066
STEP: Creating pod with conflicting port in namespace statefulset-3917 02/12/23 11:56:35.072
STEP: Waiting until pod test-pod will start running in namespace statefulset-3917 02/12/23 11:56:35.083
Feb 12 11:56:35.084: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3917" to be "running"
Feb 12 11:56:35.087: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793166ms
Feb 12 11:56:37.098: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014282147s
Feb 12 11:56:37.098: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3917 02/12/23 11:56:37.098
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3917 02/12/23 11:56:37.115
Feb 12 11:56:37.142: INFO: Observed stateful pod in namespace: statefulset-3917, name: ss-0, uid: 21ede536-52cc-4702-b0fc-5a5d5ad86830, status phase: Pending. Waiting for statefulset controller to delete.
Feb 12 11:56:37.153: INFO: Observed stateful pod in namespace: statefulset-3917, name: ss-0, uid: 21ede536-52cc-4702-b0fc-5a5d5ad86830, status phase: Failed. Waiting for statefulset controller to delete.
Feb 12 11:56:37.164: INFO: Observed stateful pod in namespace: statefulset-3917, name: ss-0, uid: 21ede536-52cc-4702-b0fc-5a5d5ad86830, status phase: Failed. Waiting for statefulset controller to delete.
Feb 12 11:56:37.166: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3917
STEP: Removing pod with conflicting port in namespace statefulset-3917 02/12/23 11:56:37.166
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3917 and will be in running state 02/12/23 11:56:37.187
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 11:56:39.213: INFO: Deleting all statefulset in ns statefulset-3917
Feb 12 11:56:39.231: INFO: Scaling statefulset ss to 0
Feb 12 11:56:49.290: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:56:49.298: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 11:56:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3917" for this suite. 02/12/23 11:56:49.318
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":206,"skipped":3645,"failed":0}
------------------------------
 [SLOW TEST] [14.350 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:34.974
    Feb 12 11:56:34.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 11:56:34.976
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:35.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:35.056
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3917 02/12/23 11:56:35.058
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 02/12/23 11:56:35.066
    STEP: Creating pod with conflicting port in namespace statefulset-3917 02/12/23 11:56:35.072
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3917 02/12/23 11:56:35.083
    Feb 12 11:56:35.084: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3917" to be "running"
    Feb 12 11:56:35.087: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793166ms
    Feb 12 11:56:37.098: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014282147s
    Feb 12 11:56:37.098: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3917 02/12/23 11:56:37.098
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3917 02/12/23 11:56:37.115
    Feb 12 11:56:37.142: INFO: Observed stateful pod in namespace: statefulset-3917, name: ss-0, uid: 21ede536-52cc-4702-b0fc-5a5d5ad86830, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 12 11:56:37.153: INFO: Observed stateful pod in namespace: statefulset-3917, name: ss-0, uid: 21ede536-52cc-4702-b0fc-5a5d5ad86830, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 12 11:56:37.164: INFO: Observed stateful pod in namespace: statefulset-3917, name: ss-0, uid: 21ede536-52cc-4702-b0fc-5a5d5ad86830, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 12 11:56:37.166: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3917
    STEP: Removing pod with conflicting port in namespace statefulset-3917 02/12/23 11:56:37.166
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3917 and will be in running state 02/12/23 11:56:37.187
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 11:56:39.213: INFO: Deleting all statefulset in ns statefulset-3917
    Feb 12 11:56:39.231: INFO: Scaling statefulset ss to 0
    Feb 12 11:56:49.290: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:56:49.298: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 11:56:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3917" for this suite. 02/12/23 11:56:49.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:49.326
Feb 12 11:56:49.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 11:56:49.327
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:49.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:49.349
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/12/23 11:56:49.351
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6693.svc.cluster.local;sleep 1; done
 02/12/23 11:56:49.357
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6693.svc.cluster.local;sleep 1; done
 02/12/23 11:56:49.357
STEP: creating a pod to probe DNS 02/12/23 11:56:49.357
STEP: submitting the pod to kubernetes 02/12/23 11:56:49.357
Feb 12 11:56:49.377: INFO: Waiting up to 15m0s for pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641" in namespace "dns-6693" to be "running"
Feb 12 11:56:49.390: INFO: Pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641": Phase="Pending", Reason="", readiness=false. Elapsed: 12.890735ms
Feb 12 11:56:51.403: INFO: Pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641": Phase="Running", Reason="", readiness=true. Elapsed: 2.025870547s
Feb 12 11:56:51.403: INFO: Pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641" satisfied condition "running"
STEP: retrieving the pod 02/12/23 11:56:51.404
STEP: looking for the results for each expected name from probers 02/12/23 11:56:51.408
Feb 12 11:56:51.413: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.423: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.429: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.434: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.437: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.443: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.447: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.450: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
Feb 12 11:56:51.450: INFO: Lookups using dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6693.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6693.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local jessie_udp@dns-test-service-2.dns-6693.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6693.svc.cluster.local]

Feb 12 11:56:56.541: INFO: DNS probes using dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641 succeeded

STEP: deleting the pod 02/12/23 11:56:56.541
STEP: deleting the test headless service 02/12/23 11:56:56.594
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 11:56:56.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6693" for this suite. 02/12/23 11:56:56.628
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":207,"skipped":3656,"failed":0}
------------------------------
 [SLOW TEST] [8.059 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:49.326
    Feb 12 11:56:49.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 11:56:49.327
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:56:49.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:56:49.349
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/12/23 11:56:49.351
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6693.svc.cluster.local;sleep 1; done
     02/12/23 11:56:49.357
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6693.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6693.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6693.svc.cluster.local;sleep 1; done
     02/12/23 11:56:49.357
    STEP: creating a pod to probe DNS 02/12/23 11:56:49.357
    STEP: submitting the pod to kubernetes 02/12/23 11:56:49.357
    Feb 12 11:56:49.377: INFO: Waiting up to 15m0s for pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641" in namespace "dns-6693" to be "running"
    Feb 12 11:56:49.390: INFO: Pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641": Phase="Pending", Reason="", readiness=false. Elapsed: 12.890735ms
    Feb 12 11:56:51.403: INFO: Pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641": Phase="Running", Reason="", readiness=true. Elapsed: 2.025870547s
    Feb 12 11:56:51.403: INFO: Pod "dns-test-67978d9a-38bf-464d-9efe-dc933fb98641" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 11:56:51.404
    STEP: looking for the results for each expected name from probers 02/12/23 11:56:51.408
    Feb 12 11:56:51.413: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.423: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.429: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.434: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.437: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.443: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.447: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.450: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6693.svc.cluster.local from pod dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641: the server could not find the requested resource (get pods dns-test-67978d9a-38bf-464d-9efe-dc933fb98641)
    Feb 12 11:56:51.450: INFO: Lookups using dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6693.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6693.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6693.svc.cluster.local jessie_udp@dns-test-service-2.dns-6693.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6693.svc.cluster.local]

    Feb 12 11:56:56.541: INFO: DNS probes using dns-6693/dns-test-67978d9a-38bf-464d-9efe-dc933fb98641 succeeded

    STEP: deleting the pod 02/12/23 11:56:56.541
    STEP: deleting the test headless service 02/12/23 11:56:56.594
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 11:56:56.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6693" for this suite. 02/12/23 11:56:56.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:56:57.388
Feb 12 11:56:57.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 11:56:57.391
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:00.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:00.185
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-185 02/12/23 11:57:00.19
STEP: creating service affinity-nodeport-transition in namespace services-185 02/12/23 11:57:00.19
STEP: creating replication controller affinity-nodeport-transition in namespace services-185 02/12/23 11:57:00.431
I0212 11:57:00.608071      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-185, replica count: 3
I0212 11:57:03.659799      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 11:57:03.696: INFO: Creating new exec pod
Feb 12 11:57:03.704: INFO: Waiting up to 5m0s for pod "execpod-affinityrw6s4" in namespace "services-185" to be "running"
Feb 12 11:57:03.713: INFO: Pod "execpod-affinityrw6s4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481816ms
Feb 12 11:57:05.723: INFO: Pod "execpod-affinityrw6s4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018891416s
Feb 12 11:57:05.723: INFO: Pod "execpod-affinityrw6s4" satisfied condition "running"
Feb 12 11:57:06.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Feb 12 11:57:06.853: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 12 11:57:06.853: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:57:06.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.54.75 80'
Feb 12 11:57:06.970: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.54.75 80\nConnection to 10.233.54.75 80 port [tcp/http] succeeded!\n"
Feb 12 11:57:06.970: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:57:06.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.102 30345'
Feb 12 11:57:07.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.102 30345\nConnection to 10.2.20.102 30345 port [tcp/*] succeeded!\n"
Feb 12 11:57:07.087: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:57:07.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 30345'
Feb 12 11:57:07.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 30345\nConnection to 10.2.20.101 30345 port [tcp/*] succeeded!\n"
Feb 12 11:57:07.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 11:57:07.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:30345/ ; done'
Feb 12 11:57:07.462: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n"
Feb 12 11:57:07.462: INFO: stdout: "\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h"
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
Feb 12 11:57:07.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:30345/ ; done'
Feb 12 11:57:07.665: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n"
Feb 12 11:57:07.665: INFO: stdout: "\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr"
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
Feb 12 11:57:07.665: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-185, will wait for the garbage collector to delete the pods 02/12/23 11:57:07.69
Feb 12 11:57:07.756: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.446101ms
Feb 12 11:57:07.856: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.662125ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 11:57:10.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-185" for this suite. 02/12/23 11:57:10.301
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":208,"skipped":3664,"failed":0}
------------------------------
 [SLOW TEST] [12.926 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:56:57.388
    Feb 12 11:56:57.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 11:56:57.391
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:00.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:00.185
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-185 02/12/23 11:57:00.19
    STEP: creating service affinity-nodeport-transition in namespace services-185 02/12/23 11:57:00.19
    STEP: creating replication controller affinity-nodeport-transition in namespace services-185 02/12/23 11:57:00.431
    I0212 11:57:00.608071      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-185, replica count: 3
    I0212 11:57:03.659799      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 11:57:03.696: INFO: Creating new exec pod
    Feb 12 11:57:03.704: INFO: Waiting up to 5m0s for pod "execpod-affinityrw6s4" in namespace "services-185" to be "running"
    Feb 12 11:57:03.713: INFO: Pod "execpod-affinityrw6s4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481816ms
    Feb 12 11:57:05.723: INFO: Pod "execpod-affinityrw6s4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018891416s
    Feb 12 11:57:05.723: INFO: Pod "execpod-affinityrw6s4" satisfied condition "running"
    Feb 12 11:57:06.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Feb 12 11:57:06.853: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 12 11:57:06.853: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:57:06.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.54.75 80'
    Feb 12 11:57:06.970: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.54.75 80\nConnection to 10.233.54.75 80 port [tcp/http] succeeded!\n"
    Feb 12 11:57:06.970: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:57:06.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.102 30345'
    Feb 12 11:57:07.087: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.102 30345\nConnection to 10.2.20.102 30345 port [tcp/*] succeeded!\n"
    Feb 12 11:57:07.087: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:57:07.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 30345'
    Feb 12 11:57:07.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 30345\nConnection to 10.2.20.101 30345 port [tcp/*] succeeded!\n"
    Feb 12 11:57:07.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 11:57:07.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:30345/ ; done'
    Feb 12 11:57:07.462: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n"
    Feb 12 11:57:07.462: INFO: stdout: "\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h\naffinity-nodeport-transition-q59fx\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-cqh5h"
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-q59fx
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.462: INFO: Received response from host: affinity-nodeport-transition-cqh5h
    Feb 12 11:57:07.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-185 exec execpod-affinityrw6s4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:30345/ ; done'
    Feb 12 11:57:07.665: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:30345/\n"
    Feb 12 11:57:07.665: INFO: stdout: "\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr\naffinity-nodeport-transition-xpmhr"
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Received response from host: affinity-nodeport-transition-xpmhr
    Feb 12 11:57:07.665: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-185, will wait for the garbage collector to delete the pods 02/12/23 11:57:07.69
    Feb 12 11:57:07.756: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.446101ms
    Feb 12 11:57:07.856: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.662125ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 11:57:10.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-185" for this suite. 02/12/23 11:57:10.301
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:57:10.317
Feb 12 11:57:10.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replicaset 02/12/23 11:57:10.318
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:10.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:10.345
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/12/23 11:57:10.353
Feb 12 11:57:10.367: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 12 11:57:15.372: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/12/23 11:57:15.372
STEP: getting scale subresource 02/12/23 11:57:15.373
STEP: updating a scale subresource 02/12/23 11:57:15.381
STEP: verifying the replicaset Spec.Replicas was modified 02/12/23 11:57:15.392
STEP: Patch a scale subresource 02/12/23 11:57:15.396
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 12 11:57:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9101" for this suite. 02/12/23 11:57:15.426
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":209,"skipped":3674,"failed":0}
------------------------------
 [SLOW TEST] [5.127 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:57:10.317
    Feb 12 11:57:10.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replicaset 02/12/23 11:57:10.318
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:10.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:10.345
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/12/23 11:57:10.353
    Feb 12 11:57:10.367: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 12 11:57:15.372: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/12/23 11:57:15.372
    STEP: getting scale subresource 02/12/23 11:57:15.373
    STEP: updating a scale subresource 02/12/23 11:57:15.381
    STEP: verifying the replicaset Spec.Replicas was modified 02/12/23 11:57:15.392
    STEP: Patch a scale subresource 02/12/23 11:57:15.396
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 12 11:57:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-9101" for this suite. 02/12/23 11:57:15.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:57:15.446
Feb 12 11:57:15.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 11:57:15.447
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:15.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:15.486
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 02/12/23 11:57:15.489
Feb 12 11:57:15.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93" in namespace "projected-6178" to be "Succeeded or Failed"
Feb 12 11:57:15.505: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.689744ms
Feb 12 11:57:17.522: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022642185s
Feb 12 11:57:19.518: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018416154s
STEP: Saw pod success 02/12/23 11:57:19.518
Feb 12 11:57:19.519: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93" satisfied condition "Succeeded or Failed"
Feb 12 11:57:19.531: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93 container client-container: <nil>
STEP: delete the pod 02/12/23 11:57:19.577
Feb 12 11:57:19.609: INFO: Waiting for pod downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93 to disappear
Feb 12 11:57:19.612: INFO: Pod downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 11:57:19.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6178" for this suite. 02/12/23 11:57:19.615
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":210,"skipped":3697,"failed":0}
------------------------------
 [4.176 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:57:15.446
    Feb 12 11:57:15.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 11:57:15.447
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:15.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:15.486
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 02/12/23 11:57:15.489
    Feb 12 11:57:15.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93" in namespace "projected-6178" to be "Succeeded or Failed"
    Feb 12 11:57:15.505: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.689744ms
    Feb 12 11:57:17.522: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022642185s
    Feb 12 11:57:19.518: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018416154s
    STEP: Saw pod success 02/12/23 11:57:19.518
    Feb 12 11:57:19.519: INFO: Pod "downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93" satisfied condition "Succeeded or Failed"
    Feb 12 11:57:19.531: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93 container client-container: <nil>
    STEP: delete the pod 02/12/23 11:57:19.577
    Feb 12 11:57:19.609: INFO: Waiting for pod downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93 to disappear
    Feb 12 11:57:19.612: INFO: Pod downwardapi-volume-04a0c6d9-14af-4395-b370-ce46bcd1db93 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 11:57:19.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6178" for this suite. 02/12/23 11:57:19.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:57:19.627
Feb 12 11:57:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 11:57:19.628
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:19.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:19.645
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9290 02/12/23 11:57:19.647
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 02/12/23 11:57:19.656
Feb 12 11:57:19.671: INFO: Found 0 stateful pods, waiting for 3
Feb 12 11:57:29.688: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 11:57:29.688: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 11:57:29.689: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 11:57:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 11:57:29.894: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 11:57:29.894: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 11:57:29.894: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/12/23 11:57:41.499
Feb 12 11:57:41.622: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/12/23 11:57:41.622
STEP: Updating Pods in reverse ordinal order 02/12/23 11:57:51.671
Feb 12 11:57:51.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 11:57:51.849: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 11:57:51.849: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 11:57:51.849: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 11:58:11.911: INFO: Waiting for StatefulSet statefulset-9290/ss2 to complete update
STEP: Rolling back to a previous revision 02/12/23 11:58:21.932
Feb 12 11:58:21.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 11:58:22.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 11:58:22.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 11:58:22.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 11:58:35.320: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/12/23 11:58:45.485
Feb 12 11:58:45.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 11:58:45.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 11:58:45.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 11:58:45.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 11:59:05.653: INFO: Deleting all statefulset in ns statefulset-9290
Feb 12 11:59:05.662: INFO: Scaling statefulset ss2 to 0
Feb 12 11:59:15.703: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 11:59:15.715: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 11:59:15.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9290" for this suite. 02/12/23 11:59:15.753
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":211,"skipped":3717,"failed":0}
------------------------------
 [SLOW TEST] [116.137 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:57:19.627
    Feb 12 11:57:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 11:57:19.628
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:57:19.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:57:19.645
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9290 02/12/23 11:57:19.647
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 02/12/23 11:57:19.656
    Feb 12 11:57:19.671: INFO: Found 0 stateful pods, waiting for 3
    Feb 12 11:57:29.688: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 11:57:29.688: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 11:57:29.689: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 11:57:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 11:57:29.894: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 11:57:29.894: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 11:57:29.894: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/12/23 11:57:41.499
    Feb 12 11:57:41.622: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/12/23 11:57:41.622
    STEP: Updating Pods in reverse ordinal order 02/12/23 11:57:51.671
    Feb 12 11:57:51.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 11:57:51.849: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 11:57:51.849: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 11:57:51.849: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 11:58:11.911: INFO: Waiting for StatefulSet statefulset-9290/ss2 to complete update
    STEP: Rolling back to a previous revision 02/12/23 11:58:21.932
    Feb 12 11:58:21.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 11:58:22.164: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 11:58:22.164: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 11:58:22.164: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 11:58:35.320: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/12/23 11:58:45.485
    Feb 12 11:58:45.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-9290 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 11:58:45.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 11:58:45.602: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 11:58:45.602: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 11:59:05.653: INFO: Deleting all statefulset in ns statefulset-9290
    Feb 12 11:59:05.662: INFO: Scaling statefulset ss2 to 0
    Feb 12 11:59:15.703: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 11:59:15.715: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 11:59:15.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9290" for this suite. 02/12/23 11:59:15.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:59:15.767
Feb 12 11:59:15.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename endpointslice 02/12/23 11:59:15.767
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:15.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:15.785
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Feb 12 11:59:15.794: INFO: Endpoints addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
Feb 12 11:59:15.794: INFO: EndpointSlices addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 12 11:59:15.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8735" for this suite. 02/12/23 11:59:15.797
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":212,"skipped":3839,"failed":0}
------------------------------
 [0.040 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:59:15.767
    Feb 12 11:59:15.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename endpointslice 02/12/23 11:59:15.767
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:15.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:15.785
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Feb 12 11:59:15.794: INFO: Endpoints addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
    Feb 12 11:59:15.794: INFO: EndpointSlices addresses: [10.2.20.101 10.2.20.102] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 12 11:59:15.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8735" for this suite. 02/12/23 11:59:15.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:59:15.81
Feb 12 11:59:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename endpointslicemirroring 02/12/23 11:59:15.81
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:15.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:15.835
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/12/23 11:59:15.852
Feb 12 11:59:15.866: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/12/23 11:59:17.881
STEP: mirroring deletion of a custom Endpoint 02/12/23 11:59:17.898
Feb 12 11:59:17.916: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Feb 12 11:59:19.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2431" for this suite. 02/12/23 11:59:19.938
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":213,"skipped":3869,"failed":0}
------------------------------
 [4.148 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:59:15.81
    Feb 12 11:59:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename endpointslicemirroring 02/12/23 11:59:15.81
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:15.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:15.835
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/12/23 11:59:15.852
    Feb 12 11:59:15.866: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/12/23 11:59:17.881
    STEP: mirroring deletion of a custom Endpoint 02/12/23 11:59:17.898
    Feb 12 11:59:17.916: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Feb 12 11:59:19.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-2431" for this suite. 02/12/23 11:59:19.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:59:19.963
Feb 12 11:59:19.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:59:19.964
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:19.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:19.987
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 12 11:59:19.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 11:59:28.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8245" for this suite. 02/12/23 11:59:28.331
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":214,"skipped":3976,"failed":0}
------------------------------
 [SLOW TEST] [8.375 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:59:19.963
    Feb 12 11:59:19.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 11:59:19.964
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:19.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:19.987
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 12 11:59:19.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 11:59:28.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8245" for this suite. 02/12/23 11:59:28.331
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 11:59:28.341
Feb 12 11:59:28.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 11:59:28.343
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:28.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:28.374
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-844fbf20-7414-48ed-a51a-92637588254f in namespace container-probe-1948 02/12/23 11:59:28.377
Feb 12 11:59:28.388: INFO: Waiting up to 5m0s for pod "liveness-844fbf20-7414-48ed-a51a-92637588254f" in namespace "container-probe-1948" to be "not pending"
Feb 12 11:59:28.395: INFO: Pod "liveness-844fbf20-7414-48ed-a51a-92637588254f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.902148ms
Feb 12 11:59:30.411: INFO: Pod "liveness-844fbf20-7414-48ed-a51a-92637588254f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022443463s
Feb 12 11:59:30.411: INFO: Pod "liveness-844fbf20-7414-48ed-a51a-92637588254f" satisfied condition "not pending"
Feb 12 11:59:30.411: INFO: Started pod liveness-844fbf20-7414-48ed-a51a-92637588254f in namespace container-probe-1948
STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:59:30.411
Feb 12 11:59:30.426: INFO: Initial restart count of pod liveness-844fbf20-7414-48ed-a51a-92637588254f is 0
Feb 12 11:59:51.065: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 1 (20.638191371s elapsed)
Feb 12 12:00:14.158: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 2 (43.730954344s elapsed)
Feb 12 12:00:30.268: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 3 (59.841274018s elapsed)
Feb 12 12:00:50.389: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 4 (1m19.962281488s elapsed)
Feb 12 12:01:50.759: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 5 (2m20.332390453s elapsed)
STEP: deleting the pod 02/12/23 12:01:50.759
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 12:01:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1948" for this suite. 02/12/23 12:01:50.802
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":215,"skipped":3978,"failed":0}
------------------------------
 [SLOW TEST] [142.470 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 11:59:28.341
    Feb 12 11:59:28.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 11:59:28.343
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 11:59:28.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 11:59:28.374
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-844fbf20-7414-48ed-a51a-92637588254f in namespace container-probe-1948 02/12/23 11:59:28.377
    Feb 12 11:59:28.388: INFO: Waiting up to 5m0s for pod "liveness-844fbf20-7414-48ed-a51a-92637588254f" in namespace "container-probe-1948" to be "not pending"
    Feb 12 11:59:28.395: INFO: Pod "liveness-844fbf20-7414-48ed-a51a-92637588254f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.902148ms
    Feb 12 11:59:30.411: INFO: Pod "liveness-844fbf20-7414-48ed-a51a-92637588254f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022443463s
    Feb 12 11:59:30.411: INFO: Pod "liveness-844fbf20-7414-48ed-a51a-92637588254f" satisfied condition "not pending"
    Feb 12 11:59:30.411: INFO: Started pod liveness-844fbf20-7414-48ed-a51a-92637588254f in namespace container-probe-1948
    STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 11:59:30.411
    Feb 12 11:59:30.426: INFO: Initial restart count of pod liveness-844fbf20-7414-48ed-a51a-92637588254f is 0
    Feb 12 11:59:51.065: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 1 (20.638191371s elapsed)
    Feb 12 12:00:14.158: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 2 (43.730954344s elapsed)
    Feb 12 12:00:30.268: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 3 (59.841274018s elapsed)
    Feb 12 12:00:50.389: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 4 (1m19.962281488s elapsed)
    Feb 12 12:01:50.759: INFO: Restart count of pod container-probe-1948/liveness-844fbf20-7414-48ed-a51a-92637588254f is now 5 (2m20.332390453s elapsed)
    STEP: deleting the pod 02/12/23 12:01:50.759
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 12:01:50.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1948" for this suite. 02/12/23 12:01:50.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:01:50.815
Feb 12 12:01:50.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 12:01:50.815
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:01:50.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:01:50.838
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 12 12:01:50.840: INFO: Creating deployment "test-recreate-deployment"
Feb 12 12:01:50.846: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 12 12:01:50.854: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 12 12:01:52.865: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 12 12:01:52.870: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 12 12:01:52.889: INFO: Updating deployment test-recreate-deployment
Feb 12 12:01:52.889: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 12:01:53.037: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-527  158d8439-8087-4bc7-9d51-716eb67fa337 22854 2 2023-02-12 12:01:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059af438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-12 12:01:53 +0000 UTC,LastTransitionTime:2023-02-12 12:01:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-12 12:01:53 +0000 UTC,LastTransitionTime:2023-02-12 12:01:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 12 12:01:53.042: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-527  558ce4f1-9659-49fa-b097-568c447e839e 22850 1 2023-02-12 12:01:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 158d8439-8087-4bc7-9d51-716eb67fa337 0xc0059af900 0xc0059af901}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"158d8439-8087-4bc7-9d51-716eb67fa337\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059af998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:01:53.042: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 12 12:01:53.042: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-527  a686720e-05ce-4a87-baaf-a32605078d3d 22842 2 2023-02-12 12:01:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 158d8439-8087-4bc7-9d51-716eb67fa337 0xc0059af7e7 0xc0059af7e8}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"158d8439-8087-4bc7-9d51-716eb67fa337\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059af898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:01:53.046: INFO: Pod "test-recreate-deployment-9d58999df-jjp9r" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-jjp9r test-recreate-deployment-9d58999df- deployment-527  64a81d3b-5a0e-4f90-bc8b-10985c902894 22853 0 2023-02-12 12:01:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 558ce4f1-9659-49fa-b097-568c447e839e 0xc0059afdd0 0xc0059afdd1}] [] [{kube-controller-manager Update v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"558ce4f1-9659-49fa-b097-568c447e839e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:01:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7s6zg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7s6zg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:01:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 12:01:53.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-527" for this suite. 02/12/23 12:01:53.059
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":216,"skipped":4066,"failed":0}
------------------------------
 [2.262 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:01:50.815
    Feb 12 12:01:50.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 12:01:50.815
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:01:50.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:01:50.838
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 12 12:01:50.840: INFO: Creating deployment "test-recreate-deployment"
    Feb 12 12:01:50.846: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 12 12:01:50.854: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 12 12:01:52.865: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 12 12:01:52.870: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 12 12:01:52.889: INFO: Updating deployment test-recreate-deployment
    Feb 12 12:01:52.889: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 12:01:53.037: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-527  158d8439-8087-4bc7-9d51-716eb67fa337 22854 2 2023-02-12 12:01:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059af438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-12 12:01:53 +0000 UTC,LastTransitionTime:2023-02-12 12:01:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-02-12 12:01:53 +0000 UTC,LastTransitionTime:2023-02-12 12:01:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 12 12:01:53.042: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-527  558ce4f1-9659-49fa-b097-568c447e839e 22850 1 2023-02-12 12:01:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 158d8439-8087-4bc7-9d51-716eb67fa337 0xc0059af900 0xc0059af901}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"158d8439-8087-4bc7-9d51-716eb67fa337\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059af998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:01:53.042: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 12 12:01:53.042: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-527  a686720e-05ce-4a87-baaf-a32605078d3d 22842 2 2023-02-12 12:01:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 158d8439-8087-4bc7-9d51-716eb67fa337 0xc0059af7e7 0xc0059af7e8}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"158d8439-8087-4bc7-9d51-716eb67fa337\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059af898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:01:53.046: INFO: Pod "test-recreate-deployment-9d58999df-jjp9r" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-jjp9r test-recreate-deployment-9d58999df- deployment-527  64a81d3b-5a0e-4f90-bc8b-10985c902894 22853 0 2023-02-12 12:01:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 558ce4f1-9659-49fa-b097-568c447e839e 0xc0059afdd0 0xc0059afdd1}] [] [{kube-controller-manager Update v1 2023-02-12 12:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"558ce4f1-9659-49fa-b097-568c447e839e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:01:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7s6zg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7s6zg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:01:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:01:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 12:01:53.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-527" for this suite. 02/12/23 12:01:53.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:01:53.078
Feb 12 12:01:53.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replication-controller 02/12/23 12:01:53.078
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:01:53.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:01:53.096
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 02/12/23 12:01:53.099
Feb 12 12:01:53.107: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9436" to be "running and ready"
Feb 12 12:01:53.118: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 11.469683ms
Feb 12 12:01:53.118: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:01:55.123: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.016150538s
Feb 12 12:01:55.123: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 12 12:01:55.123: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/12/23 12:01:55.126
STEP: Then the orphan pod is adopted 02/12/23 12:01:55.136
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 12 12:01:56.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9436" for this suite. 02/12/23 12:01:56.155
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":217,"skipped":4093,"failed":0}
------------------------------
 [3.087 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:01:53.078
    Feb 12 12:01:53.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replication-controller 02/12/23 12:01:53.078
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:01:53.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:01:53.096
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/12/23 12:01:53.099
    Feb 12 12:01:53.107: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-9436" to be "running and ready"
    Feb 12 12:01:53.118: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 11.469683ms
    Feb 12 12:01:53.118: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:01:55.123: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.016150538s
    Feb 12 12:01:55.123: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 12 12:01:55.123: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/12/23 12:01:55.126
    STEP: Then the orphan pod is adopted 02/12/23 12:01:55.136
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 12 12:01:56.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9436" for this suite. 02/12/23 12:01:56.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:01:56.167
Feb 12 12:01:56.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:01:56.167
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:01:56.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:01:56.185
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:01:56.203
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:01:56.688
STEP: Deploying the webhook pod 02/12/23 12:01:56.698
STEP: Wait for the deployment to be ready 02/12/23 12:01:56.709
Feb 12 12:01:56.717: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:01:58.755
STEP: Verifying the service has paired with the endpoint 02/12/23 12:01:58.778
Feb 12 12:01:59.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Feb 12 12:01:59.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9663-crds.webhook.example.com via the AdmissionRegistration API 02/12/23 12:02:05.325
STEP: Creating a custom resource that should be mutated by the webhook 02/12/23 12:02:05.408
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:02:08.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5165" for this suite. 02/12/23 12:02:08.023
STEP: Destroying namespace "webhook-5165-markers" for this suite. 02/12/23 12:02:08.036
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":218,"skipped":4101,"failed":0}
------------------------------
 [SLOW TEST] [11.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:01:56.167
    Feb 12 12:01:56.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:01:56.167
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:01:56.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:01:56.185
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:01:56.203
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:01:56.688
    STEP: Deploying the webhook pod 02/12/23 12:01:56.698
    STEP: Wait for the deployment to be ready 02/12/23 12:01:56.709
    Feb 12 12:01:56.717: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:01:58.755
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:01:58.778
    Feb 12 12:01:59.779: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Feb 12 12:01:59.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9663-crds.webhook.example.com via the AdmissionRegistration API 02/12/23 12:02:05.325
    STEP: Creating a custom resource that should be mutated by the webhook 02/12/23 12:02:05.408
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:02:08.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5165" for this suite. 02/12/23 12:02:08.023
    STEP: Destroying namespace "webhook-5165-markers" for this suite. 02/12/23 12:02:08.036
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:02:08.128
Feb 12 12:02:08.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:02:08.129
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:08.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:08.178
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 02/12/23 12:02:08.186
Feb 12 12:02:08.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8438 cluster-info'
Feb 12 12:02:08.311: INFO: stderr: ""
Feb 12 12:02:08.311: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:02:08.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8438" for this suite. 02/12/23 12:02:08.317
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":219,"skipped":4132,"failed":0}
------------------------------
 [0.198 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:02:08.128
    Feb 12 12:02:08.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:02:08.129
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:08.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:08.178
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 02/12/23 12:02:08.186
    Feb 12 12:02:08.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8438 cluster-info'
    Feb 12 12:02:08.311: INFO: stderr: ""
    Feb 12 12:02:08.311: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:02:08.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8438" for this suite. 02/12/23 12:02:08.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:02:08.327
Feb 12 12:02:08.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:02:08.328
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:08.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:08.357
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 02/12/23 12:02:08.359
Feb 12 12:02:08.370: INFO: Waiting up to 5m0s for pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238" in namespace "downward-api-5524" to be "Succeeded or Failed"
Feb 12 12:02:08.374: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903308ms
Feb 12 12:02:10.570: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199041986s
Feb 12 12:02:12.437: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Running", Reason="", readiness=false. Elapsed: 4.066973494s
Feb 12 12:02:14.391: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020735742s
STEP: Saw pod success 02/12/23 12:02:14.391
Feb 12 12:02:14.392: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238" satisfied condition "Succeeded or Failed"
Feb 12 12:02:14.406: INFO: Trying to get logs from node kube-3 pod downward-api-077cf292-d710-436f-8af7-5c9be430f238 container dapi-container: <nil>
STEP: delete the pod 02/12/23 12:02:14.446
Feb 12 12:02:14.463: INFO: Waiting for pod downward-api-077cf292-d710-436f-8af7-5c9be430f238 to disappear
Feb 12 12:02:14.469: INFO: Pod downward-api-077cf292-d710-436f-8af7-5c9be430f238 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 12 12:02:14.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5524" for this suite. 02/12/23 12:02:14.474
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":220,"skipped":4139,"failed":0}
------------------------------
 [SLOW TEST] [6.152 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:02:08.327
    Feb 12 12:02:08.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:02:08.328
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:08.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:08.357
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 02/12/23 12:02:08.359
    Feb 12 12:02:08.370: INFO: Waiting up to 5m0s for pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238" in namespace "downward-api-5524" to be "Succeeded or Failed"
    Feb 12 12:02:08.374: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903308ms
    Feb 12 12:02:10.570: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199041986s
    Feb 12 12:02:12.437: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Running", Reason="", readiness=false. Elapsed: 4.066973494s
    Feb 12 12:02:14.391: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020735742s
    STEP: Saw pod success 02/12/23 12:02:14.391
    Feb 12 12:02:14.392: INFO: Pod "downward-api-077cf292-d710-436f-8af7-5c9be430f238" satisfied condition "Succeeded or Failed"
    Feb 12 12:02:14.406: INFO: Trying to get logs from node kube-3 pod downward-api-077cf292-d710-436f-8af7-5c9be430f238 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 12:02:14.446
    Feb 12 12:02:14.463: INFO: Waiting for pod downward-api-077cf292-d710-436f-8af7-5c9be430f238 to disappear
    Feb 12 12:02:14.469: INFO: Pod downward-api-077cf292-d710-436f-8af7-5c9be430f238 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 12 12:02:14.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5524" for this suite. 02/12/23 12:02:14.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:02:14.484
Feb 12 12:02:14.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:02:14.485
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:14.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:14.501
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:02:14.518
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:02:15.075
STEP: Deploying the webhook pod 02/12/23 12:02:15.08
STEP: Wait for the deployment to be ready 02/12/23 12:02:15.094
Feb 12 12:02:15.105: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:02:17.139
STEP: Verifying the service has paired with the endpoint 02/12/23 12:02:17.159
Feb 12 12:02:18.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Feb 12 12:02:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/12/23 12:02:23.681
STEP: Creating a custom resource that should be denied by the webhook 02/12/23 12:02:23.711
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/12/23 12:02:25.776
STEP: Updating the custom resource with disallowed data should be denied 02/12/23 12:02:25.805
STEP: Deleting the custom resource should be denied 02/12/23 12:02:25.82
STEP: Remove the offending key and value from the custom resource data 02/12/23 12:02:25.827
STEP: Deleting the updated custom resource should be successful 02/12/23 12:02:25.838
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:02:26.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4127" for this suite. 02/12/23 12:02:26.381
STEP: Destroying namespace "webhook-4127-markers" for this suite. 02/12/23 12:02:26.397
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":221,"skipped":4157,"failed":0}
------------------------------
 [SLOW TEST] [11.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:02:14.484
    Feb 12 12:02:14.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:02:14.485
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:14.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:14.501
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:02:14.518
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:02:15.075
    STEP: Deploying the webhook pod 02/12/23 12:02:15.08
    STEP: Wait for the deployment to be ready 02/12/23 12:02:15.094
    Feb 12 12:02:15.105: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:02:17.139
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:02:17.159
    Feb 12 12:02:18.160: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Feb 12 12:02:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/12/23 12:02:23.681
    STEP: Creating a custom resource that should be denied by the webhook 02/12/23 12:02:23.711
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/12/23 12:02:25.776
    STEP: Updating the custom resource with disallowed data should be denied 02/12/23 12:02:25.805
    STEP: Deleting the custom resource should be denied 02/12/23 12:02:25.82
    STEP: Remove the offending key and value from the custom resource data 02/12/23 12:02:25.827
    STEP: Deleting the updated custom resource should be successful 02/12/23 12:02:25.838
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:02:26.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4127" for this suite. 02/12/23 12:02:26.381
    STEP: Destroying namespace "webhook-4127-markers" for this suite. 02/12/23 12:02:26.397
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:02:26.48
Feb 12 12:02:26.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 12:02:26.481
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:26.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:26.516
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-d78301bf-fa0d-4364-af8b-83899c42d1da in namespace container-probe-9796 02/12/23 12:02:26.522
Feb 12 12:02:26.542: INFO: Waiting up to 5m0s for pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da" in namespace "container-probe-9796" to be "not pending"
Feb 12 12:02:26.552: INFO: Pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452192ms
Feb 12 12:02:28.592: INFO: Pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.050259769s
Feb 12 12:02:28.592: INFO: Pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da" satisfied condition "not pending"
Feb 12 12:02:28.592: INFO: Started pod liveness-d78301bf-fa0d-4364-af8b-83899c42d1da in namespace container-probe-9796
STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 12:02:28.592
Feb 12 12:02:28.596: INFO: Initial restart count of pod liveness-d78301bf-fa0d-4364-af8b-83899c42d1da is 0
STEP: deleting the pod 02/12/23 12:06:30.167
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 12:06:30.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9796" for this suite. 02/12/23 12:06:30.2
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":222,"skipped":4157,"failed":0}
------------------------------
 [SLOW TEST] [243.730 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:02:26.48
    Feb 12 12:02:26.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 12:02:26.481
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:02:26.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:02:26.516
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-d78301bf-fa0d-4364-af8b-83899c42d1da in namespace container-probe-9796 02/12/23 12:02:26.522
    Feb 12 12:02:26.542: INFO: Waiting up to 5m0s for pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da" in namespace "container-probe-9796" to be "not pending"
    Feb 12 12:02:26.552: INFO: Pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452192ms
    Feb 12 12:02:28.592: INFO: Pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da": Phase="Running", Reason="", readiness=true. Elapsed: 2.050259769s
    Feb 12 12:02:28.592: INFO: Pod "liveness-d78301bf-fa0d-4364-af8b-83899c42d1da" satisfied condition "not pending"
    Feb 12 12:02:28.592: INFO: Started pod liveness-d78301bf-fa0d-4364-af8b-83899c42d1da in namespace container-probe-9796
    STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 12:02:28.592
    Feb 12 12:02:28.596: INFO: Initial restart count of pod liveness-d78301bf-fa0d-4364-af8b-83899c42d1da is 0
    STEP: deleting the pod 02/12/23 12:06:30.167
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 12:06:30.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9796" for this suite. 02/12/23 12:06:30.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:06:30.215
Feb 12 12:06:30.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 12:06:30.22
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:30.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:30.247
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Feb 12 12:06:30.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/12/23 12:06:37.486
Feb 12 12:06:37.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
Feb 12 12:06:38.001: INFO: stderr: ""
Feb 12 12:06:38.001: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 12 12:06:38.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 delete e2e-test-crd-publish-openapi-7830-crds test-foo'
Feb 12 12:06:38.079: INFO: stderr: ""
Feb 12 12:06:38.079: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 12 12:06:38.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 apply -f -'
Feb 12 12:06:38.268: INFO: stderr: ""
Feb 12 12:06:38.268: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 12 12:06:38.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 delete e2e-test-crd-publish-openapi-7830-crds test-foo'
Feb 12 12:06:38.324: INFO: stderr: ""
Feb 12 12:06:38.324: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/12/23 12:06:38.324
Feb 12 12:06:38.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
Feb 12 12:06:38.464: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/12/23 12:06:38.464
Feb 12 12:06:38.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
Feb 12 12:06:38.883: INFO: rc: 1
Feb 12 12:06:38.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 apply -f -'
Feb 12 12:06:39.035: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/12/23 12:06:39.035
Feb 12 12:06:39.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
Feb 12 12:06:39.169: INFO: rc: 1
Feb 12 12:06:39.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 apply -f -'
Feb 12 12:06:39.326: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/12/23 12:06:39.326
Feb 12 12:06:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds'
Feb 12 12:06:39.500: INFO: stderr: ""
Feb 12 12:06:39.500: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/12/23 12:06:39.5
Feb 12 12:06:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.metadata'
Feb 12 12:06:39.640: INFO: stderr: ""
Feb 12 12:06:39.640: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 12 12:06:39.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.spec'
Feb 12 12:06:39.778: INFO: stderr: ""
Feb 12 12:06:39.778: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 12 12:06:39.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.spec.bars'
Feb 12 12:06:39.924: INFO: stderr: ""
Feb 12 12:06:39.924: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/12/23 12:06:39.924
Feb 12 12:06:39.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.spec.bars2'
Feb 12 12:06:40.060: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:06:46.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6099" for this suite. 02/12/23 12:06:46.198
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":223,"skipped":4176,"failed":0}
------------------------------
 [SLOW TEST] [15.998 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:06:30.215
    Feb 12 12:06:30.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename crd-publish-openapi 02/12/23 12:06:30.22
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:30.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:30.247
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Feb 12 12:06:30.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/12/23 12:06:37.486
    Feb 12 12:06:37.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
    Feb 12 12:06:38.001: INFO: stderr: ""
    Feb 12 12:06:38.001: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 12 12:06:38.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 delete e2e-test-crd-publish-openapi-7830-crds test-foo'
    Feb 12 12:06:38.079: INFO: stderr: ""
    Feb 12 12:06:38.079: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 12 12:06:38.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 apply -f -'
    Feb 12 12:06:38.268: INFO: stderr: ""
    Feb 12 12:06:38.268: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 12 12:06:38.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 delete e2e-test-crd-publish-openapi-7830-crds test-foo'
    Feb 12 12:06:38.324: INFO: stderr: ""
    Feb 12 12:06:38.324: INFO: stdout: "e2e-test-crd-publish-openapi-7830-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/12/23 12:06:38.324
    Feb 12 12:06:38.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
    Feb 12 12:06:38.464: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/12/23 12:06:38.464
    Feb 12 12:06:38.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
    Feb 12 12:06:38.883: INFO: rc: 1
    Feb 12 12:06:38.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 apply -f -'
    Feb 12 12:06:39.035: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/12/23 12:06:39.035
    Feb 12 12:06:39.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 create -f -'
    Feb 12 12:06:39.169: INFO: rc: 1
    Feb 12 12:06:39.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 --namespace=crd-publish-openapi-6099 apply -f -'
    Feb 12 12:06:39.326: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/12/23 12:06:39.326
    Feb 12 12:06:39.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds'
    Feb 12 12:06:39.500: INFO: stderr: ""
    Feb 12 12:06:39.500: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/12/23 12:06:39.5
    Feb 12 12:06:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.metadata'
    Feb 12 12:06:39.640: INFO: stderr: ""
    Feb 12 12:06:39.640: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 12 12:06:39.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.spec'
    Feb 12 12:06:39.778: INFO: stderr: ""
    Feb 12 12:06:39.778: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 12 12:06:39.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.spec.bars'
    Feb 12 12:06:39.924: INFO: stderr: ""
    Feb 12 12:06:39.924: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7830-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/12/23 12:06:39.924
    Feb 12 12:06:39.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=crd-publish-openapi-6099 explain e2e-test-crd-publish-openapi-7830-crds.spec.bars2'
    Feb 12 12:06:40.060: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:06:46.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6099" for this suite. 02/12/23 12:06:46.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:06:46.213
Feb 12 12:06:46.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 12:06:46.214
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:46.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:46.283
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 02/12/23 12:06:46.288
Feb 12 12:06:46.323: INFO: Waiting up to 5m0s for pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81" in namespace "var-expansion-5761" to be "Succeeded or Failed"
Feb 12 12:06:46.328: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699038ms
Feb 12 12:06:48.334: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010483662s
Feb 12 12:06:50.333: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009162993s
STEP: Saw pod success 02/12/23 12:06:50.333
Feb 12 12:06:50.333: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81" satisfied condition "Succeeded or Failed"
Feb 12 12:06:50.337: INFO: Trying to get logs from node kube-3 pod var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81 container dapi-container: <nil>
STEP: delete the pod 02/12/23 12:06:50.348
Feb 12 12:06:50.370: INFO: Waiting for pod var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81 to disappear
Feb 12 12:06:50.373: INFO: Pod var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 12:06:50.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5761" for this suite. 02/12/23 12:06:50.377
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":224,"skipped":4189,"failed":0}
------------------------------
 [4.172 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:06:46.213
    Feb 12 12:06:46.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 12:06:46.214
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:46.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:46.283
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 02/12/23 12:06:46.288
    Feb 12 12:06:46.323: INFO: Waiting up to 5m0s for pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81" in namespace "var-expansion-5761" to be "Succeeded or Failed"
    Feb 12 12:06:46.328: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699038ms
    Feb 12 12:06:48.334: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010483662s
    Feb 12 12:06:50.333: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009162993s
    STEP: Saw pod success 02/12/23 12:06:50.333
    Feb 12 12:06:50.333: INFO: Pod "var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81" satisfied condition "Succeeded or Failed"
    Feb 12 12:06:50.337: INFO: Trying to get logs from node kube-3 pod var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 12:06:50.348
    Feb 12 12:06:50.370: INFO: Waiting for pod var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81 to disappear
    Feb 12 12:06:50.373: INFO: Pod var-expansion-d9e74ed8-bff1-45d1-8d6f-86d8b0ea3e81 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 12:06:50.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5761" for this suite. 02/12/23 12:06:50.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:06:50.389
Feb 12 12:06:50.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubelet-test 02/12/23 12:06:50.39
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:50.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:50.413
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 12 12:06:54.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8982" for this suite. 02/12/23 12:06:54.454
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":225,"skipped":4237,"failed":0}
------------------------------
 [4.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:06:50.389
    Feb 12 12:06:50.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubelet-test 02/12/23 12:06:50.39
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:50.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:50.413
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 12 12:06:54.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8982" for this suite. 02/12/23 12:06:54.454
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:06:54.464
Feb 12 12:06:54.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:06:54.465
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:54.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:54.488
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-fd214877-e6e3-480b-9fc1-6240fe8e7357 02/12/23 12:06:54.493
STEP: Creating secret with name s-test-opt-upd-861294f3-1d2c-4659-a290-1e3e92c44541 02/12/23 12:06:54.498
STEP: Creating the pod 02/12/23 12:06:54.503
Feb 12 12:06:54.514: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5" in namespace "projected-3960" to be "running and ready"
Feb 12 12:06:54.517: INFO: Pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612419ms
Feb 12 12:06:54.517: INFO: The phase of Pod pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:06:56.532: INFO: Pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5": Phase="Running", Reason="", readiness=true. Elapsed: 2.018588355s
Feb 12 12:06:56.532: INFO: The phase of Pod pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5 is Running (Ready = true)
Feb 12 12:06:56.532: INFO: Pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-fd214877-e6e3-480b-9fc1-6240fe8e7357 02/12/23 12:06:56.586
STEP: Updating secret s-test-opt-upd-861294f3-1d2c-4659-a290-1e3e92c44541 02/12/23 12:06:56.592
STEP: Creating secret with name s-test-opt-create-257a1b87-8196-4951-91fb-5b6d1c448786 02/12/23 12:06:56.598
STEP: waiting to observe update in volume 02/12/23 12:06:56.603
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 12:06:58.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3960" for this suite. 02/12/23 12:06:58.632
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":226,"skipped":4241,"failed":0}
------------------------------
 [4.177 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:06:54.464
    Feb 12 12:06:54.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:06:54.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:54.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:54.488
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-fd214877-e6e3-480b-9fc1-6240fe8e7357 02/12/23 12:06:54.493
    STEP: Creating secret with name s-test-opt-upd-861294f3-1d2c-4659-a290-1e3e92c44541 02/12/23 12:06:54.498
    STEP: Creating the pod 02/12/23 12:06:54.503
    Feb 12 12:06:54.514: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5" in namespace "projected-3960" to be "running and ready"
    Feb 12 12:06:54.517: INFO: Pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612419ms
    Feb 12 12:06:54.517: INFO: The phase of Pod pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:06:56.532: INFO: Pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5": Phase="Running", Reason="", readiness=true. Elapsed: 2.018588355s
    Feb 12 12:06:56.532: INFO: The phase of Pod pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5 is Running (Ready = true)
    Feb 12 12:06:56.532: INFO: Pod "pod-projected-secrets-cc9eb70c-dd8a-4188-aa06-08455c36ddf5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-fd214877-e6e3-480b-9fc1-6240fe8e7357 02/12/23 12:06:56.586
    STEP: Updating secret s-test-opt-upd-861294f3-1d2c-4659-a290-1e3e92c44541 02/12/23 12:06:56.592
    STEP: Creating secret with name s-test-opt-create-257a1b87-8196-4951-91fb-5b6d1c448786 02/12/23 12:06:56.598
    STEP: waiting to observe update in volume 02/12/23 12:06:56.603
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 12:06:58.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3960" for this suite. 02/12/23 12:06:58.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:06:58.642
Feb 12 12:06:58.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir-wrapper 02/12/23 12:06:58.642
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:58.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:58.668
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/12/23 12:06:58.67
STEP: Creating RC which spawns configmap-volume pods 02/12/23 12:06:58.923
Feb 12 12:06:59.024: INFO: Pod name wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f: Found 3 pods out of 5
Feb 12 12:07:04.033: INFO: Pod name wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/12/23 12:07:04.033
Feb 12 12:07:04.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:04.040: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.306275ms
Feb 12 12:07:06.047: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013318192s
Feb 12 12:07:08.046: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012979497s
Feb 12 12:07:10.085: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051235221s
Feb 12 12:07:12.067: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033824788s
Feb 12 12:07:14.136: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102461138s
Feb 12 12:07:16.050: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Running", Reason="", readiness=true. Elapsed: 12.016640532s
Feb 12 12:07:16.050: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6" satisfied condition "running"
Feb 12 12:07:16.050: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-hzjs4" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:16.057: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-hzjs4": Phase="Running", Reason="", readiness=true. Elapsed: 7.042583ms
Feb 12 12:07:16.057: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-hzjs4" satisfied condition "running"
Feb 12 12:07:16.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xbgnj" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:16.064: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xbgnj": Phase="Running", Reason="", readiness=true. Elapsed: 7.153274ms
Feb 12 12:07:16.064: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xbgnj" satisfied condition "running"
Feb 12 12:07:16.064: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xml7z" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:16.068: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xml7z": Phase="Running", Reason="", readiness=true. Elapsed: 3.972018ms
Feb 12 12:07:16.068: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xml7z" satisfied condition "running"
Feb 12 12:07:16.068: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-zm98d" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:16.073: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-zm98d": Phase="Running", Reason="", readiness=true. Elapsed: 4.41704ms
Feb 12 12:07:16.073: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-zm98d" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f in namespace emptydir-wrapper-5056, will wait for the garbage collector to delete the pods 02/12/23 12:07:16.073
Feb 12 12:07:16.141: INFO: Deleting ReplicationController wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f took: 12.834648ms
Feb 12 12:07:16.242: INFO: Terminating ReplicationController wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f pods took: 100.958428ms
STEP: Creating RC which spawns configmap-volume pods 02/12/23 12:07:19.247
Feb 12 12:07:19.260: INFO: Pod name wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95: Found 0 pods out of 5
Feb 12 12:07:24.283: INFO: Pod name wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/12/23 12:07:24.283
Feb 12 12:07:24.283: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:24.290: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.947097ms
Feb 12 12:07:26.301: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01751621s
Feb 12 12:07:28.297: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01355846s
Feb 12 12:07:30.296: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012902588s
Feb 12 12:07:32.296: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012628195s
Feb 12 12:07:34.770: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.486099952s
Feb 12 12:07:34.770: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9" satisfied condition "running"
Feb 12 12:07:34.770: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:34.902: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2": Phase="Pending", Reason="", readiness=false. Elapsed: 132.37389ms
Feb 12 12:07:36.911: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2": Phase="Running", Reason="", readiness=true. Elapsed: 2.141388649s
Feb 12 12:07:36.911: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2" satisfied condition "running"
Feb 12 12:07:36.911: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-q7zq4" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:36.919: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-q7zq4": Phase="Running", Reason="", readiness=true. Elapsed: 8.116018ms
Feb 12 12:07:36.919: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-q7zq4" satisfied condition "running"
Feb 12 12:07:36.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-v8m5c" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:36.926: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-v8m5c": Phase="Running", Reason="", readiness=true. Elapsed: 6.248562ms
Feb 12 12:07:36.926: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-v8m5c" satisfied condition "running"
Feb 12 12:07:36.926: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-xzlgz" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:36.931: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-xzlgz": Phase="Running", Reason="", readiness=true. Elapsed: 4.902633ms
Feb 12 12:07:36.931: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-xzlgz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95 in namespace emptydir-wrapper-5056, will wait for the garbage collector to delete the pods 02/12/23 12:07:36.931
Feb 12 12:07:36.997: INFO: Deleting ReplicationController wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95 took: 12.43557ms
Feb 12 12:07:37.097: INFO: Terminating ReplicationController wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95 pods took: 100.261032ms
STEP: Creating RC which spawns configmap-volume pods 02/12/23 12:07:40.604
Feb 12 12:07:40.626: INFO: Pod name wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12: Found 0 pods out of 5
Feb 12 12:07:45.631: INFO: Pod name wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/12/23 12:07:45.632
Feb 12 12:07:45.632: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:07:45.635: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476844ms
Feb 12 12:07:47.647: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015434159s
Feb 12 12:07:49.652: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020415941s
Feb 12 12:07:51.647: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015622107s
Feb 12 12:07:54.145: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.513183173s
Feb 12 12:07:55.664: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031950228s
Feb 12 12:07:58.635: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.00311867s
Feb 12 12:08:00.399: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Running", Reason="", readiness=true. Elapsed: 14.767384082s
Feb 12 12:08:00.399: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx" satisfied condition "running"
Feb 12 12:08:00.399: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-lk6pk" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:08:00.637: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-lk6pk": Phase="Running", Reason="", readiness=true. Elapsed: 238.268137ms
Feb 12 12:08:00.637: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-lk6pk" satisfied condition "running"
Feb 12 12:08:00.637: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:08:00.646: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.677989ms
Feb 12 12:08:02.651: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn": Phase="Running", Reason="", readiness=true. Elapsed: 2.013748954s
Feb 12 12:08:02.651: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn" satisfied condition "running"
Feb 12 12:08:02.651: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-p5kgx" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:08:02.657: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-p5kgx": Phase="Running", Reason="", readiness=true. Elapsed: 5.931849ms
Feb 12 12:08:02.657: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-p5kgx" satisfied condition "running"
Feb 12 12:08:02.657: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-r2klg" in namespace "emptydir-wrapper-5056" to be "running"
Feb 12 12:08:02.661: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-r2klg": Phase="Running", Reason="", readiness=true. Elapsed: 4.174981ms
Feb 12 12:08:02.661: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-r2klg" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12 in namespace emptydir-wrapper-5056, will wait for the garbage collector to delete the pods 02/12/23 12:08:02.661
Feb 12 12:08:02.737: INFO: Deleting ReplicationController wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12 took: 20.604656ms
Feb 12 12:08:02.838: INFO: Terminating ReplicationController wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12 pods took: 101.132713ms
STEP: Cleaning up the configMaps 02/12/23 12:08:06.038
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 12 12:08:06.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5056" for this suite. 02/12/23 12:08:06.372
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":227,"skipped":4249,"failed":0}
------------------------------
 [SLOW TEST] [67.736 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:06:58.642
    Feb 12 12:06:58.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir-wrapper 02/12/23 12:06:58.642
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:06:58.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:06:58.668
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/12/23 12:06:58.67
    STEP: Creating RC which spawns configmap-volume pods 02/12/23 12:06:58.923
    Feb 12 12:06:59.024: INFO: Pod name wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f: Found 3 pods out of 5
    Feb 12 12:07:04.033: INFO: Pod name wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/12/23 12:07:04.033
    Feb 12 12:07:04.033: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:04.040: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.306275ms
    Feb 12 12:07:06.047: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013318192s
    Feb 12 12:07:08.046: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012979497s
    Feb 12 12:07:10.085: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051235221s
    Feb 12 12:07:12.067: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033824788s
    Feb 12 12:07:14.136: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102461138s
    Feb 12 12:07:16.050: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6": Phase="Running", Reason="", readiness=true. Elapsed: 12.016640532s
    Feb 12 12:07:16.050: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-7hcd6" satisfied condition "running"
    Feb 12 12:07:16.050: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-hzjs4" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:16.057: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-hzjs4": Phase="Running", Reason="", readiness=true. Elapsed: 7.042583ms
    Feb 12 12:07:16.057: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-hzjs4" satisfied condition "running"
    Feb 12 12:07:16.057: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xbgnj" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:16.064: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xbgnj": Phase="Running", Reason="", readiness=true. Elapsed: 7.153274ms
    Feb 12 12:07:16.064: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xbgnj" satisfied condition "running"
    Feb 12 12:07:16.064: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xml7z" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:16.068: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xml7z": Phase="Running", Reason="", readiness=true. Elapsed: 3.972018ms
    Feb 12 12:07:16.068: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-xml7z" satisfied condition "running"
    Feb 12 12:07:16.068: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-zm98d" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:16.073: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-zm98d": Phase="Running", Reason="", readiness=true. Elapsed: 4.41704ms
    Feb 12 12:07:16.073: INFO: Pod "wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f-zm98d" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f in namespace emptydir-wrapper-5056, will wait for the garbage collector to delete the pods 02/12/23 12:07:16.073
    Feb 12 12:07:16.141: INFO: Deleting ReplicationController wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f took: 12.834648ms
    Feb 12 12:07:16.242: INFO: Terminating ReplicationController wrapped-volume-race-35f51595-6ef0-4893-92f4-c3c76959078f pods took: 100.958428ms
    STEP: Creating RC which spawns configmap-volume pods 02/12/23 12:07:19.247
    Feb 12 12:07:19.260: INFO: Pod name wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95: Found 0 pods out of 5
    Feb 12 12:07:24.283: INFO: Pod name wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/12/23 12:07:24.283
    Feb 12 12:07:24.283: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:24.290: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.947097ms
    Feb 12 12:07:26.301: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01751621s
    Feb 12 12:07:28.297: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01355846s
    Feb 12 12:07:30.296: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012902588s
    Feb 12 12:07:32.296: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012628195s
    Feb 12 12:07:34.770: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.486099952s
    Feb 12 12:07:34.770: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-4t6v9" satisfied condition "running"
    Feb 12 12:07:34.770: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:34.902: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2": Phase="Pending", Reason="", readiness=false. Elapsed: 132.37389ms
    Feb 12 12:07:36.911: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2": Phase="Running", Reason="", readiness=true. Elapsed: 2.141388649s
    Feb 12 12:07:36.911: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-kbnw2" satisfied condition "running"
    Feb 12 12:07:36.911: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-q7zq4" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:36.919: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-q7zq4": Phase="Running", Reason="", readiness=true. Elapsed: 8.116018ms
    Feb 12 12:07:36.919: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-q7zq4" satisfied condition "running"
    Feb 12 12:07:36.919: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-v8m5c" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:36.926: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-v8m5c": Phase="Running", Reason="", readiness=true. Elapsed: 6.248562ms
    Feb 12 12:07:36.926: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-v8m5c" satisfied condition "running"
    Feb 12 12:07:36.926: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-xzlgz" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:36.931: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-xzlgz": Phase="Running", Reason="", readiness=true. Elapsed: 4.902633ms
    Feb 12 12:07:36.931: INFO: Pod "wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95-xzlgz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95 in namespace emptydir-wrapper-5056, will wait for the garbage collector to delete the pods 02/12/23 12:07:36.931
    Feb 12 12:07:36.997: INFO: Deleting ReplicationController wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95 took: 12.43557ms
    Feb 12 12:07:37.097: INFO: Terminating ReplicationController wrapped-volume-race-0f85abc1-4fdc-4717-826e-cc851ecd3a95 pods took: 100.261032ms
    STEP: Creating RC which spawns configmap-volume pods 02/12/23 12:07:40.604
    Feb 12 12:07:40.626: INFO: Pod name wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12: Found 0 pods out of 5
    Feb 12 12:07:45.631: INFO: Pod name wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/12/23 12:07:45.632
    Feb 12 12:07:45.632: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:07:45.635: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476844ms
    Feb 12 12:07:47.647: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015434159s
    Feb 12 12:07:49.652: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020415941s
    Feb 12 12:07:51.647: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015622107s
    Feb 12 12:07:54.145: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.513183173s
    Feb 12 12:07:55.664: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031950228s
    Feb 12 12:07:58.635: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Pending", Reason="", readiness=false. Elapsed: 13.00311867s
    Feb 12 12:08:00.399: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx": Phase="Running", Reason="", readiness=true. Elapsed: 14.767384082s
    Feb 12 12:08:00.399: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-k52lx" satisfied condition "running"
    Feb 12 12:08:00.399: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-lk6pk" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:08:00.637: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-lk6pk": Phase="Running", Reason="", readiness=true. Elapsed: 238.268137ms
    Feb 12 12:08:00.637: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-lk6pk" satisfied condition "running"
    Feb 12 12:08:00.637: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:08:00.646: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.677989ms
    Feb 12 12:08:02.651: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn": Phase="Running", Reason="", readiness=true. Elapsed: 2.013748954s
    Feb 12 12:08:02.651: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-ndvtn" satisfied condition "running"
    Feb 12 12:08:02.651: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-p5kgx" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:08:02.657: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-p5kgx": Phase="Running", Reason="", readiness=true. Elapsed: 5.931849ms
    Feb 12 12:08:02.657: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-p5kgx" satisfied condition "running"
    Feb 12 12:08:02.657: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-r2klg" in namespace "emptydir-wrapper-5056" to be "running"
    Feb 12 12:08:02.661: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-r2klg": Phase="Running", Reason="", readiness=true. Elapsed: 4.174981ms
    Feb 12 12:08:02.661: INFO: Pod "wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12-r2klg" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12 in namespace emptydir-wrapper-5056, will wait for the garbage collector to delete the pods 02/12/23 12:08:02.661
    Feb 12 12:08:02.737: INFO: Deleting ReplicationController wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12 took: 20.604656ms
    Feb 12 12:08:02.838: INFO: Terminating ReplicationController wrapped-volume-race-51dee7a2-0268-486e-8f81-e8a05aa46f12 pods took: 101.132713ms
    STEP: Cleaning up the configMaps 02/12/23 12:08:06.038
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:08:06.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-5056" for this suite. 02/12/23 12:08:06.372
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:08:06.38
Feb 12 12:08:06.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:08:06.381
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:06.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:06.403
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 02/12/23 12:08:06.405
Feb 12 12:08:06.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 create -f -'
Feb 12 12:08:06.916: INFO: stderr: ""
Feb 12 12:08:06.916: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 12:08:06.916
Feb 12 12:08:06.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 12:08:07.015: INFO: stderr: ""
Feb 12 12:08:07.015: INFO: stdout: "update-demo-nautilus-76tdk update-demo-nautilus-mnrxx "
Feb 12 12:08:07.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-76tdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 12:08:07.063: INFO: stderr: ""
Feb 12 12:08:07.063: INFO: stdout: ""
Feb 12 12:08:07.063: INFO: update-demo-nautilus-76tdk is created but not running
Feb 12 12:08:12.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 12 12:08:12.133: INFO: stderr: ""
Feb 12 12:08:12.133: INFO: stdout: "update-demo-nautilus-76tdk update-demo-nautilus-mnrxx "
Feb 12 12:08:12.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-76tdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 12:08:12.198: INFO: stderr: ""
Feb 12 12:08:12.198: INFO: stdout: "true"
Feb 12 12:08:12.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-76tdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 12:08:12.261: INFO: stderr: ""
Feb 12 12:08:12.261: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 12:08:12.261: INFO: validating pod update-demo-nautilus-76tdk
Feb 12 12:08:12.267: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 12:08:12.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 12:08:12.267: INFO: update-demo-nautilus-76tdk is verified up and running
Feb 12 12:08:12.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-mnrxx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 12 12:08:12.327: INFO: stderr: ""
Feb 12 12:08:12.327: INFO: stdout: "true"
Feb 12 12:08:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-mnrxx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 12 12:08:12.407: INFO: stderr: ""
Feb 12 12:08:12.407: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Feb 12 12:08:12.407: INFO: validating pod update-demo-nautilus-mnrxx
Feb 12 12:08:12.413: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 12 12:08:12.413: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 12 12:08:12.413: INFO: update-demo-nautilus-mnrxx is verified up and running
STEP: using delete to clean up resources 02/12/23 12:08:12.413
Feb 12 12:08:12.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 delete --grace-period=0 --force -f -'
Feb 12 12:08:12.496: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:08:12.497: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 12 12:08:12.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get rc,svc -l name=update-demo --no-headers'
Feb 12 12:08:12.627: INFO: stderr: "No resources found in kubectl-3508 namespace.\n"
Feb 12 12:08:12.627: INFO: stdout: ""
Feb 12 12:08:12.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 12:08:12.717: INFO: stderr: ""
Feb 12 12:08:12.717: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:08:12.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3508" for this suite. 02/12/23 12:08:12.724
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":228,"skipped":4297,"failed":0}
------------------------------
 [SLOW TEST] [6.354 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:08:06.38
    Feb 12 12:08:06.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:08:06.381
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:06.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:06.403
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 02/12/23 12:08:06.405
    Feb 12 12:08:06.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 create -f -'
    Feb 12 12:08:06.916: INFO: stderr: ""
    Feb 12 12:08:06.916: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/12/23 12:08:06.916
    Feb 12 12:08:06.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 12:08:07.015: INFO: stderr: ""
    Feb 12 12:08:07.015: INFO: stdout: "update-demo-nautilus-76tdk update-demo-nautilus-mnrxx "
    Feb 12 12:08:07.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-76tdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 12:08:07.063: INFO: stderr: ""
    Feb 12 12:08:07.063: INFO: stdout: ""
    Feb 12 12:08:07.063: INFO: update-demo-nautilus-76tdk is created but not running
    Feb 12 12:08:12.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 12 12:08:12.133: INFO: stderr: ""
    Feb 12 12:08:12.133: INFO: stdout: "update-demo-nautilus-76tdk update-demo-nautilus-mnrxx "
    Feb 12 12:08:12.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-76tdk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 12:08:12.198: INFO: stderr: ""
    Feb 12 12:08:12.198: INFO: stdout: "true"
    Feb 12 12:08:12.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-76tdk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 12:08:12.261: INFO: stderr: ""
    Feb 12 12:08:12.261: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 12:08:12.261: INFO: validating pod update-demo-nautilus-76tdk
    Feb 12 12:08:12.267: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 12:08:12.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 12:08:12.267: INFO: update-demo-nautilus-76tdk is verified up and running
    Feb 12 12:08:12.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-mnrxx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 12 12:08:12.327: INFO: stderr: ""
    Feb 12 12:08:12.327: INFO: stdout: "true"
    Feb 12 12:08:12.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods update-demo-nautilus-mnrxx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 12 12:08:12.407: INFO: stderr: ""
    Feb 12 12:08:12.407: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Feb 12 12:08:12.407: INFO: validating pod update-demo-nautilus-mnrxx
    Feb 12 12:08:12.413: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 12 12:08:12.413: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 12 12:08:12.413: INFO: update-demo-nautilus-mnrxx is verified up and running
    STEP: using delete to clean up resources 02/12/23 12:08:12.413
    Feb 12 12:08:12.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 delete --grace-period=0 --force -f -'
    Feb 12 12:08:12.496: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:08:12.497: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 12 12:08:12.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get rc,svc -l name=update-demo --no-headers'
    Feb 12 12:08:12.627: INFO: stderr: "No resources found in kubectl-3508 namespace.\n"
    Feb 12 12:08:12.627: INFO: stdout: ""
    Feb 12 12:08:12.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-3508 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 12:08:12.717: INFO: stderr: ""
    Feb 12 12:08:12.717: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:08:12.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3508" for this suite. 02/12/23 12:08:12.724
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:08:12.735
Feb 12 12:08:12.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:08:12.736
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:12.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:12.775
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-3243 02/12/23 12:08:13.393
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[] 02/12/23 12:08:15.678
Feb 12 12:08:15.860: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 12 12:08:16.867: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3243 02/12/23 12:08:16.867
Feb 12 12:08:16.885: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3243" to be "running and ready"
Feb 12 12:08:16.889: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469054ms
Feb 12 12:08:16.889: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:08:18.903: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017661754s
Feb 12 12:08:18.903: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 12 12:08:18.903: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[pod1:[80]] 02/12/23 12:08:18.919
Feb 12 12:08:18.938: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/12/23 12:08:18.938
Feb 12 12:08:18.938: INFO: Creating new exec pod
Feb 12 12:08:18.951: INFO: Waiting up to 5m0s for pod "execpodgj82c" in namespace "services-3243" to be "running"
Feb 12 12:08:18.960: INFO: Pod "execpodgj82c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.94032ms
Feb 12 12:08:21.099: INFO: Pod "execpodgj82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.147427074s
Feb 12 12:08:22.971: INFO: Pod "execpodgj82c": Phase="Running", Reason="", readiness=true. Elapsed: 4.019988631s
Feb 12 12:08:22.971: INFO: Pod "execpodgj82c" satisfied condition "running"
Feb 12 12:08:23.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 12 12:08:24.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 12 12:08:24.197: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:08:24.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.96 80'
Feb 12 12:08:24.326: INFO: stderr: "+ nc -v -t -w 2 10.233.59.96 80\n+ echo hostName\nConnection to 10.233.59.96 80 port [tcp/http] succeeded!\n"
Feb 12 12:08:24.326: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3243 02/12/23 12:08:24.326
Feb 12 12:08:24.355: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3243" to be "running and ready"
Feb 12 12:08:24.361: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.582385ms
Feb 12 12:08:24.361: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:08:26.372: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017299459s
Feb 12 12:08:26.373: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 12 12:08:26.373: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[pod1:[80] pod2:[80]] 02/12/23 12:08:26.384
Feb 12 12:08:26.418: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/12/23 12:08:26.418
Feb 12 12:08:27.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 12 12:08:27.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 12 12:08:27.608: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:08:27.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.96 80'
Feb 12 12:08:27.751: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.96 80\nConnection to 10.233.59.96 80 port [tcp/http] succeeded!\n"
Feb 12 12:08:27.751: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3243 02/12/23 12:08:27.751
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[pod2:[80]] 02/12/23 12:08:27.78
Feb 12 12:08:27.808: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/12/23 12:08:27.808
Feb 12 12:08:28.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Feb 12 12:08:28.976: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 12 12:08:28.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:08:28.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.96 80'
Feb 12 12:08:29.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.96 80\nConnection to 10.233.59.96 80 port [tcp/http] succeeded!\n"
Feb 12 12:08:29.100: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3243 02/12/23 12:08:29.1
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[] 02/12/23 12:08:29.125
Feb 12 12:08:29.153: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:08:29.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3243" for this suite. 02/12/23 12:08:29.204
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":229,"skipped":4300,"failed":0}
------------------------------
 [SLOW TEST] [16.483 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:08:12.735
    Feb 12 12:08:12.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:08:12.736
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:12.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:12.775
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-3243 02/12/23 12:08:13.393
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[] 02/12/23 12:08:15.678
    Feb 12 12:08:15.860: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Feb 12 12:08:16.867: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3243 02/12/23 12:08:16.867
    Feb 12 12:08:16.885: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3243" to be "running and ready"
    Feb 12 12:08:16.889: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469054ms
    Feb 12 12:08:16.889: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:08:18.903: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017661754s
    Feb 12 12:08:18.903: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 12 12:08:18.903: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[pod1:[80]] 02/12/23 12:08:18.919
    Feb 12 12:08:18.938: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/12/23 12:08:18.938
    Feb 12 12:08:18.938: INFO: Creating new exec pod
    Feb 12 12:08:18.951: INFO: Waiting up to 5m0s for pod "execpodgj82c" in namespace "services-3243" to be "running"
    Feb 12 12:08:18.960: INFO: Pod "execpodgj82c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.94032ms
    Feb 12 12:08:21.099: INFO: Pod "execpodgj82c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.147427074s
    Feb 12 12:08:22.971: INFO: Pod "execpodgj82c": Phase="Running", Reason="", readiness=true. Elapsed: 4.019988631s
    Feb 12 12:08:22.971: INFO: Pod "execpodgj82c" satisfied condition "running"
    Feb 12 12:08:23.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 12 12:08:24.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 12 12:08:24.197: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:08:24.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.96 80'
    Feb 12 12:08:24.326: INFO: stderr: "+ nc -v -t -w 2 10.233.59.96 80\n+ echo hostName\nConnection to 10.233.59.96 80 port [tcp/http] succeeded!\n"
    Feb 12 12:08:24.326: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-3243 02/12/23 12:08:24.326
    Feb 12 12:08:24.355: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3243" to be "running and ready"
    Feb 12 12:08:24.361: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.582385ms
    Feb 12 12:08:24.361: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:08:26.372: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017299459s
    Feb 12 12:08:26.373: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 12 12:08:26.373: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[pod1:[80] pod2:[80]] 02/12/23 12:08:26.384
    Feb 12 12:08:26.418: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/12/23 12:08:26.418
    Feb 12 12:08:27.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 12 12:08:27.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 12 12:08:27.608: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:08:27.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.96 80'
    Feb 12 12:08:27.751: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.96 80\nConnection to 10.233.59.96 80 port [tcp/http] succeeded!\n"
    Feb 12 12:08:27.751: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3243 02/12/23 12:08:27.751
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[pod2:[80]] 02/12/23 12:08:27.78
    Feb 12 12:08:27.808: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/12/23 12:08:27.808
    Feb 12 12:08:28.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Feb 12 12:08:28.976: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 12 12:08:28.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:08:28.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3243 exec execpodgj82c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.96 80'
    Feb 12 12:08:29.100: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.96 80\nConnection to 10.233.59.96 80 port [tcp/http] succeeded!\n"
    Feb 12 12:08:29.100: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-3243 02/12/23 12:08:29.1
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3243 to expose endpoints map[] 02/12/23 12:08:29.125
    Feb 12 12:08:29.153: INFO: successfully validated that service endpoint-test2 in namespace services-3243 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:08:29.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3243" for this suite. 02/12/23 12:08:29.204
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:08:29.218
Feb 12 12:08:29.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:08:29.22
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:29.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:29.255
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/12/23 12:08:29.259
Feb 12 12:08:29.273: INFO: Waiting up to 5m0s for pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386" in namespace "emptydir-94" to be "Succeeded or Failed"
Feb 12 12:08:29.289: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Pending", Reason="", readiness=false. Elapsed: 16.360923ms
Feb 12 12:08:32.823: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Running", Reason="", readiness=true. Elapsed: 3.549579823s
Feb 12 12:08:33.301: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Running", Reason="", readiness=false. Elapsed: 4.02794365s
Feb 12 12:08:35.304: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030478s
STEP: Saw pod success 02/12/23 12:08:35.304
Feb 12 12:08:35.304: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386" satisfied condition "Succeeded or Failed"
Feb 12 12:08:35.317: INFO: Trying to get logs from node kube-3 pod pod-d23080c1-734a-4c67-99df-5ee3047a9386 container test-container: <nil>
STEP: delete the pod 02/12/23 12:08:35.349
Feb 12 12:08:35.370: INFO: Waiting for pod pod-d23080c1-734a-4c67-99df-5ee3047a9386 to disappear
Feb 12 12:08:35.373: INFO: Pod pod-d23080c1-734a-4c67-99df-5ee3047a9386 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:08:35.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-94" for this suite. 02/12/23 12:08:35.378
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":230,"skipped":4315,"failed":0}
------------------------------
 [SLOW TEST] [6.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:08:29.218
    Feb 12 12:08:29.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:08:29.22
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:29.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:29.255
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/12/23 12:08:29.259
    Feb 12 12:08:29.273: INFO: Waiting up to 5m0s for pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386" in namespace "emptydir-94" to be "Succeeded or Failed"
    Feb 12 12:08:29.289: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Pending", Reason="", readiness=false. Elapsed: 16.360923ms
    Feb 12 12:08:32.823: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Running", Reason="", readiness=true. Elapsed: 3.549579823s
    Feb 12 12:08:33.301: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Running", Reason="", readiness=false. Elapsed: 4.02794365s
    Feb 12 12:08:35.304: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030478s
    STEP: Saw pod success 02/12/23 12:08:35.304
    Feb 12 12:08:35.304: INFO: Pod "pod-d23080c1-734a-4c67-99df-5ee3047a9386" satisfied condition "Succeeded or Failed"
    Feb 12 12:08:35.317: INFO: Trying to get logs from node kube-3 pod pod-d23080c1-734a-4c67-99df-5ee3047a9386 container test-container: <nil>
    STEP: delete the pod 02/12/23 12:08:35.349
    Feb 12 12:08:35.370: INFO: Waiting for pod pod-d23080c1-734a-4c67-99df-5ee3047a9386 to disappear
    Feb 12 12:08:35.373: INFO: Pod pod-d23080c1-734a-4c67-99df-5ee3047a9386 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:08:35.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-94" for this suite. 02/12/23 12:08:35.378
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:08:35.386
Feb 12 12:08:35.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 12:08:35.387
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:35.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:35.409
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 12 12:08:35.412: INFO: Creating deployment "webserver-deployment"
Feb 12 12:08:35.417: INFO: Waiting for observed generation 1
Feb 12 12:08:37.426: INFO: Waiting for all required pods to come up
Feb 12 12:08:37.435: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/12/23 12:08:37.435
Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v274p" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7sxr4" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-h272k" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-j56fg" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n6n7r" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.436: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qh4bc" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.436: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rhqmh" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.436: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sd9gg" in namespace "deployment-2793" to be "running"
Feb 12 12:08:37.440: INFO: Pod "webserver-deployment-845c8977d9-7sxr4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437894ms
Feb 12 12:08:37.441: INFO: Pod "webserver-deployment-845c8977d9-sd9gg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438636ms
Feb 12 12:08:37.441: INFO: Pod "webserver-deployment-845c8977d9-v274p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585077ms
Feb 12 12:08:37.441: INFO: Pod "webserver-deployment-845c8977d9-j56fg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.197567ms
Feb 12 12:08:37.442: INFO: Pod "webserver-deployment-845c8977d9-qh4bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.986646ms
Feb 12 12:08:37.442: INFO: Pod "webserver-deployment-845c8977d9-h272k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.6899ms
Feb 12 12:08:37.444: INFO: Pod "webserver-deployment-845c8977d9-rhqmh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.533405ms
Feb 12 12:08:37.444: INFO: Pod "webserver-deployment-845c8977d9-n6n7r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.825987ms
Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-sd9gg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011032132s
Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-sd9gg" satisfied condition "running"
Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-j56fg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011834383s
Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-j56fg" satisfied condition "running"
Feb 12 12:08:39.449: INFO: Pod "webserver-deployment-845c8977d9-h272k": Phase="Running", Reason="", readiness=true. Elapsed: 2.013760656s
Feb 12 12:08:39.449: INFO: Pod "webserver-deployment-845c8977d9-h272k" satisfied condition "running"
Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-v274p": Phase="Running", Reason="", readiness=true. Elapsed: 2.015652793s
Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-v274p" satisfied condition "running"
Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-qh4bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014864398s
Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-qh4bc" satisfied condition "running"
Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-7sxr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015626639s
Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-7sxr4" satisfied condition "running"
Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-n6n7r": Phase="Running", Reason="", readiness=true. Elapsed: 2.015910705s
Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-n6n7r" satisfied condition "running"
Feb 12 12:08:39.452: INFO: Pod "webserver-deployment-845c8977d9-rhqmh": Phase="Running", Reason="", readiness=true. Elapsed: 2.015872845s
Feb 12 12:08:39.452: INFO: Pod "webserver-deployment-845c8977d9-rhqmh" satisfied condition "running"
Feb 12 12:08:39.452: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 12 12:08:39.459: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 12 12:08:39.470: INFO: Updating deployment webserver-deployment
Feb 12 12:08:39.470: INFO: Waiting for observed generation 2
Feb 12 12:08:41.766: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 12 12:08:41.936: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 12 12:08:41.941: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 12 12:08:42.128: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 12 12:08:42.128: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 12 12:08:42.200: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 12 12:08:42.329: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 12 12:08:42.329: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 12 12:08:42.585: INFO: Updating deployment webserver-deployment
Feb 12 12:08:42.585: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 12 12:08:42.738: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 12 12:08:46.566: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 12:08:47.293: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2793  40b788cb-f930-4061-9c4f-c7a08b3fce1c 25510 3 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea4538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-12 12:08:42 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-12 12:08:44 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 12 12:08:47.589: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2793  bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 25487 3 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 40b788cb-f930-4061-9c4f-c7a08b3fce1c 0xc003ea4947 0xc003ea4948}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40b788cb-f930-4061-9c4f-c7a08b3fce1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea49f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:08:47.589: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 12 12:08:47.589: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2793  91fb6d17-d3a1-4eac-be5c-39929899d487 25506 3 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 40b788cb-f930-4061-9c4f-c7a08b3fce1c 0xc003ea4a57 0xc003ea4a58}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40b788cb-f930-4061-9c4f-c7a08b3fce1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea4ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-4j72t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4j72t webserver-deployment-69b7448995- deployment-2793  eb71d7d2-d606-4b44-9003-3853c4750a05 25389 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b31b8c8e00a56c4b5498c9bef57556c7018d7d406c09a8ce8079612dd40a35ce cni.projectcalico.org/podIP:10.233.99.68/32 cni.projectcalico.org/podIPs:10.233.99.68/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea4fc7 0xc003ea4fc8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-12 12:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzgnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzgnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-66cl8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-66cl8 webserver-deployment-69b7448995- deployment-2793  517f3b87-1567-42a3-bc3e-5043646dd2eb 25522 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:c0bedced4d43ebabba10e646100ccabef11da07d0df67e05a7711d89bc560ed8 cni.projectcalico.org/podIP:10.233.120.211/32 cni.projectcalico.org/podIPs:10.233.120.211/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea51d0 0xc003ea51d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htcck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htcck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.211,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-72lkr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-72lkr webserver-deployment-69b7448995- deployment-2793  c292a453-0ec7-4125-aa4a-9953aff0d3e2 25430 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea5410 0xc003ea5411}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w27g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w27g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-dlwmd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dlwmd webserver-deployment-69b7448995- deployment-2793  a5116de0-1d82-4d4d-b64d-5fcae04748f3 25468 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea55f0 0xc003ea55f1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgshs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgshs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-jp8bm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jp8bm webserver-deployment-69b7448995- deployment-2793  1c290488-f014-436f-9977-666477b4952b 25525 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea57d0 0xc003ea57d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwlxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwlxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-k2kpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-k2kpr webserver-deployment-69b7448995- deployment-2793  bd66c6b2-7d49-407c-a16f-3eb4cde60427 25440 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:89bb93a60101d6a68a1d3f5b485723e827c65351e1cb1740e846e2d0e27c4627 cni.projectcalico.org/podIP:10.233.120.76/32 cni.projectcalico.org/podIPs:10.233.120.76/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea59d0 0xc003ea59d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk52w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk52w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.76,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-kjmnb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kjmnb webserver-deployment-69b7448995- deployment-2793  70a2cd2d-664c-4e86-8c7c-cd21f4065b2b 25509 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea5c10 0xc003ea5c11}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7g2vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7g2vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-n6cmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-n6cmc webserver-deployment-69b7448995- deployment-2793  efa82ae2-7e45-4844-9c6f-c041c2b1dc9e 25481 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea5e00 0xc003ea5e01}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hm9jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hm9jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-shg8g" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-shg8g webserver-deployment-69b7448995- deployment-2793  d02a908e-574d-4170-89f8-be3c34446592 25465 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:761f10a4b261d88260cfd5aa40c64c648f3544c62a18e4ad0735b6735799547f cni.projectcalico.org/podIP:10.233.99.69/32 cni.projectcalico.org/podIPs:10.233.99.69/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a010 0xc00777a011}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlfd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlfd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.69,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-tc9sr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tc9sr webserver-deployment-69b7448995- deployment-2793  5d1524dd-7116-4fb7-8f65-594ca5ace20a 25474 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a250 0xc00777a251}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khd8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khd8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-zkdzh" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zkdzh webserver-deployment-69b7448995- deployment-2793  1b83b7b5-495d-41e6-ae40-8d2f819ddab1 25504 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a3c0 0xc00777a3c1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnnbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnnbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-zpftq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zpftq webserver-deployment-69b7448995- deployment-2793  c487e5de-c9bc-45f7-a082-005407132bfe 25499 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a5a0 0xc00777a5a1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ssnp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ssnp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-zrjrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zrjrp webserver-deployment-69b7448995- deployment-2793  9f689fa7-0e75-4938-93d9-9fe29c7d87ef 25392 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7a36825222a9771fbf3f8b4f37bf975b279dcce4f62b2d45775634cd647b4c54 cni.projectcalico.org/podIP:10.233.120.77/32 cni.projectcalico.org/podIPs:10.233.120.77/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a7a0 0xc00777a7a1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w47vz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w47vz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-6sb7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6sb7z webserver-deployment-845c8977d9- deployment-2793  e7c75295-6ddb-4af2-89e4-ec62f0918a32 25479 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777a9a0 0xc00777a9a1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9xh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9xh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-75lmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-75lmt webserver-deployment-845c8977d9- deployment-2793  13126384-260b-47ce-a6d8-66b3fac58279 25478 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777ab00 0xc00777ab01}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts7rh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts7rh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-7sxr4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-7sxr4 webserver-deployment-845c8977d9- deployment-2793  a319960f-221e-4beb-8eb9-ea2b8a9fbeb9 25308 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:940ac2bc89f3e598ff58d9eaf315254ba7cfd70ae12cdbfd836b5f6ae9c7784c cni.projectcalico.org/podIP:10.233.99.127/32 cni.projectcalico.org/podIPs:10.233.99.127/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777ac80 0xc00777ac81}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjtvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjtvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.127,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3dcb7d80b7f115770e2cb01dee1cb7a6ceb873a7777d988c73d8091ccdf30fd6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-dcwkx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dcwkx webserver-deployment-845c8977d9- deployment-2793  4d6d058d-ac96-4c9e-afa6-1c4330a4544f 25512 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777ae97 0xc00777ae98}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnlt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnlt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-ddds5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ddds5 webserver-deployment-845c8977d9- deployment-2793  0ef55cda-7849-499b-9637-ecfa806bb794 25256 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3dc9fbaf80dd6d4f609ca8733c3e07905c99fe631851cf6027a25de2df92b4a0 cni.projectcalico.org/podIP:10.233.120.73/32 cni.projectcalico.org/podIPs:10.233.120.73/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b087 0xc00777b088}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvrkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvrkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.73,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9d7f05e3ccab76ac6e026aa2452b6256b42bbba01f8928dd8117dbd2e584f772,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-dfmsl" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dfmsl webserver-deployment-845c8977d9- deployment-2793  6ab91207-5070-4272-92c7-d7bda160fc4e 25518 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b297 0xc00777b298}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stg57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stg57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-dp9ms" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dp9ms webserver-deployment-845c8977d9- deployment-2793  e94c88fb-1cbe-4d5b-a212-019d083ae1a8 25482 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b467 0xc00777b468}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jsh4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jsh4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-h272k" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h272k webserver-deployment-845c8977d9- deployment-2793  d580b072-7232-4c43-bbc8-0e67790e3dd6 25298 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:31b2e121090f09e82bd4175bb93a42514153f1f8cb252da21da44e2adeded8ef cni.projectcalico.org/podIP:10.233.120.206/32 cni.projectcalico.org/podIPs:10.233.120.206/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b5d0 0xc00777b5d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g55sw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g55sw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.206,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ab667e285fa3dd7db522e3e8e1efe4cca8bd03346cba658030ea79b6eb6636b8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-hhzvk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hhzvk webserver-deployment-845c8977d9- deployment-2793  7b3902b2-fc38-4ec8-9b32-7809078ff917 25439 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b7d7 0xc00777b7d8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rsp5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rsp5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-j56fg" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j56fg webserver-deployment-845c8977d9- deployment-2793  5c0670a3-a766-4dab-8cc0-3960fb0186f2 25301 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6112329f986673503caa545e618070583ea5b076dcbfddb1f5d1cbb74f8116c5 cni.projectcalico.org/podIP:10.233.120.74/32 cni.projectcalico.org/podIPs:10.233.120.74/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b9c7 0xc00777b9c8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjzx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjzx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.74,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79100290b7c65f7eefaf534e3e7dce5bd80a300fb4b6814c6c37bade36c7cbba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-lqcdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lqcdt webserver-deployment-845c8977d9- deployment-2793  e805a080-4701-4966-b650-2fec52db654b 25498 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777bbd7 0xc00777bbd8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtszs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtszs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-n6n7r" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6n7r webserver-deployment-845c8977d9- deployment-2793  b3ce08d3-9bda-462e-a48a-4ef0b147c4bc 25294 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:eeadab519d4a82bc67a1fd0b462000763329f40eac8738dd41a8024e2bb4fdb2 cni.projectcalico.org/podIP:10.233.120.210/32 cni.projectcalico.org/podIPs:10.233.120.210/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777bda7 0xc00777bda8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nl9bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nl9bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.210,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://71667abce584775084116c220279f998eee3fa529bc14e22aa7c0312f57e5626,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-nqcps" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nqcps webserver-deployment-845c8977d9- deployment-2793  2787ba81-0145-471f-abc6-a51757bee11c 25460 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777bfb7 0xc00777bfb8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzll7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzll7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-pd9f6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pd9f6 webserver-deployment-845c8977d9- deployment-2793  d2e0c85b-44e2-45ba-bc96-aa161f943f44 25526 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56140 0xc002c56141}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mpqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mpqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-rhqmh" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rhqmh webserver-deployment-845c8977d9- deployment-2793  f89b7e93-18ab-4587-bbef-531d0ad4352c 25291 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:769b4bbbe3c0206807a40ad120861df8682d9edafd402a7139f49b9d8897df3a cni.projectcalico.org/podIP:10.233.120.209/32 cni.projectcalico.org/podIPs:10.233.120.209/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56307 0xc002c56308}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcghh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcghh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.209,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6657b7f3e7fa9b0b91013192ed1047741a409a8362f53cd4e59b3b9d768949d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-rxjjr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rxjjr webserver-deployment-845c8977d9- deployment-2793  9b38a709-6a36-43f0-8e7f-14104a81d9c2 25277 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:23051e6ea5f4a992cb475c4487a5b5de6eeff5b4e32729588e1f11625dbe1415 cni.projectcalico.org/podIP:10.233.99.125/32 cni.projectcalico.org/podIPs:10.233.99.125/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56537 0xc002c56538}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr6t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr6t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.125,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://14003e272068133f7a551c4bb8918baa4be8a7c3d2979c5a1da7e8337fef738e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-t8gr5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t8gr5 webserver-deployment-845c8977d9- deployment-2793  2a878cc5-1b89-40f7-baf7-f871c75a74eb 25516 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56747 0xc002c56748}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-npphh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-npphh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.731: INFO: Pod "webserver-deployment-845c8977d9-v274p" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-v274p webserver-deployment-845c8977d9- deployment-2793  7d1ad1c0-efff-4e6a-afcc-2fb0bd4f9f0f 25304 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:de1da577457461fce6f00cc913bf6d6f69b4e79926abeef409206f97cc6d08f1 cni.projectcalico.org/podIP:10.233.120.75/32 cni.projectcalico.org/podIPs:10.233.120.75/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56937 0xc002c56938}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wl2q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wl2q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.75,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5ea359515464e5b75561b8730cbf352b678cc44443ffb17938e0dce8f94d0cfb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.731: INFO: Pod "webserver-deployment-845c8977d9-x2df9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-x2df9 webserver-deployment-845c8977d9- deployment-2793  ccf5986e-e8e1-438c-8200-b4de793fbec8 25453 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56b47 0xc002c56b48}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjnwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjnwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 12 12:08:47.731: INFO: Pod "webserver-deployment-845c8977d9-xfv5b" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xfv5b webserver-deployment-845c8977d9- deployment-2793  6a131f3b-ef57-4064-920d-8decb30e575a 25477 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56d27 0xc002c56d28}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mh2b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mh2b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 12:08:47.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2793" for this suite. 02/12/23 12:08:47.771
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":231,"skipped":4318,"failed":0}
------------------------------
 [SLOW TEST] [12.539 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:08:35.386
    Feb 12 12:08:35.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 12:08:35.387
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:35.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:35.409
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 12 12:08:35.412: INFO: Creating deployment "webserver-deployment"
    Feb 12 12:08:35.417: INFO: Waiting for observed generation 1
    Feb 12 12:08:37.426: INFO: Waiting for all required pods to come up
    Feb 12 12:08:37.435: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/12/23 12:08:37.435
    Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v274p" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-7sxr4" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-h272k" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-j56fg" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.435: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n6n7r" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.436: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qh4bc" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.436: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rhqmh" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.436: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sd9gg" in namespace "deployment-2793" to be "running"
    Feb 12 12:08:37.440: INFO: Pod "webserver-deployment-845c8977d9-7sxr4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.437894ms
    Feb 12 12:08:37.441: INFO: Pod "webserver-deployment-845c8977d9-sd9gg": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438636ms
    Feb 12 12:08:37.441: INFO: Pod "webserver-deployment-845c8977d9-v274p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585077ms
    Feb 12 12:08:37.441: INFO: Pod "webserver-deployment-845c8977d9-j56fg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.197567ms
    Feb 12 12:08:37.442: INFO: Pod "webserver-deployment-845c8977d9-qh4bc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.986646ms
    Feb 12 12:08:37.442: INFO: Pod "webserver-deployment-845c8977d9-h272k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.6899ms
    Feb 12 12:08:37.444: INFO: Pod "webserver-deployment-845c8977d9-rhqmh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.533405ms
    Feb 12 12:08:37.444: INFO: Pod "webserver-deployment-845c8977d9-n6n7r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.825987ms
    Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-sd9gg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011032132s
    Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-sd9gg" satisfied condition "running"
    Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-j56fg": Phase="Running", Reason="", readiness=true. Elapsed: 2.011834383s
    Feb 12 12:08:39.447: INFO: Pod "webserver-deployment-845c8977d9-j56fg" satisfied condition "running"
    Feb 12 12:08:39.449: INFO: Pod "webserver-deployment-845c8977d9-h272k": Phase="Running", Reason="", readiness=true. Elapsed: 2.013760656s
    Feb 12 12:08:39.449: INFO: Pod "webserver-deployment-845c8977d9-h272k" satisfied condition "running"
    Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-v274p": Phase="Running", Reason="", readiness=true. Elapsed: 2.015652793s
    Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-v274p" satisfied condition "running"
    Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-qh4bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014864398s
    Feb 12 12:08:39.450: INFO: Pod "webserver-deployment-845c8977d9-qh4bc" satisfied condition "running"
    Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-7sxr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015626639s
    Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-7sxr4" satisfied condition "running"
    Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-n6n7r": Phase="Running", Reason="", readiness=true. Elapsed: 2.015910705s
    Feb 12 12:08:39.451: INFO: Pod "webserver-deployment-845c8977d9-n6n7r" satisfied condition "running"
    Feb 12 12:08:39.452: INFO: Pod "webserver-deployment-845c8977d9-rhqmh": Phase="Running", Reason="", readiness=true. Elapsed: 2.015872845s
    Feb 12 12:08:39.452: INFO: Pod "webserver-deployment-845c8977d9-rhqmh" satisfied condition "running"
    Feb 12 12:08:39.452: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 12 12:08:39.459: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 12 12:08:39.470: INFO: Updating deployment webserver-deployment
    Feb 12 12:08:39.470: INFO: Waiting for observed generation 2
    Feb 12 12:08:41.766: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 12 12:08:41.936: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 12 12:08:41.941: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 12 12:08:42.128: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 12 12:08:42.128: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 12 12:08:42.200: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 12 12:08:42.329: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 12 12:08:42.329: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 12 12:08:42.585: INFO: Updating deployment webserver-deployment
    Feb 12 12:08:42.585: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 12 12:08:42.738: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 12 12:08:46.566: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 12:08:47.293: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2793  40b788cb-f930-4061-9c4f-c7a08b3fce1c 25510 3 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea4538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-12 12:08:42 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-02-12 12:08:44 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Feb 12 12:08:47.589: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-2793  bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 25487 3 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 40b788cb-f930-4061-9c4f-c7a08b3fce1c 0xc003ea4947 0xc003ea4948}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40b788cb-f930-4061-9c4f-c7a08b3fce1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea49f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:08:47.589: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 12 12:08:47.589: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-2793  91fb6d17-d3a1-4eac-be5c-39929899d487 25506 3 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 40b788cb-f930-4061-9c4f-c7a08b3fce1c 0xc003ea4a57 0xc003ea4a58}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40b788cb-f930-4061-9c4f-c7a08b3fce1c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ea4ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-4j72t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4j72t webserver-deployment-69b7448995- deployment-2793  eb71d7d2-d606-4b44-9003-3853c4750a05 25389 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b31b8c8e00a56c4b5498c9bef57556c7018d7d406c09a8ce8079612dd40a35ce cni.projectcalico.org/podIP:10.233.99.68/32 cni.projectcalico.org/podIPs:10.233.99.68/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea4fc7 0xc003ea4fc8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-12 12:08:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzgnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzgnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-66cl8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-66cl8 webserver-deployment-69b7448995- deployment-2793  517f3b87-1567-42a3-bc3e-5043646dd2eb 25522 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:c0bedced4d43ebabba10e646100ccabef11da07d0df67e05a7711d89bc560ed8 cni.projectcalico.org/podIP:10.233.120.211/32 cni.projectcalico.org/podIPs:10.233.120.211/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea51d0 0xc003ea51d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htcck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htcck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.211,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-72lkr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-72lkr webserver-deployment-69b7448995- deployment-2793  c292a453-0ec7-4125-aa4a-9953aff0d3e2 25430 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea5410 0xc003ea5411}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w27g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w27g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-dlwmd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dlwmd webserver-deployment-69b7448995- deployment-2793  a5116de0-1d82-4d4d-b64d-5fcae04748f3 25468 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea55f0 0xc003ea55f1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kgshs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kgshs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-jp8bm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jp8bm webserver-deployment-69b7448995- deployment-2793  1c290488-f014-436f-9977-666477b4952b 25525 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea57d0 0xc003ea57d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dwlxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dwlxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-k2kpr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-k2kpr webserver-deployment-69b7448995- deployment-2793  bd66c6b2-7d49-407c-a16f-3eb4cde60427 25440 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:89bb93a60101d6a68a1d3f5b485723e827c65351e1cb1740e846e2d0e27c4627 cni.projectcalico.org/podIP:10.233.120.76/32 cni.projectcalico.org/podIPs:10.233.120.76/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea59d0 0xc003ea59d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk52w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk52w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.76,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.728: INFO: Pod "webserver-deployment-69b7448995-kjmnb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kjmnb webserver-deployment-69b7448995- deployment-2793  70a2cd2d-664c-4e86-8c7c-cd21f4065b2b 25509 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea5c10 0xc003ea5c11}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7g2vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7g2vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-n6cmc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-n6cmc webserver-deployment-69b7448995- deployment-2793  efa82ae2-7e45-4844-9c6f-c041c2b1dc9e 25481 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc003ea5e00 0xc003ea5e01}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hm9jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hm9jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-shg8g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-shg8g webserver-deployment-69b7448995- deployment-2793  d02a908e-574d-4170-89f8-be3c34446592 25465 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:761f10a4b261d88260cfd5aa40c64c648f3544c62a18e4ad0735b6735799547f cni.projectcalico.org/podIP:10.233.99.69/32 cni.projectcalico.org/podIPs:10.233.99.69/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a010 0xc00777a011}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlfd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlfd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.69,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-tc9sr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tc9sr webserver-deployment-69b7448995- deployment-2793  5d1524dd-7116-4fb7-8f65-594ca5ace20a 25474 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a250 0xc00777a251}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khd8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khd8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-zkdzh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zkdzh webserver-deployment-69b7448995- deployment-2793  1b83b7b5-495d-41e6-ae40-8d2f819ddab1 25504 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a3c0 0xc00777a3c1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fnnbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fnnbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-zpftq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zpftq webserver-deployment-69b7448995- deployment-2793  c487e5de-c9bc-45f7-a082-005407132bfe 25499 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a5a0 0xc00777a5a1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ssnp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ssnp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-69b7448995-zrjrp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zrjrp webserver-deployment-69b7448995- deployment-2793  9f689fa7-0e75-4938-93d9-9fe29c7d87ef 25392 0 2023-02-12 12:08:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:7a36825222a9771fbf3f8b4f37bf975b279dcce4f62b2d45775634cd647b4c54 cni.projectcalico.org/podIP:10.233.120.77/32 cni.projectcalico.org/podIPs:10.233.120.77/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5 0xc00777a7a0 0xc00777a7a1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc8a5bf4-b8c7-4c78-b780-14cf0c8670f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w47vz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w47vz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-6sb7z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6sb7z webserver-deployment-845c8977d9- deployment-2793  e7c75295-6ddb-4af2-89e4-ec62f0918a32 25479 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777a9a0 0xc00777a9a1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f9xh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f9xh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-75lmt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-75lmt webserver-deployment-845c8977d9- deployment-2793  13126384-260b-47ce-a6d8-66b3fac58279 25478 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777ab00 0xc00777ab01}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ts7rh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ts7rh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-7sxr4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-7sxr4 webserver-deployment-845c8977d9- deployment-2793  a319960f-221e-4beb-8eb9-ea2b8a9fbeb9 25308 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:940ac2bc89f3e598ff58d9eaf315254ba7cfd70ae12cdbfd836b5f6ae9c7784c cni.projectcalico.org/podIP:10.233.99.127/32 cni.projectcalico.org/podIPs:10.233.99.127/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777ac80 0xc00777ac81}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.127\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bjtvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bjtvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.127,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3dcb7d80b7f115770e2cb01dee1cb7a6ceb873a7777d988c73d8091ccdf30fd6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-dcwkx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dcwkx webserver-deployment-845c8977d9- deployment-2793  4d6d058d-ac96-4c9e-afa6-1c4330a4544f 25512 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777ae97 0xc00777ae98}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnlt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnlt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.729: INFO: Pod "webserver-deployment-845c8977d9-ddds5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ddds5 webserver-deployment-845c8977d9- deployment-2793  0ef55cda-7849-499b-9637-ecfa806bb794 25256 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3dc9fbaf80dd6d4f609ca8733c3e07905c99fe631851cf6027a25de2df92b4a0 cni.projectcalico.org/podIP:10.233.120.73/32 cni.projectcalico.org/podIPs:10.233.120.73/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b087 0xc00777b088}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wvrkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wvrkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.73,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9d7f05e3ccab76ac6e026aa2452b6256b42bbba01f8928dd8117dbd2e584f772,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-dfmsl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dfmsl webserver-deployment-845c8977d9- deployment-2793  6ab91207-5070-4272-92c7-d7bda160fc4e 25518 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b297 0xc00777b298}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stg57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stg57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-dp9ms" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dp9ms webserver-deployment-845c8977d9- deployment-2793  e94c88fb-1cbe-4d5b-a212-019d083ae1a8 25482 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b467 0xc00777b468}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jsh4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jsh4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-h272k" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h272k webserver-deployment-845c8977d9- deployment-2793  d580b072-7232-4c43-bbc8-0e67790e3dd6 25298 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:31b2e121090f09e82bd4175bb93a42514153f1f8cb252da21da44e2adeded8ef cni.projectcalico.org/podIP:10.233.120.206/32 cni.projectcalico.org/podIPs:10.233.120.206/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b5d0 0xc00777b5d1}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g55sw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g55sw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.206,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ab667e285fa3dd7db522e3e8e1efe4cca8bd03346cba658030ea79b6eb6636b8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-hhzvk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hhzvk webserver-deployment-845c8977d9- deployment-2793  7b3902b2-fc38-4ec8-9b32-7809078ff917 25439 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b7d7 0xc00777b7d8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rsp5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rsp5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-j56fg" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j56fg webserver-deployment-845c8977d9- deployment-2793  5c0670a3-a766-4dab-8cc0-3960fb0186f2 25301 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6112329f986673503caa545e618070583ea5b076dcbfddb1f5d1cbb74f8116c5 cni.projectcalico.org/podIP:10.233.120.74/32 cni.projectcalico.org/podIPs:10.233.120.74/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777b9c7 0xc00777b9c8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qjzx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qjzx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.74,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79100290b7c65f7eefaf534e3e7dce5bd80a300fb4b6814c6c37bade36c7cbba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-lqcdt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lqcdt webserver-deployment-845c8977d9- deployment-2793  e805a080-4701-4966-b650-2fec52db654b 25498 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777bbd7 0xc00777bbd8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtszs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtszs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-n6n7r" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n6n7r webserver-deployment-845c8977d9- deployment-2793  b3ce08d3-9bda-462e-a48a-4ef0b147c4bc 25294 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:eeadab519d4a82bc67a1fd0b462000763329f40eac8738dd41a8024e2bb4fdb2 cni.projectcalico.org/podIP:10.233.120.210/32 cni.projectcalico.org/podIPs:10.233.120.210/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777bda7 0xc00777bda8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nl9bv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nl9bv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.210,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://71667abce584775084116c220279f998eee3fa529bc14e22aa7c0312f57e5626,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-nqcps" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nqcps webserver-deployment-845c8977d9- deployment-2793  2787ba81-0145-471f-abc6-a51757bee11c 25460 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc00777bfb7 0xc00777bfb8}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzll7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzll7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-pd9f6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pd9f6 webserver-deployment-845c8977d9- deployment-2793  d2e0c85b-44e2-45ba-bc96-aa161f943f44 25526 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56140 0xc002c56141}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6mpqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6mpqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-rhqmh" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rhqmh webserver-deployment-845c8977d9- deployment-2793  f89b7e93-18ab-4587-bbef-531d0ad4352c 25291 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:769b4bbbe3c0206807a40ad120861df8682d9edafd402a7139f49b9d8897df3a cni.projectcalico.org/podIP:10.233.120.209/32 cni.projectcalico.org/podIPs:10.233.120.209/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56307 0xc002c56308}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcghh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcghh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.102,PodIP:10.233.120.209,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6657b7f3e7fa9b0b91013192ed1047741a409a8362f53cd4e59b3b9d768949d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-rxjjr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rxjjr webserver-deployment-845c8977d9- deployment-2793  9b38a709-6a36-43f0-8e7f-14104a81d9c2 25277 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:23051e6ea5f4a992cb475c4487a5b5de6eeff5b4e32729588e1f11625dbe1415 cni.projectcalico.org/podIP:10.233.99.125/32 cni.projectcalico.org/podIPs:10.233.99.125/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56537 0xc002c56538}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr6t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr6t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.125,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://14003e272068133f7a551c4bb8918baa4be8a7c3d2979c5a1da7e8337fef738e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.125,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.730: INFO: Pod "webserver-deployment-845c8977d9-t8gr5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t8gr5 webserver-deployment-845c8977d9- deployment-2793  2a878cc5-1b89-40f7-baf7-f871c75a74eb 25516 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56747 0xc002c56748}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-npphh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-npphh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:,StartTime:2023-02-12 12:08:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.731: INFO: Pod "webserver-deployment-845c8977d9-v274p" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-v274p webserver-deployment-845c8977d9- deployment-2793  7d1ad1c0-efff-4e6a-afcc-2fb0bd4f9f0f 25304 0 2023-02-12 12:08:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:de1da577457461fce6f00cc913bf6d6f69b4e79926abeef409206f97cc6d08f1 cni.projectcalico.org/podIP:10.233.120.75/32 cni.projectcalico.org/podIPs:10.233.120.75/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56937 0xc002c56938}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:08:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:08:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.120.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wl2q9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wl2q9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.101,PodIP:10.233.120.75,StartTime:2023-02-12 12:08:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:08:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5ea359515464e5b75561b8730cbf352b678cc44443ffb17938e0dce8f94d0cfb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.120.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.731: INFO: Pod "webserver-deployment-845c8977d9-x2df9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-x2df9 webserver-deployment-845c8977d9- deployment-2793  ccf5986e-e8e1-438c-8200-b4de793fbec8 25453 0 2023-02-12 12:08:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56b47 0xc002c56b48}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjnwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjnwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:,StartTime:2023-02-12 12:08:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 12 12:08:47.731: INFO: Pod "webserver-deployment-845c8977d9-xfv5b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xfv5b webserver-deployment-845c8977d9- deployment-2793  6a131f3b-ef57-4064-920d-8decb30e575a 25477 0 2023-02-12 12:08:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 91fb6d17-d3a1-4eac-be5c-39929899d487 0xc002c56d27 0xc002c56d28}] [] [{kube-controller-manager Update v1 2023-02-12 12:08:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"91fb6d17-d3a1-4eac-be5c-39929899d487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mh2b5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mh2b5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:08:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 12:08:47.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2793" for this suite. 02/12/23 12:08:47.771
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:08:47.928
Feb 12 12:08:47.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:08:47.93
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:48.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:48.035
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-25700cf2-6999-4650-9291-bc7c54abb325 02/12/23 12:08:48.044
STEP: Creating a pod to test consume configMaps 02/12/23 12:08:48.079
Feb 12 12:08:48.126: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b" in namespace "projected-5036" to be "Succeeded or Failed"
Feb 12 12:08:48.163: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.713085ms
Feb 12 12:08:50.169: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04299124s
Feb 12 12:08:52.175: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04906598s
Feb 12 12:09:14.729: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.602499474s
STEP: Saw pod success 02/12/23 12:09:14.729
Feb 12 12:09:14.729: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b" satisfied condition "Succeeded or Failed"
Feb 12 12:09:16.286: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:09:16.956
Feb 12 12:09:17.465: INFO: Waiting for pod pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b to disappear
Feb 12 12:09:17.610: INFO: Pod pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 12:09:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5036" for this suite. 02/12/23 12:09:17.638
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":232,"skipped":4319,"failed":0}
------------------------------
 [SLOW TEST] [29.734 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:08:47.928
    Feb 12 12:08:47.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:08:47.93
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:08:48.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:08:48.035
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-25700cf2-6999-4650-9291-bc7c54abb325 02/12/23 12:08:48.044
    STEP: Creating a pod to test consume configMaps 02/12/23 12:08:48.079
    Feb 12 12:08:48.126: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b" in namespace "projected-5036" to be "Succeeded or Failed"
    Feb 12 12:08:48.163: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.713085ms
    Feb 12 12:08:50.169: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04299124s
    Feb 12 12:08:52.175: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04906598s
    Feb 12 12:09:14.729: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.602499474s
    STEP: Saw pod success 02/12/23 12:09:14.729
    Feb 12 12:09:14.729: INFO: Pod "pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b" satisfied condition "Succeeded or Failed"
    Feb 12 12:09:16.286: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:09:16.956
    Feb 12 12:09:17.465: INFO: Waiting for pod pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b to disappear
    Feb 12 12:09:17.610: INFO: Pod pod-projected-configmaps-c5b4ad13-74c5-46b5-bac4-5a0612f7818b no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 12:09:17.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5036" for this suite. 02/12/23 12:09:17.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:09:17.663
Feb 12 12:09:17.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 12:09:17.664
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:09:42.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:09:42.483
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7378 02/12/23 12:09:42.485
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Feb 12 12:09:42.501: INFO: Found 0 stateful pods, waiting for 1
Feb 12 12:09:52.517: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/12/23 12:09:52.538
W0212 12:09:52.574768      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 12 12:09:52.583: INFO: Found 1 stateful pods, waiting for 2
Feb 12 12:10:02.616: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:10:02.616: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/12/23 12:10:02.629
STEP: Delete all of the StatefulSets 02/12/23 12:10:02.634
STEP: Verify that StatefulSets have been deleted 02/12/23 12:10:02.659
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 12:10:02.667: INFO: Deleting all statefulset in ns statefulset-7378
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 12:10:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7378" for this suite. 02/12/23 12:10:02.689
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":233,"skipped":4327,"failed":0}
------------------------------
 [SLOW TEST] [45.511 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:09:17.663
    Feb 12 12:09:17.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 12:09:17.664
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:09:42.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:09:42.483
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7378 02/12/23 12:09:42.485
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Feb 12 12:09:42.501: INFO: Found 0 stateful pods, waiting for 1
    Feb 12 12:09:52.517: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/12/23 12:09:52.538
    W0212 12:09:52.574768      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 12 12:09:52.583: INFO: Found 1 stateful pods, waiting for 2
    Feb 12 12:10:02.616: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:10:02.616: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/12/23 12:10:02.629
    STEP: Delete all of the StatefulSets 02/12/23 12:10:02.634
    STEP: Verify that StatefulSets have been deleted 02/12/23 12:10:02.659
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 12:10:02.667: INFO: Deleting all statefulset in ns statefulset-7378
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 12:10:02.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7378" for this suite. 02/12/23 12:10:02.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:10:03.175
Feb 12 12:10:03.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 12:10:03.176
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:10:03.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:10:03.242
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-c8a266a2-0294-401a-b5d8-3bcda9b83096 02/12/23 12:10:03.245
STEP: Creating a pod to test consume secrets 02/12/23 12:10:03.252
Feb 12 12:10:03.265: INFO: Waiting up to 5m0s for pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759" in namespace "secrets-6200" to be "Succeeded or Failed"
Feb 12 12:10:03.277: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759": Phase="Pending", Reason="", readiness=false. Elapsed: 11.605115ms
Feb 12 12:10:05.281: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015772266s
Feb 12 12:10:07.286: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021139359s
STEP: Saw pod success 02/12/23 12:10:07.286
Feb 12 12:10:07.287: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759" satisfied condition "Succeeded or Failed"
Feb 12 12:10:07.295: INFO: Trying to get logs from node kube-3 pod pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759 container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:10:07.311
Feb 12 12:10:07.327: INFO: Waiting for pod pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759 to disappear
Feb 12 12:10:07.330: INFO: Pod pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 12:10:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6200" for this suite. 02/12/23 12:10:07.333
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":234,"skipped":4340,"failed":0}
------------------------------
 [4.165 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:10:03.175
    Feb 12 12:10:03.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 12:10:03.176
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:10:03.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:10:03.242
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-c8a266a2-0294-401a-b5d8-3bcda9b83096 02/12/23 12:10:03.245
    STEP: Creating a pod to test consume secrets 02/12/23 12:10:03.252
    Feb 12 12:10:03.265: INFO: Waiting up to 5m0s for pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759" in namespace "secrets-6200" to be "Succeeded or Failed"
    Feb 12 12:10:03.277: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759": Phase="Pending", Reason="", readiness=false. Elapsed: 11.605115ms
    Feb 12 12:10:05.281: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015772266s
    Feb 12 12:10:07.286: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021139359s
    STEP: Saw pod success 02/12/23 12:10:07.286
    Feb 12 12:10:07.287: INFO: Pod "pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759" satisfied condition "Succeeded or Failed"
    Feb 12 12:10:07.295: INFO: Trying to get logs from node kube-3 pod pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759 container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:10:07.311
    Feb 12 12:10:07.327: INFO: Waiting for pod pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759 to disappear
    Feb 12 12:10:07.330: INFO: Pod pod-secrets-ceaf8cae-7912-4fa3-86d9-2dad38352759 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 12:10:07.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6200" for this suite. 02/12/23 12:10:07.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:10:07.342
Feb 12 12:10:07.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:10:07.342
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:10:07.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:10:07.365
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:10:07.391
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:10:07.628
STEP: Deploying the webhook pod 02/12/23 12:10:07.637
STEP: Wait for the deployment to be ready 02/12/23 12:10:07.648
Feb 12 12:10:07.655: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/12/23 12:10:09.697
STEP: Verifying the service has paired with the endpoint 02/12/23 12:10:09.729
Feb 12 12:10:10.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 02/12/23 12:10:10.74
STEP: create a pod 02/12/23 12:10:10.769
Feb 12 12:10:10.776: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9293" to be "running"
Feb 12 12:10:10.781: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872792ms
Feb 12 12:10:12.798: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022024705s
Feb 12 12:10:12.798: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/12/23 12:10:12.798
Feb 12 12:10:12.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=webhook-9293 attach --namespace=webhook-9293 to-be-attached-pod -i -c=container1'
Feb 12 12:10:12.864: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:10:12.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9293" for this suite. 02/12/23 12:10:12.875
STEP: Destroying namespace "webhook-9293-markers" for this suite. 02/12/23 12:10:12.884
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":235,"skipped":4357,"failed":0}
------------------------------
 [SLOW TEST] [5.650 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:10:07.342
    Feb 12 12:10:07.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:10:07.342
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:10:07.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:10:07.365
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:10:07.391
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:10:07.628
    STEP: Deploying the webhook pod 02/12/23 12:10:07.637
    STEP: Wait for the deployment to be ready 02/12/23 12:10:07.648
    Feb 12 12:10:07.655: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/12/23 12:10:09.697
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:10:09.729
    Feb 12 12:10:10.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 02/12/23 12:10:10.74
    STEP: create a pod 02/12/23 12:10:10.769
    Feb 12 12:10:10.776: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9293" to be "running"
    Feb 12 12:10:10.781: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.872792ms
    Feb 12 12:10:12.798: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022024705s
    Feb 12 12:10:12.798: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/12/23 12:10:12.798
    Feb 12 12:10:12.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=webhook-9293 attach --namespace=webhook-9293 to-be-attached-pod -i -c=container1'
    Feb 12 12:10:12.864: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:10:12.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9293" for this suite. 02/12/23 12:10:12.875
    STEP: Destroying namespace "webhook-9293-markers" for this suite. 02/12/23 12:10:12.884
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:10:12.991
Feb 12 12:10:12.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename cronjob 02/12/23 12:10:12.993
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:10:13.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:10:13.045
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/12/23 12:10:13.052
STEP: Ensuring a job is scheduled 02/12/23 12:10:13.063
STEP: Ensuring exactly one is scheduled 02/12/23 12:11:01.069
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/12/23 12:11:01.072
STEP: Ensuring no more jobs are scheduled 02/12/23 12:11:01.076
STEP: Removing cronjob 02/12/23 12:16:01.101
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 12 12:16:01.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5995" for this suite. 02/12/23 12:16:01.125
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":236,"skipped":4360,"failed":0}
------------------------------
 [SLOW TEST] [348.143 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:10:12.991
    Feb 12 12:10:12.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename cronjob 02/12/23 12:10:12.993
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:10:13.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:10:13.045
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/12/23 12:10:13.052
    STEP: Ensuring a job is scheduled 02/12/23 12:10:13.063
    STEP: Ensuring exactly one is scheduled 02/12/23 12:11:01.069
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/12/23 12:11:01.072
    STEP: Ensuring no more jobs are scheduled 02/12/23 12:11:01.076
    STEP: Removing cronjob 02/12/23 12:16:01.101
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 12 12:16:01.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5995" for this suite. 02/12/23 12:16:01.125
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:01.135
Feb 12 12:16:01.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename containers 02/12/23 12:16:01.136
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:01.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:01.158
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 02/12/23 12:16:01.165
Feb 12 12:16:01.179: INFO: Waiting up to 5m0s for pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2" in namespace "containers-869" to be "Succeeded or Failed"
Feb 12 12:16:01.185: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.518245ms
Feb 12 12:16:03.198: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019003143s
Feb 12 12:16:05.722: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542997132s
Feb 12 12:16:07.189: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010175572s
Feb 12 12:16:09.202: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022738679s
STEP: Saw pod success 02/12/23 12:16:09.202
Feb 12 12:16:09.203: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2" satisfied condition "Succeeded or Failed"
Feb 12 12:16:09.218: INFO: Trying to get logs from node kube-3 pod client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:16:09.256
Feb 12 12:16:09.326: INFO: Waiting for pod client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2 to disappear
Feb 12 12:16:09.331: INFO: Pod client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Feb 12 12:16:09.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-869" for this suite. 02/12/23 12:16:09.338
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":237,"skipped":4360,"failed":0}
------------------------------
 [SLOW TEST] [8.217 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:01.135
    Feb 12 12:16:01.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename containers 02/12/23 12:16:01.136
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:01.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:01.158
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 02/12/23 12:16:01.165
    Feb 12 12:16:01.179: INFO: Waiting up to 5m0s for pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2" in namespace "containers-869" to be "Succeeded or Failed"
    Feb 12 12:16:01.185: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.518245ms
    Feb 12 12:16:03.198: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019003143s
    Feb 12 12:16:05.722: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542997132s
    Feb 12 12:16:07.189: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010175572s
    Feb 12 12:16:09.202: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022738679s
    STEP: Saw pod success 02/12/23 12:16:09.202
    Feb 12 12:16:09.203: INFO: Pod "client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2" satisfied condition "Succeeded or Failed"
    Feb 12 12:16:09.218: INFO: Trying to get logs from node kube-3 pod client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:16:09.256
    Feb 12 12:16:09.326: INFO: Waiting for pod client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2 to disappear
    Feb 12 12:16:09.331: INFO: Pod client-containers-c77a27db-cd4c-4b1c-bf11-a75442df9eb2 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Feb 12 12:16:09.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-869" for this suite. 02/12/23 12:16:09.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:09.357
Feb 12 12:16:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replication-controller 02/12/23 12:16:09.358
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:09.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:09.386
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 02/12/23 12:16:09.426
STEP: When the matched label of one of its pods change 02/12/23 12:16:09.455
Feb 12 12:16:09.473: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/12/23 12:16:09.526
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 12 12:16:10.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-16" for this suite. 02/12/23 12:16:10.61
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":238,"skipped":4379,"failed":0}
------------------------------
 [1.260 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:09.357
    Feb 12 12:16:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replication-controller 02/12/23 12:16:09.358
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:09.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:09.386
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 02/12/23 12:16:09.426
    STEP: When the matched label of one of its pods change 02/12/23 12:16:09.455
    Feb 12 12:16:09.473: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/12/23 12:16:09.526
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 12 12:16:10.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-16" for this suite. 02/12/23 12:16:10.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:10.618
Feb 12 12:16:10.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:16:10.619
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:10.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:10.642
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-fe4ceafa-2e22-469b-a92b-c9c36cf3e948 02/12/23 12:16:10.644
STEP: Creating a pod to test consume configMaps 02/12/23 12:16:10.649
Feb 12 12:16:10.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c" in namespace "configmap-5807" to be "Succeeded or Failed"
Feb 12 12:16:10.663: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.753594ms
Feb 12 12:16:12.670: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013072715s
Feb 12 12:16:14.668: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01094668s
STEP: Saw pod success 02/12/23 12:16:14.668
Feb 12 12:16:14.668: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c" satisfied condition "Succeeded or Failed"
Feb 12 12:16:14.670: INFO: Trying to get logs from node kube-3 pod pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:16:14.678
Feb 12 12:16:14.694: INFO: Waiting for pod pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c to disappear
Feb 12 12:16:14.698: INFO: Pod pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:16:14.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5807" for this suite. 02/12/23 12:16:14.703
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":239,"skipped":4389,"failed":0}
------------------------------
 [4.094 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:10.618
    Feb 12 12:16:10.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:16:10.619
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:10.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:10.642
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-fe4ceafa-2e22-469b-a92b-c9c36cf3e948 02/12/23 12:16:10.644
    STEP: Creating a pod to test consume configMaps 02/12/23 12:16:10.649
    Feb 12 12:16:10.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c" in namespace "configmap-5807" to be "Succeeded or Failed"
    Feb 12 12:16:10.663: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.753594ms
    Feb 12 12:16:12.670: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013072715s
    Feb 12 12:16:14.668: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01094668s
    STEP: Saw pod success 02/12/23 12:16:14.668
    Feb 12 12:16:14.668: INFO: Pod "pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c" satisfied condition "Succeeded or Failed"
    Feb 12 12:16:14.670: INFO: Trying to get logs from node kube-3 pod pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:16:14.678
    Feb 12 12:16:14.694: INFO: Waiting for pod pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c to disappear
    Feb 12 12:16:14.698: INFO: Pod pod-configmaps-62e8e5ff-4cf1-43e4-9c1f-23636e9dd03c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:16:14.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5807" for this suite. 02/12/23 12:16:14.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:14.713
Feb 12 12:16:14.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:16:14.713
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.731
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Feb 12 12:16:14.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1385 version'
Feb 12 12:16:14.779: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 12 12:16:14.779: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:16:14.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1385" for this suite. 02/12/23 12:16:14.783
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":240,"skipped":4400,"failed":0}
------------------------------
 [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:14.713
    Feb 12 12:16:14.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:16:14.713
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.731
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Feb 12 12:16:14.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1385 version'
    Feb 12 12:16:14.779: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 12 12:16:14.779: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:22:09Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.6\", GitCommit:\"ff2c119726cc1f8926fb0585c74b25921e866a28\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:26Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:16:14.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1385" for this suite. 02/12/23 12:16:14.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:14.794
Feb 12 12:16:14.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 12:16:14.795
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.814
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 02/12/23 12:16:14.816
STEP: Getting a ResourceQuota 02/12/23 12:16:14.821
STEP: Listing all ResourceQuotas with LabelSelector 02/12/23 12:16:14.835
STEP: Patching the ResourceQuota 02/12/23 12:16:14.838
STEP: Deleting a Collection of ResourceQuotas 02/12/23 12:16:14.846
STEP: Verifying the deleted ResourceQuota 02/12/23 12:16:14.858
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 12:16:14.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7785" for this suite. 02/12/23 12:16:14.867
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":241,"skipped":4411,"failed":0}
------------------------------
 [0.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:14.794
    Feb 12 12:16:14.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 12:16:14.795
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.814
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 02/12/23 12:16:14.816
    STEP: Getting a ResourceQuota 02/12/23 12:16:14.821
    STEP: Listing all ResourceQuotas with LabelSelector 02/12/23 12:16:14.835
    STEP: Patching the ResourceQuota 02/12/23 12:16:14.838
    STEP: Deleting a Collection of ResourceQuotas 02/12/23 12:16:14.846
    STEP: Verifying the deleted ResourceQuota 02/12/23 12:16:14.858
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 12:16:14.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7785" for this suite. 02/12/23 12:16:14.867
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:14.878
Feb 12 12:16:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:16:14.879
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.908
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:16:14.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9451" for this suite. 02/12/23 12:16:14.952
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":242,"skipped":4413,"failed":0}
------------------------------
 [0.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:14.878
    Feb 12 12:16:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:16:14.879
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.908
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:16:14.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9451" for this suite. 02/12/23 12:16:14.952
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:14.96
Feb 12 12:16:14.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:16:14.961
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.98
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 02/12/23 12:16:14.982
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:16:14.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-683" for this suite. 02/12/23 12:16:14.99
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":243,"skipped":4416,"failed":0}
------------------------------
 [0.040 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:14.96
    Feb 12 12:16:14.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:16:14.961
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:14.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:14.98
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 02/12/23 12:16:14.982
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:16:14.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-683" for this suite. 02/12/23 12:16:14.99
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:15.003
Feb 12 12:16:15.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 12:16:15.003
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:15.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:15.02
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/12/23 12:16:15.023
STEP: Wait for the Deployment to create new ReplicaSet 02/12/23 12:16:15.03
STEP: delete the deployment 02/12/23 12:16:15.142
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/12/23 12:16:15.151
STEP: Gathering metrics 02/12/23 12:16:15.707
Feb 12 12:16:15.746: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Feb 12 12:16:15.755: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.166197ms
Feb 12 12:16:15.755: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Feb 12 12:16:15.755: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Feb 12 12:16:15.884: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 12:16:15.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-73" for this suite. 02/12/23 12:16:15.895
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":244,"skipped":4462,"failed":0}
------------------------------
 [0.907 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:15.003
    Feb 12 12:16:15.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 12:16:15.003
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:15.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:15.02
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/12/23 12:16:15.023
    STEP: Wait for the Deployment to create new ReplicaSet 02/12/23 12:16:15.03
    STEP: delete the deployment 02/12/23 12:16:15.142
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/12/23 12:16:15.151
    STEP: Gathering metrics 02/12/23 12:16:15.707
    Feb 12 12:16:15.746: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Feb 12 12:16:15.755: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.166197ms
    Feb 12 12:16:15.755: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Feb 12 12:16:15.755: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Feb 12 12:16:15.884: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 12:16:15.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-73" for this suite. 02/12/23 12:16:15.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:15.911
Feb 12 12:16:15.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:16:15.911
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:15.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:15.941
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-f63e1b4f-6489-4f45-96e2-bd6062fc0854 02/12/23 12:16:15.946
STEP: Creating a pod to test consume configMaps 02/12/23 12:16:15.954
Feb 12 12:16:15.977: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544" in namespace "projected-7662" to be "Succeeded or Failed"
Feb 12 12:16:15.984: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885442ms
Feb 12 12:16:17.998: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020926855s
Feb 12 12:16:19.991: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014311197s
STEP: Saw pod success 02/12/23 12:16:19.991
Feb 12 12:16:19.992: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544" satisfied condition "Succeeded or Failed"
Feb 12 12:16:19.997: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:16:20.012
Feb 12 12:16:20.029: INFO: Waiting for pod pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544 to disappear
Feb 12 12:16:20.035: INFO: Pod pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 12:16:20.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7662" for this suite. 02/12/23 12:16:20.041
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":245,"skipped":4477,"failed":0}
------------------------------
 [4.140 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:15.911
    Feb 12 12:16:15.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:16:15.911
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:15.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:15.941
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-f63e1b4f-6489-4f45-96e2-bd6062fc0854 02/12/23 12:16:15.946
    STEP: Creating a pod to test consume configMaps 02/12/23 12:16:15.954
    Feb 12 12:16:15.977: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544" in namespace "projected-7662" to be "Succeeded or Failed"
    Feb 12 12:16:15.984: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885442ms
    Feb 12 12:16:17.998: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020926855s
    Feb 12 12:16:19.991: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014311197s
    STEP: Saw pod success 02/12/23 12:16:19.991
    Feb 12 12:16:19.992: INFO: Pod "pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544" satisfied condition "Succeeded or Failed"
    Feb 12 12:16:19.997: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:16:20.012
    Feb 12 12:16:20.029: INFO: Waiting for pod pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544 to disappear
    Feb 12 12:16:20.035: INFO: Pod pod-projected-configmaps-9c96c1f2-35f9-4e4d-9739-dde535994544 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 12:16:20.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7662" for this suite. 02/12/23 12:16:20.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:20.052
Feb 12 12:16:20.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 12:16:20.053
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:20.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:20.094
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/12/23 12:16:20.11
STEP: delete the rc 02/12/23 12:16:42.86
STEP: wait for the rc to be deleted 02/12/23 12:16:43.345
Feb 12 12:16:50.147: INFO: 81 pods remaining
Feb 12 12:16:50.147: INFO: 74 pods has nil DeletionTimestamp
Feb 12 12:16:50.147: INFO: 
Feb 12 12:16:52.347: INFO: 64 pods remaining
Feb 12 12:16:52.349: INFO: 60 pods has nil DeletionTimestamp
Feb 12 12:16:52.350: INFO: 
Feb 12 12:16:53.476: INFO: 38 pods remaining
Feb 12 12:16:53.476: INFO: 37 pods has nil DeletionTimestamp
Feb 12 12:16:53.476: INFO: 
Feb 12 12:16:54.432: INFO: 25 pods remaining
Feb 12 12:16:54.432: INFO: 24 pods has nil DeletionTimestamp
Feb 12 12:16:54.432: INFO: 
Feb 12 12:16:55.943: INFO: 15 pods remaining
Feb 12 12:16:55.944: INFO: 10 pods has nil DeletionTimestamp
Feb 12 12:16:55.944: INFO: 
STEP: Gathering metrics 02/12/23 12:16:56.37
Feb 12 12:16:59.273: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Feb 12 12:16:59.473: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 200.247337ms
Feb 12 12:16:59.473: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Feb 12 12:16:59.473: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Feb 12 12:16:59.597: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 12:16:59.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6130" for this suite. 02/12/23 12:16:59.661
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":246,"skipped":4492,"failed":0}
------------------------------
 [SLOW TEST] [39.679 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:20.052
    Feb 12 12:16:20.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 12:16:20.053
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:20.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:20.094
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/12/23 12:16:20.11
    STEP: delete the rc 02/12/23 12:16:42.86
    STEP: wait for the rc to be deleted 02/12/23 12:16:43.345
    Feb 12 12:16:50.147: INFO: 81 pods remaining
    Feb 12 12:16:50.147: INFO: 74 pods has nil DeletionTimestamp
    Feb 12 12:16:50.147: INFO: 
    Feb 12 12:16:52.347: INFO: 64 pods remaining
    Feb 12 12:16:52.349: INFO: 60 pods has nil DeletionTimestamp
    Feb 12 12:16:52.350: INFO: 
    Feb 12 12:16:53.476: INFO: 38 pods remaining
    Feb 12 12:16:53.476: INFO: 37 pods has nil DeletionTimestamp
    Feb 12 12:16:53.476: INFO: 
    Feb 12 12:16:54.432: INFO: 25 pods remaining
    Feb 12 12:16:54.432: INFO: 24 pods has nil DeletionTimestamp
    Feb 12 12:16:54.432: INFO: 
    Feb 12 12:16:55.943: INFO: 15 pods remaining
    Feb 12 12:16:55.944: INFO: 10 pods has nil DeletionTimestamp
    Feb 12 12:16:55.944: INFO: 
    STEP: Gathering metrics 02/12/23 12:16:56.37
    Feb 12 12:16:59.273: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Feb 12 12:16:59.473: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 200.247337ms
    Feb 12 12:16:59.473: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Feb 12 12:16:59.473: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Feb 12 12:16:59.597: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 12:16:59.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6130" for this suite. 02/12/23 12:16:59.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:16:59.731
Feb 12 12:16:59.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replicaset 02/12/23 12:16:59.732
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:59.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:59.959
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 12 12:17:00.205: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 12 12:17:05.216: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/12/23 12:17:05.216
Feb 12 12:17:05.216: INFO: Waiting up to 5m0s for pod "test-rs-gk7kl" in namespace "replicaset-5518" to be "running"
Feb 12 12:17:05.224: INFO: Pod "test-rs-gk7kl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.7338ms
Feb 12 12:17:07.957: INFO: Pod "test-rs-gk7kl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740837212s
Feb 12 12:17:09.235: INFO: Pod "test-rs-gk7kl": Phase="Running", Reason="", readiness=true. Elapsed: 4.019260475s
Feb 12 12:17:09.236: INFO: Pod "test-rs-gk7kl" satisfied condition "running"
STEP: Scaling up "test-rs" replicaset  02/12/23 12:17:09.236
Feb 12 12:17:09.278: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/12/23 12:17:09.278
W0212 12:17:09.302645      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 12 12:17:09.307: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
Feb 12 12:17:09.356: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
Feb 12 12:17:09.789: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
Feb 12 12:17:09.810: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
Feb 12 12:17:13.552: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 2, AvailableReplicas 2
Feb 12 12:17:16.759: INFO: observed Replicaset test-rs in namespace replicaset-5518 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Feb 12 12:17:16.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5518" for this suite. 02/12/23 12:17:16.764
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":247,"skipped":4512,"failed":0}
------------------------------
 [SLOW TEST] [17.040 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:16:59.731
    Feb 12 12:16:59.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replicaset 02/12/23 12:16:59.732
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:16:59.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:16:59.959
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 12 12:17:00.205: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 12 12:17:05.216: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/12/23 12:17:05.216
    Feb 12 12:17:05.216: INFO: Waiting up to 5m0s for pod "test-rs-gk7kl" in namespace "replicaset-5518" to be "running"
    Feb 12 12:17:05.224: INFO: Pod "test-rs-gk7kl": Phase="Pending", Reason="", readiness=false. Elapsed: 7.7338ms
    Feb 12 12:17:07.957: INFO: Pod "test-rs-gk7kl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740837212s
    Feb 12 12:17:09.235: INFO: Pod "test-rs-gk7kl": Phase="Running", Reason="", readiness=true. Elapsed: 4.019260475s
    Feb 12 12:17:09.236: INFO: Pod "test-rs-gk7kl" satisfied condition "running"
    STEP: Scaling up "test-rs" replicaset  02/12/23 12:17:09.236
    Feb 12 12:17:09.278: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/12/23 12:17:09.278
    W0212 12:17:09.302645      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 12 12:17:09.307: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
    Feb 12 12:17:09.356: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
    Feb 12 12:17:09.789: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
    Feb 12 12:17:09.810: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 1, AvailableReplicas 1
    Feb 12 12:17:13.552: INFO: observed ReplicaSet test-rs in namespace replicaset-5518 with ReadyReplicas 2, AvailableReplicas 2
    Feb 12 12:17:16.759: INFO: observed Replicaset test-rs in namespace replicaset-5518 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Feb 12 12:17:16.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5518" for this suite. 02/12/23 12:17:16.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:16.774
Feb 12 12:17:16.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 12:17:16.774
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:16.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:16.793
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/12/23 12:17:16.795
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_tcp@PTR;sleep 1; done
 02/12/23 12:17:16.823
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_tcp@PTR;sleep 1; done
 02/12/23 12:17:16.823
STEP: creating a pod to probe DNS 02/12/23 12:17:16.823
STEP: submitting the pod to kubernetes 02/12/23 12:17:16.823
Feb 12 12:17:16.840: INFO: Waiting up to 15m0s for pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3" in namespace "dns-1668" to be "running"
Feb 12 12:17:16.857: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.177461ms
Feb 12 12:17:18.870: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030387658s
Feb 12 12:17:20.869: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029420737s
Feb 12 12:17:22.861: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021108746s
Feb 12 12:17:24.862: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Running", Reason="", readiness=true. Elapsed: 8.022040331s
Feb 12 12:17:24.862: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3" satisfied condition "running"
STEP: retrieving the pod 02/12/23 12:17:24.862
STEP: looking for the results for each expected name from probers 02/12/23 12:17:24.865
Feb 12 12:17:24.869: INFO: Unable to read wheezy_udp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.877: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.891: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.912: INFO: Unable to read jessie_udp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.918: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.922: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
Feb 12 12:17:24.933: INFO: Lookups using dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3 failed for: [wheezy_udp@dns-test-service.dns-1668.svc.cluster.local wheezy_tcp@dns-test-service.dns-1668.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local jessie_udp@dns-test-service.dns-1668.svc.cluster.local jessie_tcp@dns-test-service.dns-1668.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local]

Feb 12 12:17:30.031: INFO: DNS probes using dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3 succeeded

STEP: deleting the pod 02/12/23 12:17:30.031
STEP: deleting the test service 02/12/23 12:17:30.089
STEP: deleting the test headless service 02/12/23 12:17:30.168
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 12:17:30.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1668" for this suite. 02/12/23 12:17:30.201
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":248,"skipped":4563,"failed":0}
------------------------------
 [SLOW TEST] [13.438 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:16.774
    Feb 12 12:17:16.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 12:17:16.774
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:16.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:16.793
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/12/23 12:17:16.795
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_tcp@PTR;sleep 1; done
     02/12/23 12:17:16.823
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1668.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1668.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1668.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_udp@PTR;check="$$(dig +tcp +noall +answer +search 190.50.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.50.190_tcp@PTR;sleep 1; done
     02/12/23 12:17:16.823
    STEP: creating a pod to probe DNS 02/12/23 12:17:16.823
    STEP: submitting the pod to kubernetes 02/12/23 12:17:16.823
    Feb 12 12:17:16.840: INFO: Waiting up to 15m0s for pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3" in namespace "dns-1668" to be "running"
    Feb 12 12:17:16.857: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.177461ms
    Feb 12 12:17:18.870: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030387658s
    Feb 12 12:17:20.869: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029420737s
    Feb 12 12:17:22.861: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021108746s
    Feb 12 12:17:24.862: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3": Phase="Running", Reason="", readiness=true. Elapsed: 8.022040331s
    Feb 12 12:17:24.862: INFO: Pod "dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 12:17:24.862
    STEP: looking for the results for each expected name from probers 02/12/23 12:17:24.865
    Feb 12 12:17:24.869: INFO: Unable to read wheezy_udp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.873: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.877: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.891: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.912: INFO: Unable to read jessie_udp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.918: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.922: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local from pod dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3: the server could not find the requested resource (get pods dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3)
    Feb 12 12:17:24.933: INFO: Lookups using dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3 failed for: [wheezy_udp@dns-test-service.dns-1668.svc.cluster.local wheezy_tcp@dns-test-service.dns-1668.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local jessie_udp@dns-test-service.dns-1668.svc.cluster.local jessie_tcp@dns-test-service.dns-1668.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1668.svc.cluster.local]

    Feb 12 12:17:30.031: INFO: DNS probes using dns-1668/dns-test-d1104891-f42f-4d0a-b2a1-25e6e974cfe3 succeeded

    STEP: deleting the pod 02/12/23 12:17:30.031
    STEP: deleting the test service 02/12/23 12:17:30.089
    STEP: deleting the test headless service 02/12/23 12:17:30.168
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 12:17:30.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1668" for this suite. 02/12/23 12:17:30.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:30.215
Feb 12 12:17:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:17:30.216
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:30.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:30.247
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-fa2fcf98-22d6-47cf-9c30-ee3e70f315e7 02/12/23 12:17:30.251
STEP: Creating a pod to test consume secrets 02/12/23 12:17:30.259
Feb 12 12:17:30.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff" in namespace "projected-4624" to be "Succeeded or Failed"
Feb 12 12:17:30.276: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840033ms
Feb 12 12:17:32.293: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020393036s
Feb 12 12:17:34.287: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014912848s
STEP: Saw pod success 02/12/23 12:17:34.287
Feb 12 12:17:34.288: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff" satisfied condition "Succeeded or Failed"
Feb 12 12:17:34.301: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff container projected-secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:17:34.317
Feb 12 12:17:34.339: INFO: Waiting for pod pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff to disappear
Feb 12 12:17:34.344: INFO: Pod pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 12:17:34.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4624" for this suite. 02/12/23 12:17:34.349
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":249,"skipped":4587,"failed":0}
------------------------------
 [4.144 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:30.215
    Feb 12 12:17:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:17:30.216
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:30.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:30.247
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-fa2fcf98-22d6-47cf-9c30-ee3e70f315e7 02/12/23 12:17:30.251
    STEP: Creating a pod to test consume secrets 02/12/23 12:17:30.259
    Feb 12 12:17:30.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff" in namespace "projected-4624" to be "Succeeded or Failed"
    Feb 12 12:17:30.276: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.840033ms
    Feb 12 12:17:32.293: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020393036s
    Feb 12 12:17:34.287: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014912848s
    STEP: Saw pod success 02/12/23 12:17:34.287
    Feb 12 12:17:34.288: INFO: Pod "pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff" satisfied condition "Succeeded or Failed"
    Feb 12 12:17:34.301: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:17:34.317
    Feb 12 12:17:34.339: INFO: Waiting for pod pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff to disappear
    Feb 12 12:17:34.344: INFO: Pod pod-projected-secrets-d9fc55e5-d6ae-4f63-9b02-4f90ce842bff no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 12:17:34.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4624" for this suite. 02/12/23 12:17:34.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:34.359
Feb 12 12:17:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir-wrapper 02/12/23 12:17:34.36
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:34.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:34.379
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 12 12:17:34.400: INFO: Waiting up to 5m0s for pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d" in namespace "emptydir-wrapper-3032" to be "running and ready"
Feb 12 12:17:34.406: INFO: Pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034454ms
Feb 12 12:17:34.406: INFO: The phase of Pod pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:17:36.418: INFO: Pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01824858s
Feb 12 12:17:36.418: INFO: The phase of Pod pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d is Running (Ready = true)
Feb 12 12:17:36.418: INFO: Pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/12/23 12:17:36.436
STEP: Cleaning up the configmap 02/12/23 12:17:36.452
STEP: Cleaning up the pod 02/12/23 12:17:36.468
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Feb 12 12:17:36.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3032" for this suite. 02/12/23 12:17:36.492
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":250,"skipped":4598,"failed":0}
------------------------------
 [2.143 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:34.359
    Feb 12 12:17:34.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir-wrapper 02/12/23 12:17:34.36
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:34.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:34.379
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 12 12:17:34.400: INFO: Waiting up to 5m0s for pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d" in namespace "emptydir-wrapper-3032" to be "running and ready"
    Feb 12 12:17:34.406: INFO: Pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034454ms
    Feb 12 12:17:34.406: INFO: The phase of Pod pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:17:36.418: INFO: Pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d": Phase="Running", Reason="", readiness=true. Elapsed: 2.01824858s
    Feb 12 12:17:36.418: INFO: The phase of Pod pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d is Running (Ready = true)
    Feb 12 12:17:36.418: INFO: Pod "pod-secrets-56b085d5-f0af-4afe-974a-7103e202963d" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/12/23 12:17:36.436
    STEP: Cleaning up the configmap 02/12/23 12:17:36.452
    STEP: Cleaning up the pod 02/12/23 12:17:36.468
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:17:36.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3032" for this suite. 02/12/23 12:17:36.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:36.505
Feb 12 12:17:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 12:17:36.506
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:36.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:36.524
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 02/12/23 12:17:36.526
STEP: submitting the pod to kubernetes 02/12/23 12:17:36.526
Feb 12 12:17:36.534: INFO: Waiting up to 5m0s for pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" in namespace "pods-5939" to be "running and ready"
Feb 12 12:17:36.537: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728873ms
Feb 12 12:17:36.537: INFO: The phase of Pod pod-update-4abe370d-11ea-4590-846c-d515efb84487 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:17:38.550: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487": Phase="Running", Reason="", readiness=true. Elapsed: 2.015315098s
Feb 12 12:17:38.550: INFO: The phase of Pod pod-update-4abe370d-11ea-4590-846c-d515efb84487 is Running (Ready = true)
Feb 12 12:17:38.550: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/12/23 12:17:38.56
STEP: updating the pod 02/12/23 12:17:38.567
Feb 12 12:17:39.093: INFO: Successfully updated pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487"
Feb 12 12:17:39.093: INFO: Waiting up to 5m0s for pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" in namespace "pods-5939" to be "running"
Feb 12 12:17:39.100: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487": Phase="Running", Reason="", readiness=true. Elapsed: 7.462337ms
Feb 12 12:17:39.100: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/12/23 12:17:39.1
Feb 12 12:17:39.106: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 12:17:39.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5939" for this suite. 02/12/23 12:17:39.111
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":251,"skipped":4619,"failed":0}
------------------------------
 [2.612 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:36.505
    Feb 12 12:17:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 12:17:36.506
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:36.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:36.524
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 02/12/23 12:17:36.526
    STEP: submitting the pod to kubernetes 02/12/23 12:17:36.526
    Feb 12 12:17:36.534: INFO: Waiting up to 5m0s for pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" in namespace "pods-5939" to be "running and ready"
    Feb 12 12:17:36.537: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728873ms
    Feb 12 12:17:36.537: INFO: The phase of Pod pod-update-4abe370d-11ea-4590-846c-d515efb84487 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:17:38.550: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487": Phase="Running", Reason="", readiness=true. Elapsed: 2.015315098s
    Feb 12 12:17:38.550: INFO: The phase of Pod pod-update-4abe370d-11ea-4590-846c-d515efb84487 is Running (Ready = true)
    Feb 12 12:17:38.550: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/12/23 12:17:38.56
    STEP: updating the pod 02/12/23 12:17:38.567
    Feb 12 12:17:39.093: INFO: Successfully updated pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487"
    Feb 12 12:17:39.093: INFO: Waiting up to 5m0s for pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" in namespace "pods-5939" to be "running"
    Feb 12 12:17:39.100: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487": Phase="Running", Reason="", readiness=true. Elapsed: 7.462337ms
    Feb 12 12:17:39.100: INFO: Pod "pod-update-4abe370d-11ea-4590-846c-d515efb84487" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/12/23 12:17:39.1
    Feb 12 12:17:39.106: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 12:17:39.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5939" for this suite. 02/12/23 12:17:39.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:39.119
Feb 12 12:17:39.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:17:39.12
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:39.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:39.139
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-bfc62f8d-0d17-4245-b795-112ee7829bc5 02/12/23 12:17:39.141
STEP: Creating a pod to test consume configMaps 02/12/23 12:17:39.147
Feb 12 12:17:39.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8" in namespace "configmap-6695" to be "Succeeded or Failed"
Feb 12 12:17:39.158: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572741ms
Feb 12 12:17:41.287: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132001679s
Feb 12 12:17:43.163: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008841155s
Feb 12 12:17:45.163: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008181872s
STEP: Saw pod success 02/12/23 12:17:45.163
Feb 12 12:17:45.163: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8" satisfied condition "Succeeded or Failed"
Feb 12 12:17:45.165: INFO: Trying to get logs from node kube-3 pod pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:17:45.171
Feb 12 12:17:45.184: INFO: Waiting for pod pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8 to disappear
Feb 12 12:17:45.187: INFO: Pod pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:17:45.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6695" for this suite. 02/12/23 12:17:45.19
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":252,"skipped":4640,"failed":0}
------------------------------
 [SLOW TEST] [6.077 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:39.119
    Feb 12 12:17:39.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:17:39.12
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:39.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:39.139
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-bfc62f8d-0d17-4245-b795-112ee7829bc5 02/12/23 12:17:39.141
    STEP: Creating a pod to test consume configMaps 02/12/23 12:17:39.147
    Feb 12 12:17:39.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8" in namespace "configmap-6695" to be "Succeeded or Failed"
    Feb 12 12:17:39.158: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572741ms
    Feb 12 12:17:41.287: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132001679s
    Feb 12 12:17:43.163: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008841155s
    Feb 12 12:17:45.163: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008181872s
    STEP: Saw pod success 02/12/23 12:17:45.163
    Feb 12 12:17:45.163: INFO: Pod "pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8" satisfied condition "Succeeded or Failed"
    Feb 12 12:17:45.165: INFO: Trying to get logs from node kube-3 pod pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:17:45.171
    Feb 12 12:17:45.184: INFO: Waiting for pod pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8 to disappear
    Feb 12 12:17:45.187: INFO: Pod pod-configmaps-a145f462-60b5-42e2-b8f9-29313745a4f8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:17:45.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6695" for this suite. 02/12/23 12:17:45.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:45.198
Feb 12 12:17:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 12:17:45.199
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:45.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:45.218
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-eb874d71-7a00-42fe-bd82-0b705df06750 02/12/23 12:17:45.221
STEP: Creating a pod to test consume secrets 02/12/23 12:17:45.227
Feb 12 12:17:45.235: INFO: Waiting up to 5m0s for pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa" in namespace "secrets-6302" to be "Succeeded or Failed"
Feb 12 12:17:45.250: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.770281ms
Feb 12 12:17:47.262: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027016984s
Feb 12 12:17:49.261: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026340415s
Feb 12 12:17:51.273: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037801168s
STEP: Saw pod success 02/12/23 12:17:51.273
Feb 12 12:17:51.273: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa" satisfied condition "Succeeded or Failed"
Feb 12 12:17:51.284: INFO: Trying to get logs from node kube-3 pod pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:17:51.29
Feb 12 12:17:51.332: INFO: Waiting for pod pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa to disappear
Feb 12 12:17:51.335: INFO: Pod pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 12:17:51.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6302" for this suite. 02/12/23 12:17:51.339
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":253,"skipped":4686,"failed":0}
------------------------------
 [SLOW TEST] [6.160 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:45.198
    Feb 12 12:17:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 12:17:45.199
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:45.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:45.218
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-eb874d71-7a00-42fe-bd82-0b705df06750 02/12/23 12:17:45.221
    STEP: Creating a pod to test consume secrets 02/12/23 12:17:45.227
    Feb 12 12:17:45.235: INFO: Waiting up to 5m0s for pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa" in namespace "secrets-6302" to be "Succeeded or Failed"
    Feb 12 12:17:45.250: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.770281ms
    Feb 12 12:17:47.262: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027016984s
    Feb 12 12:17:49.261: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026340415s
    Feb 12 12:17:51.273: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037801168s
    STEP: Saw pod success 02/12/23 12:17:51.273
    Feb 12 12:17:51.273: INFO: Pod "pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa" satisfied condition "Succeeded or Failed"
    Feb 12 12:17:51.284: INFO: Trying to get logs from node kube-3 pod pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:17:51.29
    Feb 12 12:17:51.332: INFO: Waiting for pod pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa to disappear
    Feb 12 12:17:51.335: INFO: Pod pod-secrets-7c8460c3-a6ca-4827-ab92-7c30041077fa no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 12:17:51.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6302" for this suite. 02/12/23 12:17:51.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:51.361
Feb 12 12:17:51.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename watch 02/12/23 12:17:51.362
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:51.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:51.443
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/12/23 12:17:51.445
STEP: creating a new configmap 02/12/23 12:17:51.447
STEP: modifying the configmap once 02/12/23 12:17:51.477
STEP: closing the watch once it receives two notifications 02/12/23 12:17:51.501
Feb 12 12:17:51.501: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29411 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 12:17:51.501: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29412 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/12/23 12:17:51.502
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/12/23 12:17:51.536
STEP: deleting the configmap 02/12/23 12:17:51.537
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/12/23 12:17:51.55
Feb 12 12:17:51.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29413 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 12:17:51.567: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29414 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 12 12:17:51.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5584" for this suite. 02/12/23 12:17:51.571
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":254,"skipped":4718,"failed":0}
------------------------------
 [0.229 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:51.361
    Feb 12 12:17:51.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename watch 02/12/23 12:17:51.362
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:51.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:51.443
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/12/23 12:17:51.445
    STEP: creating a new configmap 02/12/23 12:17:51.447
    STEP: modifying the configmap once 02/12/23 12:17:51.477
    STEP: closing the watch once it receives two notifications 02/12/23 12:17:51.501
    Feb 12 12:17:51.501: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29411 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 12:17:51.501: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29412 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/12/23 12:17:51.502
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/12/23 12:17:51.536
    STEP: deleting the configmap 02/12/23 12:17:51.537
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/12/23 12:17:51.55
    Feb 12 12:17:51.550: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29413 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 12:17:51.567: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5584  d215537d-43fa-4f7a-871c-c818ca60af14 29414 0 2023-02-12 12:17:51 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-12 12:17:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 12 12:17:51.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5584" for this suite. 02/12/23 12:17:51.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:17:51.591
Feb 12 12:17:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 12:17:51.593
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:51.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:51.627
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Feb 12 12:17:51.638: INFO: Waiting up to 5m0s for pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e" in namespace "container-probe-2621" to be "running and ready"
Feb 12 12:17:51.643: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.95777ms
Feb 12 12:17:51.643: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:17:53.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 2.016949929s
Feb 12 12:17:53.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:17:55.657: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 4.018561327s
Feb 12 12:17:55.657: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:17:57.657: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 6.019062617s
Feb 12 12:17:57.657: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:17:59.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 8.017028508s
Feb 12 12:17:59.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:01.658: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 10.019467566s
Feb 12 12:18:01.658: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:03.657: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 12.01871716s
Feb 12 12:18:03.657: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:05.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 14.016909058s
Feb 12 12:18:05.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:07.648: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 16.009466494s
Feb 12 12:18:07.648: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:09.648: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 18.010132739s
Feb 12 12:18:09.648: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:11.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 20.016550133s
Feb 12 12:18:11.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
Feb 12 12:18:13.658: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=true. Elapsed: 22.019551611s
Feb 12 12:18:13.658: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = true)
Feb 12 12:18:13.659: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e" satisfied condition "running and ready"
Feb 12 12:18:13.673: INFO: Container started at 2023-02-12 12:17:52 +0000 UTC, pod became ready at 2023-02-12 12:18:11 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 12:18:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2621" for this suite. 02/12/23 12:18:13.68
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":255,"skipped":4726,"failed":0}
------------------------------
 [SLOW TEST] [22.100 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:17:51.591
    Feb 12 12:17:51.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 12:17:51.593
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:17:51.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:17:51.627
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Feb 12 12:17:51.638: INFO: Waiting up to 5m0s for pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e" in namespace "container-probe-2621" to be "running and ready"
    Feb 12 12:17:51.643: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.95777ms
    Feb 12 12:17:51.643: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:17:53.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 2.016949929s
    Feb 12 12:17:53.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:17:55.657: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 4.018561327s
    Feb 12 12:17:55.657: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:17:57.657: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 6.019062617s
    Feb 12 12:17:57.657: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:17:59.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 8.017028508s
    Feb 12 12:17:59.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:01.658: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 10.019467566s
    Feb 12 12:18:01.658: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:03.657: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 12.01871716s
    Feb 12 12:18:03.657: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:05.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 14.016909058s
    Feb 12 12:18:05.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:07.648: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 16.009466494s
    Feb 12 12:18:07.648: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:09.648: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 18.010132739s
    Feb 12 12:18:09.648: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:11.655: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=false. Elapsed: 20.016550133s
    Feb 12 12:18:11.655: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = false)
    Feb 12 12:18:13.658: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e": Phase="Running", Reason="", readiness=true. Elapsed: 22.019551611s
    Feb 12 12:18:13.658: INFO: The phase of Pod test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e is Running (Ready = true)
    Feb 12 12:18:13.659: INFO: Pod "test-webserver-cd1dd9cb-fe0f-4314-bc52-89a9ff89e22e" satisfied condition "running and ready"
    Feb 12 12:18:13.673: INFO: Container started at 2023-02-12 12:17:52 +0000 UTC, pod became ready at 2023-02-12 12:18:11 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 12:18:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2621" for this suite. 02/12/23 12:18:13.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:13.694
Feb 12 12:18:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:18:13.696
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:13.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:13.725
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 12:18:13.727
Feb 12 12:18:13.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-2061 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Feb 12 12:18:13.789: INFO: stderr: ""
Feb 12 12:18:13.789: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/12/23 12:18:13.789
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Feb 12 12:18:13.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-2061 delete pods e2e-test-httpd-pod'
Feb 12 12:18:16.135: INFO: stderr: ""
Feb 12 12:18:16.135: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:18:16.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2061" for this suite. 02/12/23 12:18:16.14
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":256,"skipped":4731,"failed":0}
------------------------------
 [2.452 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:13.694
    Feb 12 12:18:13.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:18:13.696
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:13.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:13.725
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 12:18:13.727
    Feb 12 12:18:13.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-2061 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Feb 12 12:18:13.789: INFO: stderr: ""
    Feb 12 12:18:13.789: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/12/23 12:18:13.789
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Feb 12 12:18:13.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-2061 delete pods e2e-test-httpd-pod'
    Feb 12 12:18:16.135: INFO: stderr: ""
    Feb 12 12:18:16.135: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:18:16.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2061" for this suite. 02/12/23 12:18:16.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:16.148
Feb 12 12:18:16.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sysctl 02/12/23 12:18:16.149
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:16.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:16.17
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/12/23 12:18:16.173
STEP: Watching for error events or started pod 02/12/23 12:18:16.181
STEP: Waiting for pod completion 02/12/23 12:18:18.196
Feb 12 12:18:18.196: INFO: Waiting up to 3m0s for pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0" in namespace "sysctl-8145" to be "completed"
Feb 12 12:18:18.205: INFO: Pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.292198ms
Feb 12 12:18:20.211: INFO: Pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015146624s
Feb 12 12:18:20.211: INFO: Pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/12/23 12:18:20.218
STEP: Getting logs from the pod 02/12/23 12:18:20.218
STEP: Checking that the sysctl is actually updated 02/12/23 12:18:20.229
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 12:18:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8145" for this suite. 02/12/23 12:18:20.233
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":257,"skipped":4737,"failed":0}
------------------------------
 [4.092 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:16.148
    Feb 12 12:18:16.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sysctl 02/12/23 12:18:16.149
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:16.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:16.17
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/12/23 12:18:16.173
    STEP: Watching for error events or started pod 02/12/23 12:18:16.181
    STEP: Waiting for pod completion 02/12/23 12:18:18.196
    Feb 12 12:18:18.196: INFO: Waiting up to 3m0s for pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0" in namespace "sysctl-8145" to be "completed"
    Feb 12 12:18:18.205: INFO: Pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.292198ms
    Feb 12 12:18:20.211: INFO: Pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015146624s
    Feb 12 12:18:20.211: INFO: Pod "sysctl-cee93d36-a860-4e08-82f3-f2d3af7c7eb0" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/12/23 12:18:20.218
    STEP: Getting logs from the pod 02/12/23 12:18:20.218
    STEP: Checking that the sysctl is actually updated 02/12/23 12:18:20.229
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 12:18:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-8145" for this suite. 02/12/23 12:18:20.233
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:20.24
Feb 12 12:18:20.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:18:20.241
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:20.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:20.258
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-68a34801-a598-4f03-9f85-51c66e2da7b5 02/12/23 12:18:20.26
STEP: Creating a pod to test consume secrets 02/12/23 12:18:20.265
Feb 12 12:18:20.274: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61" in namespace "projected-662" to be "Succeeded or Failed"
Feb 12 12:18:20.280: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376574ms
Feb 12 12:18:22.285: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011204001s
Feb 12 12:18:24.286: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012441983s
STEP: Saw pod success 02/12/23 12:18:24.286
Feb 12 12:18:24.286: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61" satisfied condition "Succeeded or Failed"
Feb 12 12:18:24.290: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:18:24.3
Feb 12 12:18:24.324: INFO: Waiting for pod pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61 to disappear
Feb 12 12:18:24.328: INFO: Pod pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 12:18:24.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-662" for this suite. 02/12/23 12:18:24.336
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":258,"skipped":4737,"failed":0}
------------------------------
 [4.104 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:20.24
    Feb 12 12:18:20.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:18:20.241
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:20.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:20.258
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-68a34801-a598-4f03-9f85-51c66e2da7b5 02/12/23 12:18:20.26
    STEP: Creating a pod to test consume secrets 02/12/23 12:18:20.265
    Feb 12 12:18:20.274: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61" in namespace "projected-662" to be "Succeeded or Failed"
    Feb 12 12:18:20.280: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376574ms
    Feb 12 12:18:22.285: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011204001s
    Feb 12 12:18:24.286: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012441983s
    STEP: Saw pod success 02/12/23 12:18:24.286
    Feb 12 12:18:24.286: INFO: Pod "pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61" satisfied condition "Succeeded or Failed"
    Feb 12 12:18:24.290: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:18:24.3
    Feb 12 12:18:24.324: INFO: Waiting for pod pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61 to disappear
    Feb 12 12:18:24.328: INFO: Pod pod-projected-secrets-336da817-12cd-451d-b836-5bce1459ad61 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 12:18:24.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-662" for this suite. 02/12/23 12:18:24.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:24.348
Feb 12 12:18:24.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:18:24.349
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:24.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:24.376
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:18:24.379
Feb 12 12:18:24.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673" in namespace "downward-api-5353" to be "Succeeded or Failed"
Feb 12 12:18:24.396: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230839ms
Feb 12 12:18:26.403: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013191719s
Feb 12 12:18:28.400: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937333s
STEP: Saw pod success 02/12/23 12:18:28.4
Feb 12 12:18:28.400: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673" satisfied condition "Succeeded or Failed"
Feb 12 12:18:28.405: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673 container client-container: <nil>
STEP: delete the pod 02/12/23 12:18:28.411
Feb 12 12:18:28.428: INFO: Waiting for pod downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673 to disappear
Feb 12 12:18:28.431: INFO: Pod downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 12:18:28.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5353" for this suite. 02/12/23 12:18:28.434
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":259,"skipped":4744,"failed":0}
------------------------------
 [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:24.348
    Feb 12 12:18:24.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:18:24.349
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:24.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:24.376
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:18:24.379
    Feb 12 12:18:24.390: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673" in namespace "downward-api-5353" to be "Succeeded or Failed"
    Feb 12 12:18:24.396: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673": Phase="Pending", Reason="", readiness=false. Elapsed: 6.230839ms
    Feb 12 12:18:26.403: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013191719s
    Feb 12 12:18:28.400: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009937333s
    STEP: Saw pod success 02/12/23 12:18:28.4
    Feb 12 12:18:28.400: INFO: Pod "downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673" satisfied condition "Succeeded or Failed"
    Feb 12 12:18:28.405: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673 container client-container: <nil>
    STEP: delete the pod 02/12/23 12:18:28.411
    Feb 12 12:18:28.428: INFO: Waiting for pod downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673 to disappear
    Feb 12 12:18:28.431: INFO: Pod downwardapi-volume-4a9be072-bc0b-4ffa-aa4b-8072f3fc8673 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 12:18:28.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5353" for this suite. 02/12/23 12:18:28.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:28.443
Feb 12 12:18:28.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pod-network-test 02/12/23 12:18:28.444
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:28.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:28.465
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-670 02/12/23 12:18:28.469
STEP: creating a selector 02/12/23 12:18:28.469
STEP: Creating the service pods in kubernetes 02/12/23 12:18:28.469
Feb 12 12:18:28.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 12:18:28.520: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-670" to be "running and ready"
Feb 12 12:18:28.532: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.072757ms
Feb 12 12:18:28.532: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:18:30.827: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307114407s
Feb 12 12:18:30.828: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:18:33.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.61921156s
Feb 12 12:18:33.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:34.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.449456782s
Feb 12 12:18:34.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:36.538: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017999168s
Feb 12 12:18:36.538: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:38.545: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024796141s
Feb 12 12:18:38.545: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:40.547: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.026514556s
Feb 12 12:18:40.547: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:42.549: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.028668399s
Feb 12 12:18:42.549: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:44.546: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025232428s
Feb 12 12:18:44.546: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:46.548: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027647865s
Feb 12 12:18:46.548: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:48.542: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02159383s
Feb 12 12:18:48.542: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:18:50.546: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025191558s
Feb 12 12:18:50.546: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 12 12:18:50.546: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 12 12:18:50.559: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-670" to be "running and ready"
Feb 12 12:18:50.569: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.157518ms
Feb 12 12:18:50.569: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 12 12:18:50.569: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 12 12:18:50.585: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-670" to be "running and ready"
Feb 12 12:18:50.596: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.113119ms
Feb 12 12:18:50.597: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 12 12:18:50.597: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/12/23 12:18:50.602
Feb 12 12:18:50.610: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-670" to be "running"
Feb 12 12:18:50.615: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86042ms
Feb 12 12:18:52.620: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009518527s
Feb 12 12:18:52.620: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 12 12:18:52.623: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 12 12:18:52.623: INFO: Breadth first check of 10.233.120.120 on host 10.2.20.101...
Feb 12 12:18:52.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.71:9080/dial?request=hostname&protocol=http&host=10.233.120.120&port=8083&tries=1'] Namespace:pod-network-test-670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:18:52.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:18:52.627: INFO: ExecWithOptions: Clientset creation
Feb 12 12:18:52.627: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-670/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.120%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 12 12:18:52.698: INFO: Waiting for responses: map[]
Feb 12 12:18:52.698: INFO: reached 10.233.120.120 after 0/1 tries
Feb 12 12:18:52.698: INFO: Breadth first check of 10.233.120.252 on host 10.2.20.102...
Feb 12 12:18:52.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.71:9080/dial?request=hostname&protocol=http&host=10.233.120.252&port=8083&tries=1'] Namespace:pod-network-test-670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:18:52.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:18:52.703: INFO: ExecWithOptions: Clientset creation
Feb 12 12:18:52.703: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-670/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.252%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 12 12:18:52.758: INFO: Waiting for responses: map[]
Feb 12 12:18:52.758: INFO: reached 10.233.120.252 after 0/1 tries
Feb 12 12:18:52.758: INFO: Breadth first check of 10.233.99.125 on host 10.2.20.103...
Feb 12 12:18:52.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.71:9080/dial?request=hostname&protocol=http&host=10.233.99.125&port=8083&tries=1'] Namespace:pod-network-test-670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:18:52.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:18:52.762: INFO: ExecWithOptions: Clientset creation
Feb 12 12:18:52.762: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-670/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.99.125%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 12 12:18:52.814: INFO: Waiting for responses: map[]
Feb 12 12:18:52.814: INFO: reached 10.233.99.125 after 0/1 tries
Feb 12 12:18:52.814: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 12 12:18:52.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-670" for this suite. 02/12/23 12:18:52.817
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":260,"skipped":4788,"failed":0}
------------------------------
 [SLOW TEST] [24.379 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:28.443
    Feb 12 12:18:28.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pod-network-test 02/12/23 12:18:28.444
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:28.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:28.465
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-670 02/12/23 12:18:28.469
    STEP: creating a selector 02/12/23 12:18:28.469
    STEP: Creating the service pods in kubernetes 02/12/23 12:18:28.469
    Feb 12 12:18:28.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 12 12:18:28.520: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-670" to be "running and ready"
    Feb 12 12:18:28.532: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.072757ms
    Feb 12 12:18:28.532: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:18:30.827: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.307114407s
    Feb 12 12:18:30.828: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:18:33.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.61921156s
    Feb 12 12:18:33.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:34.970: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.449456782s
    Feb 12 12:18:34.970: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:36.538: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017999168s
    Feb 12 12:18:36.538: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:38.545: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024796141s
    Feb 12 12:18:38.545: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:40.547: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.026514556s
    Feb 12 12:18:40.547: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:42.549: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.028668399s
    Feb 12 12:18:42.549: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:44.546: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025232428s
    Feb 12 12:18:44.546: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:46.548: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.027647865s
    Feb 12 12:18:46.548: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:48.542: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.02159383s
    Feb 12 12:18:48.542: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:18:50.546: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025191558s
    Feb 12 12:18:50.546: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 12 12:18:50.546: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 12 12:18:50.559: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-670" to be "running and ready"
    Feb 12 12:18:50.569: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.157518ms
    Feb 12 12:18:50.569: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 12 12:18:50.569: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 12 12:18:50.585: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-670" to be "running and ready"
    Feb 12 12:18:50.596: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.113119ms
    Feb 12 12:18:50.597: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 12 12:18:50.597: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/12/23 12:18:50.602
    Feb 12 12:18:50.610: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-670" to be "running"
    Feb 12 12:18:50.615: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86042ms
    Feb 12 12:18:52.620: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009518527s
    Feb 12 12:18:52.620: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 12 12:18:52.623: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 12 12:18:52.623: INFO: Breadth first check of 10.233.120.120 on host 10.2.20.101...
    Feb 12 12:18:52.626: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.71:9080/dial?request=hostname&protocol=http&host=10.233.120.120&port=8083&tries=1'] Namespace:pod-network-test-670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:18:52.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:18:52.627: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:18:52.627: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-670/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.120%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 12 12:18:52.698: INFO: Waiting for responses: map[]
    Feb 12 12:18:52.698: INFO: reached 10.233.120.120 after 0/1 tries
    Feb 12 12:18:52.698: INFO: Breadth first check of 10.233.120.252 on host 10.2.20.102...
    Feb 12 12:18:52.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.71:9080/dial?request=hostname&protocol=http&host=10.233.120.252&port=8083&tries=1'] Namespace:pod-network-test-670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:18:52.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:18:52.703: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:18:52.703: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-670/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.120.252%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 12 12:18:52.758: INFO: Waiting for responses: map[]
    Feb 12 12:18:52.758: INFO: reached 10.233.120.252 after 0/1 tries
    Feb 12 12:18:52.758: INFO: Breadth first check of 10.233.99.125 on host 10.2.20.103...
    Feb 12 12:18:52.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.99.71:9080/dial?request=hostname&protocol=http&host=10.233.99.125&port=8083&tries=1'] Namespace:pod-network-test-670 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:18:52.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:18:52.762: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:18:52.762: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-670/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.99.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.99.125%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 12 12:18:52.814: INFO: Waiting for responses: map[]
    Feb 12 12:18:52.814: INFO: reached 10.233.99.125 after 0/1 tries
    Feb 12 12:18:52.814: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 12 12:18:52.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-670" for this suite. 02/12/23 12:18:52.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:52.823
Feb 12 12:18:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename watch 02/12/23 12:18:52.824
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:52.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:52.843
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/12/23 12:18:52.845
STEP: modifying the configmap once 02/12/23 12:18:52.849
STEP: modifying the configmap a second time 02/12/23 12:18:52.857
STEP: deleting the configmap 02/12/23 12:18:52.864
STEP: creating a watch on configmaps from the resource version returned by the first update 02/12/23 12:18:52.869
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/12/23 12:18:52.87
Feb 12 12:18:52.871: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7025  abd2801a-0f65-4139-9e67-a1d566f01c5b 29755 0 2023-02-12 12:18:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-12 12:18:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 12 12:18:52.871: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7025  abd2801a-0f65-4139-9e67-a1d566f01c5b 29756 0 2023-02-12 12:18:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-12 12:18:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Feb 12 12:18:52.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7025" for this suite. 02/12/23 12:18:52.874
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":261,"skipped":4793,"failed":0}
------------------------------
 [0.067 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:52.823
    Feb 12 12:18:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename watch 02/12/23 12:18:52.824
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:52.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:52.843
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/12/23 12:18:52.845
    STEP: modifying the configmap once 02/12/23 12:18:52.849
    STEP: modifying the configmap a second time 02/12/23 12:18:52.857
    STEP: deleting the configmap 02/12/23 12:18:52.864
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/12/23 12:18:52.869
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/12/23 12:18:52.87
    Feb 12 12:18:52.871: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7025  abd2801a-0f65-4139-9e67-a1d566f01c5b 29755 0 2023-02-12 12:18:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-12 12:18:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 12 12:18:52.871: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7025  abd2801a-0f65-4139-9e67-a1d566f01c5b 29756 0 2023-02-12 12:18:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-12 12:18:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Feb 12 12:18:52.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7025" for this suite. 02/12/23 12:18:52.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:52.891
Feb 12 12:18:52.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename endpointslice 02/12/23 12:18:52.892
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:52.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:52.909
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 02/12/23 12:18:52.913
STEP: getting /apis/discovery.k8s.io 02/12/23 12:18:52.915
STEP: getting /apis/discovery.k8s.iov1 02/12/23 12:18:52.916
STEP: creating 02/12/23 12:18:52.917
STEP: getting 02/12/23 12:18:52.936
STEP: listing 02/12/23 12:18:52.939
STEP: watching 02/12/23 12:18:52.942
Feb 12 12:18:52.942: INFO: starting watch
STEP: cluster-wide listing 02/12/23 12:18:52.943
STEP: cluster-wide watching 02/12/23 12:18:52.946
Feb 12 12:18:52.946: INFO: starting watch
STEP: patching 02/12/23 12:18:52.947
STEP: updating 02/12/23 12:18:52.953
Feb 12 12:18:52.963: INFO: waiting for watch events with expected annotations
Feb 12 12:18:52.963: INFO: saw patched and updated annotations
STEP: deleting 02/12/23 12:18:52.964
STEP: deleting a collection 02/12/23 12:18:52.98
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Feb 12 12:18:53.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7987" for this suite. 02/12/23 12:18:53.007
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":262,"skipped":4809,"failed":0}
------------------------------
 [0.123 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:52.891
    Feb 12 12:18:52.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename endpointslice 02/12/23 12:18:52.892
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:52.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:52.909
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 02/12/23 12:18:52.913
    STEP: getting /apis/discovery.k8s.io 02/12/23 12:18:52.915
    STEP: getting /apis/discovery.k8s.iov1 02/12/23 12:18:52.916
    STEP: creating 02/12/23 12:18:52.917
    STEP: getting 02/12/23 12:18:52.936
    STEP: listing 02/12/23 12:18:52.939
    STEP: watching 02/12/23 12:18:52.942
    Feb 12 12:18:52.942: INFO: starting watch
    STEP: cluster-wide listing 02/12/23 12:18:52.943
    STEP: cluster-wide watching 02/12/23 12:18:52.946
    Feb 12 12:18:52.946: INFO: starting watch
    STEP: patching 02/12/23 12:18:52.947
    STEP: updating 02/12/23 12:18:52.953
    Feb 12 12:18:52.963: INFO: waiting for watch events with expected annotations
    Feb 12 12:18:52.963: INFO: saw patched and updated annotations
    STEP: deleting 02/12/23 12:18:52.964
    STEP: deleting a collection 02/12/23 12:18:52.98
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Feb 12 12:18:53.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7987" for this suite. 02/12/23 12:18:53.007
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:53.014
Feb 12 12:18:53.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:18:53.015
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:53.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:53.04
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:18:53.041
Feb 12 12:18:53.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7" in namespace "projected-7746" to be "Succeeded or Failed"
Feb 12 12:18:53.054: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522879ms
Feb 12 12:18:55.070: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020116083s
Feb 12 12:18:57.072: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021730473s
STEP: Saw pod success 02/12/23 12:18:57.072
Feb 12 12:18:57.072: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7" satisfied condition "Succeeded or Failed"
Feb 12 12:18:57.077: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7 container client-container: <nil>
STEP: delete the pod 02/12/23 12:18:57.095
Feb 12 12:18:57.119: INFO: Waiting for pod downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7 to disappear
Feb 12 12:18:57.123: INFO: Pod downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 12:18:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7746" for this suite. 02/12/23 12:18:57.128
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":263,"skipped":4811,"failed":0}
------------------------------
 [4.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:53.014
    Feb 12 12:18:53.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:18:53.015
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:53.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:53.04
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:18:53.041
    Feb 12 12:18:53.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7" in namespace "projected-7746" to be "Succeeded or Failed"
    Feb 12 12:18:53.054: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522879ms
    Feb 12 12:18:55.070: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020116083s
    Feb 12 12:18:57.072: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021730473s
    STEP: Saw pod success 02/12/23 12:18:57.072
    Feb 12 12:18:57.072: INFO: Pod "downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7" satisfied condition "Succeeded or Failed"
    Feb 12 12:18:57.077: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7 container client-container: <nil>
    STEP: delete the pod 02/12/23 12:18:57.095
    Feb 12 12:18:57.119: INFO: Waiting for pod downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7 to disappear
    Feb 12 12:18:57.123: INFO: Pod downwardapi-volume-35827b9b-01cd-42fa-bf14-b4d9ce1afbc7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 12:18:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7746" for this suite. 02/12/23 12:18:57.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:18:57.147
Feb 12 12:18:57.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename job 02/12/23 12:18:57.148
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:57.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:57.17
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 02/12/23 12:18:57.171
STEP: Ensuring active pods == parallelism 02/12/23 12:18:57.179
STEP: Orphaning one of the Job's Pods 02/12/23 12:18:59.192
Feb 12 12:18:59.742: INFO: Successfully updated pod "adopt-release-2ngbx"
STEP: Checking that the Job readopts the Pod 02/12/23 12:18:59.742
Feb 12 12:18:59.744: INFO: Waiting up to 15m0s for pod "adopt-release-2ngbx" in namespace "job-7541" to be "adopted"
Feb 12 12:18:59.760: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 16.162159ms
Feb 12 12:19:01.774: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.030133107s
Feb 12 12:19:01.774: INFO: Pod "adopt-release-2ngbx" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/12/23 12:19:01.774
Feb 12 12:19:02.287: INFO: Successfully updated pod "adopt-release-2ngbx"
STEP: Checking that the Job releases the Pod 02/12/23 12:19:02.287
Feb 12 12:19:02.287: INFO: Waiting up to 15m0s for pod "adopt-release-2ngbx" in namespace "job-7541" to be "released"
Feb 12 12:19:02.291: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.248732ms
Feb 12 12:19:04.305: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018280935s
Feb 12 12:19:04.305: INFO: Pod "adopt-release-2ngbx" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Feb 12 12:19:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7541" for this suite. 02/12/23 12:19:04.317
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":264,"skipped":4843,"failed":0}
------------------------------
 [SLOW TEST] [7.183 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:18:57.147
    Feb 12 12:18:57.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename job 02/12/23 12:18:57.148
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:18:57.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:18:57.17
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 02/12/23 12:18:57.171
    STEP: Ensuring active pods == parallelism 02/12/23 12:18:57.179
    STEP: Orphaning one of the Job's Pods 02/12/23 12:18:59.192
    Feb 12 12:18:59.742: INFO: Successfully updated pod "adopt-release-2ngbx"
    STEP: Checking that the Job readopts the Pod 02/12/23 12:18:59.742
    Feb 12 12:18:59.744: INFO: Waiting up to 15m0s for pod "adopt-release-2ngbx" in namespace "job-7541" to be "adopted"
    Feb 12 12:18:59.760: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 16.162159ms
    Feb 12 12:19:01.774: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.030133107s
    Feb 12 12:19:01.774: INFO: Pod "adopt-release-2ngbx" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/12/23 12:19:01.774
    Feb 12 12:19:02.287: INFO: Successfully updated pod "adopt-release-2ngbx"
    STEP: Checking that the Job releases the Pod 02/12/23 12:19:02.287
    Feb 12 12:19:02.287: INFO: Waiting up to 15m0s for pod "adopt-release-2ngbx" in namespace "job-7541" to be "released"
    Feb 12 12:19:02.291: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.248732ms
    Feb 12 12:19:04.305: INFO: Pod "adopt-release-2ngbx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018280935s
    Feb 12 12:19:04.305: INFO: Pod "adopt-release-2ngbx" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Feb 12 12:19:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7541" for this suite. 02/12/23 12:19:04.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:19:04.33
Feb 12 12:19:04.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 12:19:04.33
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:19:04.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:19:04.351
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 12:20:04.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3202" for this suite. 02/12/23 12:20:04.392
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":265,"skipped":4850,"failed":0}
------------------------------
 [SLOW TEST] [60.082 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:19:04.33
    Feb 12 12:19:04.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 12:19:04.33
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:19:04.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:19:04.351
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 12:20:04.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3202" for this suite. 02/12/23 12:20:04.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:20:04.412
Feb 12 12:20:04.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename subpath 02/12/23 12:20:04.414
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:04.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:04.438
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/12/23 12:20:04.44
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-7jxx 02/12/23 12:20:04.45
STEP: Creating a pod to test atomic-volume-subpath 02/12/23 12:20:04.45
Feb 12 12:20:04.459: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7jxx" in namespace "subpath-7688" to be "Succeeded or Failed"
Feb 12 12:20:04.465: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.204834ms
Feb 12 12:20:06.477: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017949084s
Feb 12 12:20:08.880: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.42132005s
Feb 12 12:20:10.477: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018551412s
Feb 12 12:20:12.469: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 8.010075036s
Feb 12 12:20:14.472: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 10.01289349s
Feb 12 12:20:16.469: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 12.010228042s
Feb 12 12:20:18.477: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 14.018340342s
Feb 12 12:20:20.469: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 16.010123137s
Feb 12 12:20:22.479: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 18.019961744s
Feb 12 12:20:24.480: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 20.021117844s
Feb 12 12:20:26.479: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 22.020649398s
Feb 12 12:20:28.480: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 24.02156568s
Feb 12 12:20:30.482: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 26.023501208s
Feb 12 12:20:32.479: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=false. Elapsed: 28.020666039s
Feb 12 12:20:34.478: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.019301613s
STEP: Saw pod success 02/12/23 12:20:34.478
Feb 12 12:20:34.479: INFO: Pod "pod-subpath-test-secret-7jxx" satisfied condition "Succeeded or Failed"
Feb 12 12:20:34.491: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-secret-7jxx container test-container-subpath-secret-7jxx: <nil>
STEP: delete the pod 02/12/23 12:20:34.538
Feb 12 12:20:34.562: INFO: Waiting for pod pod-subpath-test-secret-7jxx to disappear
Feb 12 12:20:34.564: INFO: Pod pod-subpath-test-secret-7jxx no longer exists
STEP: Deleting pod pod-subpath-test-secret-7jxx 02/12/23 12:20:34.565
Feb 12 12:20:34.565: INFO: Deleting pod "pod-subpath-test-secret-7jxx" in namespace "subpath-7688"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 12 12:20:34.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7688" for this suite. 02/12/23 12:20:34.57
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":266,"skipped":4856,"failed":0}
------------------------------
 [SLOW TEST] [30.164 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:20:04.412
    Feb 12 12:20:04.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename subpath 02/12/23 12:20:04.414
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:04.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:04.438
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/12/23 12:20:04.44
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-7jxx 02/12/23 12:20:04.45
    STEP: Creating a pod to test atomic-volume-subpath 02/12/23 12:20:04.45
    Feb 12 12:20:04.459: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7jxx" in namespace "subpath-7688" to be "Succeeded or Failed"
    Feb 12 12:20:04.465: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.204834ms
    Feb 12 12:20:06.477: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017949084s
    Feb 12 12:20:08.880: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.42132005s
    Feb 12 12:20:10.477: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018551412s
    Feb 12 12:20:12.469: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 8.010075036s
    Feb 12 12:20:14.472: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 10.01289349s
    Feb 12 12:20:16.469: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 12.010228042s
    Feb 12 12:20:18.477: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 14.018340342s
    Feb 12 12:20:20.469: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 16.010123137s
    Feb 12 12:20:22.479: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 18.019961744s
    Feb 12 12:20:24.480: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 20.021117844s
    Feb 12 12:20:26.479: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 22.020649398s
    Feb 12 12:20:28.480: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 24.02156568s
    Feb 12 12:20:30.482: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=true. Elapsed: 26.023501208s
    Feb 12 12:20:32.479: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Running", Reason="", readiness=false. Elapsed: 28.020666039s
    Feb 12 12:20:34.478: INFO: Pod "pod-subpath-test-secret-7jxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.019301613s
    STEP: Saw pod success 02/12/23 12:20:34.478
    Feb 12 12:20:34.479: INFO: Pod "pod-subpath-test-secret-7jxx" satisfied condition "Succeeded or Failed"
    Feb 12 12:20:34.491: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-secret-7jxx container test-container-subpath-secret-7jxx: <nil>
    STEP: delete the pod 02/12/23 12:20:34.538
    Feb 12 12:20:34.562: INFO: Waiting for pod pod-subpath-test-secret-7jxx to disappear
    Feb 12 12:20:34.564: INFO: Pod pod-subpath-test-secret-7jxx no longer exists
    STEP: Deleting pod pod-subpath-test-secret-7jxx 02/12/23 12:20:34.565
    Feb 12 12:20:34.565: INFO: Deleting pod "pod-subpath-test-secret-7jxx" in namespace "subpath-7688"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 12 12:20:34.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7688" for this suite. 02/12/23 12:20:34.57
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:20:34.58
Feb 12 12:20:34.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 12:20:34.581
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:34.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:34.598
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-a2b5abf4-1761-4a2a-a8ca-8bd87232bb70 02/12/23 12:20:34.6
STEP: Creating a pod to test consume secrets 02/12/23 12:20:34.604
Feb 12 12:20:34.613: INFO: Waiting up to 5m0s for pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415" in namespace "secrets-8807" to be "Succeeded or Failed"
Feb 12 12:20:34.617: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881485ms
Feb 12 12:20:36.624: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010229419s
Feb 12 12:20:38.633: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019452041s
STEP: Saw pod success 02/12/23 12:20:38.633
Feb 12 12:20:38.634: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415" satisfied condition "Succeeded or Failed"
Feb 12 12:20:38.647: INFO: Trying to get logs from node kube-3 pod pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415 container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:20:38.667
Feb 12 12:20:38.684: INFO: Waiting for pod pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415 to disappear
Feb 12 12:20:38.688: INFO: Pod pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 12:20:38.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8807" for this suite. 02/12/23 12:20:38.691
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":267,"skipped":4860,"failed":0}
------------------------------
 [4.120 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:20:34.58
    Feb 12 12:20:34.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 12:20:34.581
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:34.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:34.598
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-a2b5abf4-1761-4a2a-a8ca-8bd87232bb70 02/12/23 12:20:34.6
    STEP: Creating a pod to test consume secrets 02/12/23 12:20:34.604
    Feb 12 12:20:34.613: INFO: Waiting up to 5m0s for pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415" in namespace "secrets-8807" to be "Succeeded or Failed"
    Feb 12 12:20:34.617: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881485ms
    Feb 12 12:20:36.624: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010229419s
    Feb 12 12:20:38.633: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019452041s
    STEP: Saw pod success 02/12/23 12:20:38.633
    Feb 12 12:20:38.634: INFO: Pod "pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415" satisfied condition "Succeeded or Failed"
    Feb 12 12:20:38.647: INFO: Trying to get logs from node kube-3 pod pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415 container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:20:38.667
    Feb 12 12:20:38.684: INFO: Waiting for pod pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415 to disappear
    Feb 12 12:20:38.688: INFO: Pod pod-secrets-875dd42f-0e53-470a-b2a4-34910de7f415 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 12:20:38.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8807" for this suite. 02/12/23 12:20:38.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:20:38.703
Feb 12 12:20:38.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 12:20:38.704
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:38.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:38.722
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-78e9f26a-ce07-429b-85be-0d53b4ac89f5 02/12/23 12:20:38.742
STEP: Creating a pod to test consume secrets 02/12/23 12:20:38.748
Feb 12 12:20:38.757: INFO: Waiting up to 5m0s for pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380" in namespace "secrets-8906" to be "Succeeded or Failed"
Feb 12 12:20:38.761: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022314ms
Feb 12 12:20:40.767: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010318267s
Feb 12 12:20:42.769: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011691394s
STEP: Saw pod success 02/12/23 12:20:42.769
Feb 12 12:20:42.769: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380" satisfied condition "Succeeded or Failed"
Feb 12 12:20:42.772: INFO: Trying to get logs from node kube-3 pod pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380 container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:20:42.779
Feb 12 12:20:42.797: INFO: Waiting for pod pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380 to disappear
Feb 12 12:20:42.800: INFO: Pod pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 12:20:42.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8906" for this suite. 02/12/23 12:20:42.804
STEP: Destroying namespace "secret-namespace-6036" for this suite. 02/12/23 12:20:42.812
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":268,"skipped":4879,"failed":0}
------------------------------
 [4.120 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:20:38.703
    Feb 12 12:20:38.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 12:20:38.704
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:38.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:38.722
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-78e9f26a-ce07-429b-85be-0d53b4ac89f5 02/12/23 12:20:38.742
    STEP: Creating a pod to test consume secrets 02/12/23 12:20:38.748
    Feb 12 12:20:38.757: INFO: Waiting up to 5m0s for pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380" in namespace "secrets-8906" to be "Succeeded or Failed"
    Feb 12 12:20:38.761: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022314ms
    Feb 12 12:20:40.767: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010318267s
    Feb 12 12:20:42.769: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011691394s
    STEP: Saw pod success 02/12/23 12:20:42.769
    Feb 12 12:20:42.769: INFO: Pod "pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380" satisfied condition "Succeeded or Failed"
    Feb 12 12:20:42.772: INFO: Trying to get logs from node kube-3 pod pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380 container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:20:42.779
    Feb 12 12:20:42.797: INFO: Waiting for pod pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380 to disappear
    Feb 12 12:20:42.800: INFO: Pod pod-secrets-19ee602d-6be2-4a4a-a441-fea35d97c380 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 12:20:42.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8906" for this suite. 02/12/23 12:20:42.804
    STEP: Destroying namespace "secret-namespace-6036" for this suite. 02/12/23 12:20:42.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:20:42.837
Feb 12 12:20:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 12:20:42.838
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:42.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:42.859
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4704 02/12/23 12:20:42.861
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 02/12/23 12:20:42.867
Feb 12 12:20:42.891: INFO: Found 0 stateful pods, waiting for 3
Feb 12 12:20:52.901: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:20:52.901: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:20:52.901: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/12/23 12:20:52.93
Feb 12 12:20:52.954: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/12/23 12:20:52.954
STEP: Not applying an update when the partition is greater than the number of replicas 02/12/23 12:21:02.976
STEP: Performing a canary update 02/12/23 12:21:02.976
Feb 12 12:21:03.001: INFO: Updating stateful set ss2
Feb 12 12:21:03.023: INFO: Waiting for Pod statefulset-4704/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 02/12/23 12:21:13.034
Feb 12 12:21:13.142: INFO: Found 2 stateful pods, waiting for 3
Feb 12 12:21:23.153: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:21:23.153: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:21:23.153: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/12/23 12:21:23.16
Feb 12 12:21:23.186: INFO: Updating stateful set ss2
Feb 12 12:21:23.196: INFO: Waiting for Pod statefulset-4704/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Feb 12 12:21:33.750: INFO: Updating stateful set ss2
Feb 12 12:21:34.127: INFO: Waiting for StatefulSet statefulset-4704/ss2 to complete update
Feb 12 12:21:34.127: INFO: Waiting for Pod statefulset-4704/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 12:21:44.153: INFO: Deleting all statefulset in ns statefulset-4704
Feb 12 12:21:44.165: INFO: Scaling statefulset ss2 to 0
Feb 12 12:21:54.229: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 12:21:54.241: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 12:21:54.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4704" for this suite. 02/12/23 12:21:54.292
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":269,"skipped":4922,"failed":0}
------------------------------
 [SLOW TEST] [71.465 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:20:42.837
    Feb 12 12:20:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 12:20:42.838
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:20:42.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:20:42.859
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4704 02/12/23 12:20:42.861
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 02/12/23 12:20:42.867
    Feb 12 12:20:42.891: INFO: Found 0 stateful pods, waiting for 3
    Feb 12 12:20:52.901: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:20:52.901: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:20:52.901: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 02/12/23 12:20:52.93
    Feb 12 12:20:52.954: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/12/23 12:20:52.954
    STEP: Not applying an update when the partition is greater than the number of replicas 02/12/23 12:21:02.976
    STEP: Performing a canary update 02/12/23 12:21:02.976
    Feb 12 12:21:03.001: INFO: Updating stateful set ss2
    Feb 12 12:21:03.023: INFO: Waiting for Pod statefulset-4704/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 02/12/23 12:21:13.034
    Feb 12 12:21:13.142: INFO: Found 2 stateful pods, waiting for 3
    Feb 12 12:21:23.153: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:21:23.153: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:21:23.153: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/12/23 12:21:23.16
    Feb 12 12:21:23.186: INFO: Updating stateful set ss2
    Feb 12 12:21:23.196: INFO: Waiting for Pod statefulset-4704/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Feb 12 12:21:33.750: INFO: Updating stateful set ss2
    Feb 12 12:21:34.127: INFO: Waiting for StatefulSet statefulset-4704/ss2 to complete update
    Feb 12 12:21:34.127: INFO: Waiting for Pod statefulset-4704/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 12:21:44.153: INFO: Deleting all statefulset in ns statefulset-4704
    Feb 12 12:21:44.165: INFO: Scaling statefulset ss2 to 0
    Feb 12 12:21:54.229: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 12:21:54.241: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 12:21:54.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4704" for this suite. 02/12/23 12:21:54.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:21:54.306
Feb 12 12:21:54.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:21:54.307
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:21:54.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:21:54.325
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-3803 02/12/23 12:21:54.326
STEP: creating service affinity-nodeport in namespace services-3803 02/12/23 12:21:54.327
STEP: creating replication controller affinity-nodeport in namespace services-3803 02/12/23 12:21:54.349
I0212 12:21:54.360884      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3803, replica count: 3
I0212 12:21:57.412886      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 12:21:57.449: INFO: Creating new exec pod
Feb 12 12:21:57.458: INFO: Waiting up to 5m0s for pod "execpod-affinity8c6g6" in namespace "services-3803" to be "running"
Feb 12 12:21:57.466: INFO: Pod "execpod-affinity8c6g6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049648ms
Feb 12 12:21:59.470: INFO: Pod "execpod-affinity8c6g6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012298739s
Feb 12 12:21:59.470: INFO: Pod "execpod-affinity8c6g6" satisfied condition "running"
Feb 12 12:22:00.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Feb 12 12:22:00.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 12 12:22:00.702: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:22:00.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.51.220 80'
Feb 12 12:22:00.880: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.51.220 80\nConnection to 10.233.51.220 80 port [tcp/http] succeeded!\n"
Feb 12 12:22:00.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:22:00.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.103 32286'
Feb 12 12:22:01.052: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.103 32286\nConnection to 10.2.20.103 32286 port [tcp/*] succeeded!\n"
Feb 12 12:22:01.052: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:22:01.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32286'
Feb 12 12:22:01.191: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32286\nConnection to 10.2.20.101 32286 port [tcp/*] succeeded!\n"
Feb 12 12:22:01.191: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:22:01.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:32286/ ; done'
Feb 12 12:22:01.354: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n"
Feb 12 12:22:01.354: INFO: stdout: "\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh"
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
Feb 12 12:22:01.354: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3803, will wait for the garbage collector to delete the pods 02/12/23 12:22:01.367
Feb 12 12:22:01.435: INFO: Deleting ReplicationController affinity-nodeport took: 11.423557ms
Feb 12 12:22:01.536: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.85426ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:22:03.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3803" for this suite. 02/12/23 12:22:03.9
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":270,"skipped":4960,"failed":0}
------------------------------
 [SLOW TEST] [9.605 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:21:54.306
    Feb 12 12:21:54.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:21:54.307
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:21:54.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:21:54.325
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-3803 02/12/23 12:21:54.326
    STEP: creating service affinity-nodeport in namespace services-3803 02/12/23 12:21:54.327
    STEP: creating replication controller affinity-nodeport in namespace services-3803 02/12/23 12:21:54.349
    I0212 12:21:54.360884      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3803, replica count: 3
    I0212 12:21:57.412886      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 12:21:57.449: INFO: Creating new exec pod
    Feb 12 12:21:57.458: INFO: Waiting up to 5m0s for pod "execpod-affinity8c6g6" in namespace "services-3803" to be "running"
    Feb 12 12:21:57.466: INFO: Pod "execpod-affinity8c6g6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049648ms
    Feb 12 12:21:59.470: INFO: Pod "execpod-affinity8c6g6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012298739s
    Feb 12 12:21:59.470: INFO: Pod "execpod-affinity8c6g6" satisfied condition "running"
    Feb 12 12:22:00.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Feb 12 12:22:00.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 12 12:22:00.702: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:22:00.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.51.220 80'
    Feb 12 12:22:00.880: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.51.220 80\nConnection to 10.233.51.220 80 port [tcp/http] succeeded!\n"
    Feb 12 12:22:00.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:22:00.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.103 32286'
    Feb 12 12:22:01.052: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.103 32286\nConnection to 10.2.20.103 32286 port [tcp/*] succeeded!\n"
    Feb 12 12:22:01.052: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:22:01.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32286'
    Feb 12 12:22:01.191: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32286\nConnection to 10.2.20.101 32286 port [tcp/*] succeeded!\n"
    Feb 12 12:22:01.191: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:22:01.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3803 exec execpod-affinity8c6g6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:32286/ ; done'
    Feb 12 12:22:01.354: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32286/\n"
    Feb 12 12:22:01.354: INFO: stdout: "\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh\naffinity-nodeport-kwznh"
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Received response from host: affinity-nodeport-kwznh
    Feb 12 12:22:01.354: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3803, will wait for the garbage collector to delete the pods 02/12/23 12:22:01.367
    Feb 12 12:22:01.435: INFO: Deleting ReplicationController affinity-nodeport took: 11.423557ms
    Feb 12 12:22:01.536: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.85426ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:22:03.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3803" for this suite. 02/12/23 12:22:03.9
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:22:03.913
Feb 12 12:22:03.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:22:03.916
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:03.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:03.949
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 02/12/23 12:22:03.953
Feb 12 12:22:03.966: INFO: Waiting up to 5m0s for pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb" in namespace "emptydir-5298" to be "Succeeded or Failed"
Feb 12 12:22:03.978: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.350898ms
Feb 12 12:22:05.993: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026533301s
Feb 12 12:22:07.984: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018422999s
STEP: Saw pod success 02/12/23 12:22:07.984
Feb 12 12:22:07.985: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb" satisfied condition "Succeeded or Failed"
Feb 12 12:22:07.988: INFO: Trying to get logs from node kube-3 pod pod-044efe2b-075c-4c0d-9839-d454f19362cb container test-container: <nil>
STEP: delete the pod 02/12/23 12:22:08.005
Feb 12 12:22:08.022: INFO: Waiting for pod pod-044efe2b-075c-4c0d-9839-d454f19362cb to disappear
Feb 12 12:22:08.025: INFO: Pod pod-044efe2b-075c-4c0d-9839-d454f19362cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:22:08.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5298" for this suite. 02/12/23 12:22:08.028
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":4971,"failed":0}
------------------------------
 [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:22:03.913
    Feb 12 12:22:03.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:22:03.916
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:03.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:03.949
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/12/23 12:22:03.953
    Feb 12 12:22:03.966: INFO: Waiting up to 5m0s for pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb" in namespace "emptydir-5298" to be "Succeeded or Failed"
    Feb 12 12:22:03.978: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.350898ms
    Feb 12 12:22:05.993: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026533301s
    Feb 12 12:22:07.984: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018422999s
    STEP: Saw pod success 02/12/23 12:22:07.984
    Feb 12 12:22:07.985: INFO: Pod "pod-044efe2b-075c-4c0d-9839-d454f19362cb" satisfied condition "Succeeded or Failed"
    Feb 12 12:22:07.988: INFO: Trying to get logs from node kube-3 pod pod-044efe2b-075c-4c0d-9839-d454f19362cb container test-container: <nil>
    STEP: delete the pod 02/12/23 12:22:08.005
    Feb 12 12:22:08.022: INFO: Waiting for pod pod-044efe2b-075c-4c0d-9839-d454f19362cb to disappear
    Feb 12 12:22:08.025: INFO: Pod pod-044efe2b-075c-4c0d-9839-d454f19362cb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:22:08.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5298" for this suite. 02/12/23 12:22:08.028
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:22:08.035
Feb 12 12:22:08.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 12:22:08.035
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:08.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:08.064
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Feb 12 12:22:08.078: INFO: Waiting up to 2m0s for pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" in namespace "var-expansion-550" to be "container 0 failed with reason CreateContainerConfigError"
Feb 12 12:22:08.085: INFO: Pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.096422ms
Feb 12 12:22:10.095: INFO: Pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017321135s
Feb 12 12:22:10.095: INFO: Pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 12 12:22:10.095: INFO: Deleting pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" in namespace "var-expansion-550"
Feb 12 12:22:10.104: INFO: Wait up to 5m0s for pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 12:22:14.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-550" for this suite. 02/12/23 12:22:14.132
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":272,"skipped":4971,"failed":0}
------------------------------
 [SLOW TEST] [6.119 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:22:08.035
    Feb 12 12:22:08.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 12:22:08.035
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:08.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:08.064
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Feb 12 12:22:08.078: INFO: Waiting up to 2m0s for pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" in namespace "var-expansion-550" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 12 12:22:08.085: INFO: Pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.096422ms
    Feb 12 12:22:10.095: INFO: Pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017321135s
    Feb 12 12:22:10.095: INFO: Pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 12 12:22:10.095: INFO: Deleting pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" in namespace "var-expansion-550"
    Feb 12 12:22:10.104: INFO: Wait up to 5m0s for pod "var-expansion-72f597b3-b540-4b16-b303-f5824fa4432c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 12:22:14.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-550" for this suite. 02/12/23 12:22:14.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:22:14.157
Feb 12 12:22:14.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:22:14.159
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:14.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:14.185
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-3696 02/12/23 12:22:14.188
STEP: creating replication controller nodeport-test in namespace services-3696 02/12/23 12:22:14.203
I0212 12:22:14.219660      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3696, replica count: 2
I0212 12:22:17.270197      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 12:22:17.270: INFO: Creating new exec pod
Feb 12 12:22:17.278: INFO: Waiting up to 5m0s for pod "execpodwftwp" in namespace "services-3696" to be "running"
Feb 12 12:22:17.282: INFO: Pod "execpodwftwp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.660991ms
Feb 12 12:22:19.285: INFO: Pod "execpodwftwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007055571s
Feb 12 12:22:19.285: INFO: Pod "execpodwftwp" satisfied condition "running"
Feb 12 12:22:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 12 12:22:20.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 12 12:22:20.397: INFO: stdout: ""
Feb 12 12:22:21.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 12 12:22:21.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 12 12:22:21.723: INFO: stdout: ""
Feb 12 12:22:22.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Feb 12 12:22:22.532: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 12 12:22:22.532: INFO: stdout: "nodeport-test-k7m64"
Feb 12 12:22:22.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.186 80'
Feb 12 12:22:22.647: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.186 80\nConnection to 10.233.55.186 80 port [tcp/http] succeeded!\n"
Feb 12 12:22:22.647: INFO: stdout: "nodeport-test-gm6r8"
Feb 12 12:22:22.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.103 31546'
Feb 12 12:22:22.763: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.103 31546\nConnection to 10.2.20.103 31546 port [tcp/*] succeeded!\n"
Feb 12 12:22:22.763: INFO: stdout: "nodeport-test-k7m64"
Feb 12 12:22:22.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.102 31546'
Feb 12 12:22:22.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.102 31546\nConnection to 10.2.20.102 31546 port [tcp/*] succeeded!\n"
Feb 12 12:22:22.886: INFO: stdout: "nodeport-test-k7m64"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:22:22.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3696" for this suite. 02/12/23 12:22:22.89
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":273,"skipped":5000,"failed":0}
------------------------------
 [SLOW TEST] [8.739 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:22:14.157
    Feb 12 12:22:14.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:22:14.159
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:14.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:14.185
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-3696 02/12/23 12:22:14.188
    STEP: creating replication controller nodeport-test in namespace services-3696 02/12/23 12:22:14.203
    I0212 12:22:14.219660      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3696, replica count: 2
    I0212 12:22:17.270197      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 12:22:17.270: INFO: Creating new exec pod
    Feb 12 12:22:17.278: INFO: Waiting up to 5m0s for pod "execpodwftwp" in namespace "services-3696" to be "running"
    Feb 12 12:22:17.282: INFO: Pod "execpodwftwp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.660991ms
    Feb 12 12:22:19.285: INFO: Pod "execpodwftwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007055571s
    Feb 12 12:22:19.285: INFO: Pod "execpodwftwp" satisfied condition "running"
    Feb 12 12:22:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 12 12:22:20.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 12 12:22:20.397: INFO: stdout: ""
    Feb 12 12:22:21.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 12 12:22:21.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 12 12:22:21.723: INFO: stdout: ""
    Feb 12 12:22:22.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Feb 12 12:22:22.532: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 12 12:22:22.532: INFO: stdout: "nodeport-test-k7m64"
    Feb 12 12:22:22.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.186 80'
    Feb 12 12:22:22.647: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.186 80\nConnection to 10.233.55.186 80 port [tcp/http] succeeded!\n"
    Feb 12 12:22:22.647: INFO: stdout: "nodeport-test-gm6r8"
    Feb 12 12:22:22.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.103 31546'
    Feb 12 12:22:22.763: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.103 31546\nConnection to 10.2.20.103 31546 port [tcp/*] succeeded!\n"
    Feb 12 12:22:22.763: INFO: stdout: "nodeport-test-k7m64"
    Feb 12 12:22:22.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-3696 exec execpodwftwp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.102 31546'
    Feb 12 12:22:22.886: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.102 31546\nConnection to 10.2.20.102 31546 port [tcp/*] succeeded!\n"
    Feb 12 12:22:22.886: INFO: stdout: "nodeport-test-k7m64"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:22:22.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3696" for this suite. 02/12/23 12:22:22.89
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:22:22.897
Feb 12 12:22:22.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:22:22.898
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:22.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:22.922
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-7528/configmap-test-cea9f355-5b73-41d0-8962-3e631094b67d 02/12/23 12:22:22.925
STEP: Creating a pod to test consume configMaps 02/12/23 12:22:22.935
Feb 12 12:22:22.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936" in namespace "configmap-7528" to be "Succeeded or Failed"
Feb 12 12:22:22.949: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 3.889157ms
Feb 12 12:22:25.280: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3350275s
Feb 12 12:22:27.241: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295815847s
Feb 12 12:22:29.078: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132701213s
Feb 12 12:22:30.954: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009032237s
Feb 12 12:22:32.956: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.011060909s
STEP: Saw pod success 02/12/23 12:22:32.956
Feb 12 12:22:32.956: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936" satisfied condition "Succeeded or Failed"
Feb 12 12:22:32.960: INFO: Trying to get logs from node kube-3 pod pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936 container env-test: <nil>
STEP: delete the pod 02/12/23 12:22:32.968
Feb 12 12:22:32.991: INFO: Waiting for pod pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936 to disappear
Feb 12 12:22:32.995: INFO: Pod pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:22:32.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7528" for this suite. 02/12/23 12:22:33
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":274,"skipped":5007,"failed":0}
------------------------------
 [SLOW TEST] [10.110 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:22:22.897
    Feb 12 12:22:22.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:22:22.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:22.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:22.922
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-7528/configmap-test-cea9f355-5b73-41d0-8962-3e631094b67d 02/12/23 12:22:22.925
    STEP: Creating a pod to test consume configMaps 02/12/23 12:22:22.935
    Feb 12 12:22:22.945: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936" in namespace "configmap-7528" to be "Succeeded or Failed"
    Feb 12 12:22:22.949: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 3.889157ms
    Feb 12 12:22:25.280: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3350275s
    Feb 12 12:22:27.241: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295815847s
    Feb 12 12:22:29.078: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132701213s
    Feb 12 12:22:30.954: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009032237s
    Feb 12 12:22:32.956: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.011060909s
    STEP: Saw pod success 02/12/23 12:22:32.956
    Feb 12 12:22:32.956: INFO: Pod "pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936" satisfied condition "Succeeded or Failed"
    Feb 12 12:22:32.960: INFO: Trying to get logs from node kube-3 pod pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936 container env-test: <nil>
    STEP: delete the pod 02/12/23 12:22:32.968
    Feb 12 12:22:32.991: INFO: Waiting for pod pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936 to disappear
    Feb 12 12:22:32.995: INFO: Pod pod-configmaps-1cfaab3a-daf4-451e-9c2d-e6ccb2cfd936 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:22:32.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7528" for this suite. 02/12/23 12:22:33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:22:33.013
Feb 12 12:22:33.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:22:33.013
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:33.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:33.035
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 02/12/23 12:22:33.041
Feb 12 12:22:33.048: INFO: Waiting up to 5m0s for pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955" in namespace "emptydir-5149" to be "Succeeded or Failed"
Feb 12 12:22:33.053: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380061ms
Feb 12 12:22:35.067: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018224547s
Feb 12 12:22:37.067: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01886876s
STEP: Saw pod success 02/12/23 12:22:37.067
Feb 12 12:22:37.068: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955" satisfied condition "Succeeded or Failed"
Feb 12 12:22:37.088: INFO: Trying to get logs from node kube-3 pod pod-4e7140cc-612a-4f8b-a518-dee8082d0955 container test-container: <nil>
STEP: delete the pod 02/12/23 12:22:37.107
Feb 12 12:22:37.130: INFO: Waiting for pod pod-4e7140cc-612a-4f8b-a518-dee8082d0955 to disappear
Feb 12 12:22:37.135: INFO: Pod pod-4e7140cc-612a-4f8b-a518-dee8082d0955 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:22:37.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5149" for this suite. 02/12/23 12:22:37.139
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":275,"skipped":5023,"failed":0}
------------------------------
 [4.132 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:22:33.013
    Feb 12 12:22:33.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:22:33.013
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:33.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:33.035
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/12/23 12:22:33.041
    Feb 12 12:22:33.048: INFO: Waiting up to 5m0s for pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955" in namespace "emptydir-5149" to be "Succeeded or Failed"
    Feb 12 12:22:33.053: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380061ms
    Feb 12 12:22:35.067: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018224547s
    Feb 12 12:22:37.067: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01886876s
    STEP: Saw pod success 02/12/23 12:22:37.067
    Feb 12 12:22:37.068: INFO: Pod "pod-4e7140cc-612a-4f8b-a518-dee8082d0955" satisfied condition "Succeeded or Failed"
    Feb 12 12:22:37.088: INFO: Trying to get logs from node kube-3 pod pod-4e7140cc-612a-4f8b-a518-dee8082d0955 container test-container: <nil>
    STEP: delete the pod 02/12/23 12:22:37.107
    Feb 12 12:22:37.130: INFO: Waiting for pod pod-4e7140cc-612a-4f8b-a518-dee8082d0955 to disappear
    Feb 12 12:22:37.135: INFO: Pod pod-4e7140cc-612a-4f8b-a518-dee8082d0955 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:22:37.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5149" for this suite. 02/12/23 12:22:37.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:22:37.146
Feb 12 12:22:37.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename cronjob 02/12/23 12:22:37.147
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:37.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:37.167
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/12/23 12:22:37.169
STEP: Ensuring more than one job is running at a time 02/12/23 12:22:37.175
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/12/23 12:24:01.188
STEP: Removing cronjob 02/12/23 12:24:01.2
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 12 12:24:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4131" for this suite. 02/12/23 12:24:01.246
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":276,"skipped":5032,"failed":0}
------------------------------
 [SLOW TEST] [84.116 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:22:37.146
    Feb 12 12:22:37.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename cronjob 02/12/23 12:22:37.147
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:22:37.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:22:37.167
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/12/23 12:22:37.169
    STEP: Ensuring more than one job is running at a time 02/12/23 12:22:37.175
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/12/23 12:24:01.188
    STEP: Removing cronjob 02/12/23 12:24:01.2
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 12 12:24:01.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4131" for this suite. 02/12/23 12:24:01.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:24:01.264
Feb 12 12:24:01.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:24:01.265
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:01.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:01.311
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:24:01.315
Feb 12 12:24:01.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1" in namespace "downward-api-2775" to be "Succeeded or Failed"
Feb 12 12:24:01.329: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.342754ms
Feb 12 12:24:03.341: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016092598s
Feb 12 12:24:05.335: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00917939s
STEP: Saw pod success 02/12/23 12:24:05.335
Feb 12 12:24:05.335: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1" satisfied condition "Succeeded or Failed"
Feb 12 12:24:05.339: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1 container client-container: <nil>
STEP: delete the pod 02/12/23 12:24:05.346
Feb 12 12:24:05.364: INFO: Waiting for pod downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1 to disappear
Feb 12 12:24:05.367: INFO: Pod downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 12:24:05.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2775" for this suite. 02/12/23 12:24:05.371
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":277,"skipped":5053,"failed":0}
------------------------------
 [4.113 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:24:01.264
    Feb 12 12:24:01.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:24:01.265
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:01.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:01.311
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:24:01.315
    Feb 12 12:24:01.325: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1" in namespace "downward-api-2775" to be "Succeeded or Failed"
    Feb 12 12:24:01.329: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.342754ms
    Feb 12 12:24:03.341: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016092598s
    Feb 12 12:24:05.335: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00917939s
    STEP: Saw pod success 02/12/23 12:24:05.335
    Feb 12 12:24:05.335: INFO: Pod "downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1" satisfied condition "Succeeded or Failed"
    Feb 12 12:24:05.339: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1 container client-container: <nil>
    STEP: delete the pod 02/12/23 12:24:05.346
    Feb 12 12:24:05.364: INFO: Waiting for pod downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1 to disappear
    Feb 12 12:24:05.367: INFO: Pod downwardapi-volume-9a6506be-e277-46bf-9d7b-70964846a4f1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 12:24:05.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2775" for this suite. 02/12/23 12:24:05.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:24:05.379
Feb 12 12:24:05.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename discovery 02/12/23 12:24:05.38
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:05.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:05.401
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/12/23 12:24:05.404
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 12 12:24:05.813: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 12 12:24:05.814: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 12 12:24:05.814: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 12 12:24:05.814: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 12 12:24:05.814: INFO: Checking APIGroup: apps
Feb 12 12:24:05.815: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 12 12:24:05.815: INFO: Versions found [{apps/v1 v1}]
Feb 12 12:24:05.815: INFO: apps/v1 matches apps/v1
Feb 12 12:24:05.815: INFO: Checking APIGroup: events.k8s.io
Feb 12 12:24:05.815: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 12 12:24:05.815: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 12 12:24:05.815: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 12 12:24:05.815: INFO: Checking APIGroup: authentication.k8s.io
Feb 12 12:24:05.816: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 12 12:24:05.816: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 12 12:24:05.816: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 12 12:24:05.816: INFO: Checking APIGroup: authorization.k8s.io
Feb 12 12:24:05.817: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 12 12:24:05.817: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 12 12:24:05.817: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 12 12:24:05.817: INFO: Checking APIGroup: autoscaling
Feb 12 12:24:05.817: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 12 12:24:05.817: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Feb 12 12:24:05.817: INFO: autoscaling/v2 matches autoscaling/v2
Feb 12 12:24:05.817: INFO: Checking APIGroup: batch
Feb 12 12:24:05.818: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 12 12:24:05.818: INFO: Versions found [{batch/v1 v1}]
Feb 12 12:24:05.818: INFO: batch/v1 matches batch/v1
Feb 12 12:24:05.818: INFO: Checking APIGroup: certificates.k8s.io
Feb 12 12:24:05.819: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 12 12:24:05.819: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 12 12:24:05.819: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 12 12:24:05.819: INFO: Checking APIGroup: networking.k8s.io
Feb 12 12:24:05.819: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 12 12:24:05.819: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 12 12:24:05.819: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 12 12:24:05.819: INFO: Checking APIGroup: policy
Feb 12 12:24:05.820: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 12 12:24:05.820: INFO: Versions found [{policy/v1 v1}]
Feb 12 12:24:05.820: INFO: policy/v1 matches policy/v1
Feb 12 12:24:05.820: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 12 12:24:05.821: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 12 12:24:05.821: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 12 12:24:05.821: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 12 12:24:05.821: INFO: Checking APIGroup: storage.k8s.io
Feb 12 12:24:05.822: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 12 12:24:05.822: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 12 12:24:05.822: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 12 12:24:05.822: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 12 12:24:05.822: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 12 12:24:05.822: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 12 12:24:05.822: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 12 12:24:05.822: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 12 12:24:05.823: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 12 12:24:05.823: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 12 12:24:05.823: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 12 12:24:05.823: INFO: Checking APIGroup: scheduling.k8s.io
Feb 12 12:24:05.824: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 12 12:24:05.824: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 12 12:24:05.824: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 12 12:24:05.824: INFO: Checking APIGroup: coordination.k8s.io
Feb 12 12:24:05.824: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 12 12:24:05.824: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 12 12:24:05.824: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 12 12:24:05.824: INFO: Checking APIGroup: node.k8s.io
Feb 12 12:24:05.825: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 12 12:24:05.825: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 12 12:24:05.825: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 12 12:24:05.825: INFO: Checking APIGroup: discovery.k8s.io
Feb 12 12:24:05.825: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 12 12:24:05.825: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 12 12:24:05.825: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 12 12:24:05.825: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 12 12:24:05.826: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Feb 12 12:24:05.826: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Feb 12 12:24:05.826: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Feb 12 12:24:05.826: INFO: Checking APIGroup: crd.projectcalico.org
Feb 12 12:24:05.827: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 12 12:24:05.827: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 12 12:24:05.827: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Feb 12 12:24:05.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8866" for this suite. 02/12/23 12:24:05.831
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":278,"skipped":5069,"failed":0}
------------------------------
 [0.459 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:24:05.379
    Feb 12 12:24:05.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename discovery 02/12/23 12:24:05.38
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:05.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:05.401
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/12/23 12:24:05.404
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 12 12:24:05.813: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 12 12:24:05.814: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 12 12:24:05.814: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 12 12:24:05.814: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 12 12:24:05.814: INFO: Checking APIGroup: apps
    Feb 12 12:24:05.815: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 12 12:24:05.815: INFO: Versions found [{apps/v1 v1}]
    Feb 12 12:24:05.815: INFO: apps/v1 matches apps/v1
    Feb 12 12:24:05.815: INFO: Checking APIGroup: events.k8s.io
    Feb 12 12:24:05.815: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 12 12:24:05.815: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 12 12:24:05.815: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 12 12:24:05.815: INFO: Checking APIGroup: authentication.k8s.io
    Feb 12 12:24:05.816: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 12 12:24:05.816: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 12 12:24:05.816: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 12 12:24:05.816: INFO: Checking APIGroup: authorization.k8s.io
    Feb 12 12:24:05.817: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 12 12:24:05.817: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 12 12:24:05.817: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 12 12:24:05.817: INFO: Checking APIGroup: autoscaling
    Feb 12 12:24:05.817: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 12 12:24:05.817: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Feb 12 12:24:05.817: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 12 12:24:05.817: INFO: Checking APIGroup: batch
    Feb 12 12:24:05.818: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 12 12:24:05.818: INFO: Versions found [{batch/v1 v1}]
    Feb 12 12:24:05.818: INFO: batch/v1 matches batch/v1
    Feb 12 12:24:05.818: INFO: Checking APIGroup: certificates.k8s.io
    Feb 12 12:24:05.819: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 12 12:24:05.819: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 12 12:24:05.819: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 12 12:24:05.819: INFO: Checking APIGroup: networking.k8s.io
    Feb 12 12:24:05.819: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 12 12:24:05.819: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 12 12:24:05.819: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 12 12:24:05.819: INFO: Checking APIGroup: policy
    Feb 12 12:24:05.820: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 12 12:24:05.820: INFO: Versions found [{policy/v1 v1}]
    Feb 12 12:24:05.820: INFO: policy/v1 matches policy/v1
    Feb 12 12:24:05.820: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 12 12:24:05.821: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 12 12:24:05.821: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 12 12:24:05.821: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 12 12:24:05.821: INFO: Checking APIGroup: storage.k8s.io
    Feb 12 12:24:05.822: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 12 12:24:05.822: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 12 12:24:05.822: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 12 12:24:05.822: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 12 12:24:05.822: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 12 12:24:05.822: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 12 12:24:05.822: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 12 12:24:05.822: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 12 12:24:05.823: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 12 12:24:05.823: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 12 12:24:05.823: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 12 12:24:05.823: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 12 12:24:05.824: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 12 12:24:05.824: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 12 12:24:05.824: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 12 12:24:05.824: INFO: Checking APIGroup: coordination.k8s.io
    Feb 12 12:24:05.824: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 12 12:24:05.824: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 12 12:24:05.824: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 12 12:24:05.824: INFO: Checking APIGroup: node.k8s.io
    Feb 12 12:24:05.825: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 12 12:24:05.825: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 12 12:24:05.825: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 12 12:24:05.825: INFO: Checking APIGroup: discovery.k8s.io
    Feb 12 12:24:05.825: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 12 12:24:05.825: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 12 12:24:05.825: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 12 12:24:05.825: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 12 12:24:05.826: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Feb 12 12:24:05.826: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Feb 12 12:24:05.826: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Feb 12 12:24:05.826: INFO: Checking APIGroup: crd.projectcalico.org
    Feb 12 12:24:05.827: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Feb 12 12:24:05.827: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Feb 12 12:24:05.827: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Feb 12 12:24:05.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-8866" for this suite. 02/12/23 12:24:05.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:24:05.84
Feb 12 12:24:05.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:24:05.841
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:05.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:05.858
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:24:05.86
Feb 12 12:24:05.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a" in namespace "downward-api-8206" to be "Succeeded or Failed"
Feb 12 12:24:05.873: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.100639ms
Feb 12 12:24:07.889: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a": Phase="Running", Reason="", readiness=false. Elapsed: 2.019057376s
Feb 12 12:24:09.877: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007499631s
STEP: Saw pod success 02/12/23 12:24:09.877
Feb 12 12:24:09.877: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a" satisfied condition "Succeeded or Failed"
Feb 12 12:24:09.885: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a container client-container: <nil>
STEP: delete the pod 02/12/23 12:24:09.894
Feb 12 12:24:09.907: INFO: Waiting for pod downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a to disappear
Feb 12 12:24:09.910: INFO: Pod downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 12:24:09.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8206" for this suite. 02/12/23 12:24:09.914
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":279,"skipped":5080,"failed":0}
------------------------------
 [4.082 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:24:05.84
    Feb 12 12:24:05.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:24:05.841
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:05.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:05.858
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:24:05.86
    Feb 12 12:24:05.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a" in namespace "downward-api-8206" to be "Succeeded or Failed"
    Feb 12 12:24:05.873: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.100639ms
    Feb 12 12:24:07.889: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a": Phase="Running", Reason="", readiness=false. Elapsed: 2.019057376s
    Feb 12 12:24:09.877: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007499631s
    STEP: Saw pod success 02/12/23 12:24:09.877
    Feb 12 12:24:09.877: INFO: Pod "downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a" satisfied condition "Succeeded or Failed"
    Feb 12 12:24:09.885: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a container client-container: <nil>
    STEP: delete the pod 02/12/23 12:24:09.894
    Feb 12 12:24:09.907: INFO: Waiting for pod downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a to disappear
    Feb 12 12:24:09.910: INFO: Pod downwardapi-volume-94e486a6-24d1-42d1-9736-c0f92473953a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 12:24:09.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8206" for this suite. 02/12/23 12:24:09.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:24:09.922
Feb 12 12:24:09.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 12:24:09.923
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:09.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:09.941
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 12 12:24:09.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:25:13.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4234" for this suite. 02/12/23 12:25:13.442
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":280,"skipped":5088,"failed":0}
------------------------------
 [SLOW TEST] [63.530 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:24:09.922
    Feb 12 12:24:09.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename custom-resource-definition 02/12/23 12:24:09.923
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:24:09.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:24:09.941
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 12 12:24:09.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:25:13.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4234" for this suite. 02/12/23 12:25:13.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:25:13.453
Feb 12 12:25:13.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename aggregator 02/12/23 12:25:13.454
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:25:13.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:25:13.482
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 12 12:25:13.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/12/23 12:25:13.484
Feb 12 12:25:13.895: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 12 12:25:15.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:17.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:19.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:21.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:28.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:29.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:31.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:33.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:35.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:44.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:25:46.115: INFO: Waited 115.428688ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/12/23 12:25:46.158
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/12/23 12:25:46.162
STEP: List APIServices 02/12/23 12:25:46.172
Feb 12 12:25:46.179: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Feb 12 12:25:51.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7191" for this suite. 02/12/23 12:25:51.581
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":281,"skipped":5101,"failed":0}
------------------------------
 [SLOW TEST] [38.140 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:25:13.453
    Feb 12 12:25:13.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename aggregator 02/12/23 12:25:13.454
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:25:13.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:25:13.482
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 12 12:25:13.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/12/23 12:25:13.484
    Feb 12 12:25:13.895: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Feb 12 12:25:15.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:17.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:19.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:21.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:28.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:29.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:31.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:33.989: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:35.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:44.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 25, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:25:46.115: INFO: Waited 115.428688ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/12/23 12:25:46.158
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/12/23 12:25:46.162
    STEP: List APIServices 02/12/23 12:25:46.172
    Feb 12 12:25:46.179: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Feb 12 12:25:51.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-7191" for this suite. 02/12/23 12:25:51.581
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:25:51.594
Feb 12 12:25:51.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename init-container 02/12/23 12:25:51.598
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:25:51.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:25:51.624
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 02/12/23 12:25:51.628
Feb 12 12:25:51.628: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 12:25:57.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4070" for this suite. 02/12/23 12:25:57.208
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":282,"skipped":5103,"failed":0}
------------------------------
 [SLOW TEST] [5.635 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:25:51.594
    Feb 12 12:25:51.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename init-container 02/12/23 12:25:51.598
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:25:51.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:25:51.624
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 02/12/23 12:25:51.628
    Feb 12 12:25:51.628: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 12:25:57.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4070" for this suite. 02/12/23 12:25:57.208
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:25:57.23
Feb 12 12:25:57.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 12:25:57.231
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:25:57.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:25:57.354
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 02/12/23 12:25:57.356
STEP: waiting for pod running 02/12/23 12:25:57.394
Feb 12 12:25:57.395: INFO: Waiting up to 2m0s for pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" in namespace "var-expansion-3373" to be "running"
Feb 12 12:25:57.629: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24": Phase="Pending", Reason="", readiness=false. Elapsed: 234.600783ms
Feb 12 12:25:59.643: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24": Phase="Running", Reason="", readiness=true. Elapsed: 2.248070546s
Feb 12 12:25:59.643: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" satisfied condition "running"
STEP: creating a file in subpath 02/12/23 12:25:59.643
Feb 12 12:25:59.657: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3373 PodName:var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:25:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:25:59.659: INFO: ExecWithOptions: Clientset creation
Feb 12 12:25:59.660: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3373/pods/var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/12/23 12:25:59.858
Feb 12 12:25:59.867: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3373 PodName:var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:25:59.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:25:59.868: INFO: ExecWithOptions: Clientset creation
Feb 12 12:25:59.868: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3373/pods/var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/12/23 12:25:59.941
Feb 12 12:26:00.480: INFO: Successfully updated pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24"
STEP: waiting for annotated pod running 02/12/23 12:26:00.48
Feb 12 12:26:00.481: INFO: Waiting up to 2m0s for pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" in namespace "var-expansion-3373" to be "running"
Feb 12 12:26:00.485: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24": Phase="Running", Reason="", readiness=true. Elapsed: 3.981246ms
Feb 12 12:26:00.485: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" satisfied condition "running"
STEP: deleting the pod gracefully 02/12/23 12:26:00.485
Feb 12 12:26:00.485: INFO: Deleting pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" in namespace "var-expansion-3373"
Feb 12 12:26:00.494: INFO: Wait up to 5m0s for pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 12:26:34.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3373" for this suite. 02/12/23 12:26:34.531
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":283,"skipped":5107,"failed":0}
------------------------------
 [SLOW TEST] [37.317 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:25:57.23
    Feb 12 12:25:57.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 12:25:57.231
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:25:57.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:25:57.354
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 02/12/23 12:25:57.356
    STEP: waiting for pod running 02/12/23 12:25:57.394
    Feb 12 12:25:57.395: INFO: Waiting up to 2m0s for pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" in namespace "var-expansion-3373" to be "running"
    Feb 12 12:25:57.629: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24": Phase="Pending", Reason="", readiness=false. Elapsed: 234.600783ms
    Feb 12 12:25:59.643: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24": Phase="Running", Reason="", readiness=true. Elapsed: 2.248070546s
    Feb 12 12:25:59.643: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" satisfied condition "running"
    STEP: creating a file in subpath 02/12/23 12:25:59.643
    Feb 12 12:25:59.657: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3373 PodName:var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:25:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:25:59.659: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:25:59.660: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3373/pods/var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/12/23 12:25:59.858
    Feb 12 12:25:59.867: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3373 PodName:var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:25:59.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:25:59.868: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:25:59.868: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3373/pods/var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/12/23 12:25:59.941
    Feb 12 12:26:00.480: INFO: Successfully updated pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24"
    STEP: waiting for annotated pod running 02/12/23 12:26:00.48
    Feb 12 12:26:00.481: INFO: Waiting up to 2m0s for pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" in namespace "var-expansion-3373" to be "running"
    Feb 12 12:26:00.485: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24": Phase="Running", Reason="", readiness=true. Elapsed: 3.981246ms
    Feb 12 12:26:00.485: INFO: Pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" satisfied condition "running"
    STEP: deleting the pod gracefully 02/12/23 12:26:00.485
    Feb 12 12:26:00.485: INFO: Deleting pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" in namespace "var-expansion-3373"
    Feb 12 12:26:00.494: INFO: Wait up to 5m0s for pod "var-expansion-c6e33e25-8082-4cfb-a3f1-d5e75ded1a24" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 12:26:34.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3373" for this suite. 02/12/23 12:26:34.531
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:26:34.549
Feb 12 12:26:34.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename runtimeclass 02/12/23 12:26:34.55
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:34.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:34.569
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/12/23 12:26:34.571
STEP: getting /apis/node.k8s.io 02/12/23 12:26:34.573
STEP: getting /apis/node.k8s.io/v1 02/12/23 12:26:34.573
STEP: creating 02/12/23 12:26:34.574
STEP: watching 02/12/23 12:26:34.589
Feb 12 12:26:34.589: INFO: starting watch
STEP: getting 02/12/23 12:26:34.594
STEP: listing 02/12/23 12:26:34.598
STEP: patching 02/12/23 12:26:34.6
STEP: updating 02/12/23 12:26:34.606
Feb 12 12:26:34.611: INFO: waiting for watch events with expected annotations
STEP: deleting 02/12/23 12:26:34.611
STEP: deleting a collection 02/12/23 12:26:34.622
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Feb 12 12:26:34.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3989" for this suite. 02/12/23 12:26:34.64
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":284,"skipped":5108,"failed":0}
------------------------------
 [0.097 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:26:34.549
    Feb 12 12:26:34.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename runtimeclass 02/12/23 12:26:34.55
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:34.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:34.569
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/12/23 12:26:34.571
    STEP: getting /apis/node.k8s.io 02/12/23 12:26:34.573
    STEP: getting /apis/node.k8s.io/v1 02/12/23 12:26:34.573
    STEP: creating 02/12/23 12:26:34.574
    STEP: watching 02/12/23 12:26:34.589
    Feb 12 12:26:34.589: INFO: starting watch
    STEP: getting 02/12/23 12:26:34.594
    STEP: listing 02/12/23 12:26:34.598
    STEP: patching 02/12/23 12:26:34.6
    STEP: updating 02/12/23 12:26:34.606
    Feb 12 12:26:34.611: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/12/23 12:26:34.611
    STEP: deleting a collection 02/12/23 12:26:34.622
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Feb 12 12:26:34.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3989" for this suite. 02/12/23 12:26:34.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:26:34.648
Feb 12 12:26:34.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubelet-test 02/12/23 12:26:34.649
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:34.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:34.666
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 12 12:26:34.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-367" for this suite. 02/12/23 12:26:34.712
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":285,"skipped":5179,"failed":0}
------------------------------
 [0.071 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:26:34.648
    Feb 12 12:26:34.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubelet-test 02/12/23 12:26:34.649
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:34.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:34.666
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 12 12:26:34.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-367" for this suite. 02/12/23 12:26:34.712
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:26:34.72
Feb 12 12:26:34.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 12:26:34.722
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:34.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:34.742
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/12/23 12:26:34.744
STEP: delete the rc 02/12/23 12:26:39.756
STEP: wait for all pods to be garbage collected 02/12/23 12:26:39.764
STEP: Gathering metrics 02/12/23 12:26:44.79
Feb 12 12:26:44.837: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Feb 12 12:26:44.844: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.244587ms
Feb 12 12:26:44.844: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Feb 12 12:26:44.844: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Feb 12 12:26:44.912: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 12:26:44.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4425" for this suite. 02/12/23 12:26:44.916
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":286,"skipped":5181,"failed":0}
------------------------------
 [SLOW TEST] [10.204 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:26:34.72
    Feb 12 12:26:34.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 12:26:34.722
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:34.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:34.742
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/12/23 12:26:34.744
    STEP: delete the rc 02/12/23 12:26:39.756
    STEP: wait for all pods to be garbage collected 02/12/23 12:26:39.764
    STEP: Gathering metrics 02/12/23 12:26:44.79
    Feb 12 12:26:44.837: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Feb 12 12:26:44.844: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 6.244587ms
    Feb 12 12:26:44.844: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Feb 12 12:26:44.844: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Feb 12 12:26:44.912: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 12:26:44.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4425" for this suite. 02/12/23 12:26:44.916
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:26:44.925
Feb 12 12:26:44.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename cronjob 02/12/23 12:26:44.926
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:44.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:44.944
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/12/23 12:26:44.946
STEP: Ensuring no jobs are scheduled 02/12/23 12:26:44.952
STEP: Ensuring no job exists by listing jobs explicitly 02/12/23 12:31:44.974
STEP: Removing cronjob 02/12/23 12:31:44.983
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Feb 12 12:31:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4644" for this suite. 02/12/23 12:31:45.001
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":287,"skipped":5185,"failed":0}
------------------------------
 [SLOW TEST] [300.085 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:26:44.925
    Feb 12 12:26:44.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename cronjob 02/12/23 12:26:44.926
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:26:44.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:26:44.944
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/12/23 12:26:44.946
    STEP: Ensuring no jobs are scheduled 02/12/23 12:26:44.952
    STEP: Ensuring no job exists by listing jobs explicitly 02/12/23 12:31:44.974
    STEP: Removing cronjob 02/12/23 12:31:44.983
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Feb 12 12:31:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4644" for this suite. 02/12/23 12:31:45.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:31:45.017
Feb 12 12:31:45.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:31:45.017
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:31:45.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:31:45.036
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-8ef4c68e-318d-4571-a3fe-dd7eb82a3db4 02/12/23 12:31:45.038
STEP: Creating a pod to test consume configMaps 02/12/23 12:31:45.042
Feb 12 12:31:45.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52" in namespace "projected-4500" to be "Succeeded or Failed"
Feb 12 12:31:45.054: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895096ms
Feb 12 12:31:47.067: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015508412s
Feb 12 12:31:49.067: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01564349s
STEP: Saw pod success 02/12/23 12:31:49.067
Feb 12 12:31:49.068: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52" satisfied condition "Succeeded or Failed"
Feb 12 12:31:49.080: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:31:49.131
Feb 12 12:31:49.153: INFO: Waiting for pod pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52 to disappear
Feb 12 12:31:49.155: INFO: Pod pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 12:31:49.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4500" for this suite. 02/12/23 12:31:49.16
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":288,"skipped":5223,"failed":0}
------------------------------
 [4.151 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:31:45.017
    Feb 12 12:31:45.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:31:45.017
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:31:45.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:31:45.036
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-8ef4c68e-318d-4571-a3fe-dd7eb82a3db4 02/12/23 12:31:45.038
    STEP: Creating a pod to test consume configMaps 02/12/23 12:31:45.042
    Feb 12 12:31:45.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52" in namespace "projected-4500" to be "Succeeded or Failed"
    Feb 12 12:31:45.054: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895096ms
    Feb 12 12:31:47.067: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015508412s
    Feb 12 12:31:49.067: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01564349s
    STEP: Saw pod success 02/12/23 12:31:49.067
    Feb 12 12:31:49.068: INFO: Pod "pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52" satisfied condition "Succeeded or Failed"
    Feb 12 12:31:49.080: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:31:49.131
    Feb 12 12:31:49.153: INFO: Waiting for pod pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52 to disappear
    Feb 12 12:31:49.155: INFO: Pod pod-projected-configmaps-004782a8-5e8d-4f16-861c-9dc28a80ed52 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 12:31:49.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4500" for this suite. 02/12/23 12:31:49.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:31:49.169
Feb 12 12:31:49.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-probe 02/12/23 12:31:49.17
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:31:49.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:31:49.188
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd in namespace container-probe-7121 02/12/23 12:31:49.189
Feb 12 12:31:49.202: INFO: Waiting up to 5m0s for pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd" in namespace "container-probe-7121" to be "not pending"
Feb 12 12:31:49.207: INFO: Pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.427411ms
Feb 12 12:31:51.238: INFO: Pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd": Phase="Running", Reason="", readiness=true. Elapsed: 2.035675003s
Feb 12 12:31:51.238: INFO: Pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd" satisfied condition "not pending"
Feb 12 12:31:51.239: INFO: Started pod test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd in namespace container-probe-7121
STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 12:31:51.239
Feb 12 12:31:51.251: INFO: Initial restart count of pod test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd is 0
STEP: deleting the pod 02/12/23 12:35:52.824
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Feb 12 12:35:52.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7121" for this suite. 02/12/23 12:35:52.867
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":289,"skipped":5248,"failed":0}
------------------------------
 [SLOW TEST] [243.707 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:31:49.169
    Feb 12 12:31:49.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-probe 02/12/23 12:31:49.17
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:31:49.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:31:49.188
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd in namespace container-probe-7121 02/12/23 12:31:49.189
    Feb 12 12:31:49.202: INFO: Waiting up to 5m0s for pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd" in namespace "container-probe-7121" to be "not pending"
    Feb 12 12:31:49.207: INFO: Pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.427411ms
    Feb 12 12:31:51.238: INFO: Pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd": Phase="Running", Reason="", readiness=true. Elapsed: 2.035675003s
    Feb 12 12:31:51.238: INFO: Pod "test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd" satisfied condition "not pending"
    Feb 12 12:31:51.239: INFO: Started pod test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd in namespace container-probe-7121
    STEP: checking the pod's current state and verifying that restartCount is present 02/12/23 12:31:51.239
    Feb 12 12:31:51.251: INFO: Initial restart count of pod test-webserver-30dc25ad-e53a-4c81-9faf-8ef400725ecd is 0
    STEP: deleting the pod 02/12/23 12:35:52.824
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Feb 12 12:35:52.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7121" for this suite. 02/12/23 12:35:52.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:35:52.877
Feb 12 12:35:52.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 12:35:52.879
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:35:52.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:35:52.905
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 02/12/23 12:35:52.909
STEP: setting up watch 02/12/23 12:35:52.909
STEP: submitting the pod to kubernetes 02/12/23 12:35:53.019
STEP: verifying the pod is in kubernetes 02/12/23 12:35:53.036
STEP: verifying pod creation was observed 02/12/23 12:35:53.041
Feb 12 12:35:53.042: INFO: Waiting up to 5m0s for pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d" in namespace "pods-4371" to be "running"
Feb 12 12:35:53.048: INFO: Pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.659383ms
Feb 12 12:35:55.053: INFO: Pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011731584s
Feb 12 12:35:55.053: INFO: Pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d" satisfied condition "running"
STEP: deleting the pod gracefully 02/12/23 12:35:55.057
STEP: verifying pod deletion was observed 02/12/23 12:35:55.065
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 12:35:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4371" for this suite. 02/12/23 12:35:56.959
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":290,"skipped":5264,"failed":0}
------------------------------
 [4.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:35:52.877
    Feb 12 12:35:52.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 12:35:52.879
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:35:52.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:35:52.905
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 02/12/23 12:35:52.909
    STEP: setting up watch 02/12/23 12:35:52.909
    STEP: submitting the pod to kubernetes 02/12/23 12:35:53.019
    STEP: verifying the pod is in kubernetes 02/12/23 12:35:53.036
    STEP: verifying pod creation was observed 02/12/23 12:35:53.041
    Feb 12 12:35:53.042: INFO: Waiting up to 5m0s for pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d" in namespace "pods-4371" to be "running"
    Feb 12 12:35:53.048: INFO: Pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.659383ms
    Feb 12 12:35:55.053: INFO: Pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011731584s
    Feb 12 12:35:55.053: INFO: Pod "pod-submit-remove-46598498-c773-48e3-94c0-db03b093c42d" satisfied condition "running"
    STEP: deleting the pod gracefully 02/12/23 12:35:55.057
    STEP: verifying pod deletion was observed 02/12/23 12:35:55.065
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 12:35:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4371" for this suite. 02/12/23 12:35:56.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:35:56.967
Feb 12 12:35:56.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:35:56.968
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:35:57.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:35:57.038
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:35:57.052
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:35:57.277
STEP: Deploying the webhook pod 02/12/23 12:35:57.285
STEP: Wait for the deployment to be ready 02/12/23 12:35:57.299
Feb 12 12:35:57.311: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:35:59.349
STEP: Verifying the service has paired with the endpoint 02/12/23 12:35:59.374
Feb 12 12:36:00.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/12/23 12:36:00.387
STEP: create a configmap that should be updated by the webhook 02/12/23 12:36:00.41
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:36:00.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4712" for this suite. 02/12/23 12:36:00.433
STEP: Destroying namespace "webhook-4712-markers" for this suite. 02/12/23 12:36:00.44
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":291,"skipped":5270,"failed":0}
------------------------------
 [3.549 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:35:56.967
    Feb 12 12:35:56.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:35:56.968
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:35:57.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:35:57.038
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:35:57.052
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:35:57.277
    STEP: Deploying the webhook pod 02/12/23 12:35:57.285
    STEP: Wait for the deployment to be ready 02/12/23 12:35:57.299
    Feb 12 12:35:57.311: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:35:59.349
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:35:59.374
    Feb 12 12:36:00.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/12/23 12:36:00.387
    STEP: create a configmap that should be updated by the webhook 02/12/23 12:36:00.41
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:36:00.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4712" for this suite. 02/12/23 12:36:00.433
    STEP: Destroying namespace "webhook-4712-markers" for this suite. 02/12/23 12:36:00.44
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:36:00.523
Feb 12 12:36:00.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 12:36:00.524
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:36:00.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:36:00.56
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 02/12/23 12:36:00.57
STEP: Creating a ResourceQuota 02/12/23 12:36:05.575
STEP: Ensuring resource quota status is calculated 02/12/23 12:36:05.589
STEP: Creating a Pod that fits quota 02/12/23 12:36:07.594
STEP: Ensuring ResourceQuota status captures the pod usage 02/12/23 12:36:08.132
STEP: Not allowing a pod to be created that exceeds remaining quota 02/12/23 12:36:10.169
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/12/23 12:36:10.177
STEP: Ensuring a pod cannot update its resource requirements 02/12/23 12:36:10.184
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/12/23 12:36:10.198
STEP: Deleting the pod 02/12/23 12:36:12.212
STEP: Ensuring resource quota status released the pod usage 02/12/23 12:36:12.397
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 12:36:14.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9887" for this suite. 02/12/23 12:36:14.404
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":292,"skipped":5322,"failed":0}
------------------------------
 [SLOW TEST] [13.900 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:36:00.523
    Feb 12 12:36:00.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 12:36:00.524
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:36:00.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:36:00.56
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 02/12/23 12:36:00.57
    STEP: Creating a ResourceQuota 02/12/23 12:36:05.575
    STEP: Ensuring resource quota status is calculated 02/12/23 12:36:05.589
    STEP: Creating a Pod that fits quota 02/12/23 12:36:07.594
    STEP: Ensuring ResourceQuota status captures the pod usage 02/12/23 12:36:08.132
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/12/23 12:36:10.169
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/12/23 12:36:10.177
    STEP: Ensuring a pod cannot update its resource requirements 02/12/23 12:36:10.184
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/12/23 12:36:10.198
    STEP: Deleting the pod 02/12/23 12:36:12.212
    STEP: Ensuring resource quota status released the pod usage 02/12/23 12:36:12.397
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 12:36:14.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9887" for this suite. 02/12/23 12:36:14.404
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:36:14.423
Feb 12 12:36:14.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 12:36:14.425
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:36:14.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:36:14.555
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/12/23 12:36:14.595
STEP: delete the rc 02/12/23 12:36:27.19
STEP: wait for the rc to be deleted 02/12/23 12:36:29.34
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/12/23 12:36:35.101
STEP: Gathering metrics 02/12/23 12:37:05.152
Feb 12 12:37:05.194: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Feb 12 12:37:05.199: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.793518ms
Feb 12 12:37:05.199: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Feb 12 12:37:05.199: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Feb 12 12:37:05.270: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 12 12:37:05.270: INFO: Deleting pod "simpletest.rc-2nhhb" in namespace "gc-3075"
Feb 12 12:37:05.351: INFO: Deleting pod "simpletest.rc-42569" in namespace "gc-3075"
Feb 12 12:37:05.382: INFO: Deleting pod "simpletest.rc-44q6w" in namespace "gc-3075"
Feb 12 12:37:05.409: INFO: Deleting pod "simpletest.rc-49c6k" in namespace "gc-3075"
Feb 12 12:37:05.444: INFO: Deleting pod "simpletest.rc-4l79d" in namespace "gc-3075"
Feb 12 12:37:05.470: INFO: Deleting pod "simpletest.rc-4pnnw" in namespace "gc-3075"
Feb 12 12:37:05.496: INFO: Deleting pod "simpletest.rc-55kpf" in namespace "gc-3075"
Feb 12 12:37:05.526: INFO: Deleting pod "simpletest.rc-5ldpm" in namespace "gc-3075"
Feb 12 12:37:05.585: INFO: Deleting pod "simpletest.rc-6962c" in namespace "gc-3075"
Feb 12 12:37:05.609: INFO: Deleting pod "simpletest.rc-6hnkk" in namespace "gc-3075"
Feb 12 12:37:05.683: INFO: Deleting pod "simpletest.rc-6jd57" in namespace "gc-3075"
Feb 12 12:37:05.734: INFO: Deleting pod "simpletest.rc-6kmkq" in namespace "gc-3075"
Feb 12 12:37:05.821: INFO: Deleting pod "simpletest.rc-6ldnq" in namespace "gc-3075"
Feb 12 12:37:08.486: INFO: Deleting pod "simpletest.rc-6rmn6" in namespace "gc-3075"
Feb 12 12:37:08.946: INFO: Deleting pod "simpletest.rc-6xs4j" in namespace "gc-3075"
Feb 12 12:37:09.821: INFO: Deleting pod "simpletest.rc-6zf6c" in namespace "gc-3075"
Feb 12 12:37:09.898: INFO: Deleting pod "simpletest.rc-6zf9q" in namespace "gc-3075"
Feb 12 12:37:10.048: INFO: Deleting pod "simpletest.rc-76qw8" in namespace "gc-3075"
Feb 12 12:37:10.169: INFO: Deleting pod "simpletest.rc-7gtgs" in namespace "gc-3075"
Feb 12 12:37:10.232: INFO: Deleting pod "simpletest.rc-7kqkx" in namespace "gc-3075"
Feb 12 12:37:10.280: INFO: Deleting pod "simpletest.rc-7vfnj" in namespace "gc-3075"
Feb 12 12:37:10.349: INFO: Deleting pod "simpletest.rc-7z6mn" in namespace "gc-3075"
Feb 12 12:37:10.396: INFO: Deleting pod "simpletest.rc-8cg4s" in namespace "gc-3075"
Feb 12 12:37:10.495: INFO: Deleting pod "simpletest.rc-8ks42" in namespace "gc-3075"
Feb 12 12:37:10.665: INFO: Deleting pod "simpletest.rc-8n7hs" in namespace "gc-3075"
Feb 12 12:37:10.749: INFO: Deleting pod "simpletest.rc-925x5" in namespace "gc-3075"
Feb 12 12:37:10.852: INFO: Deleting pod "simpletest.rc-92lsm" in namespace "gc-3075"
Feb 12 12:37:10.953: INFO: Deleting pod "simpletest.rc-94trk" in namespace "gc-3075"
Feb 12 12:37:11.066: INFO: Deleting pod "simpletest.rc-98q27" in namespace "gc-3075"
Feb 12 12:37:11.140: INFO: Deleting pod "simpletest.rc-9d8qs" in namespace "gc-3075"
Feb 12 12:37:11.193: INFO: Deleting pod "simpletest.rc-9nlwl" in namespace "gc-3075"
Feb 12 12:37:11.294: INFO: Deleting pod "simpletest.rc-b9r27" in namespace "gc-3075"
Feb 12 12:37:11.352: INFO: Deleting pod "simpletest.rc-bkkc8" in namespace "gc-3075"
Feb 12 12:37:14.288: INFO: Deleting pod "simpletest.rc-bnrsx" in namespace "gc-3075"
Feb 12 12:37:14.760: INFO: Deleting pod "simpletest.rc-ccqsz" in namespace "gc-3075"
Feb 12 12:37:15.185: INFO: Deleting pod "simpletest.rc-d7lsj" in namespace "gc-3075"
Feb 12 12:37:15.252: INFO: Deleting pod "simpletest.rc-d7z4t" in namespace "gc-3075"
Feb 12 12:37:15.487: INFO: Deleting pod "simpletest.rc-djl27" in namespace "gc-3075"
Feb 12 12:37:15.604: INFO: Deleting pod "simpletest.rc-dzz7v" in namespace "gc-3075"
Feb 12 12:37:15.653: INFO: Deleting pod "simpletest.rc-f9442" in namespace "gc-3075"
Feb 12 12:37:15.704: INFO: Deleting pod "simpletest.rc-f95n5" in namespace "gc-3075"
Feb 12 12:37:15.767: INFO: Deleting pod "simpletest.rc-fqchf" in namespace "gc-3075"
Feb 12 12:37:15.822: INFO: Deleting pod "simpletest.rc-g5llx" in namespace "gc-3075"
Feb 12 12:37:15.872: INFO: Deleting pod "simpletest.rc-g77q8" in namespace "gc-3075"
Feb 12 12:37:15.961: INFO: Deleting pod "simpletest.rc-gc469" in namespace "gc-3075"
Feb 12 12:37:16.077: INFO: Deleting pod "simpletest.rc-gkh65" in namespace "gc-3075"
Feb 12 12:37:16.194: INFO: Deleting pod "simpletest.rc-gq86w" in namespace "gc-3075"
Feb 12 12:37:16.288: INFO: Deleting pod "simpletest.rc-hcll8" in namespace "gc-3075"
Feb 12 12:37:16.403: INFO: Deleting pod "simpletest.rc-hk8hx" in namespace "gc-3075"
Feb 12 12:37:16.510: INFO: Deleting pod "simpletest.rc-hm2p8" in namespace "gc-3075"
Feb 12 12:37:16.615: INFO: Deleting pod "simpletest.rc-hnsth" in namespace "gc-3075"
Feb 12 12:37:16.718: INFO: Deleting pod "simpletest.rc-hrnb5" in namespace "gc-3075"
Feb 12 12:37:16.794: INFO: Deleting pod "simpletest.rc-jdks7" in namespace "gc-3075"
Feb 12 12:37:16.876: INFO: Deleting pod "simpletest.rc-jhvmh" in namespace "gc-3075"
Feb 12 12:37:17.006: INFO: Deleting pod "simpletest.rc-jnqfs" in namespace "gc-3075"
Feb 12 12:37:17.086: INFO: Deleting pod "simpletest.rc-jqtnh" in namespace "gc-3075"
Feb 12 12:37:17.193: INFO: Deleting pod "simpletest.rc-jsf4f" in namespace "gc-3075"
Feb 12 12:37:17.297: INFO: Deleting pod "simpletest.rc-jwq4n" in namespace "gc-3075"
Feb 12 12:37:17.423: INFO: Deleting pod "simpletest.rc-kc4c7" in namespace "gc-3075"
Feb 12 12:37:17.486: INFO: Deleting pod "simpletest.rc-knvgr" in namespace "gc-3075"
Feb 12 12:37:17.546: INFO: Deleting pod "simpletest.rc-kpfqw" in namespace "gc-3075"
Feb 12 12:37:17.664: INFO: Deleting pod "simpletest.rc-kzwld" in namespace "gc-3075"
Feb 12 12:37:17.769: INFO: Deleting pod "simpletest.rc-l42nd" in namespace "gc-3075"
Feb 12 12:37:17.815: INFO: Deleting pod "simpletest.rc-l8l4w" in namespace "gc-3075"
Feb 12 12:37:17.881: INFO: Deleting pod "simpletest.rc-l9xgh" in namespace "gc-3075"
Feb 12 12:37:20.305: INFO: Deleting pod "simpletest.rc-lttwj" in namespace "gc-3075"
Feb 12 12:37:21.670: INFO: Deleting pod "simpletest.rc-m4dbf" in namespace "gc-3075"
Feb 12 12:37:21.910: INFO: Deleting pod "simpletest.rc-mbr2l" in namespace "gc-3075"
Feb 12 12:37:22.097: INFO: Deleting pod "simpletest.rc-mfds4" in namespace "gc-3075"
Feb 12 12:37:22.266: INFO: Deleting pod "simpletest.rc-mlq4v" in namespace "gc-3075"
Feb 12 12:37:22.299: INFO: Deleting pod "simpletest.rc-mpsm7" in namespace "gc-3075"
Feb 12 12:37:22.359: INFO: Deleting pod "simpletest.rc-nxd4s" in namespace "gc-3075"
Feb 12 12:37:22.457: INFO: Deleting pod "simpletest.rc-nxjwn" in namespace "gc-3075"
Feb 12 12:37:22.523: INFO: Deleting pod "simpletest.rc-nzg5s" in namespace "gc-3075"
Feb 12 12:37:22.578: INFO: Deleting pod "simpletest.rc-p64s9" in namespace "gc-3075"
Feb 12 12:37:22.629: INFO: Deleting pod "simpletest.rc-pc5qv" in namespace "gc-3075"
Feb 12 12:37:22.704: INFO: Deleting pod "simpletest.rc-pgq6b" in namespace "gc-3075"
Feb 12 12:37:22.779: INFO: Deleting pod "simpletest.rc-ptwf7" in namespace "gc-3075"
Feb 12 12:37:22.841: INFO: Deleting pod "simpletest.rc-q7mww" in namespace "gc-3075"
Feb 12 12:37:22.931: INFO: Deleting pod "simpletest.rc-qr58c" in namespace "gc-3075"
Feb 12 12:37:22.993: INFO: Deleting pod "simpletest.rc-r676m" in namespace "gc-3075"
Feb 12 12:37:23.084: INFO: Deleting pod "simpletest.rc-rq7wt" in namespace "gc-3075"
Feb 12 12:37:23.166: INFO: Deleting pod "simpletest.rc-rw58s" in namespace "gc-3075"
Feb 12 12:37:24.696: INFO: Deleting pod "simpletest.rc-rzjfx" in namespace "gc-3075"
Feb 12 12:37:26.427: INFO: Deleting pod "simpletest.rc-scnbv" in namespace "gc-3075"
Feb 12 12:37:26.891: INFO: Deleting pod "simpletest.rc-ssxfr" in namespace "gc-3075"
Feb 12 12:37:27.219: INFO: Deleting pod "simpletest.rc-t4gw8" in namespace "gc-3075"
Feb 12 12:37:27.319: INFO: Deleting pod "simpletest.rc-tpmjg" in namespace "gc-3075"
Feb 12 12:37:27.410: INFO: Deleting pod "simpletest.rc-tqrrz" in namespace "gc-3075"
Feb 12 12:37:27.528: INFO: Deleting pod "simpletest.rc-vfx72" in namespace "gc-3075"
Feb 12 12:37:27.561: INFO: Deleting pod "simpletest.rc-vl2ps" in namespace "gc-3075"
Feb 12 12:37:27.637: INFO: Deleting pod "simpletest.rc-vn4lc" in namespace "gc-3075"
Feb 12 12:37:27.704: INFO: Deleting pod "simpletest.rc-wbcnm" in namespace "gc-3075"
Feb 12 12:37:27.771: INFO: Deleting pod "simpletest.rc-wkl2x" in namespace "gc-3075"
Feb 12 12:37:27.958: INFO: Deleting pod "simpletest.rc-ws52t" in namespace "gc-3075"
Feb 12 12:37:28.022: INFO: Deleting pod "simpletest.rc-wt5x6" in namespace "gc-3075"
Feb 12 12:37:28.186: INFO: Deleting pod "simpletest.rc-xh97t" in namespace "gc-3075"
Feb 12 12:37:28.254: INFO: Deleting pod "simpletest.rc-xqgnt" in namespace "gc-3075"
Feb 12 12:37:28.356: INFO: Deleting pod "simpletest.rc-z7pfp" in namespace "gc-3075"
Feb 12 12:37:28.427: INFO: Deleting pod "simpletest.rc-zblzg" in namespace "gc-3075"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 12:37:28.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3075" for this suite. 02/12/23 12:37:28.54
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":293,"skipped":5325,"failed":0}
------------------------------
 [SLOW TEST] [74.181 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:36:14.423
    Feb 12 12:36:14.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 12:36:14.425
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:36:14.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:36:14.555
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/12/23 12:36:14.595
    STEP: delete the rc 02/12/23 12:36:27.19
    STEP: wait for the rc to be deleted 02/12/23 12:36:29.34
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/12/23 12:36:35.101
    STEP: Gathering metrics 02/12/23 12:37:05.152
    Feb 12 12:37:05.194: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Feb 12 12:37:05.199: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.793518ms
    Feb 12 12:37:05.199: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Feb 12 12:37:05.199: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Feb 12 12:37:05.270: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 12 12:37:05.270: INFO: Deleting pod "simpletest.rc-2nhhb" in namespace "gc-3075"
    Feb 12 12:37:05.351: INFO: Deleting pod "simpletest.rc-42569" in namespace "gc-3075"
    Feb 12 12:37:05.382: INFO: Deleting pod "simpletest.rc-44q6w" in namespace "gc-3075"
    Feb 12 12:37:05.409: INFO: Deleting pod "simpletest.rc-49c6k" in namespace "gc-3075"
    Feb 12 12:37:05.444: INFO: Deleting pod "simpletest.rc-4l79d" in namespace "gc-3075"
    Feb 12 12:37:05.470: INFO: Deleting pod "simpletest.rc-4pnnw" in namespace "gc-3075"
    Feb 12 12:37:05.496: INFO: Deleting pod "simpletest.rc-55kpf" in namespace "gc-3075"
    Feb 12 12:37:05.526: INFO: Deleting pod "simpletest.rc-5ldpm" in namespace "gc-3075"
    Feb 12 12:37:05.585: INFO: Deleting pod "simpletest.rc-6962c" in namespace "gc-3075"
    Feb 12 12:37:05.609: INFO: Deleting pod "simpletest.rc-6hnkk" in namespace "gc-3075"
    Feb 12 12:37:05.683: INFO: Deleting pod "simpletest.rc-6jd57" in namespace "gc-3075"
    Feb 12 12:37:05.734: INFO: Deleting pod "simpletest.rc-6kmkq" in namespace "gc-3075"
    Feb 12 12:37:05.821: INFO: Deleting pod "simpletest.rc-6ldnq" in namespace "gc-3075"
    Feb 12 12:37:08.486: INFO: Deleting pod "simpletest.rc-6rmn6" in namespace "gc-3075"
    Feb 12 12:37:08.946: INFO: Deleting pod "simpletest.rc-6xs4j" in namespace "gc-3075"
    Feb 12 12:37:09.821: INFO: Deleting pod "simpletest.rc-6zf6c" in namespace "gc-3075"
    Feb 12 12:37:09.898: INFO: Deleting pod "simpletest.rc-6zf9q" in namespace "gc-3075"
    Feb 12 12:37:10.048: INFO: Deleting pod "simpletest.rc-76qw8" in namespace "gc-3075"
    Feb 12 12:37:10.169: INFO: Deleting pod "simpletest.rc-7gtgs" in namespace "gc-3075"
    Feb 12 12:37:10.232: INFO: Deleting pod "simpletest.rc-7kqkx" in namespace "gc-3075"
    Feb 12 12:37:10.280: INFO: Deleting pod "simpletest.rc-7vfnj" in namespace "gc-3075"
    Feb 12 12:37:10.349: INFO: Deleting pod "simpletest.rc-7z6mn" in namespace "gc-3075"
    Feb 12 12:37:10.396: INFO: Deleting pod "simpletest.rc-8cg4s" in namespace "gc-3075"
    Feb 12 12:37:10.495: INFO: Deleting pod "simpletest.rc-8ks42" in namespace "gc-3075"
    Feb 12 12:37:10.665: INFO: Deleting pod "simpletest.rc-8n7hs" in namespace "gc-3075"
    Feb 12 12:37:10.749: INFO: Deleting pod "simpletest.rc-925x5" in namespace "gc-3075"
    Feb 12 12:37:10.852: INFO: Deleting pod "simpletest.rc-92lsm" in namespace "gc-3075"
    Feb 12 12:37:10.953: INFO: Deleting pod "simpletest.rc-94trk" in namespace "gc-3075"
    Feb 12 12:37:11.066: INFO: Deleting pod "simpletest.rc-98q27" in namespace "gc-3075"
    Feb 12 12:37:11.140: INFO: Deleting pod "simpletest.rc-9d8qs" in namespace "gc-3075"
    Feb 12 12:37:11.193: INFO: Deleting pod "simpletest.rc-9nlwl" in namespace "gc-3075"
    Feb 12 12:37:11.294: INFO: Deleting pod "simpletest.rc-b9r27" in namespace "gc-3075"
    Feb 12 12:37:11.352: INFO: Deleting pod "simpletest.rc-bkkc8" in namespace "gc-3075"
    Feb 12 12:37:14.288: INFO: Deleting pod "simpletest.rc-bnrsx" in namespace "gc-3075"
    Feb 12 12:37:14.760: INFO: Deleting pod "simpletest.rc-ccqsz" in namespace "gc-3075"
    Feb 12 12:37:15.185: INFO: Deleting pod "simpletest.rc-d7lsj" in namespace "gc-3075"
    Feb 12 12:37:15.252: INFO: Deleting pod "simpletest.rc-d7z4t" in namespace "gc-3075"
    Feb 12 12:37:15.487: INFO: Deleting pod "simpletest.rc-djl27" in namespace "gc-3075"
    Feb 12 12:37:15.604: INFO: Deleting pod "simpletest.rc-dzz7v" in namespace "gc-3075"
    Feb 12 12:37:15.653: INFO: Deleting pod "simpletest.rc-f9442" in namespace "gc-3075"
    Feb 12 12:37:15.704: INFO: Deleting pod "simpletest.rc-f95n5" in namespace "gc-3075"
    Feb 12 12:37:15.767: INFO: Deleting pod "simpletest.rc-fqchf" in namespace "gc-3075"
    Feb 12 12:37:15.822: INFO: Deleting pod "simpletest.rc-g5llx" in namespace "gc-3075"
    Feb 12 12:37:15.872: INFO: Deleting pod "simpletest.rc-g77q8" in namespace "gc-3075"
    Feb 12 12:37:15.961: INFO: Deleting pod "simpletest.rc-gc469" in namespace "gc-3075"
    Feb 12 12:37:16.077: INFO: Deleting pod "simpletest.rc-gkh65" in namespace "gc-3075"
    Feb 12 12:37:16.194: INFO: Deleting pod "simpletest.rc-gq86w" in namespace "gc-3075"
    Feb 12 12:37:16.288: INFO: Deleting pod "simpletest.rc-hcll8" in namespace "gc-3075"
    Feb 12 12:37:16.403: INFO: Deleting pod "simpletest.rc-hk8hx" in namespace "gc-3075"
    Feb 12 12:37:16.510: INFO: Deleting pod "simpletest.rc-hm2p8" in namespace "gc-3075"
    Feb 12 12:37:16.615: INFO: Deleting pod "simpletest.rc-hnsth" in namespace "gc-3075"
    Feb 12 12:37:16.718: INFO: Deleting pod "simpletest.rc-hrnb5" in namespace "gc-3075"
    Feb 12 12:37:16.794: INFO: Deleting pod "simpletest.rc-jdks7" in namespace "gc-3075"
    Feb 12 12:37:16.876: INFO: Deleting pod "simpletest.rc-jhvmh" in namespace "gc-3075"
    Feb 12 12:37:17.006: INFO: Deleting pod "simpletest.rc-jnqfs" in namespace "gc-3075"
    Feb 12 12:37:17.086: INFO: Deleting pod "simpletest.rc-jqtnh" in namespace "gc-3075"
    Feb 12 12:37:17.193: INFO: Deleting pod "simpletest.rc-jsf4f" in namespace "gc-3075"
    Feb 12 12:37:17.297: INFO: Deleting pod "simpletest.rc-jwq4n" in namespace "gc-3075"
    Feb 12 12:37:17.423: INFO: Deleting pod "simpletest.rc-kc4c7" in namespace "gc-3075"
    Feb 12 12:37:17.486: INFO: Deleting pod "simpletest.rc-knvgr" in namespace "gc-3075"
    Feb 12 12:37:17.546: INFO: Deleting pod "simpletest.rc-kpfqw" in namespace "gc-3075"
    Feb 12 12:37:17.664: INFO: Deleting pod "simpletest.rc-kzwld" in namespace "gc-3075"
    Feb 12 12:37:17.769: INFO: Deleting pod "simpletest.rc-l42nd" in namespace "gc-3075"
    Feb 12 12:37:17.815: INFO: Deleting pod "simpletest.rc-l8l4w" in namespace "gc-3075"
    Feb 12 12:37:17.881: INFO: Deleting pod "simpletest.rc-l9xgh" in namespace "gc-3075"
    Feb 12 12:37:20.305: INFO: Deleting pod "simpletest.rc-lttwj" in namespace "gc-3075"
    Feb 12 12:37:21.670: INFO: Deleting pod "simpletest.rc-m4dbf" in namespace "gc-3075"
    Feb 12 12:37:21.910: INFO: Deleting pod "simpletest.rc-mbr2l" in namespace "gc-3075"
    Feb 12 12:37:22.097: INFO: Deleting pod "simpletest.rc-mfds4" in namespace "gc-3075"
    Feb 12 12:37:22.266: INFO: Deleting pod "simpletest.rc-mlq4v" in namespace "gc-3075"
    Feb 12 12:37:22.299: INFO: Deleting pod "simpletest.rc-mpsm7" in namespace "gc-3075"
    Feb 12 12:37:22.359: INFO: Deleting pod "simpletest.rc-nxd4s" in namespace "gc-3075"
    Feb 12 12:37:22.457: INFO: Deleting pod "simpletest.rc-nxjwn" in namespace "gc-3075"
    Feb 12 12:37:22.523: INFO: Deleting pod "simpletest.rc-nzg5s" in namespace "gc-3075"
    Feb 12 12:37:22.578: INFO: Deleting pod "simpletest.rc-p64s9" in namespace "gc-3075"
    Feb 12 12:37:22.629: INFO: Deleting pod "simpletest.rc-pc5qv" in namespace "gc-3075"
    Feb 12 12:37:22.704: INFO: Deleting pod "simpletest.rc-pgq6b" in namespace "gc-3075"
    Feb 12 12:37:22.779: INFO: Deleting pod "simpletest.rc-ptwf7" in namespace "gc-3075"
    Feb 12 12:37:22.841: INFO: Deleting pod "simpletest.rc-q7mww" in namespace "gc-3075"
    Feb 12 12:37:22.931: INFO: Deleting pod "simpletest.rc-qr58c" in namespace "gc-3075"
    Feb 12 12:37:22.993: INFO: Deleting pod "simpletest.rc-r676m" in namespace "gc-3075"
    Feb 12 12:37:23.084: INFO: Deleting pod "simpletest.rc-rq7wt" in namespace "gc-3075"
    Feb 12 12:37:23.166: INFO: Deleting pod "simpletest.rc-rw58s" in namespace "gc-3075"
    Feb 12 12:37:24.696: INFO: Deleting pod "simpletest.rc-rzjfx" in namespace "gc-3075"
    Feb 12 12:37:26.427: INFO: Deleting pod "simpletest.rc-scnbv" in namespace "gc-3075"
    Feb 12 12:37:26.891: INFO: Deleting pod "simpletest.rc-ssxfr" in namespace "gc-3075"
    Feb 12 12:37:27.219: INFO: Deleting pod "simpletest.rc-t4gw8" in namespace "gc-3075"
    Feb 12 12:37:27.319: INFO: Deleting pod "simpletest.rc-tpmjg" in namespace "gc-3075"
    Feb 12 12:37:27.410: INFO: Deleting pod "simpletest.rc-tqrrz" in namespace "gc-3075"
    Feb 12 12:37:27.528: INFO: Deleting pod "simpletest.rc-vfx72" in namespace "gc-3075"
    Feb 12 12:37:27.561: INFO: Deleting pod "simpletest.rc-vl2ps" in namespace "gc-3075"
    Feb 12 12:37:27.637: INFO: Deleting pod "simpletest.rc-vn4lc" in namespace "gc-3075"
    Feb 12 12:37:27.704: INFO: Deleting pod "simpletest.rc-wbcnm" in namespace "gc-3075"
    Feb 12 12:37:27.771: INFO: Deleting pod "simpletest.rc-wkl2x" in namespace "gc-3075"
    Feb 12 12:37:27.958: INFO: Deleting pod "simpletest.rc-ws52t" in namespace "gc-3075"
    Feb 12 12:37:28.022: INFO: Deleting pod "simpletest.rc-wt5x6" in namespace "gc-3075"
    Feb 12 12:37:28.186: INFO: Deleting pod "simpletest.rc-xh97t" in namespace "gc-3075"
    Feb 12 12:37:28.254: INFO: Deleting pod "simpletest.rc-xqgnt" in namespace "gc-3075"
    Feb 12 12:37:28.356: INFO: Deleting pod "simpletest.rc-z7pfp" in namespace "gc-3075"
    Feb 12 12:37:28.427: INFO: Deleting pod "simpletest.rc-zblzg" in namespace "gc-3075"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 12:37:28.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3075" for this suite. 02/12/23 12:37:28.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:37:28.605
Feb 12 12:37:28.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename subpath 02/12/23 12:37:28.606
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:37:28.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:37:28.753
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/12/23 12:37:28.767
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-tc5k 02/12/23 12:37:28.813
STEP: Creating a pod to test atomic-volume-subpath 02/12/23 12:37:28.813
Feb 12 12:37:28.840: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tc5k" in namespace "subpath-4582" to be "Succeeded or Failed"
Feb 12 12:37:28.896: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 56.490697ms
Feb 12 12:37:31.198: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358051079s
Feb 12 12:37:33.070: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230104813s
Feb 12 12:37:34.902: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061830705s
Feb 12 12:37:36.903: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 8.063227211s
Feb 12 12:37:38.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 10.061055089s
Feb 12 12:37:40.908: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 12.067955132s
Feb 12 12:37:43.200: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 14.359755904s
Feb 12 12:37:44.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 16.061254948s
Feb 12 12:37:46.910: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 18.07001247s
Feb 12 12:37:48.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 20.060640273s
Feb 12 12:37:50.913: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 22.072823567s
Feb 12 12:37:52.907: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 24.06719352s
Feb 12 12:37:54.906: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 26.065808822s
Feb 12 12:37:56.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=false. Elapsed: 28.060922689s
Feb 12 12:37:58.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.060973617s
STEP: Saw pod success 02/12/23 12:37:58.901
Feb 12 12:37:58.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k" satisfied condition "Succeeded or Failed"
Feb 12 12:37:58.904: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-downwardapi-tc5k container test-container-subpath-downwardapi-tc5k: <nil>
STEP: delete the pod 02/12/23 12:37:58.918
Feb 12 12:37:58.933: INFO: Waiting for pod pod-subpath-test-downwardapi-tc5k to disappear
Feb 12 12:37:58.937: INFO: Pod pod-subpath-test-downwardapi-tc5k no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tc5k 02/12/23 12:37:58.937
Feb 12 12:37:58.937: INFO: Deleting pod "pod-subpath-test-downwardapi-tc5k" in namespace "subpath-4582"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Feb 12 12:37:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4582" for this suite. 02/12/23 12:37:58.946
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":294,"skipped":5341,"failed":0}
------------------------------
 [SLOW TEST] [30.348 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:37:28.605
    Feb 12 12:37:28.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename subpath 02/12/23 12:37:28.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:37:28.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:37:28.753
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/12/23 12:37:28.767
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-tc5k 02/12/23 12:37:28.813
    STEP: Creating a pod to test atomic-volume-subpath 02/12/23 12:37:28.813
    Feb 12 12:37:28.840: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tc5k" in namespace "subpath-4582" to be "Succeeded or Failed"
    Feb 12 12:37:28.896: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 56.490697ms
    Feb 12 12:37:31.198: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.358051079s
    Feb 12 12:37:33.070: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.230104813s
    Feb 12 12:37:34.902: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061830705s
    Feb 12 12:37:36.903: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 8.063227211s
    Feb 12 12:37:38.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 10.061055089s
    Feb 12 12:37:40.908: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 12.067955132s
    Feb 12 12:37:43.200: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 14.359755904s
    Feb 12 12:37:44.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 16.061254948s
    Feb 12 12:37:46.910: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 18.07001247s
    Feb 12 12:37:48.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 20.060640273s
    Feb 12 12:37:50.913: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 22.072823567s
    Feb 12 12:37:52.907: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 24.06719352s
    Feb 12 12:37:54.906: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=true. Elapsed: 26.065808822s
    Feb 12 12:37:56.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Running", Reason="", readiness=false. Elapsed: 28.060922689s
    Feb 12 12:37:58.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.060973617s
    STEP: Saw pod success 02/12/23 12:37:58.901
    Feb 12 12:37:58.901: INFO: Pod "pod-subpath-test-downwardapi-tc5k" satisfied condition "Succeeded or Failed"
    Feb 12 12:37:58.904: INFO: Trying to get logs from node kube-3 pod pod-subpath-test-downwardapi-tc5k container test-container-subpath-downwardapi-tc5k: <nil>
    STEP: delete the pod 02/12/23 12:37:58.918
    Feb 12 12:37:58.933: INFO: Waiting for pod pod-subpath-test-downwardapi-tc5k to disappear
    Feb 12 12:37:58.937: INFO: Pod pod-subpath-test-downwardapi-tc5k no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-tc5k 02/12/23 12:37:58.937
    Feb 12 12:37:58.937: INFO: Deleting pod "pod-subpath-test-downwardapi-tc5k" in namespace "subpath-4582"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Feb 12 12:37:58.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4582" for this suite. 02/12/23 12:37:58.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:37:58.953
Feb 12 12:37:58.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename disruption 02/12/23 12:37:58.954
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:37:58.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:37:58.975
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 02/12/23 12:37:58.981
STEP: Updating PodDisruptionBudget status 02/12/23 12:38:00.989
STEP: Waiting for all pods to be running 02/12/23 12:38:01
Feb 12 12:38:01.010: INFO: running pods: 0 < 1
STEP: locating a running pod 02/12/23 12:38:03.016
STEP: Waiting for the pdb to be processed 02/12/23 12:38:03.027
STEP: Patching PodDisruptionBudget status 02/12/23 12:38:03.039
STEP: Waiting for the pdb to be processed 02/12/23 12:38:03.05
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Feb 12 12:38:03.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4681" for this suite. 02/12/23 12:38:03.056
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":295,"skipped":5351,"failed":0}
------------------------------
 [4.112 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:37:58.953
    Feb 12 12:37:58.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename disruption 02/12/23 12:37:58.954
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:37:58.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:37:58.975
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 02/12/23 12:37:58.981
    STEP: Updating PodDisruptionBudget status 02/12/23 12:38:00.989
    STEP: Waiting for all pods to be running 02/12/23 12:38:01
    Feb 12 12:38:01.010: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/12/23 12:38:03.016
    STEP: Waiting for the pdb to be processed 02/12/23 12:38:03.027
    STEP: Patching PodDisruptionBudget status 02/12/23 12:38:03.039
    STEP: Waiting for the pdb to be processed 02/12/23 12:38:03.05
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Feb 12 12:38:03.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4681" for this suite. 02/12/23 12:38:03.056
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:38:03.066
Feb 12 12:38:03.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-runtime 02/12/23 12:38:03.067
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:38:03.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:38:03.092
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 02/12/23 12:38:03.094
STEP: wait for the container to reach Succeeded 02/12/23 12:38:03.101
STEP: get the container status 02/12/23 12:38:07.156
STEP: the container should be terminated 02/12/23 12:38:07.167
STEP: the termination message should be set 02/12/23 12:38:07.167
Feb 12 12:38:07.168: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/12/23 12:38:07.168
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 12 12:38:07.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3" for this suite. 02/12/23 12:38:07.212
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":296,"skipped":5361,"failed":0}
------------------------------
 [4.153 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:38:03.066
    Feb 12 12:38:03.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-runtime 02/12/23 12:38:03.067
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:38:03.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:38:03.092
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 02/12/23 12:38:03.094
    STEP: wait for the container to reach Succeeded 02/12/23 12:38:03.101
    STEP: get the container status 02/12/23 12:38:07.156
    STEP: the container should be terminated 02/12/23 12:38:07.167
    STEP: the termination message should be set 02/12/23 12:38:07.167
    Feb 12 12:38:07.168: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/12/23 12:38:07.168
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 12 12:38:07.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3" for this suite. 02/12/23 12:38:07.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:38:07.22
Feb 12 12:38:07.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename statefulset 02/12/23 12:38:07.22
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:38:07.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:38:07.238
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7951 02/12/23 12:38:07.24
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7951 02/12/23 12:38:07.247
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7951 02/12/23 12:38:07.255
Feb 12 12:38:07.263: INFO: Found 0 stateful pods, waiting for 1
Feb 12 12:38:17.278: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/12/23 12:38:17.278
Feb 12 12:38:17.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 12:38:17.604: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 12:38:17.604: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 12:38:17.604: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 12:38:17.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 12 12:38:27.628: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 12:38:27.628: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 12:38:27.651: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Feb 12 12:38:27.651: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  }]
Feb 12 12:38:27.652: INFO: 
Feb 12 12:38:27.652: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 12 12:38:28.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995870473s
Feb 12 12:38:29.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992034691s
Feb 12 12:38:30.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980348766s
Feb 12 12:38:31.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969767856s
Feb 12 12:38:32.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962540934s
Feb 12 12:38:33.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95052123s
Feb 12 12:38:34.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.935537622s
Feb 12 12:38:35.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.922934185s
Feb 12 12:38:36.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.979976ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7951 02/12/23 12:38:37.741
Feb 12 12:38:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 12:38:37.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 12 12:38:37.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 12:38:37.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 12:38:37.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 12:38:37.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 12 12:38:37.970: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 12:38:37.970: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 12:38:37.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 12 12:38:38.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 12 12:38:38.108: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 12 12:38:38.108: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 12 12:38:38.120: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 12 12:38:48.134: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:38:48.134: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 12 12:38:48.134: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/12/23 12:38:48.134
Feb 12 12:38:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 12:38:48.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 12:38:48.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 12:38:48.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 12:38:48.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 12:38:48.403: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 12:38:48.403: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 12:38:48.403: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 12:38:48.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 12 12:38:48.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 12 12:38:48.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 12 12:38:48.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 12 12:38:48.542: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 12:38:48.546: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 12 12:38:58.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 12:38:58.555: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 12:38:58.555: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 12 12:38:58.573: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Feb 12 12:38:58.573: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  }]
Feb 12 12:38:58.573: INFO: ss-1  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
Feb 12 12:38:58.573: INFO: ss-2  kube-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
Feb 12 12:38:58.574: INFO: 
Feb 12 12:38:58.574: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 12 12:38:59.581: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Feb 12 12:38:59.581: INFO: ss-0  kube-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  }]
Feb 12 12:38:59.582: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
Feb 12 12:38:59.582: INFO: ss-2  kube-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
Feb 12 12:38:59.582: INFO: 
Feb 12 12:38:59.582: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 12 12:39:00.593: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987298478s
Feb 12 12:39:01.608: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975920798s
Feb 12 12:39:02.612: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961542989s
Feb 12 12:39:03.624: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.95727313s
Feb 12 12:39:04.628: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.944871202s
Feb 12 12:39:05.642: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.940908587s
Feb 12 12:39:06.657: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.92696202s
Feb 12 12:39:07.660: INFO: Verifying statefulset ss doesn't scale past 0 for another 912.618185ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7951 02/12/23 12:39:08.662
Feb 12 12:39:08.674: INFO: Scaling statefulset ss to 0
Feb 12 12:39:08.703: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Feb 12 12:39:08.709: INFO: Deleting all statefulset in ns statefulset-7951
Feb 12 12:39:08.715: INFO: Scaling statefulset ss to 0
Feb 12 12:39:08.732: INFO: Waiting for statefulset status.replicas updated to 0
Feb 12 12:39:08.739: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Feb 12 12:39:08.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7951" for this suite. 02/12/23 12:39:08.764
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":297,"skipped":5366,"failed":0}
------------------------------
 [SLOW TEST] [61.556 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:38:07.22
    Feb 12 12:38:07.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename statefulset 02/12/23 12:38:07.22
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:38:07.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:38:07.238
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7951 02/12/23 12:38:07.24
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7951 02/12/23 12:38:07.247
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7951 02/12/23 12:38:07.255
    Feb 12 12:38:07.263: INFO: Found 0 stateful pods, waiting for 1
    Feb 12 12:38:17.278: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/12/23 12:38:17.278
    Feb 12 12:38:17.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 12:38:17.604: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 12:38:17.604: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 12:38:17.604: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 12:38:17.618: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 12 12:38:27.628: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 12:38:27.628: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 12:38:27.651: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Feb 12 12:38:27.651: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  }]
    Feb 12 12:38:27.652: INFO: 
    Feb 12 12:38:27.652: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb 12 12:38:28.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995870473s
    Feb 12 12:38:29.666: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992034691s
    Feb 12 12:38:30.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980348766s
    Feb 12 12:38:31.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969767856s
    Feb 12 12:38:32.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962540934s
    Feb 12 12:38:33.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95052123s
    Feb 12 12:38:34.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.935537622s
    Feb 12 12:38:35.735: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.922934185s
    Feb 12 12:38:36.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.979976ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7951 02/12/23 12:38:37.741
    Feb 12 12:38:37.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 12:38:37.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 12 12:38:37.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 12:38:37.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 12:38:37.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 12:38:37.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 12 12:38:37.970: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 12:38:37.970: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 12:38:37.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 12 12:38:38.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 12 12:38:38.108: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 12 12:38:38.108: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 12 12:38:38.120: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Feb 12 12:38:48.134: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:38:48.134: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 12 12:38:48.134: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/12/23 12:38:48.134
    Feb 12 12:38:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 12:38:48.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 12:38:48.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 12:38:48.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 12:38:48.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 12:38:48.403: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 12:38:48.403: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 12:38:48.403: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 12:38:48.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=statefulset-7951 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 12 12:38:48.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 12 12:38:48.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 12 12:38:48.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 12 12:38:48.542: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 12:38:48.546: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 12 12:38:58.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 12:38:58.555: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 12:38:58.555: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 12 12:38:58.573: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Feb 12 12:38:58.573: INFO: ss-0  kube-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  }]
    Feb 12 12:38:58.573: INFO: ss-1  kube-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
    Feb 12 12:38:58.573: INFO: ss-2  kube-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
    Feb 12 12:38:58.574: INFO: 
    Feb 12 12:38:58.574: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 12 12:38:59.581: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Feb 12 12:38:59.581: INFO: ss-0  kube-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:07 +0000 UTC  }]
    Feb 12 12:38:59.582: INFO: ss-1  kube-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
    Feb 12 12:38:59.582: INFO: ss-2  kube-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-12 12:38:27 +0000 UTC  }]
    Feb 12 12:38:59.582: INFO: 
    Feb 12 12:38:59.582: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 12 12:39:00.593: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.987298478s
    Feb 12 12:39:01.608: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.975920798s
    Feb 12 12:39:02.612: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961542989s
    Feb 12 12:39:03.624: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.95727313s
    Feb 12 12:39:04.628: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.944871202s
    Feb 12 12:39:05.642: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.940908587s
    Feb 12 12:39:06.657: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.92696202s
    Feb 12 12:39:07.660: INFO: Verifying statefulset ss doesn't scale past 0 for another 912.618185ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7951 02/12/23 12:39:08.662
    Feb 12 12:39:08.674: INFO: Scaling statefulset ss to 0
    Feb 12 12:39:08.703: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Feb 12 12:39:08.709: INFO: Deleting all statefulset in ns statefulset-7951
    Feb 12 12:39:08.715: INFO: Scaling statefulset ss to 0
    Feb 12 12:39:08.732: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 12 12:39:08.739: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Feb 12 12:39:08.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7951" for this suite. 02/12/23 12:39:08.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:08.78
Feb 12 12:39:08.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:39:08.781
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:08.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:08.799
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 02/12/23 12:39:08.802
Feb 12 12:39:08.811: INFO: Waiting up to 5m0s for pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d" in namespace "emptydir-1243" to be "Succeeded or Failed"
Feb 12 12:39:08.819: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.751164ms
Feb 12 12:39:10.831: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020759488s
Feb 12 12:39:12.833: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022264229s
STEP: Saw pod success 02/12/23 12:39:12.833
Feb 12 12:39:12.834: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d" satisfied condition "Succeeded or Failed"
Feb 12 12:39:12.846: INFO: Trying to get logs from node kube-3 pod pod-0bfeb063-da2d-49ac-a948-3cb24899599d container test-container: <nil>
STEP: delete the pod 02/12/23 12:39:12.873
Feb 12 12:39:12.906: INFO: Waiting for pod pod-0bfeb063-da2d-49ac-a948-3cb24899599d to disappear
Feb 12 12:39:12.909: INFO: Pod pod-0bfeb063-da2d-49ac-a948-3cb24899599d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:39:12.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1243" for this suite. 02/12/23 12:39:12.914
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":298,"skipped":5406,"failed":0}
------------------------------
 [4.151 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:08.78
    Feb 12 12:39:08.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:39:08.781
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:08.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:08.799
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/12/23 12:39:08.802
    Feb 12 12:39:08.811: INFO: Waiting up to 5m0s for pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d" in namespace "emptydir-1243" to be "Succeeded or Failed"
    Feb 12 12:39:08.819: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.751164ms
    Feb 12 12:39:10.831: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020759488s
    Feb 12 12:39:12.833: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022264229s
    STEP: Saw pod success 02/12/23 12:39:12.833
    Feb 12 12:39:12.834: INFO: Pod "pod-0bfeb063-da2d-49ac-a948-3cb24899599d" satisfied condition "Succeeded or Failed"
    Feb 12 12:39:12.846: INFO: Trying to get logs from node kube-3 pod pod-0bfeb063-da2d-49ac-a948-3cb24899599d container test-container: <nil>
    STEP: delete the pod 02/12/23 12:39:12.873
    Feb 12 12:39:12.906: INFO: Waiting for pod pod-0bfeb063-da2d-49ac-a948-3cb24899599d to disappear
    Feb 12 12:39:12.909: INFO: Pod pod-0bfeb063-da2d-49ac-a948-3cb24899599d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:39:12.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1243" for this suite. 02/12/23 12:39:12.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:12.932
Feb 12 12:39:12.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:39:12.933
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:12.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:12.958
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:39:12.984
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:13.406
STEP: Deploying the webhook pod 02/12/23 12:39:13.414
STEP: Wait for the deployment to be ready 02/12/23 12:39:13.427
Feb 12 12:39:13.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:39:15.467
STEP: Verifying the service has paired with the endpoint 02/12/23 12:39:15.495
Feb 12 12:39:16.496: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 02/12/23 12:39:16.511
STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 12:39:16.535
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/12/23 12:39:16.543
STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 12:39:16.557
STEP: Patching a validating webhook configuration's rules to include the create operation 02/12/23 12:39:16.579
STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 12:39:16.587
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:39:16.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1400" for this suite. 02/12/23 12:39:16.603
STEP: Destroying namespace "webhook-1400-markers" for this suite. 02/12/23 12:39:16.612
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":299,"skipped":5421,"failed":0}
------------------------------
 [3.766 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:12.932
    Feb 12 12:39:12.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:39:12.933
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:12.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:12.958
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:39:12.984
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:13.406
    STEP: Deploying the webhook pod 02/12/23 12:39:13.414
    STEP: Wait for the deployment to be ready 02/12/23 12:39:13.427
    Feb 12 12:39:13.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:39:15.467
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:39:15.495
    Feb 12 12:39:16.496: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 02/12/23 12:39:16.511
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 12:39:16.535
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/12/23 12:39:16.543
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 12:39:16.557
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/12/23 12:39:16.579
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/12/23 12:39:16.587
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:16.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1400" for this suite. 02/12/23 12:39:16.603
    STEP: Destroying namespace "webhook-1400-markers" for this suite. 02/12/23 12:39:16.612
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:16.698
Feb 12 12:39:16.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:39:16.7
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:16.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:16.733
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:39:16.777
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:17.113
STEP: Deploying the webhook pod 02/12/23 12:39:17.118
STEP: Wait for the deployment to be ready 02/12/23 12:39:17.13
Feb 12 12:39:17.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:39:19.186
STEP: Verifying the service has paired with the endpoint 02/12/23 12:39:19.209
Feb 12 12:39:20.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/12/23 12:39:20.223
STEP: create a pod that should be updated by the webhook 02/12/23 12:39:20.248
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:39:20.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-658" for this suite. 02/12/23 12:39:20.282
STEP: Destroying namespace "webhook-658-markers" for this suite. 02/12/23 12:39:20.291
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":300,"skipped":5429,"failed":0}
------------------------------
 [3.666 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:16.698
    Feb 12 12:39:16.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:39:16.7
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:16.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:16.733
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:39:16.777
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:17.113
    STEP: Deploying the webhook pod 02/12/23 12:39:17.118
    STEP: Wait for the deployment to be ready 02/12/23 12:39:17.13
    Feb 12 12:39:17.143: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:39:19.186
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:39:19.209
    Feb 12 12:39:20.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/12/23 12:39:20.223
    STEP: create a pod that should be updated by the webhook 02/12/23 12:39:20.248
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:20.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-658" for this suite. 02/12/23 12:39:20.282
    STEP: Destroying namespace "webhook-658-markers" for this suite. 02/12/23 12:39:20.291
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:20.365
Feb 12 12:39:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-runtime 02/12/23 12:39:20.366
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:20.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:20.401
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 02/12/23 12:39:20.406
STEP: wait for the container to reach Failed 02/12/23 12:39:20.418
STEP: get the container status 02/12/23 12:39:26.859
STEP: the container should be terminated 02/12/23 12:39:26.87
STEP: the termination message should be set 02/12/23 12:39:26.87
Feb 12 12:39:26.871: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/12/23 12:39:26.871
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 12 12:39:26.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8643" for this suite. 02/12/23 12:39:26.961
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":301,"skipped":5431,"failed":0}
------------------------------
 [SLOW TEST] [6.611 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:20.365
    Feb 12 12:39:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-runtime 02/12/23 12:39:20.366
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:20.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:20.401
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 02/12/23 12:39:20.406
    STEP: wait for the container to reach Failed 02/12/23 12:39:20.418
    STEP: get the container status 02/12/23 12:39:26.859
    STEP: the container should be terminated 02/12/23 12:39:26.87
    STEP: the termination message should be set 02/12/23 12:39:26.87
    Feb 12 12:39:26.871: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/12/23 12:39:26.871
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 12 12:39:26.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8643" for this suite. 02/12/23 12:39:26.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:26.982
Feb 12 12:39:26.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:39:26.982
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:27.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:27.034
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:39:27.096
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:27.954
STEP: Deploying the webhook pod 02/12/23 12:39:27.962
STEP: Wait for the deployment to be ready 02/12/23 12:39:27.977
Feb 12 12:39:27.988: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:39:30.023
STEP: Verifying the service has paired with the endpoint 02/12/23 12:39:30.049
Feb 12 12:39:31.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 02/12/23 12:39:31.059
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/12/23 12:39:31.088
STEP: Creating a configMap that should not be mutated 02/12/23 12:39:31.098
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/12/23 12:39:31.11
STEP: Creating a configMap that should be mutated 02/12/23 12:39:31.118
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:39:31.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7619" for this suite. 02/12/23 12:39:31.145
STEP: Destroying namespace "webhook-7619-markers" for this suite. 02/12/23 12:39:31.15
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":302,"skipped":5450,"failed":0}
------------------------------
 [4.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:26.982
    Feb 12 12:39:26.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:39:26.982
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:27.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:27.034
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:39:27.096
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:27.954
    STEP: Deploying the webhook pod 02/12/23 12:39:27.962
    STEP: Wait for the deployment to be ready 02/12/23 12:39:27.977
    Feb 12 12:39:27.988: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:39:30.023
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:39:30.049
    Feb 12 12:39:31.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 02/12/23 12:39:31.059
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/12/23 12:39:31.088
    STEP: Creating a configMap that should not be mutated 02/12/23 12:39:31.098
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/12/23 12:39:31.11
    STEP: Creating a configMap that should be mutated 02/12/23 12:39:31.118
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:31.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7619" for this suite. 02/12/23 12:39:31.145
    STEP: Destroying namespace "webhook-7619-markers" for this suite. 02/12/23 12:39:31.15
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:31.226
Feb 12 12:39:31.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename namespaces 02/12/23 12:39:31.227
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:31.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:31.27
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 02/12/23 12:39:31.275
STEP: patching the Namespace 02/12/23 12:39:31.299
STEP: get the Namespace and ensuring it has the label 02/12/23 12:39:31.311
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:39:31.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7473" for this suite. 02/12/23 12:39:31.319
STEP: Destroying namespace "nspatchtest-dc374497-8428-44e6-8888-a9610744abff-2944" for this suite. 02/12/23 12:39:31.329
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":303,"skipped":5461,"failed":0}
------------------------------
 [0.113 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:31.226
    Feb 12 12:39:31.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename namespaces 02/12/23 12:39:31.227
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:31.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:31.27
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 02/12/23 12:39:31.275
    STEP: patching the Namespace 02/12/23 12:39:31.299
    STEP: get the Namespace and ensuring it has the label 02/12/23 12:39:31.311
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:31.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7473" for this suite. 02/12/23 12:39:31.319
    STEP: Destroying namespace "nspatchtest-dc374497-8428-44e6-8888-a9610744abff-2944" for this suite. 02/12/23 12:39:31.329
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:31.339
Feb 12 12:39:31.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename podtemplate 02/12/23 12:39:31.34
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:31.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:31.369
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/12/23 12:39:31.372
Feb 12 12:39:31.379: INFO: created test-podtemplate-1
Feb 12 12:39:31.386: INFO: created test-podtemplate-2
Feb 12 12:39:31.392: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/12/23 12:39:31.392
STEP: delete collection of pod templates 02/12/23 12:39:31.395
Feb 12 12:39:31.396: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/12/23 12:39:31.42
Feb 12 12:39:31.420: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Feb 12 12:39:31.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9311" for this suite. 02/12/23 12:39:31.429
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":304,"skipped":5462,"failed":0}
------------------------------
 [0.097 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:31.339
    Feb 12 12:39:31.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename podtemplate 02/12/23 12:39:31.34
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:31.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:31.369
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/12/23 12:39:31.372
    Feb 12 12:39:31.379: INFO: created test-podtemplate-1
    Feb 12 12:39:31.386: INFO: created test-podtemplate-2
    Feb 12 12:39:31.392: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/12/23 12:39:31.392
    STEP: delete collection of pod templates 02/12/23 12:39:31.395
    Feb 12 12:39:31.396: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/12/23 12:39:31.42
    Feb 12 12:39:31.420: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Feb 12 12:39:31.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9311" for this suite. 02/12/23 12:39:31.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:31.44
Feb 12 12:39:31.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:39:31.441
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:31.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:31.461
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-ff25d0c4-c62f-45e8-885f-2c8b84e0e70d 02/12/23 12:39:31.466
STEP: Creating the pod 02/12/23 12:39:31.471
Feb 12 12:39:31.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443" in namespace "configmap-386" to be "running"
Feb 12 12:39:31.486: INFO: Pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271522ms
Feb 12 12:39:33.499: INFO: Pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443": Phase="Running", Reason="", readiness=false. Elapsed: 2.018586001s
Feb 12 12:39:33.500: INFO: Pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443" satisfied condition "running"
STEP: Waiting for pod with text data 02/12/23 12:39:33.5
STEP: Waiting for pod with binary data 02/12/23 12:39:33.522
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:39:33.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-386" for this suite. 02/12/23 12:39:33.554
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":305,"skipped":5501,"failed":0}
------------------------------
 [2.130 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:31.44
    Feb 12 12:39:31.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:39:31.441
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:31.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:31.461
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-ff25d0c4-c62f-45e8-885f-2c8b84e0e70d 02/12/23 12:39:31.466
    STEP: Creating the pod 02/12/23 12:39:31.471
    Feb 12 12:39:31.480: INFO: Waiting up to 5m0s for pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443" in namespace "configmap-386" to be "running"
    Feb 12 12:39:31.486: INFO: Pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271522ms
    Feb 12 12:39:33.499: INFO: Pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443": Phase="Running", Reason="", readiness=false. Elapsed: 2.018586001s
    Feb 12 12:39:33.500: INFO: Pod "pod-configmaps-02d6ac90-7ed3-441e-854d-10fe446d2443" satisfied condition "running"
    STEP: Waiting for pod with text data 02/12/23 12:39:33.5
    STEP: Waiting for pod with binary data 02/12/23 12:39:33.522
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:39:33.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-386" for this suite. 02/12/23 12:39:33.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:33.571
Feb 12 12:39:33.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 12:39:33.573
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:33.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:33.593
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Feb 12 12:39:33.612: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 12:39:33.618
Feb 12 12:39:33.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 12:39:33.627: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 12:39:34.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 12:39:34.636: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 12:39:35.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 12:39:35.658: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 02/12/23 12:39:35.704
STEP: Check that daemon pods images are updated. 02/12/23 12:39:35.726
Feb 12 12:39:35.730: INFO: Wrong image for pod: daemon-set-8rc97. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:35.731: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:35.731: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:36.906: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:36.906: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:40.245: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:40.245: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:40.814: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:40.814: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:41.784: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:41.784: INFO: Pod daemon-set-ls5vc is not available
Feb 12 12:39:41.784: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:42.755: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:42.755: INFO: Pod daemon-set-ls5vc is not available
Feb 12 12:39:42.756: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:43.752: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:44.741: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Feb 12 12:39:44.741: INFO: Pod daemon-set-t2pxh is not available
Feb 12 12:39:46.743: INFO: Pod daemon-set-ndndx is not available
STEP: Check that daemon pods are still running on every node of the cluster. 02/12/23 12:39:46.748
Feb 12 12:39:46.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 12:39:46.756: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 12:39:47.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 12:39:47.782: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/12/23 12:39:47.828
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5827, will wait for the garbage collector to delete the pods 02/12/23 12:39:47.829
Feb 12 12:39:47.899: INFO: Deleting DaemonSet.extensions daemon-set took: 14.851583ms
Feb 12 12:39:48.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.014265ms
Feb 12 12:39:50.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 12:39:50.106: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 12 12:39:50.109: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37024"},"items":null}

Feb 12 12:39:50.111: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37024"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:39:50.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5827" for this suite. 02/12/23 12:39:50.127
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":306,"skipped":5509,"failed":0}
------------------------------
 [SLOW TEST] [16.562 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:33.571
    Feb 12 12:39:33.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 12:39:33.573
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:33.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:33.593
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Feb 12 12:39:33.612: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 12:39:33.618
    Feb 12 12:39:33.627: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 12:39:33.627: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 12:39:34.635: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 12:39:34.636: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 12:39:35.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 12:39:35.658: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 02/12/23 12:39:35.704
    STEP: Check that daemon pods images are updated. 02/12/23 12:39:35.726
    Feb 12 12:39:35.730: INFO: Wrong image for pod: daemon-set-8rc97. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:35.731: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:35.731: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:36.906: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:36.906: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:40.245: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:40.245: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:40.814: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:40.814: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:41.784: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:41.784: INFO: Pod daemon-set-ls5vc is not available
    Feb 12 12:39:41.784: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:42.755: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:42.755: INFO: Pod daemon-set-ls5vc is not available
    Feb 12 12:39:42.756: INFO: Wrong image for pod: daemon-set-n522v. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:43.752: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:44.741: INFO: Wrong image for pod: daemon-set-bwgbv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Feb 12 12:39:44.741: INFO: Pod daemon-set-t2pxh is not available
    Feb 12 12:39:46.743: INFO: Pod daemon-set-ndndx is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 02/12/23 12:39:46.748
    Feb 12 12:39:46.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 12:39:46.756: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 12:39:47.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 12:39:47.782: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/12/23 12:39:47.828
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5827, will wait for the garbage collector to delete the pods 02/12/23 12:39:47.829
    Feb 12 12:39:47.899: INFO: Deleting DaemonSet.extensions daemon-set took: 14.851583ms
    Feb 12 12:39:48.000: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.014265ms
    Feb 12 12:39:50.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 12:39:50.106: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 12 12:39:50.109: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37024"},"items":null}

    Feb 12 12:39:50.111: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37024"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:50.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5827" for this suite. 02/12/23 12:39:50.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:50.139
Feb 12 12:39:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename namespaces 02/12/23 12:39:50.139
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:50.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:50.165
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 02/12/23 12:39:50.167
Feb 12 12:39:50.175: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/12/23 12:39:50.175
Feb 12 12:39:50.182: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/12/23 12:39:50.182
Feb 12 12:39:50.192: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:39:50.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3587" for this suite. 02/12/23 12:39:50.196
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":307,"skipped":5604,"failed":0}
------------------------------
 [0.064 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:50.139
    Feb 12 12:39:50.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename namespaces 02/12/23 12:39:50.139
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:50.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:50.165
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 02/12/23 12:39:50.167
    Feb 12 12:39:50.175: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/12/23 12:39:50.175
    Feb 12 12:39:50.182: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/12/23 12:39:50.182
    Feb 12 12:39:50.192: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:50.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3587" for this suite. 02/12/23 12:39:50.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:50.205
Feb 12 12:39:50.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename daemonsets 02/12/23 12:39:50.206
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:50.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:50.228
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 02/12/23 12:39:50.248
STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 12:39:50.255
Feb 12 12:39:50.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 12:39:50.262: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 12:39:51.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 12 12:39:51.270: INFO: Node kube-1 is running 0 daemon pod, expected 1
Feb 12 12:39:52.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 12:39:52.284: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/12/23 12:39:52.29
Feb 12 12:39:52.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 12:39:52.320: INFO: Node kube-3 is running 0 daemon pod, expected 1
Feb 12 12:39:53.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 12:39:53.331: INFO: Node kube-3 is running 0 daemon pod, expected 1
Feb 12 12:39:54.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 12 12:39:54.328: INFO: Node kube-3 is running 0 daemon pod, expected 1
Feb 12 12:39:55.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 12 12:39:55.332: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 02/12/23 12:39:55.337
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3622, will wait for the garbage collector to delete the pods 02/12/23 12:39:55.337
Feb 12 12:39:55.405: INFO: Deleting DaemonSet.extensions daemon-set took: 11.109647ms
Feb 12 12:39:55.507: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.123029ms
Feb 12 12:39:59.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 12 12:39:59.310: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 12 12:39:59.313: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37195"},"items":null}

Feb 12 12:39:59.316: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37195"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:39:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3622" for this suite. 02/12/23 12:39:59.333
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":308,"skipped":5642,"failed":0}
------------------------------
 [SLOW TEST] [9.137 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:50.205
    Feb 12 12:39:50.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename daemonsets 02/12/23 12:39:50.206
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:50.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:50.228
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 02/12/23 12:39:50.248
    STEP: Check that daemon pods launch on every node of the cluster. 02/12/23 12:39:50.255
    Feb 12 12:39:50.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 12:39:50.262: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 12:39:51.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 12 12:39:51.270: INFO: Node kube-1 is running 0 daemon pod, expected 1
    Feb 12 12:39:52.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 12:39:52.284: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/12/23 12:39:52.29
    Feb 12 12:39:52.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 12:39:52.320: INFO: Node kube-3 is running 0 daemon pod, expected 1
    Feb 12 12:39:53.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 12:39:53.331: INFO: Node kube-3 is running 0 daemon pod, expected 1
    Feb 12 12:39:54.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 12 12:39:54.328: INFO: Node kube-3 is running 0 daemon pod, expected 1
    Feb 12 12:39:55.332: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 12 12:39:55.332: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 02/12/23 12:39:55.337
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3622, will wait for the garbage collector to delete the pods 02/12/23 12:39:55.337
    Feb 12 12:39:55.405: INFO: Deleting DaemonSet.extensions daemon-set took: 11.109647ms
    Feb 12 12:39:55.507: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.123029ms
    Feb 12 12:39:59.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 12 12:39:59.310: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 12 12:39:59.313: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37195"},"items":null}

    Feb 12 12:39:59.316: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37195"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:39:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3622" for this suite. 02/12/23 12:39:59.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:39:59.343
Feb 12 12:39:59.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:39:59.344
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:59.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:59.417
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:39:59.436
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:59.837
STEP: Deploying the webhook pod 02/12/23 12:39:59.845
STEP: Wait for the deployment to be ready 02/12/23 12:39:59.862
Feb 12 12:39:59.868: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/12/23 12:40:01.919
STEP: Verifying the service has paired with the endpoint 02/12/23 12:40:01.935
Feb 12 12:40:02.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 02/12/23 12:40:02.942
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/12/23 12:40:02.943
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/12/23 12:40:02.943
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/12/23 12:40:02.943
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/12/23 12:40:02.944
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/12/23 12:40:02.944
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/12/23 12:40:02.945
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:40:02.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4921" for this suite. 02/12/23 12:40:02.95
STEP: Destroying namespace "webhook-4921-markers" for this suite. 02/12/23 12:40:02.961
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":309,"skipped":5656,"failed":0}
------------------------------
 [3.730 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:39:59.343
    Feb 12 12:39:59.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:39:59.344
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:39:59.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:39:59.417
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:39:59.436
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:39:59.837
    STEP: Deploying the webhook pod 02/12/23 12:39:59.845
    STEP: Wait for the deployment to be ready 02/12/23 12:39:59.862
    Feb 12 12:39:59.868: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/12/23 12:40:01.919
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:40:01.935
    Feb 12 12:40:02.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 02/12/23 12:40:02.942
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/12/23 12:40:02.943
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/12/23 12:40:02.943
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/12/23 12:40:02.943
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/12/23 12:40:02.944
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/12/23 12:40:02.944
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/12/23 12:40:02.945
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:40:02.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4921" for this suite. 02/12/23 12:40:02.95
    STEP: Destroying namespace "webhook-4921-markers" for this suite. 02/12/23 12:40:02.961
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:03.074
Feb 12 12:40:03.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 12:40:03.075
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:03.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:03.123
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 02/12/23 12:40:03.128
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/12/23 12:40:03.136
STEP: patching the secret 02/12/23 12:40:03.142
STEP: deleting the secret using a LabelSelector 02/12/23 12:40:03.157
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/12/23 12:40:03.184
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Feb 12 12:40:03.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8131" for this suite. 02/12/23 12:40:03.198
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":310,"skipped":5668,"failed":0}
------------------------------
 [0.135 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:03.074
    Feb 12 12:40:03.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 12:40:03.075
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:03.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:03.123
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 02/12/23 12:40:03.128
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/12/23 12:40:03.136
    STEP: patching the secret 02/12/23 12:40:03.142
    STEP: deleting the secret using a LabelSelector 02/12/23 12:40:03.157
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/12/23 12:40:03.184
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 12:40:03.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8131" for this suite. 02/12/23 12:40:03.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:03.212
Feb 12 12:40:03.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:40:03.212
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:03.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:03.247
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:40:03.25
Feb 12 12:40:03.263: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8" in namespace "projected-8923" to be "Succeeded or Failed"
Feb 12 12:40:03.277: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.556465ms
Feb 12 12:40:05.628: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.364548084s
Feb 12 12:40:08.587: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.323857388s
Feb 12 12:40:09.282: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018357029s
Feb 12 12:40:11.282: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018421451s
STEP: Saw pod success 02/12/23 12:40:11.282
Feb 12 12:40:11.282: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8" satisfied condition "Succeeded or Failed"
Feb 12 12:40:11.287: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8 container client-container: <nil>
STEP: delete the pod 02/12/23 12:40:11.293
Feb 12 12:40:11.310: INFO: Waiting for pod downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8 to disappear
Feb 12 12:40:11.313: INFO: Pod downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 12:40:11.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8923" for this suite. 02/12/23 12:40:11.318
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":311,"skipped":5739,"failed":0}
------------------------------
 [SLOW TEST] [8.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:03.212
    Feb 12 12:40:03.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:40:03.212
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:03.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:03.247
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:40:03.25
    Feb 12 12:40:03.263: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8" in namespace "projected-8923" to be "Succeeded or Failed"
    Feb 12 12:40:03.277: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.556465ms
    Feb 12 12:40:05.628: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.364548084s
    Feb 12 12:40:08.587: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.323857388s
    Feb 12 12:40:09.282: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018357029s
    Feb 12 12:40:11.282: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018421451s
    STEP: Saw pod success 02/12/23 12:40:11.282
    Feb 12 12:40:11.282: INFO: Pod "downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8" satisfied condition "Succeeded or Failed"
    Feb 12 12:40:11.287: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8 container client-container: <nil>
    STEP: delete the pod 02/12/23 12:40:11.293
    Feb 12 12:40:11.310: INFO: Waiting for pod downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8 to disappear
    Feb 12 12:40:11.313: INFO: Pod downwardapi-volume-a8bf63ff-3f00-483c-b0fc-d5e74d7570d8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 12:40:11.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8923" for this suite. 02/12/23 12:40:11.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:11.326
Feb 12 12:40:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:40:11.327
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:11.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:11.346
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:40:11.348
Feb 12 12:40:11.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558" in namespace "downward-api-1735" to be "Succeeded or Failed"
Feb 12 12:40:11.366: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558": Phase="Pending", Reason="", readiness=false. Elapsed: 5.110574ms
Feb 12 12:40:13.381: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020127375s
Feb 12 12:40:15.378: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017706896s
STEP: Saw pod success 02/12/23 12:40:15.379
Feb 12 12:40:15.379: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558" satisfied condition "Succeeded or Failed"
Feb 12 12:40:15.388: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558 container client-container: <nil>
STEP: delete the pod 02/12/23 12:40:15.403
Feb 12 12:40:15.423: INFO: Waiting for pod downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558 to disappear
Feb 12 12:40:15.426: INFO: Pod downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Feb 12 12:40:15.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1735" for this suite. 02/12/23 12:40:15.429
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":312,"skipped":5745,"failed":0}
------------------------------
 [4.113 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:11.326
    Feb 12 12:40:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:40:11.327
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:11.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:11.346
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:40:11.348
    Feb 12 12:40:11.361: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558" in namespace "downward-api-1735" to be "Succeeded or Failed"
    Feb 12 12:40:11.366: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558": Phase="Pending", Reason="", readiness=false. Elapsed: 5.110574ms
    Feb 12 12:40:13.381: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020127375s
    Feb 12 12:40:15.378: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017706896s
    STEP: Saw pod success 02/12/23 12:40:15.379
    Feb 12 12:40:15.379: INFO: Pod "downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558" satisfied condition "Succeeded or Failed"
    Feb 12 12:40:15.388: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558 container client-container: <nil>
    STEP: delete the pod 02/12/23 12:40:15.403
    Feb 12 12:40:15.423: INFO: Waiting for pod downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558 to disappear
    Feb 12 12:40:15.426: INFO: Pod downwardapi-volume-0c65a7e4-5c88-420a-af99-4c5d96af6558 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Feb 12 12:40:15.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1735" for this suite. 02/12/23 12:40:15.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:15.439
Feb 12 12:40:15.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename container-runtime 02/12/23 12:40:15.44
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:15.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:15.459
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 02/12/23 12:40:15.461
STEP: wait for the container to reach Succeeded 02/12/23 12:40:15.469
STEP: get the container status 02/12/23 12:40:19.509
STEP: the container should be terminated 02/12/23 12:40:19.515
STEP: the termination message should be set 02/12/23 12:40:19.515
Feb 12 12:40:19.515: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/12/23 12:40:19.515
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Feb 12 12:40:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7867" for this suite. 02/12/23 12:40:19.541
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":313,"skipped":5761,"failed":0}
------------------------------
 [4.110 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:15.439
    Feb 12 12:40:15.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename container-runtime 02/12/23 12:40:15.44
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:15.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:15.459
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 02/12/23 12:40:15.461
    STEP: wait for the container to reach Succeeded 02/12/23 12:40:15.469
    STEP: get the container status 02/12/23 12:40:19.509
    STEP: the container should be terminated 02/12/23 12:40:19.515
    STEP: the termination message should be set 02/12/23 12:40:19.515
    Feb 12 12:40:19.515: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/12/23 12:40:19.515
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Feb 12 12:40:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7867" for this suite. 02/12/23 12:40:19.541
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:19.549
Feb 12 12:40:19.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename namespaces 02/12/23 12:40:19.551
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:19.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:19.576
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 02/12/23 12:40:19.579
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:19.606
STEP: Creating a pod in the namespace 02/12/23 12:40:19.614
STEP: Waiting for the pod to have running status 02/12/23 12:40:19.625
Feb 12 12:40:19.625: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5647" to be "running"
Feb 12 12:40:19.633: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.759154ms
Feb 12 12:40:21.642: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017407454s
Feb 12 12:40:21.642: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/12/23 12:40:21.642
STEP: Waiting for the namespace to be removed. 02/12/23 12:40:21.65
STEP: Recreating the namespace 02/12/23 12:40:32.656
STEP: Verifying there are no pods in the namespace 02/12/23 12:40:32.671
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:40:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2575" for this suite. 02/12/23 12:40:32.678
STEP: Destroying namespace "nsdeletetest-5647" for this suite. 02/12/23 12:40:32.685
Feb 12 12:40:32.688: INFO: Namespace nsdeletetest-5647 was already deleted
STEP: Destroying namespace "nsdeletetest-8765" for this suite. 02/12/23 12:40:32.688
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":314,"skipped":5761,"failed":0}
------------------------------
 [SLOW TEST] [13.146 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:19.549
    Feb 12 12:40:19.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename namespaces 02/12/23 12:40:19.551
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:19.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:19.576
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 02/12/23 12:40:19.579
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:19.606
    STEP: Creating a pod in the namespace 02/12/23 12:40:19.614
    STEP: Waiting for the pod to have running status 02/12/23 12:40:19.625
    Feb 12 12:40:19.625: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5647" to be "running"
    Feb 12 12:40:19.633: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.759154ms
    Feb 12 12:40:21.642: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017407454s
    Feb 12 12:40:21.642: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/12/23 12:40:21.642
    STEP: Waiting for the namespace to be removed. 02/12/23 12:40:21.65
    STEP: Recreating the namespace 02/12/23 12:40:32.656
    STEP: Verifying there are no pods in the namespace 02/12/23 12:40:32.671
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:40:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2575" for this suite. 02/12/23 12:40:32.678
    STEP: Destroying namespace "nsdeletetest-5647" for this suite. 02/12/23 12:40:32.685
    Feb 12 12:40:32.688: INFO: Namespace nsdeletetest-5647 was already deleted
    STEP: Destroying namespace "nsdeletetest-8765" for this suite. 02/12/23 12:40:32.688
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:32.696
Feb 12 12:40:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubelet-test 02/12/23 12:40:32.696
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:32.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:32.716
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/12/23 12:40:32.726
Feb 12 12:40:32.726: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426" in namespace "kubelet-test-3127" to be "completed"
Feb 12 12:40:32.734: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426": Phase="Pending", Reason="", readiness=false. Elapsed: 7.013236ms
Feb 12 12:40:34.749: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022413614s
Feb 12 12:40:36.747: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020705715s
Feb 12 12:40:36.747: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Feb 12 12:40:36.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3127" for this suite. 02/12/23 12:40:36.784
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":315,"skipped":5762,"failed":0}
------------------------------
 [4.196 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:32.696
    Feb 12 12:40:32.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubelet-test 02/12/23 12:40:32.696
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:32.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:32.716
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/12/23 12:40:32.726
    Feb 12 12:40:32.726: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426" in namespace "kubelet-test-3127" to be "completed"
    Feb 12 12:40:32.734: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426": Phase="Pending", Reason="", readiness=false. Elapsed: 7.013236ms
    Feb 12 12:40:34.749: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022413614s
    Feb 12 12:40:36.747: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020705715s
    Feb 12 12:40:36.747: INFO: Pod "agnhost-host-aliasesc61b1f01-5259-4887-8673-cd848298f426" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Feb 12 12:40:36.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3127" for this suite. 02/12/23 12:40:36.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:36.895
Feb 12 12:40:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:40:36.897
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:38.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:38.8
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 02/12/23 12:40:38.809
Feb 12 12:40:40.289: INFO: Waiting up to 5m0s for pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812" in namespace "downward-api-3823" to be "Succeeded or Failed"
Feb 12 12:40:40.757: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812": Phase="Pending", Reason="", readiness=false. Elapsed: 467.378289ms
Feb 12 12:40:42.763: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474120861s
Feb 12 12:40:44.774: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.484696209s
STEP: Saw pod success 02/12/23 12:40:44.775
Feb 12 12:40:44.776: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812" satisfied condition "Succeeded or Failed"
Feb 12 12:40:44.784: INFO: Trying to get logs from node kube-3 pod downward-api-0ea7392c-f119-45bb-b088-de68f94b5812 container dapi-container: <nil>
STEP: delete the pod 02/12/23 12:40:44.794
Feb 12 12:40:44.812: INFO: Waiting for pod downward-api-0ea7392c-f119-45bb-b088-de68f94b5812 to disappear
Feb 12 12:40:44.814: INFO: Pod downward-api-0ea7392c-f119-45bb-b088-de68f94b5812 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 12 12:40:44.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3823" for this suite. 02/12/23 12:40:44.817
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":316,"skipped":5792,"failed":0}
------------------------------
 [SLOW TEST] [7.928 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:36.895
    Feb 12 12:40:36.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:40:36.897
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:38.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:38.8
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 02/12/23 12:40:38.809
    Feb 12 12:40:40.289: INFO: Waiting up to 5m0s for pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812" in namespace "downward-api-3823" to be "Succeeded or Failed"
    Feb 12 12:40:40.757: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812": Phase="Pending", Reason="", readiness=false. Elapsed: 467.378289ms
    Feb 12 12:40:42.763: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474120861s
    Feb 12 12:40:44.774: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.484696209s
    STEP: Saw pod success 02/12/23 12:40:44.775
    Feb 12 12:40:44.776: INFO: Pod "downward-api-0ea7392c-f119-45bb-b088-de68f94b5812" satisfied condition "Succeeded or Failed"
    Feb 12 12:40:44.784: INFO: Trying to get logs from node kube-3 pod downward-api-0ea7392c-f119-45bb-b088-de68f94b5812 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 12:40:44.794
    Feb 12 12:40:44.812: INFO: Waiting for pod downward-api-0ea7392c-f119-45bb-b088-de68f94b5812 to disappear
    Feb 12 12:40:44.814: INFO: Pod downward-api-0ea7392c-f119-45bb-b088-de68f94b5812 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 12 12:40:44.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3823" for this suite. 02/12/23 12:40:44.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:40:44.824
Feb 12 12:40:44.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:40:44.825
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:44.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:44.844
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-2735 02/12/23 12:40:44.845
Feb 12 12:40:44.854: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2735" to be "running and ready"
Feb 12 12:40:44.866: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.457573ms
Feb 12 12:40:44.866: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:40:46.878: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.023941176s
Feb 12 12:40:46.878: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 12 12:40:46.879: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 12 12:40:46.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 12 12:40:47.056: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 12 12:40:47.056: INFO: stdout: "ipvs"
Feb 12 12:40:47.056: INFO: proxyMode: ipvs
Feb 12 12:40:47.070: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 12:40:47.074: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-2735 02/12/23 12:40:47.074
STEP: creating replication controller affinity-nodeport-timeout in namespace services-2735 02/12/23 12:40:47.089
I0212 12:40:47.105201      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2735, replica count: 3
I0212 12:40:50.156649      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 12:40:50.171: INFO: Creating new exec pod
Feb 12 12:40:50.178: INFO: Waiting up to 5m0s for pod "execpod-affinity79s4d" in namespace "services-2735" to be "running"
Feb 12 12:40:50.183: INFO: Pod "execpod-affinity79s4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936501ms
Feb 12 12:40:52.188: INFO: Pod "execpod-affinity79s4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009413487s
Feb 12 12:40:52.188: INFO: Pod "execpod-affinity79s4d" satisfied condition "running"
Feb 12 12:40:53.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Feb 12 12:40:53.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Feb 12 12:40:53.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:40:53.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.46 80'
Feb 12 12:40:53.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.46 80\nConnection to 10.233.43.46 80 port [tcp/http] succeeded!\n"
Feb 12 12:40:53.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:40:53.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32110'
Feb 12 12:40:53.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32110\nConnection to 10.2.20.101 32110 port [tcp/*] succeeded!\n"
Feb 12 12:40:53.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:40:53.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.103 32110'
Feb 12 12:40:53.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.103 32110\nConnection to 10.2.20.103 32110 port [tcp/*] succeeded!\n"
Feb 12 12:40:53.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:40:53.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:32110/ ; done'
Feb 12 12:40:54.001: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n"
Feb 12 12:40:54.001: INFO: stdout: "\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms"
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
Feb 12 12:40:54.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.2.20.101:32110/'
Feb 12 12:40:54.106: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n"
Feb 12 12:40:54.106: INFO: stdout: "affinity-nodeport-timeout-7ktms"
Feb 12 12:43:04.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.2.20.101:32110/'
Feb 12 12:43:04.457: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n"
Feb 12 12:43:04.457: INFO: stdout: "affinity-nodeport-timeout-dgglt"
Feb 12 12:43:04.457: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2735, will wait for the garbage collector to delete the pods 02/12/23 12:43:04.477
Feb 12 12:43:04.539: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.263505ms
Feb 12 12:43:04.641: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.155419ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:43:10.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2735" for this suite. 02/12/23 12:43:10.39
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":317,"skipped":5824,"failed":0}
------------------------------
 [SLOW TEST] [145.609 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:40:44.824
    Feb 12 12:40:44.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:40:44.825
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:40:44.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:40:44.844
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-2735 02/12/23 12:40:44.845
    Feb 12 12:40:44.854: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-2735" to be "running and ready"
    Feb 12 12:40:44.866: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 11.457573ms
    Feb 12 12:40:44.866: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:40:46.878: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.023941176s
    Feb 12 12:40:46.878: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 12 12:40:46.879: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 12 12:40:46.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 12 12:40:47.056: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb 12 12:40:47.056: INFO: stdout: "ipvs"
    Feb 12 12:40:47.056: INFO: proxyMode: ipvs
    Feb 12 12:40:47.070: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 12 12:40:47.074: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-2735 02/12/23 12:40:47.074
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-2735 02/12/23 12:40:47.089
    I0212 12:40:47.105201      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-2735, replica count: 3
    I0212 12:40:50.156649      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 12:40:50.171: INFO: Creating new exec pod
    Feb 12 12:40:50.178: INFO: Waiting up to 5m0s for pod "execpod-affinity79s4d" in namespace "services-2735" to be "running"
    Feb 12 12:40:50.183: INFO: Pod "execpod-affinity79s4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936501ms
    Feb 12 12:40:52.188: INFO: Pod "execpod-affinity79s4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009413487s
    Feb 12 12:40:52.188: INFO: Pod "execpod-affinity79s4d" satisfied condition "running"
    Feb 12 12:40:53.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Feb 12 12:40:53.380: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Feb 12 12:40:53.380: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:40:53.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.46 80'
    Feb 12 12:40:53.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.46 80\nConnection to 10.233.43.46 80 port [tcp/http] succeeded!\n"
    Feb 12 12:40:53.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:40:53.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32110'
    Feb 12 12:40:53.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32110\nConnection to 10.2.20.101 32110 port [tcp/*] succeeded!\n"
    Feb 12 12:40:53.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:40:53.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.103 32110'
    Feb 12 12:40:53.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.103 32110\nConnection to 10.2.20.103 32110 port [tcp/*] succeeded!\n"
    Feb 12 12:40:53.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:40:53.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.2.20.101:32110/ ; done'
    Feb 12 12:40:54.001: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n"
    Feb 12 12:40:54.001: INFO: stdout: "\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms\naffinity-nodeport-timeout-7ktms"
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Received response from host: affinity-nodeport-timeout-7ktms
    Feb 12 12:40:54.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.2.20.101:32110/'
    Feb 12 12:40:54.106: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n"
    Feb 12 12:40:54.106: INFO: stdout: "affinity-nodeport-timeout-7ktms"
    Feb 12 12:43:04.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-2735 exec execpod-affinity79s4d -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.2.20.101:32110/'
    Feb 12 12:43:04.457: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.2.20.101:32110/\n"
    Feb 12 12:43:04.457: INFO: stdout: "affinity-nodeport-timeout-dgglt"
    Feb 12 12:43:04.457: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-2735, will wait for the garbage collector to delete the pods 02/12/23 12:43:04.477
    Feb 12 12:43:04.539: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.263505ms
    Feb 12 12:43:04.641: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.155419ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:43:10.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2735" for this suite. 02/12/23 12:43:10.39
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:43:10.437
Feb 12 12:43:10.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 12:43:10.438
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:10.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:10.509
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 02/12/23 12:43:10.511
Feb 12 12:43:10.541: INFO: Waiting up to 5m0s for pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e" in namespace "var-expansion-6653" to be "Succeeded or Failed"
Feb 12 12:43:10.560: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.285625ms
Feb 12 12:43:12.582: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040065711s
Feb 12 12:43:14.565: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023885317s
STEP: Saw pod success 02/12/23 12:43:14.566
Feb 12 12:43:14.566: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e" satisfied condition "Succeeded or Failed"
Feb 12 12:43:14.569: INFO: Trying to get logs from node kube-3 pod var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e container dapi-container: <nil>
STEP: delete the pod 02/12/23 12:43:14.585
Feb 12 12:43:14.600: INFO: Waiting for pod var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e to disappear
Feb 12 12:43:14.603: INFO: Pod var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 12:43:14.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6653" for this suite. 02/12/23 12:43:14.606
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":318,"skipped":5901,"failed":0}
------------------------------
 [4.176 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:43:10.437
    Feb 12 12:43:10.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 12:43:10.438
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:10.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:10.509
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 02/12/23 12:43:10.511
    Feb 12 12:43:10.541: INFO: Waiting up to 5m0s for pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e" in namespace "var-expansion-6653" to be "Succeeded or Failed"
    Feb 12 12:43:10.560: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.285625ms
    Feb 12 12:43:12.582: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040065711s
    Feb 12 12:43:14.565: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023885317s
    STEP: Saw pod success 02/12/23 12:43:14.566
    Feb 12 12:43:14.566: INFO: Pod "var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e" satisfied condition "Succeeded or Failed"
    Feb 12 12:43:14.569: INFO: Trying to get logs from node kube-3 pod var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e container dapi-container: <nil>
    STEP: delete the pod 02/12/23 12:43:14.585
    Feb 12 12:43:14.600: INFO: Waiting for pod var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e to disappear
    Feb 12 12:43:14.603: INFO: Pod var-expansion-92587d36-4ba4-4ec8-b214-2773f134587e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 12:43:14.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6653" for this suite. 02/12/23 12:43:14.606
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:43:14.613
Feb 12 12:43:14.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pod-network-test 02/12/23 12:43:14.615
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:14.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:14.636
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-141 02/12/23 12:43:14.638
STEP: creating a selector 02/12/23 12:43:14.638
STEP: Creating the service pods in kubernetes 02/12/23 12:43:14.638
Feb 12 12:43:14.638: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 12 12:43:14.667: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-141" to be "running and ready"
Feb 12 12:43:14.680: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.702299ms
Feb 12 12:43:14.680: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:43:16.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025539476s
Feb 12 12:43:16.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:43:18.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.024838797s
Feb 12 12:43:18.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:43:20.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019917341s
Feb 12 12:43:20.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:43:22.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025117168s
Feb 12 12:43:22.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:43:24.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025931411s
Feb 12 12:43:24.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 12 12:43:26.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.025698186s
Feb 12 12:43:26.693: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 12 12:43:26.693: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 12 12:43:26.698: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-141" to be "running and ready"
Feb 12 12:43:26.702: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.569224ms
Feb 12 12:43:26.702: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 12 12:43:26.702: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 12 12:43:26.705: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-141" to be "running and ready"
Feb 12 12:43:26.708: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.981946ms
Feb 12 12:43:26.708: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 12 12:43:26.708: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 02/12/23 12:43:26.713
Feb 12 12:43:26.729: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-141" to be "running"
Feb 12 12:43:26.742: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.546254ms
Feb 12 12:43:28.754: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024745299s
Feb 12 12:43:28.754: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 12 12:43:28.763: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-141" to be "running"
Feb 12 12:43:28.769: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.83892ms
Feb 12 12:43:28.769: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 12 12:43:28.774: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Feb 12 12:43:28.774: INFO: Going to poll 10.233.120.113 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 12 12:43:28.779: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-141 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:43:28.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:43:28.779: INFO: ExecWithOptions: Clientset creation
Feb 12 12:43:28.779: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-141/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 12:43:28.888: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 12 12:43:28.888: INFO: Going to poll 10.233.120.221 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 12 12:43:28.893: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.221:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-141 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:43:28.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:43:28.894: INFO: ExecWithOptions: Clientset creation
Feb 12 12:43:28.894: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-141/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.221%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 12:43:28.968: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 12 12:43:28.968: INFO: Going to poll 10.233.99.111 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Feb 12 12:43:28.973: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.99.111:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-141 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:43:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:43:28.973: INFO: ExecWithOptions: Clientset creation
Feb 12 12:43:28.974: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-141/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.99.111%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 12 12:43:29.033: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Feb 12 12:43:29.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-141" for this suite. 02/12/23 12:43:29.038
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":319,"skipped":5905,"failed":0}
------------------------------
 [SLOW TEST] [14.433 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:43:14.613
    Feb 12 12:43:14.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pod-network-test 02/12/23 12:43:14.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:14.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:14.636
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-141 02/12/23 12:43:14.638
    STEP: creating a selector 02/12/23 12:43:14.638
    STEP: Creating the service pods in kubernetes 02/12/23 12:43:14.638
    Feb 12 12:43:14.638: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 12 12:43:14.667: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-141" to be "running and ready"
    Feb 12 12:43:14.680: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.702299ms
    Feb 12 12:43:14.680: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:43:16.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025539476s
    Feb 12 12:43:16.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:43:18.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.024838797s
    Feb 12 12:43:18.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:43:20.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.019917341s
    Feb 12 12:43:20.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:43:22.692: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025117168s
    Feb 12 12:43:22.692: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:43:24.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025931411s
    Feb 12 12:43:24.693: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 12 12:43:26.693: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.025698186s
    Feb 12 12:43:26.693: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 12 12:43:26.693: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 12 12:43:26.698: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-141" to be "running and ready"
    Feb 12 12:43:26.702: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.569224ms
    Feb 12 12:43:26.702: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 12 12:43:26.702: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 12 12:43:26.705: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-141" to be "running and ready"
    Feb 12 12:43:26.708: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.981946ms
    Feb 12 12:43:26.708: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 12 12:43:26.708: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 02/12/23 12:43:26.713
    Feb 12 12:43:26.729: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-141" to be "running"
    Feb 12 12:43:26.742: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.546254ms
    Feb 12 12:43:28.754: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024745299s
    Feb 12 12:43:28.754: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 12 12:43:28.763: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-141" to be "running"
    Feb 12 12:43:28.769: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 5.83892ms
    Feb 12 12:43:28.769: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 12 12:43:28.774: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Feb 12 12:43:28.774: INFO: Going to poll 10.233.120.113 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 12 12:43:28.779: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-141 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:43:28.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:43:28.779: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:43:28.779: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-141/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 12:43:28.888: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 12 12:43:28.888: INFO: Going to poll 10.233.120.221 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 12 12:43:28.893: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.120.221:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-141 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:43:28.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:43:28.894: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:43:28.894: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-141/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.120.221%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 12:43:28.968: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 12 12:43:28.968: INFO: Going to poll 10.233.99.111 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Feb 12 12:43:28.973: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.99.111:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-141 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:43:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:43:28.973: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:43:28.974: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-141/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.99.111%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 12 12:43:29.033: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Feb 12 12:43:29.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-141" for this suite. 02/12/23 12:43:29.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:43:29.047
Feb 12 12:43:29.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename svcaccounts 02/12/23 12:43:29.048
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:29.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:29.064
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Feb 12 12:43:29.084: INFO: created pod pod-service-account-defaultsa
Feb 12 12:43:29.084: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 12 12:43:29.092: INFO: created pod pod-service-account-mountsa
Feb 12 12:43:29.092: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 12 12:43:29.115: INFO: created pod pod-service-account-nomountsa
Feb 12 12:43:29.115: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 12 12:43:29.135: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 12 12:43:29.135: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 12 12:43:29.158: INFO: created pod pod-service-account-mountsa-mountspec
Feb 12 12:43:29.158: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 12 12:43:29.170: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 12 12:43:29.170: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 12 12:43:29.185: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 12 12:43:29.185: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 12 12:43:29.200: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 12 12:43:29.200: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 12 12:43:29.216: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 12 12:43:29.216: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Feb 12 12:43:29.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1112" for this suite. 02/12/23 12:43:29.241
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":320,"skipped":5942,"failed":0}
------------------------------
 [0.221 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:43:29.047
    Feb 12 12:43:29.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename svcaccounts 02/12/23 12:43:29.048
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:29.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:29.064
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Feb 12 12:43:29.084: INFO: created pod pod-service-account-defaultsa
    Feb 12 12:43:29.084: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 12 12:43:29.092: INFO: created pod pod-service-account-mountsa
    Feb 12 12:43:29.092: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 12 12:43:29.115: INFO: created pod pod-service-account-nomountsa
    Feb 12 12:43:29.115: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 12 12:43:29.135: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 12 12:43:29.135: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 12 12:43:29.158: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 12 12:43:29.158: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 12 12:43:29.170: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 12 12:43:29.170: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 12 12:43:29.185: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 12 12:43:29.185: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 12 12:43:29.200: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 12 12:43:29.200: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 12 12:43:29.216: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 12 12:43:29.216: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Feb 12 12:43:29.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1112" for this suite. 02/12/23 12:43:29.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:43:29.271
Feb 12 12:43:29.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 12:43:29.271
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:29.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:29.327
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb 12 12:43:29.354: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 12 12:43:34.357: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/12/23 12:43:34.357
Feb 12 12:43:34.357: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 12 12:43:36.370: INFO: Creating deployment "test-rollover-deployment"
Feb 12 12:43:36.384: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 12 12:43:38.401: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 12 12:43:38.418: INFO: Ensure that both replica sets have 1 created replica
Feb 12 12:43:38.429: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 12 12:43:38.449: INFO: Updating deployment test-rollover-deployment
Feb 12 12:43:38.449: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 12 12:43:40.458: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 12 12:43:40.469: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 12 12:43:40.475: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 12:43:40.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:43:42.488: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 12:43:42.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:43:44.500: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 12:43:44.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:43:46.491: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 12:43:46.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:43:48.499: INFO: all replica sets need to contain the pod-template-hash label
Feb 12 12:43:48.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 12 12:43:50.852: INFO: 
Feb 12 12:43:50.852: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 12:43:51.094: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-258  4ec8100a-e0eb-4630-9e8b-cf8bf1908066 38484 2 2023-02-12 12:43:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003280b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-12 12:43:36 +0000 UTC,LastTransitionTime:2023-02-12 12:43:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-12 12:43:49 +0000 UTC,LastTransitionTime:2023-02-12 12:43:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 12:43:51.254: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-258  4d158318-70a7-492a-afe2-96c03349b73e 38474 2 2023-02-12 12:43:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ec8100a-e0eb-4630-9e8b-cf8bf1908066 0xc00727e737 0xc00727e738}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec8100a-e0eb-4630-9e8b-cf8bf1908066\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00727e7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:43:51.254: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 12 12:43:51.255: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-258  89954ba5-7aa4-44ae-9daa-fdf63fbd96f0 38483 2 2023-02-12 12:43:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ec8100a-e0eb-4630-9e8b-cf8bf1908066 0xc00727e4e7 0xc00727e4e8}] [] [{e2e.test Update apps/v1 2023-02-12 12:43:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec8100a-e0eb-4630-9e8b-cf8bf1908066\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00727e5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:43:51.255: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-258  5d0adc6b-ea9c-47ce-b91e-8a5062a1e2cc 38431 2 2023-02-12 12:43:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ec8100a-e0eb-4630-9e8b-cf8bf1908066 0xc00727e617 0xc00727e618}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec8100a-e0eb-4630-9e8b-cf8bf1908066\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00727e6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:43:51.558: INFO: Pod "test-rollover-deployment-6d45fd857b-pxns8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-pxns8 test-rollover-deployment-6d45fd857b- deployment-258  5e4b4349-7067-4586-b9c2-646780775379 38447 0 2023-02-12 12:43:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:0c9de71f45f749f0703fd2bf572a701ac6494750bdc8bc3caf738b47e7cb8fb6 cni.projectcalico.org/podIP:10.233.99.124/32 cni.projectcalico.org/podIPs:10.233.99.124/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 4d158318-70a7-492a-afe2-96c03349b73e 0xc00727ed57 0xc00727ed58}] [] [{kube-controller-manager Update v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d158318-70a7-492a-afe2-96c03349b73e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:43:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzkln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzkln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.124,StartTime:2023-02-12 12:43:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:43:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://003b7b9a08d966e28532f69302cb4d21bf2495fbb89c312f8f09e3e71397da04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 12:43:51.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-258" for this suite. 02/12/23 12:43:51.569
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":321,"skipped":5977,"failed":0}
------------------------------
 [SLOW TEST] [22.611 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:43:29.271
    Feb 12 12:43:29.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 12:43:29.271
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:29.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:29.327
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb 12 12:43:29.354: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 12 12:43:34.357: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/12/23 12:43:34.357
    Feb 12 12:43:34.357: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 12 12:43:36.370: INFO: Creating deployment "test-rollover-deployment"
    Feb 12 12:43:36.384: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 12 12:43:38.401: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 12 12:43:38.418: INFO: Ensure that both replica sets have 1 created replica
    Feb 12 12:43:38.429: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 12 12:43:38.449: INFO: Updating deployment test-rollover-deployment
    Feb 12 12:43:38.449: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 12 12:43:40.458: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 12 12:43:40.469: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 12 12:43:40.475: INFO: all replica sets need to contain the pod-template-hash label
    Feb 12 12:43:40.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:43:42.488: INFO: all replica sets need to contain the pod-template-hash label
    Feb 12 12:43:42.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:43:44.500: INFO: all replica sets need to contain the pod-template-hash label
    Feb 12 12:43:44.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:43:46.491: INFO: all replica sets need to contain the pod-template-hash label
    Feb 12 12:43:46.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:43:48.499: INFO: all replica sets need to contain the pod-template-hash label
    Feb 12 12:43:48.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 43, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 12 12:43:50.852: INFO: 
    Feb 12 12:43:50.852: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 12:43:51.094: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-258  4ec8100a-e0eb-4630-9e8b-cf8bf1908066 38484 2 2023-02-12 12:43:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003280b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-12 12:43:36 +0000 UTC,LastTransitionTime:2023-02-12 12:43:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-02-12 12:43:49 +0000 UTC,LastTransitionTime:2023-02-12 12:43:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 12 12:43:51.254: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-258  4d158318-70a7-492a-afe2-96c03349b73e 38474 2 2023-02-12 12:43:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4ec8100a-e0eb-4630-9e8b-cf8bf1908066 0xc00727e737 0xc00727e738}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec8100a-e0eb-4630-9e8b-cf8bf1908066\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00727e7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:43:51.254: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 12 12:43:51.255: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-258  89954ba5-7aa4-44ae-9daa-fdf63fbd96f0 38483 2 2023-02-12 12:43:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4ec8100a-e0eb-4630-9e8b-cf8bf1908066 0xc00727e4e7 0xc00727e4e8}] [] [{e2e.test Update apps/v1 2023-02-12 12:43:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec8100a-e0eb-4630-9e8b-cf8bf1908066\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00727e5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:43:51.255: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-258  5d0adc6b-ea9c-47ce-b91e-8a5062a1e2cc 38431 2 2023-02-12 12:43:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4ec8100a-e0eb-4630-9e8b-cf8bf1908066 0xc00727e617 0xc00727e618}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec8100a-e0eb-4630-9e8b-cf8bf1908066\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00727e6c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:43:51.558: INFO: Pod "test-rollover-deployment-6d45fd857b-pxns8" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-pxns8 test-rollover-deployment-6d45fd857b- deployment-258  5e4b4349-7067-4586-b9c2-646780775379 38447 0 2023-02-12 12:43:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:0c9de71f45f749f0703fd2bf572a701ac6494750bdc8bc3caf738b47e7cb8fb6 cni.projectcalico.org/podIP:10.233.99.124/32 cni.projectcalico.org/podIPs:10.233.99.124/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 4d158318-70a7-492a-afe2-96c03349b73e 0xc00727ed57 0xc00727ed58}] [] [{kube-controller-manager Update v1 2023-02-12 12:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d158318-70a7-492a-afe2-96c03349b73e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:43:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:43:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzkln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzkln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:43:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.124,StartTime:2023-02-12 12:43:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:43:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://003b7b9a08d966e28532f69302cb4d21bf2495fbb89c312f8f09e3e71397da04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 12:43:51.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-258" for this suite. 02/12/23 12:43:51.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:43:51.906
Feb 12 12:43:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:43:51.909
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:52.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:52.578
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-3c37fc22-4266-4428-b6a3-d7a32805ae13 02/12/23 12:43:52.581
STEP: Creating a pod to test consume configMaps 02/12/23 12:43:52.603
Feb 12 12:43:52.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4" in namespace "configmap-1220" to be "Succeeded or Failed"
Feb 12 12:43:52.746: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4": Phase="Pending", Reason="", readiness=false. Elapsed: 45.836816ms
Feb 12 12:43:54.756: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055873727s
Feb 12 12:43:56.761: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060707231s
STEP: Saw pod success 02/12/23 12:43:56.762
Feb 12 12:43:56.762: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4" satisfied condition "Succeeded or Failed"
Feb 12 12:43:56.767: INFO: Trying to get logs from node kube-3 pod pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4 container agnhost-container: <nil>
STEP: delete the pod 02/12/23 12:43:56.773
Feb 12 12:43:56.790: INFO: Waiting for pod pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4 to disappear
Feb 12 12:43:56.793: INFO: Pod pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:43:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1220" for this suite. 02/12/23 12:43:56.797
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":322,"skipped":6005,"failed":0}
------------------------------
 [4.898 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:43:51.906
    Feb 12 12:43:51.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:43:51.909
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:52.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:52.578
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-3c37fc22-4266-4428-b6a3-d7a32805ae13 02/12/23 12:43:52.581
    STEP: Creating a pod to test consume configMaps 02/12/23 12:43:52.603
    Feb 12 12:43:52.700: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4" in namespace "configmap-1220" to be "Succeeded or Failed"
    Feb 12 12:43:52.746: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4": Phase="Pending", Reason="", readiness=false. Elapsed: 45.836816ms
    Feb 12 12:43:54.756: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055873727s
    Feb 12 12:43:56.761: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060707231s
    STEP: Saw pod success 02/12/23 12:43:56.762
    Feb 12 12:43:56.762: INFO: Pod "pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4" satisfied condition "Succeeded or Failed"
    Feb 12 12:43:56.767: INFO: Trying to get logs from node kube-3 pod pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4 container agnhost-container: <nil>
    STEP: delete the pod 02/12/23 12:43:56.773
    Feb 12 12:43:56.790: INFO: Waiting for pod pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4 to disappear
    Feb 12 12:43:56.793: INFO: Pod pod-configmaps-ae7c5d30-2f4c-48ee-bf0e-128a0a85dbc4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:43:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1220" for this suite. 02/12/23 12:43:56.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:43:56.806
Feb 12 12:43:56.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:43:56.807
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:56.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:56.826
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 12:43:56.828
Feb 12 12:43:56.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-6755 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 12 12:43:56.890: INFO: stderr: ""
Feb 12 12:43:56.890: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/12/23 12:43:56.89
Feb 12 12:43:56.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-6755 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Feb 12 12:43:57.697: INFO: stderr: ""
Feb 12 12:43:57.697: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 12:43:57.697
Feb 12 12:43:57.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-6755 delete pods e2e-test-httpd-pod'
Feb 12 12:44:00.597: INFO: stderr: ""
Feb 12 12:44:00.597: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:44:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6755" for this suite. 02/12/23 12:44:00.6
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":323,"skipped":6033,"failed":0}
------------------------------
 [3.806 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:43:56.806
    Feb 12 12:43:56.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:43:56.807
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:43:56.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:43:56.826
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 12:43:56.828
    Feb 12 12:43:56.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-6755 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 12 12:43:56.890: INFO: stderr: ""
    Feb 12 12:43:56.890: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/12/23 12:43:56.89
    Feb 12 12:43:56.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-6755 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Feb 12 12:43:57.697: INFO: stderr: ""
    Feb 12 12:43:57.697: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 02/12/23 12:43:57.697
    Feb 12 12:43:57.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-6755 delete pods e2e-test-httpd-pod'
    Feb 12 12:44:00.597: INFO: stderr: ""
    Feb 12 12:44:00.597: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:44:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6755" for this suite. 02/12/23 12:44:00.6
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:00.613
Feb 12 12:44:00.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename downward-api 02/12/23 12:44:00.614
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:00.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:00.634
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 02/12/23 12:44:00.636
Feb 12 12:44:00.646: INFO: Waiting up to 5m0s for pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766" in namespace "downward-api-1680" to be "Succeeded or Failed"
Feb 12 12:44:00.649: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766": Phase="Pending", Reason="", readiness=false. Elapsed: 3.169116ms
Feb 12 12:44:02.655: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008810584s
Feb 12 12:44:04.653: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007107977s
STEP: Saw pod success 02/12/23 12:44:04.653
Feb 12 12:44:04.653: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766" satisfied condition "Succeeded or Failed"
Feb 12 12:44:04.656: INFO: Trying to get logs from node kube-3 pod downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766 container dapi-container: <nil>
STEP: delete the pod 02/12/23 12:44:04.664
Feb 12 12:44:04.682: INFO: Waiting for pod downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766 to disappear
Feb 12 12:44:04.684: INFO: Pod downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Feb 12 12:44:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1680" for this suite. 02/12/23 12:44:04.687
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":324,"skipped":6033,"failed":0}
------------------------------
 [4.080 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:00.613
    Feb 12 12:44:00.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename downward-api 02/12/23 12:44:00.614
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:00.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:00.634
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 02/12/23 12:44:00.636
    Feb 12 12:44:00.646: INFO: Waiting up to 5m0s for pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766" in namespace "downward-api-1680" to be "Succeeded or Failed"
    Feb 12 12:44:00.649: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766": Phase="Pending", Reason="", readiness=false. Elapsed: 3.169116ms
    Feb 12 12:44:02.655: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008810584s
    Feb 12 12:44:04.653: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007107977s
    STEP: Saw pod success 02/12/23 12:44:04.653
    Feb 12 12:44:04.653: INFO: Pod "downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766" satisfied condition "Succeeded or Failed"
    Feb 12 12:44:04.656: INFO: Trying to get logs from node kube-3 pod downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766 container dapi-container: <nil>
    STEP: delete the pod 02/12/23 12:44:04.664
    Feb 12 12:44:04.682: INFO: Waiting for pod downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766 to disappear
    Feb 12 12:44:04.684: INFO: Pod downward-api-94b8ba1c-0bb2-475f-bf84-cbebbf545766 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Feb 12 12:44:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1680" for this suite. 02/12/23 12:44:04.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:04.694
Feb 12 12:44:04.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 12:44:04.695
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:04.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:04.713
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/12/23 12:44:04.719
Feb 12 12:44:04.719: INFO: Creating simple deployment test-deployment-rdkq5
Feb 12 12:44:04.737: INFO: deployment "test-deployment-rdkq5" doesn't have the required revision set
STEP: Getting /status 02/12/23 12:44:06.774
Feb 12 12:44:06.785: INFO: Deployment test-deployment-rdkq5 has Conditions: [{Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 02/12/23 12:44:06.785
Feb 12 12:44:06.805: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 44, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 44, 4, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rdkq5-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/12/23 12:44:06.805
Feb 12 12:44:06.807: INFO: Observed &Deployment event: ADDED
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rdkq5-777898ffcc" is progressing.}
Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
Feb 12 12:44:06.807: INFO: Found Deployment test-deployment-rdkq5 in namespace deployment-7891 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 12 12:44:06.807: INFO: Deployment test-deployment-rdkq5 has an updated status
STEP: patching the Statefulset Status 02/12/23 12:44:06.807
Feb 12 12:44:06.808: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 12 12:44:06.818: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/12/23 12:44:06.818
Feb 12 12:44:06.821: INFO: Observed &Deployment event: ADDED
Feb 12 12:44:06.821: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
Feb 12 12:44:06.821: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.821: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
Feb 12 12:44:06.821: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 12 12:44:06.821: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rdkq5-777898ffcc" is progressing.}
Feb 12 12:44:06.822: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
Feb 12 12:44:06.823: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.823: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 12 12:44:06.823: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
Feb 12 12:44:06.823: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 12 12:44:06.823: INFO: Observed &Deployment event: MODIFIED
Feb 12 12:44:06.823: INFO: Found deployment test-deployment-rdkq5 in namespace deployment-7891 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 12 12:44:06.823: INFO: Deployment test-deployment-rdkq5 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 12:44:06.843: INFO: Deployment "test-deployment-rdkq5":
&Deployment{ObjectMeta:{test-deployment-rdkq5  deployment-7891  427b2f2b-ab20-4b55-9be1-e8a457f5834a 38676 1 2023-02-12 12:44:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-12 12:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-12 12:44:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-12 12:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038e06c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rdkq5-777898ffcc",LastUpdateTime:2023-02-12 12:44:06 +0000 UTC,LastTransitionTime:2023-02-12 12:44:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 12:44:06.850: INFO: New ReplicaSet "test-deployment-rdkq5-777898ffcc" of Deployment "test-deployment-rdkq5":
&ReplicaSet{ObjectMeta:{test-deployment-rdkq5-777898ffcc  deployment-7891  bf932ebc-5acb-4005-b37c-87e6a1c1888e 38657 1 2023-02-12 12:44:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rdkq5 427b2f2b-ab20-4b55-9be1-e8a457f5834a 0xc0045bfa70 0xc0045bfa71}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427b2f2b-ab20-4b55-9be1-e8a457f5834a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:44:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045bfb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:44:06.854: INFO: Pod "test-deployment-rdkq5-777898ffcc-vsd46" is available:
&Pod{ObjectMeta:{test-deployment-rdkq5-777898ffcc-vsd46 test-deployment-rdkq5-777898ffcc- deployment-7891  275fa5be-ffed-4592-8135-212f1c4039c4 38656 0 2023-02-12 12:44:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:b18753cecea2ca50563e9e822ca58785d1ff4eebc90b20f5b17e3e135879833e cni.projectcalico.org/podIP:10.233.99.69/32 cni.projectcalico.org/podIPs:10.233.99.69/32] [{apps/v1 ReplicaSet test-deployment-rdkq5-777898ffcc bf932ebc-5acb-4005-b37c-87e6a1c1888e 0xc0038e10b0 0xc0038e10b1}] [] [{kube-controller-manager Update v1 2023-02-12 12:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf932ebc-5acb-4005-b37c-87e6a1c1888e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:44:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:44:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf6zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf6zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.69,StartTime:2023-02-12 12:44:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:44:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79b776721450181654b6d246a31e5c9a690e6d2fb6e239a9cf07ea9ccf133cec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 12:44:06.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7891" for this suite. 02/12/23 12:44:06.858
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":325,"skipped":6060,"failed":0}
------------------------------
 [2.173 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:04.694
    Feb 12 12:44:04.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 12:44:04.695
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:04.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:04.713
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/12/23 12:44:04.719
    Feb 12 12:44:04.719: INFO: Creating simple deployment test-deployment-rdkq5
    Feb 12 12:44:04.737: INFO: deployment "test-deployment-rdkq5" doesn't have the required revision set
    STEP: Getting /status 02/12/23 12:44:06.774
    Feb 12 12:44:06.785: INFO: Deployment test-deployment-rdkq5 has Conditions: [{Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 02/12/23 12:44:06.785
    Feb 12 12:44:06.805: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 44, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 44, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 44, 4, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rdkq5-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/12/23 12:44:06.805
    Feb 12 12:44:06.807: INFO: Observed &Deployment event: ADDED
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
    Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rdkq5-777898ffcc" is progressing.}
    Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
    Feb 12 12:44:06.807: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 12 12:44:06.807: INFO: Observed Deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
    Feb 12 12:44:06.807: INFO: Found Deployment test-deployment-rdkq5 in namespace deployment-7891 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 12 12:44:06.807: INFO: Deployment test-deployment-rdkq5 has an updated status
    STEP: patching the Statefulset Status 02/12/23 12:44:06.807
    Feb 12 12:44:06.808: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 12 12:44:06.818: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/12/23 12:44:06.818
    Feb 12 12:44:06.821: INFO: Observed &Deployment event: ADDED
    Feb 12 12:44:06.821: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
    Feb 12 12:44:06.821: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.821: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rdkq5-777898ffcc"}
    Feb 12 12:44:06.821: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 12 12:44:06.821: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:04 +0000 UTC 2023-02-12 12:44:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rdkq5-777898ffcc" is progressing.}
    Feb 12 12:44:06.822: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 12 12:44:06.822: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
    Feb 12 12:44:06.823: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.823: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 12 12:44:06.823: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-12 12:44:05 +0000 UTC 2023-02-12 12:44:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rdkq5-777898ffcc" has successfully progressed.}
    Feb 12 12:44:06.823: INFO: Observed deployment test-deployment-rdkq5 in namespace deployment-7891 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 12 12:44:06.823: INFO: Observed &Deployment event: MODIFIED
    Feb 12 12:44:06.823: INFO: Found deployment test-deployment-rdkq5 in namespace deployment-7891 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 12 12:44:06.823: INFO: Deployment test-deployment-rdkq5 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 12:44:06.843: INFO: Deployment "test-deployment-rdkq5":
    &Deployment{ObjectMeta:{test-deployment-rdkq5  deployment-7891  427b2f2b-ab20-4b55-9be1-e8a457f5834a 38676 1 2023-02-12 12:44:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-12 12:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-12 12:44:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-12 12:44:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038e06c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rdkq5-777898ffcc",LastUpdateTime:2023-02-12 12:44:06 +0000 UTC,LastTransitionTime:2023-02-12 12:44:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 12 12:44:06.850: INFO: New ReplicaSet "test-deployment-rdkq5-777898ffcc" of Deployment "test-deployment-rdkq5":
    &ReplicaSet{ObjectMeta:{test-deployment-rdkq5-777898ffcc  deployment-7891  bf932ebc-5acb-4005-b37c-87e6a1c1888e 38657 1 2023-02-12 12:44:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rdkq5 427b2f2b-ab20-4b55-9be1-e8a457f5834a 0xc0045bfa70 0xc0045bfa71}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427b2f2b-ab20-4b55-9be1-e8a457f5834a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:44:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045bfb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:44:06.854: INFO: Pod "test-deployment-rdkq5-777898ffcc-vsd46" is available:
    &Pod{ObjectMeta:{test-deployment-rdkq5-777898ffcc-vsd46 test-deployment-rdkq5-777898ffcc- deployment-7891  275fa5be-ffed-4592-8135-212f1c4039c4 38656 0 2023-02-12 12:44:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:b18753cecea2ca50563e9e822ca58785d1ff4eebc90b20f5b17e3e135879833e cni.projectcalico.org/podIP:10.233.99.69/32 cni.projectcalico.org/podIPs:10.233.99.69/32] [{apps/v1 ReplicaSet test-deployment-rdkq5-777898ffcc bf932ebc-5acb-4005-b37c-87e6a1c1888e 0xc0038e10b0 0xc0038e10b1}] [] [{kube-controller-manager Update v1 2023-02-12 12:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf932ebc-5acb-4005-b37c-87e6a1c1888e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:44:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:44:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf6zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf6zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:44:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.69,StartTime:2023-02-12 12:44:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:44:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://79b776721450181654b6d246a31e5c9a690e6d2fb6e239a9cf07ea9ccf133cec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 12:44:06.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7891" for this suite. 02/12/23 12:44:06.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:06.868
Feb 12 12:44:06.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename proxy 02/12/23 12:44:06.869
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:06.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:06.896
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 12 12:44:06.899: INFO: Creating pod...
Feb 12 12:44:06.909: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3583" to be "running"
Feb 12 12:44:06.916: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62068ms
Feb 12 12:44:09.494: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.585347455s
Feb 12 12:44:10.971: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062137024s
Feb 12 12:44:12.927: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018392147s
Feb 12 12:44:14.927: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 8.017441495s
Feb 12 12:44:14.927: INFO: Pod "agnhost" satisfied condition "running"
Feb 12 12:44:14.927: INFO: Creating service...
Feb 12 12:44:14.945: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=DELETE
Feb 12 12:44:14.957: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 12 12:44:14.957: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=OPTIONS
Feb 12 12:44:14.964: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 12 12:44:14.964: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=PATCH
Feb 12 12:44:14.969: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 12 12:44:14.970: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=POST
Feb 12 12:44:14.975: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 12 12:44:14.976: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=PUT
Feb 12 12:44:14.980: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 12 12:44:14.980: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 12 12:44:14.988: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 12 12:44:14.988: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 12 12:44:14.996: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 12 12:44:14.996: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 12 12:44:15.002: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 12 12:44:15.002: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=POST
Feb 12 12:44:15.011: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 12 12:44:15.011: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=PUT
Feb 12 12:44:15.019: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 12 12:44:15.019: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=GET
Feb 12 12:44:15.023: INFO: http.Client request:GET StatusCode:301
Feb 12 12:44:15.023: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=GET
Feb 12 12:44:15.030: INFO: http.Client request:GET StatusCode:301
Feb 12 12:44:15.030: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=HEAD
Feb 12 12:44:15.033: INFO: http.Client request:HEAD StatusCode:301
Feb 12 12:44:15.033: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 12 12:44:15.042: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Feb 12 12:44:15.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3583" for this suite. 02/12/23 12:44:15.049
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":326,"skipped":6070,"failed":0}
------------------------------
 [SLOW TEST] [8.194 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:06.868
    Feb 12 12:44:06.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename proxy 02/12/23 12:44:06.869
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:06.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:06.896
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 12 12:44:06.899: INFO: Creating pod...
    Feb 12 12:44:06.909: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3583" to be "running"
    Feb 12 12:44:06.916: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62068ms
    Feb 12 12:44:09.494: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.585347455s
    Feb 12 12:44:10.971: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062137024s
    Feb 12 12:44:12.927: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018392147s
    Feb 12 12:44:14.927: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 8.017441495s
    Feb 12 12:44:14.927: INFO: Pod "agnhost" satisfied condition "running"
    Feb 12 12:44:14.927: INFO: Creating service...
    Feb 12 12:44:14.945: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=DELETE
    Feb 12 12:44:14.957: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 12 12:44:14.957: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=OPTIONS
    Feb 12 12:44:14.964: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 12 12:44:14.964: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=PATCH
    Feb 12 12:44:14.969: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 12 12:44:14.970: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=POST
    Feb 12 12:44:14.975: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 12 12:44:14.976: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=PUT
    Feb 12 12:44:14.980: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 12 12:44:14.980: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 12 12:44:14.988: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 12 12:44:14.988: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 12 12:44:14.996: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 12 12:44:14.996: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 12 12:44:15.002: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 12 12:44:15.002: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=POST
    Feb 12 12:44:15.011: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 12 12:44:15.011: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 12 12:44:15.019: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 12 12:44:15.019: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=GET
    Feb 12 12:44:15.023: INFO: http.Client request:GET StatusCode:301
    Feb 12 12:44:15.023: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=GET
    Feb 12 12:44:15.030: INFO: http.Client request:GET StatusCode:301
    Feb 12 12:44:15.030: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/pods/agnhost/proxy?method=HEAD
    Feb 12 12:44:15.033: INFO: http.Client request:HEAD StatusCode:301
    Feb 12 12:44:15.033: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3583/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 12 12:44:15.042: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Feb 12 12:44:15.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-3583" for this suite. 02/12/23 12:44:15.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:15.067
Feb 12 12:44:15.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:44:15.068
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:15.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:15.098
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 02/12/23 12:44:15.101
Feb 12 12:44:15.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 create -f -'
Feb 12 12:44:15.290: INFO: stderr: ""
Feb 12 12:44:15.290: INFO: stdout: "pod/pause created\n"
Feb 12 12:44:15.290: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 12 12:44:15.291: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1720" to be "running and ready"
Feb 12 12:44:15.298: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.931923ms
Feb 12 12:44:15.298: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kube-3' to be 'Running' but was 'Pending'
Feb 12 12:44:17.305: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014263397s
Feb 12 12:44:17.305: INFO: Pod "pause" satisfied condition "running and ready"
Feb 12 12:44:17.305: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 02/12/23 12:44:17.305
Feb 12 12:44:17.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 label pods pause testing-label=testing-label-value'
Feb 12 12:44:17.442: INFO: stderr: ""
Feb 12 12:44:17.442: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/12/23 12:44:17.442
Feb 12 12:44:17.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get pod pause -L testing-label'
Feb 12 12:44:17.530: INFO: stderr: ""
Feb 12 12:44:17.530: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/12/23 12:44:17.53
Feb 12 12:44:17.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 label pods pause testing-label-'
Feb 12 12:44:17.598: INFO: stderr: ""
Feb 12 12:44:17.598: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/12/23 12:44:17.598
Feb 12 12:44:17.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get pod pause -L testing-label'
Feb 12 12:44:17.653: INFO: stderr: ""
Feb 12 12:44:17.653: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 02/12/23 12:44:17.653
Feb 12 12:44:17.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 delete --grace-period=0 --force -f -'
Feb 12 12:44:17.723: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:44:17.724: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 12 12:44:17.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get rc,svc -l name=pause --no-headers'
Feb 12 12:44:17.784: INFO: stderr: "No resources found in kubectl-1720 namespace.\n"
Feb 12 12:44:17.784: INFO: stdout: ""
Feb 12 12:44:17.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 12 12:44:17.834: INFO: stderr: ""
Feb 12 12:44:17.834: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:44:17.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1720" for this suite. 02/12/23 12:44:17.838
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":327,"skipped":6081,"failed":0}
------------------------------
 [2.777 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:15.067
    Feb 12 12:44:15.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:44:15.068
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:15.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:15.098
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 02/12/23 12:44:15.101
    Feb 12 12:44:15.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 create -f -'
    Feb 12 12:44:15.290: INFO: stderr: ""
    Feb 12 12:44:15.290: INFO: stdout: "pod/pause created\n"
    Feb 12 12:44:15.290: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 12 12:44:15.291: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1720" to be "running and ready"
    Feb 12 12:44:15.298: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.931923ms
    Feb 12 12:44:15.298: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'kube-3' to be 'Running' but was 'Pending'
    Feb 12 12:44:17.305: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014263397s
    Feb 12 12:44:17.305: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 12 12:44:17.305: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 02/12/23 12:44:17.305
    Feb 12 12:44:17.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 label pods pause testing-label=testing-label-value'
    Feb 12 12:44:17.442: INFO: stderr: ""
    Feb 12 12:44:17.442: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/12/23 12:44:17.442
    Feb 12 12:44:17.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get pod pause -L testing-label'
    Feb 12 12:44:17.530: INFO: stderr: ""
    Feb 12 12:44:17.530: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/12/23 12:44:17.53
    Feb 12 12:44:17.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 label pods pause testing-label-'
    Feb 12 12:44:17.598: INFO: stderr: ""
    Feb 12 12:44:17.598: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/12/23 12:44:17.598
    Feb 12 12:44:17.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get pod pause -L testing-label'
    Feb 12 12:44:17.653: INFO: stderr: ""
    Feb 12 12:44:17.653: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 02/12/23 12:44:17.653
    Feb 12 12:44:17.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 delete --grace-period=0 --force -f -'
    Feb 12 12:44:17.723: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:44:17.724: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 12 12:44:17.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get rc,svc -l name=pause --no-headers'
    Feb 12 12:44:17.784: INFO: stderr: "No resources found in kubectl-1720 namespace.\n"
    Feb 12 12:44:17.784: INFO: stdout: ""
    Feb 12 12:44:17.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-1720 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 12 12:44:17.834: INFO: stderr: ""
    Feb 12 12:44:17.834: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:44:17.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1720" for this suite. 02/12/23 12:44:17.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:17.844
Feb 12 12:44:17.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename events 02/12/23 12:44:17.845
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:17.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:17.863
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/12/23 12:44:17.865
STEP: get a list of Events with a label in the current namespace 02/12/23 12:44:17.904
STEP: delete a list of events 02/12/23 12:44:17.907
Feb 12 12:44:17.907: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/12/23 12:44:17.933
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Feb 12 12:44:17.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9154" for this suite. 02/12/23 12:44:17.941
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":328,"skipped":6086,"failed":0}
------------------------------
 [0.105 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:17.844
    Feb 12 12:44:17.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename events 02/12/23 12:44:17.845
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:17.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:17.863
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/12/23 12:44:17.865
    STEP: get a list of Events with a label in the current namespace 02/12/23 12:44:17.904
    STEP: delete a list of events 02/12/23 12:44:17.907
    Feb 12 12:44:17.907: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/12/23 12:44:17.933
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Feb 12 12:44:17.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9154" for this suite. 02/12/23 12:44:17.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:17.955
Feb 12 12:44:17.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:44:17.956
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:17.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:17.982
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 02/12/23 12:44:17.984
Feb 12 12:44:17.999: INFO: Waiting up to 5m0s for pod "pod-018dd782-3817-4ddb-8460-606088d2e072" in namespace "emptydir-4288" to be "Succeeded or Failed"
Feb 12 12:44:18.010: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Pending", Reason="", readiness=false. Elapsed: 11.176862ms
Feb 12 12:44:26.169: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Running", Reason="", readiness=true. Elapsed: 8.170818149s
Feb 12 12:44:28.015: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Running", Reason="", readiness=false. Elapsed: 10.016461481s
Feb 12 12:44:30.026: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.027759933s
STEP: Saw pod success 02/12/23 12:44:30.027
Feb 12 12:44:30.028: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072" satisfied condition "Succeeded or Failed"
Feb 12 12:44:30.041: INFO: Trying to get logs from node kube-3 pod pod-018dd782-3817-4ddb-8460-606088d2e072 container test-container: <nil>
STEP: delete the pod 02/12/23 12:44:30.059
Feb 12 12:44:30.077: INFO: Waiting for pod pod-018dd782-3817-4ddb-8460-606088d2e072 to disappear
Feb 12 12:44:30.081: INFO: Pod pod-018dd782-3817-4ddb-8460-606088d2e072 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:44:30.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4288" for this suite. 02/12/23 12:44:30.085
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":329,"skipped":6132,"failed":0}
------------------------------
 [SLOW TEST] [12.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:17.955
    Feb 12 12:44:17.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:44:17.956
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:17.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:17.982
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/12/23 12:44:17.984
    Feb 12 12:44:17.999: INFO: Waiting up to 5m0s for pod "pod-018dd782-3817-4ddb-8460-606088d2e072" in namespace "emptydir-4288" to be "Succeeded or Failed"
    Feb 12 12:44:18.010: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Pending", Reason="", readiness=false. Elapsed: 11.176862ms
    Feb 12 12:44:26.169: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Running", Reason="", readiness=true. Elapsed: 8.170818149s
    Feb 12 12:44:28.015: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Running", Reason="", readiness=false. Elapsed: 10.016461481s
    Feb 12 12:44:30.026: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.027759933s
    STEP: Saw pod success 02/12/23 12:44:30.027
    Feb 12 12:44:30.028: INFO: Pod "pod-018dd782-3817-4ddb-8460-606088d2e072" satisfied condition "Succeeded or Failed"
    Feb 12 12:44:30.041: INFO: Trying to get logs from node kube-3 pod pod-018dd782-3817-4ddb-8460-606088d2e072 container test-container: <nil>
    STEP: delete the pod 02/12/23 12:44:30.059
    Feb 12 12:44:30.077: INFO: Waiting for pod pod-018dd782-3817-4ddb-8460-606088d2e072 to disappear
    Feb 12 12:44:30.081: INFO: Pod pod-018dd782-3817-4ddb-8460-606088d2e072 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:44:30.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4288" for this suite. 02/12/23 12:44:30.085
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:30.093
Feb 12 12:44:30.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename pods 02/12/23 12:44:30.094
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:30.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:30.114
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Feb 12 12:44:30.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: creating the pod 02/12/23 12:44:30.116
STEP: submitting the pod to kubernetes 02/12/23 12:44:30.116
Feb 12 12:44:30.128: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19" in namespace "pods-8295" to be "running and ready"
Feb 12 12:44:30.136: INFO: Pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.400466ms
Feb 12 12:44:30.136: INFO: The phase of Pod pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:44:32.152: INFO: Pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19": Phase="Running", Reason="", readiness=true. Elapsed: 2.0233629s
Feb 12 12:44:32.152: INFO: The phase of Pod pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19 is Running (Ready = true)
Feb 12 12:44:32.152: INFO: Pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Feb 12 12:44:32.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8295" for this suite. 02/12/23 12:44:32.208
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":330,"skipped":6134,"failed":0}
------------------------------
 [2.127 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:30.093
    Feb 12 12:44:30.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename pods 02/12/23 12:44:30.094
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:30.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:30.114
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Feb 12 12:44:30.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: creating the pod 02/12/23 12:44:30.116
    STEP: submitting the pod to kubernetes 02/12/23 12:44:30.116
    Feb 12 12:44:30.128: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19" in namespace "pods-8295" to be "running and ready"
    Feb 12 12:44:30.136: INFO: Pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.400466ms
    Feb 12 12:44:30.136: INFO: The phase of Pod pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:44:32.152: INFO: Pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19": Phase="Running", Reason="", readiness=true. Elapsed: 2.0233629s
    Feb 12 12:44:32.152: INFO: The phase of Pod pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19 is Running (Ready = true)
    Feb 12 12:44:32.152: INFO: Pod "pod-logs-websocket-d1734aaa-98ee-4c6b-b9e1-41daef552f19" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Feb 12 12:44:32.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8295" for this suite. 02/12/23 12:44:32.208
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:44:32.22
Feb 12 12:44:32.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename taint-single-pod 02/12/23 12:44:32.221
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:32.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:32.242
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Feb 12 12:44:32.245: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 12:45:32.286: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Feb 12 12:45:32.291: INFO: Starting informer...
STEP: Starting pod... 02/12/23 12:45:32.291
Feb 12 12:45:32.513: INFO: Pod is running on kube-3. Tainting Node
STEP: Trying to apply a taint on the Node 02/12/23 12:45:32.513
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:45:32.571
STEP: Waiting short time to make sure Pod is queued for deletion 02/12/23 12:45:32.6
Feb 12 12:45:32.600: INFO: Pod wasn't evicted. Proceeding
Feb 12 12:45:32.600: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:45:32.631
STEP: Waiting some time to make sure that toleration time passed. 02/12/23 12:45:32.652
Feb 12 12:46:47.654: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:46:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-931" for this suite. 02/12/23 12:46:47.661
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":331,"skipped":6134,"failed":0}
------------------------------
 [SLOW TEST] [135.449 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:44:32.22
    Feb 12 12:44:32.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename taint-single-pod 02/12/23 12:44:32.221
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:44:32.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:44:32.242
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Feb 12 12:44:32.245: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 12 12:45:32.286: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Feb 12 12:45:32.291: INFO: Starting informer...
    STEP: Starting pod... 02/12/23 12:45:32.291
    Feb 12 12:45:32.513: INFO: Pod is running on kube-3. Tainting Node
    STEP: Trying to apply a taint on the Node 02/12/23 12:45:32.513
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:45:32.571
    STEP: Waiting short time to make sure Pod is queued for deletion 02/12/23 12:45:32.6
    Feb 12 12:45:32.600: INFO: Pod wasn't evicted. Proceeding
    Feb 12 12:45:32.600: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:45:32.631
    STEP: Waiting some time to make sure that toleration time passed. 02/12/23 12:45:32.652
    Feb 12 12:46:47.654: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:46:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-931" for this suite. 02/12/23 12:46:47.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:46:47.672
Feb 12 12:46:47.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 12:46:47.673
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:46:47.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:46:47.691
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Feb 12 12:46:47.700: INFO: Waiting up to 2m0s for pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" in namespace "var-expansion-2218" to be "container 0 failed with reason CreateContainerConfigError"
Feb 12 12:46:47.709: INFO: Pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996041ms
Feb 12 12:46:49.722: INFO: Pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0220522s
Feb 12 12:46:49.722: INFO: Pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 12 12:46:49.722: INFO: Deleting pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" in namespace "var-expansion-2218"
Feb 12 12:46:49.742: INFO: Wait up to 5m0s for pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 12:46:53.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2218" for this suite. 02/12/23 12:46:53.767
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":332,"skipped":6156,"failed":0}
------------------------------
 [SLOW TEST] [6.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:46:47.672
    Feb 12 12:46:47.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 12:46:47.673
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:46:47.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:46:47.691
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Feb 12 12:46:47.700: INFO: Waiting up to 2m0s for pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" in namespace "var-expansion-2218" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 12 12:46:47.709: INFO: Pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996041ms
    Feb 12 12:46:49.722: INFO: Pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0220522s
    Feb 12 12:46:49.722: INFO: Pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 12 12:46:49.722: INFO: Deleting pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" in namespace "var-expansion-2218"
    Feb 12 12:46:49.742: INFO: Wait up to 5m0s for pod "var-expansion-9993f51d-ebe8-4cc6-9ab7-f988fa4741ac" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 12:46:53.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2218" for this suite. 02/12/23 12:46:53.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:46:53.782
Feb 12 12:46:53.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename secrets 02/12/23 12:46:53.782
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:46:53.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:46:53.803
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-54412175-a81f-4aaa-87e5-5fcbddf793c1 02/12/23 12:46:53.805
STEP: Creating a pod to test consume secrets 02/12/23 12:46:53.81
Feb 12 12:46:53.819: INFO: Waiting up to 5m0s for pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427" in namespace "secrets-2462" to be "Succeeded or Failed"
Feb 12 12:46:53.828: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427": Phase="Pending", Reason="", readiness=false. Elapsed: 8.97079ms
Feb 12 12:46:55.836: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017129179s
Feb 12 12:46:57.833: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014022497s
STEP: Saw pod success 02/12/23 12:46:57.833
Feb 12 12:46:57.833: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427" satisfied condition "Succeeded or Failed"
Feb 12 12:46:57.836: INFO: Trying to get logs from node kube-3 pod pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427 container secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:46:57.859
Feb 12 12:46:57.878: INFO: Waiting for pod pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427 to disappear
Feb 12 12:46:57.896: INFO: Pod pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Feb 12 12:46:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2462" for this suite. 02/12/23 12:46:57.908
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":333,"skipped":6167,"failed":0}
------------------------------
 [4.135 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:46:53.782
    Feb 12 12:46:53.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename secrets 02/12/23 12:46:53.782
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:46:53.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:46:53.803
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-54412175-a81f-4aaa-87e5-5fcbddf793c1 02/12/23 12:46:53.805
    STEP: Creating a pod to test consume secrets 02/12/23 12:46:53.81
    Feb 12 12:46:53.819: INFO: Waiting up to 5m0s for pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427" in namespace "secrets-2462" to be "Succeeded or Failed"
    Feb 12 12:46:53.828: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427": Phase="Pending", Reason="", readiness=false. Elapsed: 8.97079ms
    Feb 12 12:46:55.836: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017129179s
    Feb 12 12:46:57.833: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014022497s
    STEP: Saw pod success 02/12/23 12:46:57.833
    Feb 12 12:46:57.833: INFO: Pod "pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427" satisfied condition "Succeeded or Failed"
    Feb 12 12:46:57.836: INFO: Trying to get logs from node kube-3 pod pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427 container secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:46:57.859
    Feb 12 12:46:57.878: INFO: Waiting for pod pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427 to disappear
    Feb 12 12:46:57.896: INFO: Pod pod-secrets-87843bd6-3df6-4c36-b7a1-d318ae0b4427 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Feb 12 12:46:57.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2462" for this suite. 02/12/23 12:46:57.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:46:57.922
Feb 12 12:46:57.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:46:57.923
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:46:57.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:46:57.943
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-19b24d0b-98a8-4c4a-89d7-f9fd279abde6 02/12/23 12:46:57.948
STEP: Creating a pod to test consume configMaps 02/12/23 12:46:57.953
Feb 12 12:46:57.962: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613" in namespace "projected-1431" to be "Succeeded or Failed"
Feb 12 12:46:57.967: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 4.641304ms
Feb 12 12:47:00.240: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277925758s
Feb 12 12:47:01.971: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008573679s
Feb 12 12:47:03.983: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020071974s
Feb 12 12:47:05.982: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019158831s
STEP: Saw pod success 02/12/23 12:47:05.982
Feb 12 12:47:05.983: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613" satisfied condition "Succeeded or Failed"
Feb 12 12:47:05.993: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613 container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/12/23 12:47:06.017
Feb 12 12:47:06.041: INFO: Waiting for pod pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613 to disappear
Feb 12 12:47:06.044: INFO: Pod pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Feb 12 12:47:06.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1431" for this suite. 02/12/23 12:47:06.048
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":334,"skipped":6185,"failed":0}
------------------------------
 [SLOW TEST] [8.132 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:46:57.922
    Feb 12 12:46:57.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:46:57.923
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:46:57.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:46:57.943
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-19b24d0b-98a8-4c4a-89d7-f9fd279abde6 02/12/23 12:46:57.948
    STEP: Creating a pod to test consume configMaps 02/12/23 12:46:57.953
    Feb 12 12:46:57.962: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613" in namespace "projected-1431" to be "Succeeded or Failed"
    Feb 12 12:46:57.967: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 4.641304ms
    Feb 12 12:47:00.240: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277925758s
    Feb 12 12:47:01.971: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008573679s
    Feb 12 12:47:03.983: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Pending", Reason="", readiness=false. Elapsed: 6.020071974s
    Feb 12 12:47:05.982: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019158831s
    STEP: Saw pod success 02/12/23 12:47:05.982
    Feb 12 12:47:05.983: INFO: Pod "pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613" satisfied condition "Succeeded or Failed"
    Feb 12 12:47:05.993: INFO: Trying to get logs from node kube-3 pod pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:47:06.017
    Feb 12 12:47:06.041: INFO: Waiting for pod pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613 to disappear
    Feb 12 12:47:06.044: INFO: Pod pod-projected-configmaps-beeb3a49-a854-4c15-9cd9-324056d2c613 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Feb 12 12:47:06.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1431" for this suite. 02/12/23 12:47:06.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:47:06.055
Feb 12 12:47:06.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename init-container 02/12/23 12:47:06.056
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:47:06.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:47:06.075
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 02/12/23 12:47:06.076
Feb 12 12:47:06.076: INFO: PodSpec: initContainers in spec.initContainers
Feb 12 12:47:45.665: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ff9ad5f4-7646-4ddb-8715-234c548bd464", GenerateName:"", Namespace:"init-container-7058", SelfLink:"", UID:"77e6cc24-728d-4f0c-a03f-cdee384a20e7", ResourceVersion:"39450", Generation:0, CreationTimestamp:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"76954985"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"3e6bb3f5bfd5beee55f5d2815270f7157e91a5e49d576f1cae9e78afd17c4885", "cni.projectcalico.org/podIP":"10.233.99.119/32", "cni.projectcalico.org/podIPs":"10.233.99.119/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba6030), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba6060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 12, 12, 47, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba60c0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-2svn2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003275ca0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2svn2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2svn2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2svn2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004b29f18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0033ec8c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004b29fa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004b29fc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004b29fc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004b29fcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e736b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.2.20.103", PodIP:"10.233.99.119", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.99.119"}}, StartTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033eca10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033eca80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://c4f1017d78175b911f7df4c9a579ef4f5d29595c2e86db3b0a6da71160f1fcf9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003275d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003275d00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0038e004f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 12:47:45.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7058" for this suite. 02/12/23 12:47:45.671
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":335,"skipped":6204,"failed":0}
------------------------------
 [SLOW TEST] [39.628 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:47:06.055
    Feb 12 12:47:06.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename init-container 02/12/23 12:47:06.056
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:47:06.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:47:06.075
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 02/12/23 12:47:06.076
    Feb 12 12:47:06.076: INFO: PodSpec: initContainers in spec.initContainers
    Feb 12 12:47:45.665: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ff9ad5f4-7646-4ddb-8715-234c548bd464", GenerateName:"", Namespace:"init-container-7058", SelfLink:"", UID:"77e6cc24-728d-4f0c-a03f-cdee384a20e7", ResourceVersion:"39450", Generation:0, CreationTimestamp:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"76954985"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"3e6bb3f5bfd5beee55f5d2815270f7157e91a5e49d576f1cae9e78afd17c4885", "cni.projectcalico.org/podIP":"10.233.99.119/32", "cni.projectcalico.org/podIPs":"10.233.99.119/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba6030), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba6060), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 12, 12, 47, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000ba60c0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-2svn2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003275ca0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2svn2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2svn2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-2svn2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004b29f18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"kube-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0033ec8c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004b29fa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004b29fc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004b29fc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004b29fcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e736b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.2.20.103", PodIP:"10.233.99.119", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.99.119"}}, StartTime:time.Date(2023, time.February, 12, 12, 47, 6, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033eca10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033eca80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://c4f1017d78175b911f7df4c9a579ef4f5d29595c2e86db3b0a6da71160f1fcf9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003275d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003275d00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc0038e004f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 12:47:45.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7058" for this suite. 02/12/23 12:47:45.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:47:45.684
Feb 12 12:47:45.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename taint-multiple-pods 02/12/23 12:47:45.685
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:47:45.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:47:45.706
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Feb 12 12:47:45.708: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 12:48:45.765: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Feb 12 12:48:45.774: INFO: Starting informer...
STEP: Starting pods... 02/12/23 12:48:45.774
Feb 12 12:48:46.016: INFO: Pod1 is running on kube-3. Tainting Node
Feb 12 12:48:46.240: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3173" to be "running"
Feb 12 12:48:46.243: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151985ms
Feb 12 12:48:48.253: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012743566s
Feb 12 12:48:48.253: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 12 12:48:48.253: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3173" to be "running"
Feb 12 12:48:48.258: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.763132ms
Feb 12 12:48:48.258: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 12 12:48:48.258: INFO: Pod2 is running on kube-3. Tainting Node
STEP: Trying to apply a taint on the Node 02/12/23 12:48:48.258
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:48:48.275
STEP: Waiting for Pod1 and Pod2 to be deleted 02/12/23 12:48:48.282
Feb 12 12:48:54.026: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 12 12:49:14.126: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:49:14.138
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:49:14.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3173" for this suite. 02/12/23 12:49:14.15
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":336,"skipped":6212,"failed":0}
------------------------------
 [SLOW TEST] [88.473 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:47:45.684
    Feb 12 12:47:45.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename taint-multiple-pods 02/12/23 12:47:45.685
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:47:45.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:47:45.706
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Feb 12 12:47:45.708: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 12 12:48:45.765: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Feb 12 12:48:45.774: INFO: Starting informer...
    STEP: Starting pods... 02/12/23 12:48:45.774
    Feb 12 12:48:46.016: INFO: Pod1 is running on kube-3. Tainting Node
    Feb 12 12:48:46.240: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-3173" to be "running"
    Feb 12 12:48:46.243: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151985ms
    Feb 12 12:48:48.253: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012743566s
    Feb 12 12:48:48.253: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 12 12:48:48.253: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-3173" to be "running"
    Feb 12 12:48:48.258: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.763132ms
    Feb 12 12:48:48.258: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 12 12:48:48.258: INFO: Pod2 is running on kube-3. Tainting Node
    STEP: Trying to apply a taint on the Node 02/12/23 12:48:48.258
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:48:48.275
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/12/23 12:48:48.282
    Feb 12 12:48:54.026: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 12 12:49:14.126: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/12/23 12:49:14.138
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:49:14.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-3173" for this suite. 02/12/23 12:49:14.15
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:14.158
Feb 12 12:49:14.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:49:14.16
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:14.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:14.179
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 02/12/23 12:49:14.181
Feb 12 12:49:14.189: INFO: Waiting up to 5m0s for pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2" in namespace "projected-7595" to be "running and ready"
Feb 12 12:49:14.193: INFO: Pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825024ms
Feb 12 12:49:14.193: INFO: The phase of Pod labelsupdate625f4da8-b408-4780-941f-4b845d7705c2 is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:49:16.209: INFO: Pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019810777s
Feb 12 12:49:16.209: INFO: The phase of Pod labelsupdate625f4da8-b408-4780-941f-4b845d7705c2 is Running (Ready = true)
Feb 12 12:49:16.209: INFO: Pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2" satisfied condition "running and ready"
Feb 12 12:49:16.796: INFO: Successfully updated pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 12:49:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7595" for this suite. 02/12/23 12:49:20.861
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":337,"skipped":6214,"failed":0}
------------------------------
 [SLOW TEST] [6.714 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:14.158
    Feb 12 12:49:14.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:49:14.16
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:14.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:14.179
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 02/12/23 12:49:14.181
    Feb 12 12:49:14.189: INFO: Waiting up to 5m0s for pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2" in namespace "projected-7595" to be "running and ready"
    Feb 12 12:49:14.193: INFO: Pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825024ms
    Feb 12 12:49:14.193: INFO: The phase of Pod labelsupdate625f4da8-b408-4780-941f-4b845d7705c2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:49:16.209: INFO: Pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.019810777s
    Feb 12 12:49:16.209: INFO: The phase of Pod labelsupdate625f4da8-b408-4780-941f-4b845d7705c2 is Running (Ready = true)
    Feb 12 12:49:16.209: INFO: Pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2" satisfied condition "running and ready"
    Feb 12 12:49:16.796: INFO: Successfully updated pod "labelsupdate625f4da8-b408-4780-941f-4b845d7705c2"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 12:49:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7595" for this suite. 02/12/23 12:49:20.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:20.889
Feb 12 12:49:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:49:20.891
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:20.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:20.913
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 02/12/23 12:49:20.915
Feb 12 12:49:20.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 12 12:49:20.985: INFO: stderr: ""
Feb 12 12:49:20.985: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 02/12/23 12:49:20.985
Feb 12 12:49:20.985: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 12 12:49:20.985: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5333" to be "running and ready, or succeeded"
Feb 12 12:49:20.990: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453521ms
Feb 12 12:49:20.990: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kube-3' to be 'Running' but was 'Pending'
Feb 12 12:49:23.141: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.155814804s
Feb 12 12:49:23.141: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 12 12:49:23.141: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/12/23 12:49:23.141
Feb 12 12:49:23.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator'
Feb 12 12:49:23.206: INFO: stderr: ""
Feb 12 12:49:23.206: INFO: stdout: "I0212 12:49:21.707965       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/fbzf 329\nI0212 12:49:21.908111       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/5cg 253\nI0212 12:49:22.108541       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/8xc 254\nI0212 12:49:22.309075       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/hbt 216\nI0212 12:49:22.508714       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kp47 544\nI0212 12:49:22.708140       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g6v 484\nI0212 12:49:22.908556       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/qndz 593\nI0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\n"
STEP: limiting log lines 02/12/23 12:49:23.206
Feb 12 12:49:23.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --tail=1'
Feb 12 12:49:23.268: INFO: stderr: ""
Feb 12 12:49:23.268: INFO: stdout: "I0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\n"
Feb 12 12:49:23.268: INFO: got output "I0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\n"
STEP: limiting log bytes 02/12/23 12:49:23.268
Feb 12 12:49:23.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --limit-bytes=1'
Feb 12 12:49:23.355: INFO: stderr: ""
Feb 12 12:49:23.355: INFO: stdout: "I"
Feb 12 12:49:23.355: INFO: got output "I"
STEP: exposing timestamps 02/12/23 12:49:23.355
Feb 12 12:49:23.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 12 12:49:23.409: INFO: stderr: ""
Feb 12 12:49:23.409: INFO: stdout: "2023-02-12T12:49:23.308993833Z I0212 12:49:23.308839       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4smw 451\n"
Feb 12 12:49:23.409: INFO: got output "2023-02-12T12:49:23.308993833Z I0212 12:49:23.308839       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4smw 451\n"
STEP: restricting to a time range 02/12/23 12:49:23.409
Feb 12 12:49:25.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --since=1s'
Feb 12 12:49:26.121: INFO: stderr: ""
Feb 12 12:49:26.121: INFO: stdout: "I0212 12:49:25.308644       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8kn 575\nI0212 12:49:25.508129       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/s7rx 522\nI0212 12:49:25.708761       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5fp 260\nI0212 12:49:25.908485       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/fnpz 425\nI0212 12:49:26.108970       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/mxlr 495\n"
Feb 12 12:49:26.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --since=24h'
Feb 12 12:49:26.203: INFO: stderr: ""
Feb 12 12:49:26.203: INFO: stdout: "I0212 12:49:21.707965       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/fbzf 329\nI0212 12:49:21.908111       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/5cg 253\nI0212 12:49:22.108541       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/8xc 254\nI0212 12:49:22.309075       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/hbt 216\nI0212 12:49:22.508714       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kp47 544\nI0212 12:49:22.708140       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g6v 484\nI0212 12:49:22.908556       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/qndz 593\nI0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\nI0212 12:49:23.308839       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4smw 451\nI0212 12:49:23.508258       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/4ln 565\nI0212 12:49:23.708894       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/8pr 348\nI0212 12:49:23.908250       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/xbm 539\nI0212 12:49:24.108775       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/q6f 382\nI0212 12:49:24.308215       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/5jt 412\nI0212 12:49:24.508881       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/r96 337\nI0212 12:49:24.708208       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/nqf2 566\nI0212 12:49:24.908738       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/w2fd 343\nI0212 12:49:25.108118       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/2xxw 299\nI0212 12:49:25.308644       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8kn 575\nI0212 12:49:25.508129       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/s7rx 522\nI0212 12:49:25.708761       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5fp 260\nI0212 12:49:25.908485       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/fnpz 425\nI0212 12:49:26.108970       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/mxlr 495\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Feb 12 12:49:26.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 delete pod logs-generator'
Feb 12 12:49:27.181: INFO: stderr: ""
Feb 12 12:49:27.181: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:49:27.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5333" for this suite. 02/12/23 12:49:27.185
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":338,"skipped":6268,"failed":0}
------------------------------
 [SLOW TEST] [6.308 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:20.889
    Feb 12 12:49:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:49:20.891
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:20.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:20.913
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 02/12/23 12:49:20.915
    Feb 12 12:49:20.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 12 12:49:20.985: INFO: stderr: ""
    Feb 12 12:49:20.985: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 02/12/23 12:49:20.985
    Feb 12 12:49:20.985: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 12 12:49:20.985: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5333" to be "running and ready, or succeeded"
    Feb 12 12:49:20.990: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453521ms
    Feb 12 12:49:20.990: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'kube-3' to be 'Running' but was 'Pending'
    Feb 12 12:49:23.141: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.155814804s
    Feb 12 12:49:23.141: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 12 12:49:23.141: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/12/23 12:49:23.141
    Feb 12 12:49:23.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator'
    Feb 12 12:49:23.206: INFO: stderr: ""
    Feb 12 12:49:23.206: INFO: stdout: "I0212 12:49:21.707965       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/fbzf 329\nI0212 12:49:21.908111       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/5cg 253\nI0212 12:49:22.108541       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/8xc 254\nI0212 12:49:22.309075       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/hbt 216\nI0212 12:49:22.508714       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kp47 544\nI0212 12:49:22.708140       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g6v 484\nI0212 12:49:22.908556       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/qndz 593\nI0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\n"
    STEP: limiting log lines 02/12/23 12:49:23.206
    Feb 12 12:49:23.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --tail=1'
    Feb 12 12:49:23.268: INFO: stderr: ""
    Feb 12 12:49:23.268: INFO: stdout: "I0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\n"
    Feb 12 12:49:23.268: INFO: got output "I0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\n"
    STEP: limiting log bytes 02/12/23 12:49:23.268
    Feb 12 12:49:23.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --limit-bytes=1'
    Feb 12 12:49:23.355: INFO: stderr: ""
    Feb 12 12:49:23.355: INFO: stdout: "I"
    Feb 12 12:49:23.355: INFO: got output "I"
    STEP: exposing timestamps 02/12/23 12:49:23.355
    Feb 12 12:49:23.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 12 12:49:23.409: INFO: stderr: ""
    Feb 12 12:49:23.409: INFO: stdout: "2023-02-12T12:49:23.308993833Z I0212 12:49:23.308839       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4smw 451\n"
    Feb 12 12:49:23.409: INFO: got output "2023-02-12T12:49:23.308993833Z I0212 12:49:23.308839       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4smw 451\n"
    STEP: restricting to a time range 02/12/23 12:49:23.409
    Feb 12 12:49:25.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --since=1s'
    Feb 12 12:49:26.121: INFO: stderr: ""
    Feb 12 12:49:26.121: INFO: stdout: "I0212 12:49:25.308644       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8kn 575\nI0212 12:49:25.508129       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/s7rx 522\nI0212 12:49:25.708761       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5fp 260\nI0212 12:49:25.908485       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/fnpz 425\nI0212 12:49:26.108970       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/mxlr 495\n"
    Feb 12 12:49:26.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 logs logs-generator logs-generator --since=24h'
    Feb 12 12:49:26.203: INFO: stderr: ""
    Feb 12 12:49:26.203: INFO: stdout: "I0212 12:49:21.707965       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/fbzf 329\nI0212 12:49:21.908111       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/5cg 253\nI0212 12:49:22.108541       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/8xc 254\nI0212 12:49:22.309075       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/hbt 216\nI0212 12:49:22.508714       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kp47 544\nI0212 12:49:22.708140       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/g6v 484\nI0212 12:49:22.908556       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/qndz 593\nI0212 12:49:23.108936       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/hxls 437\nI0212 12:49:23.308839       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/4smw 451\nI0212 12:49:23.508258       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/4ln 565\nI0212 12:49:23.708894       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/8pr 348\nI0212 12:49:23.908250       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/xbm 539\nI0212 12:49:24.108775       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/q6f 382\nI0212 12:49:24.308215       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/5jt 412\nI0212 12:49:24.508881       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/r96 337\nI0212 12:49:24.708208       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/nqf2 566\nI0212 12:49:24.908738       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/w2fd 343\nI0212 12:49:25.108118       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/2xxw 299\nI0212 12:49:25.308644       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8kn 575\nI0212 12:49:25.508129       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/s7rx 522\nI0212 12:49:25.708761       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/5fp 260\nI0212 12:49:25.908485       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/fnpz 425\nI0212 12:49:26.108970       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/mxlr 495\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Feb 12 12:49:26.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-5333 delete pod logs-generator'
    Feb 12 12:49:27.181: INFO: stderr: ""
    Feb 12 12:49:27.181: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:49:27.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5333" for this suite. 02/12/23 12:49:27.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:27.197
Feb 12 12:49:27.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:49:27.198
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:27.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:27.224
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/12/23 12:49:27.227
Feb 12 12:49:27.237: INFO: Waiting up to 5m0s for pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1" in namespace "emptydir-1449" to be "Succeeded or Failed"
Feb 12 12:49:27.246: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.856485ms
Feb 12 12:49:29.260: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022796992s
Feb 12 12:49:31.261: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02319168s
STEP: Saw pod success 02/12/23 12:49:31.261
Feb 12 12:49:31.261: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1" satisfied condition "Succeeded or Failed"
Feb 12 12:49:31.273: INFO: Trying to get logs from node kube-3 pod pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1 container test-container: <nil>
STEP: delete the pod 02/12/23 12:49:31.292
Feb 12 12:49:31.313: INFO: Waiting for pod pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1 to disappear
Feb 12 12:49:31.316: INFO: Pod pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:49:31.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1449" for this suite. 02/12/23 12:49:31.319
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":339,"skipped":6273,"failed":0}
------------------------------
 [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:27.197
    Feb 12 12:49:27.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:49:27.198
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:27.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:27.224
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/12/23 12:49:27.227
    Feb 12 12:49:27.237: INFO: Waiting up to 5m0s for pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1" in namespace "emptydir-1449" to be "Succeeded or Failed"
    Feb 12 12:49:27.246: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.856485ms
    Feb 12 12:49:29.260: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022796992s
    Feb 12 12:49:31.261: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02319168s
    STEP: Saw pod success 02/12/23 12:49:31.261
    Feb 12 12:49:31.261: INFO: Pod "pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1" satisfied condition "Succeeded or Failed"
    Feb 12 12:49:31.273: INFO: Trying to get logs from node kube-3 pod pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1 container test-container: <nil>
    STEP: delete the pod 02/12/23 12:49:31.292
    Feb 12 12:49:31.313: INFO: Waiting for pod pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1 to disappear
    Feb 12 12:49:31.316: INFO: Pod pod-3ae8ab3d-33e2-42ec-b125-36866913f9b1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:49:31.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1449" for this suite. 02/12/23 12:49:31.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:31.328
Feb 12 12:49:31.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename ephemeral-containers-test 02/12/23 12:49:31.329
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:31.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:31.346
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/12/23 12:49:31.348
Feb 12 12:49:31.360: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-900" to be "running and ready"
Feb 12 12:49:31.365: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.657401ms
Feb 12 12:49:31.365: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:49:33.375: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015687732s
Feb 12 12:49:33.375: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 12 12:49:33.376: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/12/23 12:49:33.38
Feb 12 12:49:33.398: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-900" to be "container debugger running"
Feb 12 12:49:33.402: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.259973ms
Feb 12 12:49:35.406: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007727433s
Feb 12 12:49:35.406: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/12/23 12:49:35.406
Feb 12 12:49:35.406: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-900 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:49:35.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:49:35.407: INFO: ExecWithOptions: Clientset creation
Feb 12 12:49:35.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-900/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 12 12:49:35.494: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Feb 12 12:49:35.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-900" for this suite. 02/12/23 12:49:35.507
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":340,"skipped":6289,"failed":0}
------------------------------
 [4.188 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:31.328
    Feb 12 12:49:31.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/12/23 12:49:31.329
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:31.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:31.346
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/12/23 12:49:31.348
    Feb 12 12:49:31.360: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-900" to be "running and ready"
    Feb 12 12:49:31.365: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.657401ms
    Feb 12 12:49:31.365: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:49:33.375: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015687732s
    Feb 12 12:49:33.375: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 12 12:49:33.376: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/12/23 12:49:33.38
    Feb 12 12:49:33.398: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-900" to be "container debugger running"
    Feb 12 12:49:33.402: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.259973ms
    Feb 12 12:49:35.406: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007727433s
    Feb 12 12:49:35.406: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/12/23 12:49:35.406
    Feb 12 12:49:35.406: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-900 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:49:35.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:49:35.407: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:49:35.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-900/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 12 12:49:35.494: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Feb 12 12:49:35.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-900" for this suite. 02/12/23 12:49:35.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:35.517
Feb 12 12:49:35.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:49:35.517
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:35.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:35.537
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:49:35.554
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:49:35.742
STEP: Deploying the webhook pod 02/12/23 12:49:35.75
STEP: Wait for the deployment to be ready 02/12/23 12:49:35.762
Feb 12 12:49:35.779: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 12 12:49:37.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/12/23 12:49:39.816
STEP: Verifying the service has paired with the endpoint 02/12/23 12:49:39.831
Feb 12 12:49:40.831: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Feb 12 12:49:40.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8026-crds.webhook.example.com via the AdmissionRegistration API 02/12/23 12:49:46.346
STEP: Creating a custom resource while v1 is storage version 02/12/23 12:49:46.361
STEP: Patching Custom Resource Definition to set v2 as storage 02/12/23 12:49:48.413
STEP: Patching the custom resource while v2 is storage version 02/12/23 12:49:48.434
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:49:49.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8049" for this suite. 02/12/23 12:49:49.034
STEP: Destroying namespace "webhook-8049-markers" for this suite. 02/12/23 12:49:49.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":341,"skipped":6308,"failed":0}
------------------------------
 [SLOW TEST] [13.623 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:35.517
    Feb 12 12:49:35.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:49:35.517
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:35.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:35.537
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:49:35.554
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:49:35.742
    STEP: Deploying the webhook pod 02/12/23 12:49:35.75
    STEP: Wait for the deployment to be ready 02/12/23 12:49:35.762
    Feb 12 12:49:35.779: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Feb 12 12:49:37.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 12, 12, 49, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/12/23 12:49:39.816
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:49:39.831
    Feb 12 12:49:40.831: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Feb 12 12:49:40.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8026-crds.webhook.example.com via the AdmissionRegistration API 02/12/23 12:49:46.346
    STEP: Creating a custom resource while v1 is storage version 02/12/23 12:49:46.361
    STEP: Patching Custom Resource Definition to set v2 as storage 02/12/23 12:49:48.413
    STEP: Patching the custom resource while v2 is storage version 02/12/23 12:49:48.434
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:49:49.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8049" for this suite. 02/12/23 12:49:49.034
    STEP: Destroying namespace "webhook-8049-markers" for this suite. 02/12/23 12:49:49.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:49.146
Feb 12 12:49:49.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:49:49.147
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:49.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:49.191
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-70b2f5b8-13a7-4960-a8eb-e2cec5efff02 02/12/23 12:49:49.197
STEP: Creating a pod to test consume secrets 02/12/23 12:49:49.212
Feb 12 12:49:49.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f" in namespace "projected-6400" to be "Succeeded or Failed"
Feb 12 12:49:49.234: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233882ms
Feb 12 12:49:51.246: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019477062s
Feb 12 12:49:53.245: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018365444s
STEP: Saw pod success 02/12/23 12:49:53.245
Feb 12 12:49:53.246: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f" satisfied condition "Succeeded or Failed"
Feb 12 12:49:53.256: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f container projected-secret-volume-test: <nil>
STEP: delete the pod 02/12/23 12:49:53.275
Feb 12 12:49:53.298: INFO: Waiting for pod pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f to disappear
Feb 12 12:49:53.301: INFO: Pod pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Feb 12 12:49:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6400" for this suite. 02/12/23 12:49:53.306
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6334,"failed":0}
------------------------------
 [4.178 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:49.146
    Feb 12 12:49:49.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:49:49.147
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:49.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:49.191
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-70b2f5b8-13a7-4960-a8eb-e2cec5efff02 02/12/23 12:49:49.197
    STEP: Creating a pod to test consume secrets 02/12/23 12:49:49.212
    Feb 12 12:49:49.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f" in namespace "projected-6400" to be "Succeeded or Failed"
    Feb 12 12:49:49.234: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233882ms
    Feb 12 12:49:51.246: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019477062s
    Feb 12 12:49:53.245: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018365444s
    STEP: Saw pod success 02/12/23 12:49:53.245
    Feb 12 12:49:53.246: INFO: Pod "pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f" satisfied condition "Succeeded or Failed"
    Feb 12 12:49:53.256: INFO: Trying to get logs from node kube-3 pod pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:49:53.275
    Feb 12 12:49:53.298: INFO: Waiting for pod pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f to disappear
    Feb 12 12:49:53.301: INFO: Pod pod-projected-secrets-8614c362-818c-4987-8ee0-2ab9a635730f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Feb 12 12:49:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6400" for this suite. 02/12/23 12:49:53.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:49:53.325
Feb 12 12:49:53.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:49:53.326
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:53.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:53.346
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-6174 02/12/23 12:49:53.349
STEP: creating service affinity-clusterip in namespace services-6174 02/12/23 12:49:53.349
STEP: creating replication controller affinity-clusterip in namespace services-6174 02/12/23 12:49:53.373
I0212 12:49:53.387425      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6174, replica count: 3
I0212 12:49:56.439878      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0212 12:49:59.440831      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 12:49:59.464: INFO: Creating new exec pod
Feb 12 12:49:59.478: INFO: Waiting up to 5m0s for pod "execpod-affinity65kk7" in namespace "services-6174" to be "running"
Feb 12 12:49:59.483: INFO: Pod "execpod-affinity65kk7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.185139ms
Feb 12 12:50:01.498: INFO: Pod "execpod-affinity65kk7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019119282s
Feb 12 12:50:01.498: INFO: Pod "execpod-affinity65kk7" satisfied condition "running"
Feb 12 12:50:02.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6174 exec execpod-affinity65kk7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Feb 12 12:50:02.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 12 12:50:02.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:50:02.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6174 exec execpod-affinity65kk7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.22.71 80'
Feb 12 12:50:03.005: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.22.71 80\nConnection to 10.233.22.71 80 port [tcp/http] succeeded!\n"
Feb 12 12:50:03.006: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:50:03.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6174 exec execpod-affinity65kk7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.22.71:80/ ; done'
Feb 12 12:50:03.201: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n"
Feb 12 12:50:03.201: INFO: stdout: "\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst"
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
Feb 12 12:50:03.201: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6174, will wait for the garbage collector to delete the pods 02/12/23 12:50:03.216
Feb 12 12:50:03.286: INFO: Deleting ReplicationController affinity-clusterip took: 10.06162ms
Feb 12 12:50:03.387: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.761896ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:50:06.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6174" for this suite. 02/12/23 12:50:06.021
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":343,"skipped":6363,"failed":0}
------------------------------
 [SLOW TEST] [12.707 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:49:53.325
    Feb 12 12:49:53.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:49:53.326
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:49:53.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:49:53.346
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-6174 02/12/23 12:49:53.349
    STEP: creating service affinity-clusterip in namespace services-6174 02/12/23 12:49:53.349
    STEP: creating replication controller affinity-clusterip in namespace services-6174 02/12/23 12:49:53.373
    I0212 12:49:53.387425      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6174, replica count: 3
    I0212 12:49:56.439878      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0212 12:49:59.440831      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 12:49:59.464: INFO: Creating new exec pod
    Feb 12 12:49:59.478: INFO: Waiting up to 5m0s for pod "execpod-affinity65kk7" in namespace "services-6174" to be "running"
    Feb 12 12:49:59.483: INFO: Pod "execpod-affinity65kk7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.185139ms
    Feb 12 12:50:01.498: INFO: Pod "execpod-affinity65kk7": Phase="Running", Reason="", readiness=true. Elapsed: 2.019119282s
    Feb 12 12:50:01.498: INFO: Pod "execpod-affinity65kk7" satisfied condition "running"
    Feb 12 12:50:02.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6174 exec execpod-affinity65kk7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Feb 12 12:50:02.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 12 12:50:02.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:50:02.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6174 exec execpod-affinity65kk7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.22.71 80'
    Feb 12 12:50:03.005: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.22.71 80\nConnection to 10.233.22.71 80 port [tcp/http] succeeded!\n"
    Feb 12 12:50:03.006: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:50:03.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6174 exec execpod-affinity65kk7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.22.71:80/ ; done'
    Feb 12 12:50:03.201: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.22.71:80/\n"
    Feb 12 12:50:03.201: INFO: stdout: "\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst\naffinity-clusterip-p6jst"
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Received response from host: affinity-clusterip-p6jst
    Feb 12 12:50:03.201: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6174, will wait for the garbage collector to delete the pods 02/12/23 12:50:03.216
    Feb 12 12:50:03.286: INFO: Deleting ReplicationController affinity-clusterip took: 10.06162ms
    Feb 12 12:50:03.387: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.761896ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:50:06.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6174" for this suite. 02/12/23 12:50:06.021
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:50:06.034
Feb 12 12:50:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:50:06.035
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:50:06.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:50:06.058
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 02/12/23 12:50:06.062
Feb 12 12:50:06.062: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 12 12:50:06.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
Feb 12 12:50:06.763: INFO: stderr: ""
Feb 12 12:50:06.763: INFO: stdout: "service/agnhost-replica created\n"
Feb 12 12:50:06.763: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 12 12:50:06.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
Feb 12 12:50:07.475: INFO: stderr: ""
Feb 12 12:50:07.476: INFO: stdout: "service/agnhost-primary created\n"
Feb 12 12:50:07.476: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 12 12:50:07.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
Feb 12 12:50:09.091: INFO: stderr: ""
Feb 12 12:50:09.091: INFO: stdout: "service/frontend created\n"
Feb 12 12:50:09.092: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 12 12:50:09.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
Feb 12 12:50:10.671: INFO: stderr: ""
Feb 12 12:50:10.672: INFO: stdout: "deployment.apps/frontend created\n"
Feb 12 12:50:10.672: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 12 12:50:10.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
Feb 12 12:50:11.234: INFO: stderr: ""
Feb 12 12:50:11.234: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 12 12:50:11.235: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 12 12:50:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
Feb 12 12:50:11.734: INFO: stderr: ""
Feb 12 12:50:11.734: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/12/23 12:50:11.734
Feb 12 12:50:11.734: INFO: Waiting for all frontend pods to be Running.
Feb 12 12:50:16.785: INFO: Waiting for frontend to serve content.
Feb 12 12:50:16.832: INFO: Trying to add a new entry to the guestbook.
Feb 12 12:50:16.844: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/12/23 12:50:16.853
Feb 12 12:50:16.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
Feb 12 12:50:16.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:50:16.963: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/12/23 12:50:16.963
Feb 12 12:50:16.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
Feb 12 12:50:17.065: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:50:17.065: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/12/23 12:50:17.065
Feb 12 12:50:17.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
Feb 12 12:50:17.170: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:50:17.170: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/12/23 12:50:17.17
Feb 12 12:50:17.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
Feb 12 12:50:17.257: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:50:17.257: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/12/23 12:50:17.257
Feb 12 12:50:17.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
Feb 12 12:50:17.453: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:50:17.453: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/12/23 12:50:17.453
Feb 12 12:50:17.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
Feb 12 12:50:18.264: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 12 12:50:18.264: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:50:18.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-669" for this suite. 02/12/23 12:50:18.554
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":344,"skipped":6390,"failed":0}
------------------------------
 [SLOW TEST] [14.374 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:50:06.034
    Feb 12 12:50:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:50:06.035
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:50:06.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:50:06.058
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 02/12/23 12:50:06.062
    Feb 12 12:50:06.062: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 12 12:50:06.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
    Feb 12 12:50:06.763: INFO: stderr: ""
    Feb 12 12:50:06.763: INFO: stdout: "service/agnhost-replica created\n"
    Feb 12 12:50:06.763: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 12 12:50:06.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
    Feb 12 12:50:07.475: INFO: stderr: ""
    Feb 12 12:50:07.476: INFO: stdout: "service/agnhost-primary created\n"
    Feb 12 12:50:07.476: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 12 12:50:07.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
    Feb 12 12:50:09.091: INFO: stderr: ""
    Feb 12 12:50:09.091: INFO: stdout: "service/frontend created\n"
    Feb 12 12:50:09.092: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 12 12:50:09.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
    Feb 12 12:50:10.671: INFO: stderr: ""
    Feb 12 12:50:10.672: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 12 12:50:10.672: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 12 12:50:10.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
    Feb 12 12:50:11.234: INFO: stderr: ""
    Feb 12 12:50:11.234: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 12 12:50:11.235: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 12 12:50:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 create -f -'
    Feb 12 12:50:11.734: INFO: stderr: ""
    Feb 12 12:50:11.734: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/12/23 12:50:11.734
    Feb 12 12:50:11.734: INFO: Waiting for all frontend pods to be Running.
    Feb 12 12:50:16.785: INFO: Waiting for frontend to serve content.
    Feb 12 12:50:16.832: INFO: Trying to add a new entry to the guestbook.
    Feb 12 12:50:16.844: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/12/23 12:50:16.853
    Feb 12 12:50:16.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
    Feb 12 12:50:16.963: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:50:16.963: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/12/23 12:50:16.963
    Feb 12 12:50:16.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
    Feb 12 12:50:17.065: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:50:17.065: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/12/23 12:50:17.065
    Feb 12 12:50:17.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
    Feb 12 12:50:17.170: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:50:17.170: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/12/23 12:50:17.17
    Feb 12 12:50:17.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
    Feb 12 12:50:17.257: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:50:17.257: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/12/23 12:50:17.257
    Feb 12 12:50:17.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
    Feb 12 12:50:17.453: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:50:17.453: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/12/23 12:50:17.453
    Feb 12 12:50:17.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-669 delete --grace-period=0 --force -f -'
    Feb 12 12:50:18.264: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 12 12:50:18.264: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:50:18.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-669" for this suite. 02/12/23 12:50:18.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:50:20.409
Feb 12 12:50:20.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:50:20.41
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:50:20.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:50:20.901
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-6179 02/12/23 12:50:21.036
Feb 12 12:50:21.182: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6179" to be "running and ready"
Feb 12 12:50:21.193: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 10.682879ms
Feb 12 12:50:21.193: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Feb 12 12:50:23.196: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.013668511s
Feb 12 12:50:23.196: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Feb 12 12:50:23.196: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Feb 12 12:50:23.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Feb 12 12:50:23.322: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Feb 12 12:50:23.322: INFO: stdout: "ipvs"
Feb 12 12:50:23.322: INFO: proxyMode: ipvs
Feb 12 12:50:23.338: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Feb 12 12:50:23.341: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6179 02/12/23 12:50:23.341
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6179 02/12/23 12:50:23.354
I0212 12:50:23.369202      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6179, replica count: 3
I0212 12:50:26.421140      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 12:50:26.448: INFO: Creating new exec pod
Feb 12 12:50:26.462: INFO: Waiting up to 5m0s for pod "execpod-affinitycr5tj" in namespace "services-6179" to be "running"
Feb 12 12:50:26.467: INFO: Pod "execpod-affinitycr5tj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.52944ms
Feb 12 12:50:28.482: INFO: Pod "execpod-affinitycr5tj": Phase="Running", Reason="", readiness=true. Elapsed: 2.020352105s
Feb 12 12:50:28.482: INFO: Pod "execpod-affinitycr5tj" satisfied condition "running"
Feb 12 12:50:29.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Feb 12 12:50:29.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Feb 12 12:50:29.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:50:29.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.31.182 80'
Feb 12 12:50:29.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.31.182 80\nConnection to 10.233.31.182 80 port [tcp/http] succeeded!\n"
Feb 12 12:50:29.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Feb 12 12:50:29.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.31.182:80/ ; done'
Feb 12 12:50:30.001: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n"
Feb 12 12:50:30.001: INFO: stdout: "\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f"
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
Feb 12 12:50:30.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.31.182:80/'
Feb 12 12:50:30.121: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n"
Feb 12 12:50:30.121: INFO: stdout: "affinity-clusterip-timeout-rtz6f"
Feb 12 12:52:40.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.31.182:80/'
Feb 12 12:52:40.428: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n"
Feb 12 12:52:40.428: INFO: stdout: "affinity-clusterip-timeout-2vt67"
Feb 12 12:52:40.428: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6179, will wait for the garbage collector to delete the pods 02/12/23 12:52:40.449
Feb 12 12:52:40.515: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.611857ms
Feb 12 12:52:40.615: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.564221ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:52:46.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6179" for this suite. 02/12/23 12:52:46.211
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":345,"skipped":6415,"failed":0}
------------------------------
 [SLOW TEST] [145.832 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:50:20.409
    Feb 12 12:50:20.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:50:20.41
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:50:20.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:50:20.901
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-6179 02/12/23 12:50:21.036
    Feb 12 12:50:21.182: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6179" to be "running and ready"
    Feb 12 12:50:21.193: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 10.682879ms
    Feb 12 12:50:21.193: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Feb 12 12:50:23.196: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.013668511s
    Feb 12 12:50:23.196: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Feb 12 12:50:23.196: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Feb 12 12:50:23.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Feb 12 12:50:23.322: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Feb 12 12:50:23.322: INFO: stdout: "ipvs"
    Feb 12 12:50:23.322: INFO: proxyMode: ipvs
    Feb 12 12:50:23.338: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Feb 12 12:50:23.341: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-6179 02/12/23 12:50:23.341
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-6179 02/12/23 12:50:23.354
    I0212 12:50:23.369202      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6179, replica count: 3
    I0212 12:50:26.421140      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 12:50:26.448: INFO: Creating new exec pod
    Feb 12 12:50:26.462: INFO: Waiting up to 5m0s for pod "execpod-affinitycr5tj" in namespace "services-6179" to be "running"
    Feb 12 12:50:26.467: INFO: Pod "execpod-affinitycr5tj": Phase="Pending", Reason="", readiness=false. Elapsed: 5.52944ms
    Feb 12 12:50:28.482: INFO: Pod "execpod-affinitycr5tj": Phase="Running", Reason="", readiness=true. Elapsed: 2.020352105s
    Feb 12 12:50:28.482: INFO: Pod "execpod-affinitycr5tj" satisfied condition "running"
    Feb 12 12:50:29.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Feb 12 12:50:29.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Feb 12 12:50:29.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:50:29.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.31.182 80'
    Feb 12 12:50:29.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.31.182 80\nConnection to 10.233.31.182 80 port [tcp/http] succeeded!\n"
    Feb 12 12:50:29.774: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Feb 12 12:50:29.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.31.182:80/ ; done'
    Feb 12 12:50:30.001: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n"
    Feb 12 12:50:30.001: INFO: stdout: "\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f\naffinity-clusterip-timeout-rtz6f"
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Received response from host: affinity-clusterip-timeout-rtz6f
    Feb 12 12:50:30.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.31.182:80/'
    Feb 12 12:50:30.121: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n"
    Feb 12 12:50:30.121: INFO: stdout: "affinity-clusterip-timeout-rtz6f"
    Feb 12 12:52:40.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-6179 exec execpod-affinitycr5tj -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.31.182:80/'
    Feb 12 12:52:40.428: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.31.182:80/\n"
    Feb 12 12:52:40.428: INFO: stdout: "affinity-clusterip-timeout-2vt67"
    Feb 12 12:52:40.428: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6179, will wait for the garbage collector to delete the pods 02/12/23 12:52:40.449
    Feb 12 12:52:40.515: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.611857ms
    Feb 12 12:52:40.615: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.564221ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:52:46.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6179" for this suite. 02/12/23 12:52:46.211
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:52:46.241
Feb 12 12:52:46.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:52:46.242
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:46.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:46.311
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 02/12/23 12:52:46.321
Feb 12 12:52:46.354: INFO: Waiting up to 5m0s for pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b" in namespace "emptydir-6114" to be "Succeeded or Failed"
Feb 12 12:52:46.425: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b": Phase="Pending", Reason="", readiness=false. Elapsed: 71.426855ms
Feb 12 12:52:48.442: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0877064s
Feb 12 12:52:50.437: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08344075s
STEP: Saw pod success 02/12/23 12:52:50.437
Feb 12 12:52:50.438: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b" satisfied condition "Succeeded or Failed"
Feb 12 12:52:50.450: INFO: Trying to get logs from node kube-3 pod pod-4472a451-0232-4dad-953f-6a3a6439de9b container test-container: <nil>
STEP: delete the pod 02/12/23 12:52:50.496
Feb 12 12:52:50.512: INFO: Waiting for pod pod-4472a451-0232-4dad-953f-6a3a6439de9b to disappear
Feb 12 12:52:50.515: INFO: Pod pod-4472a451-0232-4dad-953f-6a3a6439de9b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:52:50.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6114" for this suite. 02/12/23 12:52:50.518
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":346,"skipped":6423,"failed":0}
------------------------------
 [4.284 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:52:46.241
    Feb 12 12:52:46.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:52:46.242
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:46.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:46.311
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/12/23 12:52:46.321
    Feb 12 12:52:46.354: INFO: Waiting up to 5m0s for pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b" in namespace "emptydir-6114" to be "Succeeded or Failed"
    Feb 12 12:52:46.425: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b": Phase="Pending", Reason="", readiness=false. Elapsed: 71.426855ms
    Feb 12 12:52:48.442: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0877064s
    Feb 12 12:52:50.437: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08344075s
    STEP: Saw pod success 02/12/23 12:52:50.437
    Feb 12 12:52:50.438: INFO: Pod "pod-4472a451-0232-4dad-953f-6a3a6439de9b" satisfied condition "Succeeded or Failed"
    Feb 12 12:52:50.450: INFO: Trying to get logs from node kube-3 pod pod-4472a451-0232-4dad-953f-6a3a6439de9b container test-container: <nil>
    STEP: delete the pod 02/12/23 12:52:50.496
    Feb 12 12:52:50.512: INFO: Waiting for pod pod-4472a451-0232-4dad-953f-6a3a6439de9b to disappear
    Feb 12 12:52:50.515: INFO: Pod pod-4472a451-0232-4dad-953f-6a3a6439de9b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:52:50.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6114" for this suite. 02/12/23 12:52:50.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:52:50.526
Feb 12 12:52:50.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename emptydir 02/12/23 12:52:50.527
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:50.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:50.544
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 02/12/23 12:52:50.546
Feb 12 12:52:50.554: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1" in namespace "emptydir-6853" to be "running"
Feb 12 12:52:50.561: INFO: Pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756248ms
Feb 12 12:52:52.574: INFO: Pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1": Phase="Running", Reason="", readiness=false. Elapsed: 2.020215166s
Feb 12 12:52:52.574: INFO: Pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/12/23 12:52:52.574
Feb 12 12:52:52.574: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6853 PodName:pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 12 12:52:52.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
Feb 12 12:52:52.575: INFO: ExecWithOptions: Clientset creation
Feb 12 12:52:52.576: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-6853/pods/pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 12 12:52:52.638: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Feb 12 12:52:52.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6853" for this suite. 02/12/23 12:52:52.644
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":347,"skipped":6443,"failed":0}
------------------------------
 [2.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:52:50.526
    Feb 12 12:52:50.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename emptydir 02/12/23 12:52:50.527
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:50.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:50.544
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 02/12/23 12:52:50.546
    Feb 12 12:52:50.554: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1" in namespace "emptydir-6853" to be "running"
    Feb 12 12:52:50.561: INFO: Pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756248ms
    Feb 12 12:52:52.574: INFO: Pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1": Phase="Running", Reason="", readiness=false. Elapsed: 2.020215166s
    Feb 12 12:52:52.574: INFO: Pod "pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/12/23 12:52:52.574
    Feb 12 12:52:52.574: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6853 PodName:pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 12 12:52:52.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    Feb 12 12:52:52.575: INFO: ExecWithOptions: Clientset creation
    Feb 12 12:52:52.576: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-6853/pods/pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 12 12:52:52.638: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Feb 12 12:52:52.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6853" for this suite. 02/12/23 12:52:52.644
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:52:52.653
Feb 12 12:52:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename replication-controller 02/12/23 12:52:52.653
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:52.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:52.673
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 02/12/23 12:52:52.677
STEP: waiting for RC to be added 02/12/23 12:52:52.684
STEP: waiting for available Replicas 02/12/23 12:52:52.684
STEP: patching ReplicationController 02/12/23 12:52:53.444
STEP: waiting for RC to be modified 02/12/23 12:52:53.454
STEP: patching ReplicationController status 02/12/23 12:52:53.454
STEP: waiting for RC to be modified 02/12/23 12:52:53.459
STEP: waiting for available Replicas 02/12/23 12:52:53.459
STEP: fetching ReplicationController status 02/12/23 12:52:53.464
STEP: patching ReplicationController scale 02/12/23 12:52:53.468
STEP: waiting for RC to be modified 02/12/23 12:52:53.474
STEP: waiting for ReplicationController's scale to be the max amount 02/12/23 12:52:53.474
STEP: fetching ReplicationController; ensuring that it's patched 02/12/23 12:52:54.786
STEP: updating ReplicationController status 02/12/23 12:52:54.789
STEP: waiting for RC to be modified 02/12/23 12:52:54.796
STEP: listing all ReplicationControllers 02/12/23 12:52:54.797
STEP: checking that ReplicationController has expected values 02/12/23 12:52:54.801
STEP: deleting ReplicationControllers by collection 02/12/23 12:52:54.801
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/12/23 12:52:54.81
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Feb 12 12:52:54.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-333" for this suite. 02/12/23 12:52:54.868
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":348,"skipped":6446,"failed":0}
------------------------------
 [2.225 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:52:52.653
    Feb 12 12:52:52.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename replication-controller 02/12/23 12:52:52.653
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:52.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:52.673
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 02/12/23 12:52:52.677
    STEP: waiting for RC to be added 02/12/23 12:52:52.684
    STEP: waiting for available Replicas 02/12/23 12:52:52.684
    STEP: patching ReplicationController 02/12/23 12:52:53.444
    STEP: waiting for RC to be modified 02/12/23 12:52:53.454
    STEP: patching ReplicationController status 02/12/23 12:52:53.454
    STEP: waiting for RC to be modified 02/12/23 12:52:53.459
    STEP: waiting for available Replicas 02/12/23 12:52:53.459
    STEP: fetching ReplicationController status 02/12/23 12:52:53.464
    STEP: patching ReplicationController scale 02/12/23 12:52:53.468
    STEP: waiting for RC to be modified 02/12/23 12:52:53.474
    STEP: waiting for ReplicationController's scale to be the max amount 02/12/23 12:52:53.474
    STEP: fetching ReplicationController; ensuring that it's patched 02/12/23 12:52:54.786
    STEP: updating ReplicationController status 02/12/23 12:52:54.789
    STEP: waiting for RC to be modified 02/12/23 12:52:54.796
    STEP: listing all ReplicationControllers 02/12/23 12:52:54.797
    STEP: checking that ReplicationController has expected values 02/12/23 12:52:54.801
    STEP: deleting ReplicationControllers by collection 02/12/23 12:52:54.801
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/12/23 12:52:54.81
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Feb 12 12:52:54.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-333" for this suite. 02/12/23 12:52:54.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:52:54.88
Feb 12 12:52:54.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-pred 02/12/23 12:52:54.881
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:54.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:54.908
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Feb 12 12:52:54.912: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 12 12:52:54.919: INFO: Waiting for terminating namespaces to be deleted...
Feb 12 12:52:54.922: INFO: 
Logging pods the apiserver thinks is on node kube-1 before test
Feb 12 12:52:54.927: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container calico-node ready: true, restart count 1
Feb 12 12:52:54.927: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container coredns ready: true, restart count 0
Feb 12 12:52:54.927: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container kube-apiserver ready: true, restart count 2
Feb 12 12:52:54.927: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container kube-controller-manager ready: true, restart count 7
Feb 12 12:52:54.927: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 12:52:54.927: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container kube-scheduler ready: true, restart count 6
Feb 12 12:52:54.927: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 12:52:54.927: INFO: rc-test-2qtws from replication-controller-333 started at 2023-02-12 12:52:53 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container rc-test ready: true, restart count 0
Feb 12 12:52:54.927: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 12:52:54.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 12:52:54.927: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 12:52:54.927: INFO: 
Logging pods the apiserver thinks is on node kube-2 before test
Feb 12 12:52:54.933: INFO: calico-kube-controllers-75748cc9fd-mhvb4 from kube-system started at 2023-02-12 12:45:32 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.933: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 12 12:52:54.934: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 12:52:54.934: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container coredns ready: true, restart count 0
Feb 12 12:52:54.934: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container autoscaler ready: true, restart count 0
Feb 12 12:52:54.934: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 12 12:52:54.934: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container kube-controller-manager ready: true, restart count 7
Feb 12 12:52:54.934: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 12:52:54.934: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container kube-scheduler ready: true, restart count 7
Feb 12 12:52:54.934: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 12:52:54.934: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 12:52:54.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 12:52:54.934: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 12 12:52:54.934: INFO: 
Logging pods the apiserver thinks is on node kube-3 before test
Feb 12 12:52:54.940: INFO: pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1 from emptydir-6853 started at 2023-02-12 12:52:50 +0000 UTC (2 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container busybox-main-container ready: true, restart count 0
Feb 12 12:52:54.940: INFO: 	Container busybox-sub-container ready: false, restart count 0
Feb 12 12:52:54.940: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container calico-node ready: true, restart count 0
Feb 12 12:52:54.940: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 12 12:52:54.940: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container nginx-proxy ready: true, restart count 0
Feb 12 12:52:54.940: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container node-cache ready: true, restart count 0
Feb 12 12:52:54.940: INFO: rc-test-ktvp2 from replication-controller-333 started at 2023-02-12 12:52:52 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container rc-test ready: true, restart count 0
Feb 12 12:52:54.940: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 12 12:52:54.940: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container e2e ready: true, restart count 0
Feb 12 12:52:54.940: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 12:52:54.940: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
Feb 12 12:52:54.940: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 12 12:52:54.940: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node kube-1 02/12/23 12:52:54.961
STEP: verifying the node has the label node kube-2 02/12/23 12:52:54.979
STEP: verifying the node has the label node kube-3 02/12/23 12:52:55
Feb 12 12:52:55.026: INFO: Pod pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1 requesting resource cpu=0m on Node kube-3
Feb 12 12:52:55.026: INFO: Pod calico-kube-controllers-75748cc9fd-mhvb4 requesting resource cpu=30m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod calico-node-frzt9 requesting resource cpu=150m on Node kube-3
Feb 12 12:52:55.026: INFO: Pod calico-node-gmxgh requesting resource cpu=150m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod calico-node-p72st requesting resource cpu=150m on Node kube-1
Feb 12 12:52:55.026: INFO: Pod coredns-588bb58b94-2kvfg requesting resource cpu=100m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod coredns-588bb58b94-c4894 requesting resource cpu=100m on Node kube-1
Feb 12 12:52:55.026: INFO: Pod dns-autoscaler-5b9959d7fc-kj96w requesting resource cpu=20m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod kube-apiserver-kube-1 requesting resource cpu=250m on Node kube-1
Feb 12 12:52:55.026: INFO: Pod kube-apiserver-kube-2 requesting resource cpu=250m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod kube-controller-manager-kube-1 requesting resource cpu=200m on Node kube-1
Feb 12 12:52:55.026: INFO: Pod kube-controller-manager-kube-2 requesting resource cpu=200m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod kube-proxy-955lq requesting resource cpu=0m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod kube-proxy-dxsbj requesting resource cpu=0m on Node kube-1
Feb 12 12:52:55.026: INFO: Pod kube-proxy-f2kld requesting resource cpu=0m on Node kube-3
Feb 12 12:52:55.026: INFO: Pod kube-scheduler-kube-1 requesting resource cpu=100m on Node kube-1
Feb 12 12:52:55.026: INFO: Pod kube-scheduler-kube-2 requesting resource cpu=100m on Node kube-2
Feb 12 12:52:55.026: INFO: Pod nginx-proxy-kube-3 requesting resource cpu=25m on Node kube-3
Feb 12 12:52:55.027: INFO: Pod nodelocaldns-6sjhv requesting resource cpu=100m on Node kube-1
Feb 12 12:52:55.027: INFO: Pod nodelocaldns-v9d9w requesting resource cpu=100m on Node kube-2
Feb 12 12:52:55.027: INFO: Pod nodelocaldns-vgzrk requesting resource cpu=100m on Node kube-3
Feb 12 12:52:55.027: INFO: Pod rc-test-2qtws requesting resource cpu=0m on Node kube-1
Feb 12 12:52:55.027: INFO: Pod rc-test-ktvp2 requesting resource cpu=0m on Node kube-3
Feb 12 12:52:55.027: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-3
Feb 12 12:52:55.027: INFO: Pod sonobuoy-e2e-job-97bc94b81c354ee7 requesting resource cpu=0m on Node kube-3
Feb 12 12:52:55.027: INFO: Pod sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv requesting resource cpu=0m on Node kube-1
Feb 12 12:52:55.027: INFO: Pod sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 requesting resource cpu=0m on Node kube-3
Feb 12 12:52:55.027: INFO: Pod sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb requesting resource cpu=0m on Node kube-2
STEP: Starting Pods to consume most of the cluster CPU. 02/12/23 12:52:55.027
Feb 12 12:52:55.027: INFO: Creating a pod which consumes cpu=2170m on Node kube-1
Feb 12 12:52:55.046: INFO: Creating a pod which consumes cpu=2135m on Node kube-2
Feb 12 12:52:55.062: INFO: Creating a pod which consumes cpu=2607m on Node kube-3
Feb 12 12:52:55.083: INFO: Waiting up to 5m0s for pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94" in namespace "sched-pred-6204" to be "running"
Feb 12 12:52:55.092: INFO: Pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94": Phase="Pending", Reason="", readiness=false. Elapsed: 9.061164ms
Feb 12 12:52:57.115: INFO: Pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94": Phase="Running", Reason="", readiness=true. Elapsed: 2.03207156s
Feb 12 12:52:57.115: INFO: Pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94" satisfied condition "running"
Feb 12 12:52:57.116: INFO: Waiting up to 5m0s for pod "filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24" in namespace "sched-pred-6204" to be "running"
Feb 12 12:52:57.128: INFO: Pod "filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24": Phase="Running", Reason="", readiness=true. Elapsed: 11.796702ms
Feb 12 12:52:57.128: INFO: Pod "filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24" satisfied condition "running"
Feb 12 12:52:57.128: INFO: Waiting up to 5m0s for pod "filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61" in namespace "sched-pred-6204" to be "running"
Feb 12 12:52:57.143: INFO: Pod "filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61": Phase="Running", Reason="", readiness=true. Elapsed: 14.138061ms
Feb 12 12:52:57.143: INFO: Pod "filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/12/23 12:52:57.143
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d1348f729e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6204/filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24 to kube-2] 02/12/23 12:52:57.149
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d16181651a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/12/23 12:52:57.149
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d1635efd2f], Reason = [Created], Message = [Created container filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24] 02/12/23 12:52:57.149
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d16ba5aa38], Reason = [Started], Message = [Started container filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24] 02/12/23 12:52:57.15
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d13348cbd9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6204/filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94 to kube-1] 02/12/23 12:52:57.15
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d163d7b26c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/12/23 12:52:57.15
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d16592d547], Reason = [Created], Message = [Created container filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94] 02/12/23 12:52:57.15
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d16d23676f], Reason = [Started], Message = [Started container filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94] 02/12/23 12:52:57.15
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d135d98dd7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6204/filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61 to kube-3] 02/12/23 12:52:57.15
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d163d131aa], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/12/23 12:52:57.151
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d1663ed164], Reason = [Created], Message = [Created container filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61] 02/12/23 12:52:57.151
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d16cb7243c], Reason = [Started], Message = [Started container filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61] 02/12/23 12:52:57.151
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174314d1b090d5a6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 02/12/23 12:52:57.164
STEP: removing the label node off the node kube-3 02/12/23 12:52:58.168
STEP: verifying the node doesn't have the label node 02/12/23 12:52:58.185
STEP: removing the label node off the node kube-1 02/12/23 12:52:58.194
STEP: verifying the node doesn't have the label node 02/12/23 12:52:58.215
STEP: removing the label node off the node kube-2 02/12/23 12:52:58.22
STEP: verifying the node doesn't have the label node 02/12/23 12:52:58.239
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:52:58.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6204" for this suite. 02/12/23 12:52:58.254
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":349,"skipped":6490,"failed":0}
------------------------------
 [3.393 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:52:54.88
    Feb 12 12:52:54.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-pred 02/12/23 12:52:54.881
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:54.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:54.908
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Feb 12 12:52:54.912: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 12 12:52:54.919: INFO: Waiting for terminating namespaces to be deleted...
    Feb 12 12:52:54.922: INFO: 
    Logging pods the apiserver thinks is on node kube-1 before test
    Feb 12 12:52:54.927: INFO: calico-node-p72st from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container calico-node ready: true, restart count 1
    Feb 12 12:52:54.927: INFO: coredns-588bb58b94-c4894 from kube-system started at 2023-02-12 11:01:05 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 12:52:54.927: INFO: kube-apiserver-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container kube-apiserver ready: true, restart count 2
    Feb 12 12:52:54.927: INFO: kube-controller-manager-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container kube-controller-manager ready: true, restart count 7
    Feb 12 12:52:54.927: INFO: kube-proxy-dxsbj from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 12:52:54.927: INFO: kube-scheduler-kube-1 from kube-system started at 2023-02-12 10:55:31 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container kube-scheduler ready: true, restart count 6
    Feb 12 12:52:54.927: INFO: nodelocaldns-6sjhv from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 12:52:54.927: INFO: rc-test-2qtws from replication-controller-333 started at 2023-02-12 12:52:53 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container rc-test ready: true, restart count 0
    Feb 12 12:52:54.927: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 12:52:54.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 12:52:54.927: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 12:52:54.927: INFO: 
    Logging pods the apiserver thinks is on node kube-2 before test
    Feb 12 12:52:54.933: INFO: calico-kube-controllers-75748cc9fd-mhvb4 from kube-system started at 2023-02-12 12:45:32 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.933: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: calico-node-gmxgh from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: coredns-588bb58b94-2kvfg from kube-system started at 2023-02-12 11:02:25 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container coredns ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: dns-autoscaler-5b9959d7fc-kj96w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container autoscaler ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: kube-apiserver-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container kube-apiserver ready: true, restart count 1
    Feb 12 12:52:54.934: INFO: kube-controller-manager-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container kube-controller-manager ready: true, restart count 7
    Feb 12 12:52:54.934: INFO: kube-proxy-955lq from kube-system started at 2023-02-12 10:57:14 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: kube-scheduler-kube-2 from kube-system started at 2023-02-12 10:55:53 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container kube-scheduler ready: true, restart count 7
    Feb 12 12:52:54.934: INFO: nodelocaldns-v9d9w from kube-system started at 2023-02-12 11:01:41 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 12:52:54.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 12 12:52:54.934: INFO: 
    Logging pods the apiserver thinks is on node kube-3 before test
    Feb 12 12:52:54.940: INFO: pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1 from emptydir-6853 started at 2023-02-12 12:52:50 +0000 UTC (2 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container busybox-main-container ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: 	Container busybox-sub-container ready: false, restart count 0
    Feb 12 12:52:54.940: INFO: calico-node-frzt9 from kube-system started at 2023-02-12 10:58:11 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container calico-node ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: kube-proxy-f2kld from kube-system started at 2023-02-12 10:57:18 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container kube-proxy ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: nginx-proxy-kube-3 from kube-system started at 2023-02-12 10:57:00 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container nginx-proxy ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: nodelocaldns-vgzrk from kube-system started at 2023-02-12 11:01:42 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container node-cache ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: rc-test-ktvp2 from replication-controller-333 started at 2023-02-12 12:52:52 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container rc-test ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: sonobuoy from sonobuoy started at 2023-02-12 11:05:44 +0000 UTC (1 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: sonobuoy-e2e-job-97bc94b81c354ee7 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container e2e ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 from sonobuoy started at 2023-02-12 11:05:53 +0000 UTC (2 container statuses recorded)
    Feb 12 12:52:54.940: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 12 12:52:54.940: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node kube-1 02/12/23 12:52:54.961
    STEP: verifying the node has the label node kube-2 02/12/23 12:52:54.979
    STEP: verifying the node has the label node kube-3 02/12/23 12:52:55
    Feb 12 12:52:55.026: INFO: Pod pod-sharedvolume-177b37b9-4cd5-45c5-af5a-1a0c44791eb1 requesting resource cpu=0m on Node kube-3
    Feb 12 12:52:55.026: INFO: Pod calico-kube-controllers-75748cc9fd-mhvb4 requesting resource cpu=30m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod calico-node-frzt9 requesting resource cpu=150m on Node kube-3
    Feb 12 12:52:55.026: INFO: Pod calico-node-gmxgh requesting resource cpu=150m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod calico-node-p72st requesting resource cpu=150m on Node kube-1
    Feb 12 12:52:55.026: INFO: Pod coredns-588bb58b94-2kvfg requesting resource cpu=100m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod coredns-588bb58b94-c4894 requesting resource cpu=100m on Node kube-1
    Feb 12 12:52:55.026: INFO: Pod dns-autoscaler-5b9959d7fc-kj96w requesting resource cpu=20m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod kube-apiserver-kube-1 requesting resource cpu=250m on Node kube-1
    Feb 12 12:52:55.026: INFO: Pod kube-apiserver-kube-2 requesting resource cpu=250m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod kube-controller-manager-kube-1 requesting resource cpu=200m on Node kube-1
    Feb 12 12:52:55.026: INFO: Pod kube-controller-manager-kube-2 requesting resource cpu=200m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod kube-proxy-955lq requesting resource cpu=0m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod kube-proxy-dxsbj requesting resource cpu=0m on Node kube-1
    Feb 12 12:52:55.026: INFO: Pod kube-proxy-f2kld requesting resource cpu=0m on Node kube-3
    Feb 12 12:52:55.026: INFO: Pod kube-scheduler-kube-1 requesting resource cpu=100m on Node kube-1
    Feb 12 12:52:55.026: INFO: Pod kube-scheduler-kube-2 requesting resource cpu=100m on Node kube-2
    Feb 12 12:52:55.026: INFO: Pod nginx-proxy-kube-3 requesting resource cpu=25m on Node kube-3
    Feb 12 12:52:55.027: INFO: Pod nodelocaldns-6sjhv requesting resource cpu=100m on Node kube-1
    Feb 12 12:52:55.027: INFO: Pod nodelocaldns-v9d9w requesting resource cpu=100m on Node kube-2
    Feb 12 12:52:55.027: INFO: Pod nodelocaldns-vgzrk requesting resource cpu=100m on Node kube-3
    Feb 12 12:52:55.027: INFO: Pod rc-test-2qtws requesting resource cpu=0m on Node kube-1
    Feb 12 12:52:55.027: INFO: Pod rc-test-ktvp2 requesting resource cpu=0m on Node kube-3
    Feb 12 12:52:55.027: INFO: Pod sonobuoy requesting resource cpu=0m on Node kube-3
    Feb 12 12:52:55.027: INFO: Pod sonobuoy-e2e-job-97bc94b81c354ee7 requesting resource cpu=0m on Node kube-3
    Feb 12 12:52:55.027: INFO: Pod sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-7x9gv requesting resource cpu=0m on Node kube-1
    Feb 12 12:52:55.027: INFO: Pod sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-dh2g4 requesting resource cpu=0m on Node kube-3
    Feb 12 12:52:55.027: INFO: Pod sonobuoy-systemd-logs-daemon-set-67e042c5cb944d7a-xsccb requesting resource cpu=0m on Node kube-2
    STEP: Starting Pods to consume most of the cluster CPU. 02/12/23 12:52:55.027
    Feb 12 12:52:55.027: INFO: Creating a pod which consumes cpu=2170m on Node kube-1
    Feb 12 12:52:55.046: INFO: Creating a pod which consumes cpu=2135m on Node kube-2
    Feb 12 12:52:55.062: INFO: Creating a pod which consumes cpu=2607m on Node kube-3
    Feb 12 12:52:55.083: INFO: Waiting up to 5m0s for pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94" in namespace "sched-pred-6204" to be "running"
    Feb 12 12:52:55.092: INFO: Pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94": Phase="Pending", Reason="", readiness=false. Elapsed: 9.061164ms
    Feb 12 12:52:57.115: INFO: Pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94": Phase="Running", Reason="", readiness=true. Elapsed: 2.03207156s
    Feb 12 12:52:57.115: INFO: Pod "filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94" satisfied condition "running"
    Feb 12 12:52:57.116: INFO: Waiting up to 5m0s for pod "filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24" in namespace "sched-pred-6204" to be "running"
    Feb 12 12:52:57.128: INFO: Pod "filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24": Phase="Running", Reason="", readiness=true. Elapsed: 11.796702ms
    Feb 12 12:52:57.128: INFO: Pod "filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24" satisfied condition "running"
    Feb 12 12:52:57.128: INFO: Waiting up to 5m0s for pod "filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61" in namespace "sched-pred-6204" to be "running"
    Feb 12 12:52:57.143: INFO: Pod "filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61": Phase="Running", Reason="", readiness=true. Elapsed: 14.138061ms
    Feb 12 12:52:57.143: INFO: Pod "filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/12/23 12:52:57.143
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d1348f729e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6204/filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24 to kube-2] 02/12/23 12:52:57.149
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d16181651a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/12/23 12:52:57.149
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d1635efd2f], Reason = [Created], Message = [Created container filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24] 02/12/23 12:52:57.149
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24.174314d16ba5aa38], Reason = [Started], Message = [Started container filler-pod-cb9a8ccc-d9aa-44d4-8ca0-d7f19d0f3c24] 02/12/23 12:52:57.15
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d13348cbd9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6204/filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94 to kube-1] 02/12/23 12:52:57.15
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d163d7b26c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/12/23 12:52:57.15
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d16592d547], Reason = [Created], Message = [Created container filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94] 02/12/23 12:52:57.15
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94.174314d16d23676f], Reason = [Started], Message = [Started container filler-pod-cddb3164-4247-4de7-a93c-ff2f69730a94] 02/12/23 12:52:57.15
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d135d98dd7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6204/filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61 to kube-3] 02/12/23 12:52:57.15
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d163d131aa], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 02/12/23 12:52:57.151
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d1663ed164], Reason = [Created], Message = [Created container filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61] 02/12/23 12:52:57.151
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61.174314d16cb7243c], Reason = [Started], Message = [Started container filler-pod-f9c9c665-930c-4f48-8d83-76a7529c2c61] 02/12/23 12:52:57.151
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.174314d1b090d5a6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 02/12/23 12:52:57.164
    STEP: removing the label node off the node kube-3 02/12/23 12:52:58.168
    STEP: verifying the node doesn't have the label node 02/12/23 12:52:58.185
    STEP: removing the label node off the node kube-1 02/12/23 12:52:58.194
    STEP: verifying the node doesn't have the label node 02/12/23 12:52:58.215
    STEP: removing the label node off the node kube-2 02/12/23 12:52:58.22
    STEP: verifying the node doesn't have the label node 02/12/23 12:52:58.239
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:52:58.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6204" for this suite. 02/12/23 12:52:58.254
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:52:58.275
Feb 12 12:52:58.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename prestop 02/12/23 12:52:58.276
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:58.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:58.302
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9915 02/12/23 12:52:58.305
STEP: Waiting for pods to come up. 02/12/23 12:52:58.315
Feb 12 12:52:58.315: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9915" to be "running"
Feb 12 12:52:58.322: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649885ms
Feb 12 12:53:00.568: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25310651s
Feb 12 12:53:03.093: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778009913s
Feb 12 12:53:04.426: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 6.110849192s
Feb 12 12:53:04.426: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9915 02/12/23 12:53:04.487
Feb 12 12:53:04.505: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9915" to be "running"
Feb 12 12:53:04.535: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015944ms
Feb 12 12:53:06.549: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.04360766s
Feb 12 12:53:06.549: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/12/23 12:53:06.549
Feb 12 12:53:11.607: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/12/23 12:53:11.608
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Feb 12 12:53:11.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9915" for this suite. 02/12/23 12:53:11.644
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":350,"skipped":6498,"failed":0}
------------------------------
 [SLOW TEST] [13.381 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:52:58.275
    Feb 12 12:52:58.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename prestop 02/12/23 12:52:58.276
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:52:58.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:52:58.302
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9915 02/12/23 12:52:58.305
    STEP: Waiting for pods to come up. 02/12/23 12:52:58.315
    Feb 12 12:52:58.315: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9915" to be "running"
    Feb 12 12:52:58.322: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649885ms
    Feb 12 12:53:00.568: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25310651s
    Feb 12 12:53:03.093: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778009913s
    Feb 12 12:53:04.426: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 6.110849192s
    Feb 12 12:53:04.426: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9915 02/12/23 12:53:04.487
    Feb 12 12:53:04.505: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9915" to be "running"
    Feb 12 12:53:04.535: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015944ms
    Feb 12 12:53:06.549: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.04360766s
    Feb 12 12:53:06.549: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/12/23 12:53:06.549
    Feb 12 12:53:11.607: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/12/23 12:53:11.608
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Feb 12 12:53:11.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9915" for this suite. 02/12/23 12:53:11.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:53:11.661
Feb 12 12:53:11.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename gc 02/12/23 12:53:11.662
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:53:11.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:53:11.686
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/12/23 12:53:11.689
STEP: Wait for the Deployment to create new ReplicaSet 02/12/23 12:53:11.698
STEP: delete the deployment 02/12/23 12:53:12.21
STEP: wait for all rs to be garbage collected 02/12/23 12:53:12.221
STEP: expected 0 rs, got 1 rs 02/12/23 12:53:12.233
STEP: expected 0 pods, got 2 pods 02/12/23 12:53:12.239
STEP: Gathering metrics 02/12/23 12:53:12.764
Feb 12 12:53:12.796: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
Feb 12 12:53:12.801: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.900414ms
Feb 12 12:53:12.801: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
Feb 12 12:53:12.801: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
Feb 12 12:53:12.864: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Feb 12 12:53:12.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9646" for this suite. 02/12/23 12:53:12.869
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":351,"skipped":6510,"failed":0}
------------------------------
 [1.219 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:53:11.661
    Feb 12 12:53:11.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename gc 02/12/23 12:53:11.662
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:53:11.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:53:11.686
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/12/23 12:53:11.689
    STEP: Wait for the Deployment to create new ReplicaSet 02/12/23 12:53:11.698
    STEP: delete the deployment 02/12/23 12:53:12.21
    STEP: wait for all rs to be garbage collected 02/12/23 12:53:12.221
    STEP: expected 0 rs, got 1 rs 02/12/23 12:53:12.233
    STEP: expected 0 pods, got 2 pods 02/12/23 12:53:12.239
    STEP: Gathering metrics 02/12/23 12:53:12.764
    Feb 12 12:53:12.796: INFO: Waiting up to 5m0s for pod "kube-controller-manager-kube-2" in namespace "kube-system" to be "running and ready"
    Feb 12 12:53:12.801: INFO: Pod "kube-controller-manager-kube-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.900414ms
    Feb 12 12:53:12.801: INFO: The phase of Pod kube-controller-manager-kube-2 is Running (Ready = true)
    Feb 12 12:53:12.801: INFO: Pod "kube-controller-manager-kube-2" satisfied condition "running and ready"
    Feb 12 12:53:12.864: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Feb 12 12:53:12.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9646" for this suite. 02/12/23 12:53:12.869
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:53:12.88
Feb 12 12:53:12.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename var-expansion 02/12/23 12:53:12.881
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:53:12.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:53:12.904
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 02/12/23 12:53:12.906
Feb 12 12:53:12.917: INFO: Waiting up to 2m0s for pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" in namespace "var-expansion-6304" to be "running"
Feb 12 12:53:12.924: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 6.892191ms
Feb 12 12:53:14.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016023602s
Feb 12 12:53:16.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013491065s
Feb 12 12:53:18.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019165045s
Feb 12 12:53:20.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017263382s
Feb 12 12:53:22.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013128614s
Feb 12 12:53:24.929: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01233515s
Feb 12 12:53:26.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020555318s
Feb 12 12:53:28.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015731522s
Feb 12 12:53:30.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01929718s
Feb 12 12:53:32.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016200694s
Feb 12 12:53:34.938: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020972026s
Feb 12 12:53:36.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018493004s
Feb 12 12:53:38.949: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 26.032277804s
Feb 12 12:53:40.940: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 28.022888771s
Feb 12 12:53:42.927: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010670224s
Feb 12 12:53:44.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010967689s
Feb 12 12:53:46.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014900905s
Feb 12 12:53:48.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016483392s
Feb 12 12:53:50.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01540997s
Feb 12 12:53:52.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 40.019620781s
Feb 12 12:53:54.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019143949s
Feb 12 12:53:56.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015693886s
Feb 12 12:53:58.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 46.020745655s
Feb 12 12:54:00.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016002537s
Feb 12 12:54:02.931: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 50.014776536s
Feb 12 12:54:04.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010980862s
Feb 12 12:54:06.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019231168s
Feb 12 12:54:08.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017794461s
Feb 12 12:54:10.938: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 58.021456859s
Feb 12 12:54:12.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015534843s
Feb 12 12:54:14.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011429518s
Feb 12 12:54:16.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019998012s
Feb 12 12:54:18.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012859988s
Feb 12 12:54:20.939: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.02212855s
Feb 12 12:54:22.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.017316718s
Feb 12 12:54:24.929: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012396108s
Feb 12 12:54:26.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017246319s
Feb 12 12:54:28.940: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022977968s
Feb 12 12:54:30.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.018087077s
Feb 12 12:54:32.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017840578s
Feb 12 12:54:34.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017384254s
Feb 12 12:54:36.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014864476s
Feb 12 12:54:38.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.017572964s
Feb 12 12:54:40.940: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02356866s
Feb 12 12:54:42.929: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011891764s
Feb 12 12:54:44.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016945509s
Feb 12 12:54:46.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01986963s
Feb 12 12:54:48.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015097592s
Feb 12 12:54:50.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01930835s
Feb 12 12:54:52.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013610531s
Feb 12 12:54:54.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010846849s
Feb 12 12:54:56.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.018553664s
Feb 12 12:54:58.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019209905s
Feb 12 12:55:00.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015633409s
Feb 12 12:55:02.938: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02145765s
Feb 12 12:55:04.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.016103896s
Feb 12 12:55:06.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.020393284s
Feb 12 12:55:08.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019254582s
Feb 12 12:55:10.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018815634s
Feb 12 12:55:12.931: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014083059s
Feb 12 12:55:12.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017743199s
STEP: updating the pod 02/12/23 12:55:12.934
Feb 12 12:55:13.472: INFO: Successfully updated pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189"
STEP: waiting for pod running 02/12/23 12:55:13.472
Feb 12 12:55:13.472: INFO: Waiting up to 2m0s for pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" in namespace "var-expansion-6304" to be "running"
Feb 12 12:55:13.489: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 16.643308ms
Feb 12 12:55:15.504: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Running", Reason="", readiness=true. Elapsed: 2.032197474s
Feb 12 12:55:15.505: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" satisfied condition "running"
STEP: deleting the pod gracefully 02/12/23 12:55:15.505
Feb 12 12:55:15.505: INFO: Deleting pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" in namespace "var-expansion-6304"
Feb 12 12:55:15.527: INFO: Wait up to 5m0s for pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Feb 12 12:55:47.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6304" for this suite. 02/12/23 12:55:47.576
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":352,"skipped":6511,"failed":0}
------------------------------
 [SLOW TEST] [154.709 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:53:12.88
    Feb 12 12:53:12.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename var-expansion 02/12/23 12:53:12.881
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:53:12.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:53:12.904
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 02/12/23 12:53:12.906
    Feb 12 12:53:12.917: INFO: Waiting up to 2m0s for pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" in namespace "var-expansion-6304" to be "running"
    Feb 12 12:53:12.924: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 6.892191ms
    Feb 12 12:53:14.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016023602s
    Feb 12 12:53:16.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013491065s
    Feb 12 12:53:18.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019165045s
    Feb 12 12:53:20.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017263382s
    Feb 12 12:53:22.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013128614s
    Feb 12 12:53:24.929: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01233515s
    Feb 12 12:53:26.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020555318s
    Feb 12 12:53:28.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015731522s
    Feb 12 12:53:30.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01929718s
    Feb 12 12:53:32.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016200694s
    Feb 12 12:53:34.938: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020972026s
    Feb 12 12:53:36.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018493004s
    Feb 12 12:53:38.949: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 26.032277804s
    Feb 12 12:53:40.940: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 28.022888771s
    Feb 12 12:53:42.927: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010670224s
    Feb 12 12:53:44.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 32.010967689s
    Feb 12 12:53:46.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014900905s
    Feb 12 12:53:48.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016483392s
    Feb 12 12:53:50.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01540997s
    Feb 12 12:53:52.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 40.019620781s
    Feb 12 12:53:54.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019143949s
    Feb 12 12:53:56.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015693886s
    Feb 12 12:53:58.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 46.020745655s
    Feb 12 12:54:00.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016002537s
    Feb 12 12:54:02.931: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 50.014776536s
    Feb 12 12:54:04.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 52.010980862s
    Feb 12 12:54:06.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 54.019231168s
    Feb 12 12:54:08.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 56.017794461s
    Feb 12 12:54:10.938: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 58.021456859s
    Feb 12 12:54:12.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015534843s
    Feb 12 12:54:14.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011429518s
    Feb 12 12:54:16.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.019998012s
    Feb 12 12:54:18.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012859988s
    Feb 12 12:54:20.939: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.02212855s
    Feb 12 12:54:22.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.017316718s
    Feb 12 12:54:24.929: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012396108s
    Feb 12 12:54:26.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017246319s
    Feb 12 12:54:28.940: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.022977968s
    Feb 12 12:54:30.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.018087077s
    Feb 12 12:54:32.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017840578s
    Feb 12 12:54:34.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017384254s
    Feb 12 12:54:36.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014864476s
    Feb 12 12:54:38.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.017572964s
    Feb 12 12:54:40.940: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.02356866s
    Feb 12 12:54:42.929: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011891764s
    Feb 12 12:54:44.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016945509s
    Feb 12 12:54:46.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.01986963s
    Feb 12 12:54:48.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015097592s
    Feb 12 12:54:50.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.01930835s
    Feb 12 12:54:52.930: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013610531s
    Feb 12 12:54:54.928: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010846849s
    Feb 12 12:54:56.935: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.018553664s
    Feb 12 12:54:58.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019209905s
    Feb 12 12:55:00.932: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015633409s
    Feb 12 12:55:02.938: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.02145765s
    Feb 12 12:55:04.933: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.016103896s
    Feb 12 12:55:06.937: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.020393284s
    Feb 12 12:55:08.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019254582s
    Feb 12 12:55:10.936: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.018815634s
    Feb 12 12:55:12.931: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014083059s
    Feb 12 12:55:12.934: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.017743199s
    STEP: updating the pod 02/12/23 12:55:12.934
    Feb 12 12:55:13.472: INFO: Successfully updated pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189"
    STEP: waiting for pod running 02/12/23 12:55:13.472
    Feb 12 12:55:13.472: INFO: Waiting up to 2m0s for pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" in namespace "var-expansion-6304" to be "running"
    Feb 12 12:55:13.489: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Pending", Reason="", readiness=false. Elapsed: 16.643308ms
    Feb 12 12:55:15.504: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189": Phase="Running", Reason="", readiness=true. Elapsed: 2.032197474s
    Feb 12 12:55:15.505: INFO: Pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" satisfied condition "running"
    STEP: deleting the pod gracefully 02/12/23 12:55:15.505
    Feb 12 12:55:15.505: INFO: Deleting pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" in namespace "var-expansion-6304"
    Feb 12 12:55:15.527: INFO: Wait up to 5m0s for pod "var-expansion-51d25929-c1e4-4bbe-9df2-5234f28e1189" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Feb 12 12:55:47.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6304" for this suite. 02/12/23 12:55:47.576
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:55:47.59
Feb 12 12:55:47.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename services 02/12/23 12:55:47.592
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:55:47.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:55:47.615
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4009 02/12/23 12:55:47.617
STEP: changing the ExternalName service to type=NodePort 02/12/23 12:55:47.626
STEP: creating replication controller externalname-service in namespace services-4009 02/12/23 12:55:47.652
I0212 12:55:47.662737      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4009, replica count: 2
I0212 12:55:50.714351      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 12 12:55:50.714: INFO: Creating new exec pod
Feb 12 12:55:50.728: INFO: Waiting up to 5m0s for pod "execpodslbc5" in namespace "services-4009" to be "running"
Feb 12 12:55:50.733: INFO: Pod "execpodslbc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.56919ms
Feb 12 12:55:52.736: INFO: Pod "execpodslbc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007811081s
Feb 12 12:55:52.736: INFO: Pod "execpodslbc5" satisfied condition "running"
Feb 12 12:55:53.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 12 12:55:54.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 12 12:55:54.042: INFO: stdout: ""
Feb 12 12:55:55.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Feb 12 12:55:55.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 12 12:55:55.194: INFO: stdout: "externalname-service-s7xd2"
Feb 12 12:55:55.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.66 80'
Feb 12 12:55:55.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.66 80\nConnection to 10.233.24.66 80 port [tcp/http] succeeded!\n"
Feb 12 12:55:55.299: INFO: stdout: ""
Feb 12 12:55:56.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.66 80'
Feb 12 12:55:56.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.66 80\nConnection to 10.233.24.66 80 port [tcp/http] succeeded!\n"
Feb 12 12:55:56.604: INFO: stdout: "externalname-service-s7xd2"
Feb 12 12:55:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.102 32732'
Feb 12 12:55:56.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.102 32732\nConnection to 10.2.20.102 32732 port [tcp/*] succeeded!\n"
Feb 12 12:55:56.726: INFO: stdout: "externalname-service-sw8vf"
Feb 12 12:55:56.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32732'
Feb 12 12:55:56.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32732\nConnection to 10.2.20.101 32732 port [tcp/*] succeeded!\n"
Feb 12 12:55:56.857: INFO: stdout: ""
Feb 12 12:55:57.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32732'
Feb 12 12:55:57.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32732\nConnection to 10.2.20.101 32732 port [tcp/*] succeeded!\n"
Feb 12 12:55:57.978: INFO: stdout: "externalname-service-s7xd2"
Feb 12 12:55:57.978: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Feb 12 12:55:58.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4009" for this suite. 02/12/23 12:55:58.027
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":353,"skipped":6513,"failed":0}
------------------------------
 [SLOW TEST] [10.445 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:55:47.59
    Feb 12 12:55:47.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename services 02/12/23 12:55:47.592
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:55:47.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:55:47.615
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4009 02/12/23 12:55:47.617
    STEP: changing the ExternalName service to type=NodePort 02/12/23 12:55:47.626
    STEP: creating replication controller externalname-service in namespace services-4009 02/12/23 12:55:47.652
    I0212 12:55:47.662737      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4009, replica count: 2
    I0212 12:55:50.714351      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 12 12:55:50.714: INFO: Creating new exec pod
    Feb 12 12:55:50.728: INFO: Waiting up to 5m0s for pod "execpodslbc5" in namespace "services-4009" to be "running"
    Feb 12 12:55:50.733: INFO: Pod "execpodslbc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.56919ms
    Feb 12 12:55:52.736: INFO: Pod "execpodslbc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007811081s
    Feb 12 12:55:52.736: INFO: Pod "execpodslbc5" satisfied condition "running"
    Feb 12 12:55:53.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 12 12:55:54.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 12 12:55:54.042: INFO: stdout: ""
    Feb 12 12:55:55.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Feb 12 12:55:55.194: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 12 12:55:55.194: INFO: stdout: "externalname-service-s7xd2"
    Feb 12 12:55:55.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.66 80'
    Feb 12 12:55:55.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.66 80\nConnection to 10.233.24.66 80 port [tcp/http] succeeded!\n"
    Feb 12 12:55:55.299: INFO: stdout: ""
    Feb 12 12:55:56.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.24.66 80'
    Feb 12 12:55:56.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.24.66 80\nConnection to 10.233.24.66 80 port [tcp/http] succeeded!\n"
    Feb 12 12:55:56.604: INFO: stdout: "externalname-service-s7xd2"
    Feb 12 12:55:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.102 32732'
    Feb 12 12:55:56.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.102 32732\nConnection to 10.2.20.102 32732 port [tcp/*] succeeded!\n"
    Feb 12 12:55:56.726: INFO: stdout: "externalname-service-sw8vf"
    Feb 12 12:55:56.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32732'
    Feb 12 12:55:56.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32732\nConnection to 10.2.20.101 32732 port [tcp/*] succeeded!\n"
    Feb 12 12:55:56.857: INFO: stdout: ""
    Feb 12 12:55:57.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=services-4009 exec execpodslbc5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.2.20.101 32732'
    Feb 12 12:55:57.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.2.20.101 32732\nConnection to 10.2.20.101 32732 port [tcp/*] succeeded!\n"
    Feb 12 12:55:57.978: INFO: stdout: "externalname-service-s7xd2"
    Feb 12 12:55:57.978: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Feb 12 12:55:58.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4009" for this suite. 02/12/23 12:55:58.027
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:55:58.039
Feb 12 12:55:58.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename configmap 02/12/23 12:55:58.04
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:55:58.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:55:58.076
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-2425/configmap-test-b044583c-6142-4b0f-9176-cc11ffb1c8d7 02/12/23 12:55:58.081
STEP: Creating a pod to test consume configMaps 02/12/23 12:55:58.091
Feb 12 12:55:58.100: INFO: Waiting up to 5m0s for pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b" in namespace "configmap-2425" to be "Succeeded or Failed"
Feb 12 12:55:58.110: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.544284ms
Feb 12 12:56:00.113: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012057123s
Feb 12 12:56:02.123: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021712938s
STEP: Saw pod success 02/12/23 12:56:02.123
Feb 12 12:56:02.123: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b" satisfied condition "Succeeded or Failed"
Feb 12 12:56:02.134: INFO: Trying to get logs from node kube-3 pod pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b container env-test: <nil>
STEP: delete the pod 02/12/23 12:56:02.163
Feb 12 12:56:02.180: INFO: Waiting for pod pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b to disappear
Feb 12 12:56:02.183: INFO: Pod pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Feb 12 12:56:02.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2425" for this suite. 02/12/23 12:56:02.186
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":354,"skipped":6577,"failed":0}
------------------------------
 [4.152 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:55:58.039
    Feb 12 12:55:58.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename configmap 02/12/23 12:55:58.04
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:55:58.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:55:58.076
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-2425/configmap-test-b044583c-6142-4b0f-9176-cc11ffb1c8d7 02/12/23 12:55:58.081
    STEP: Creating a pod to test consume configMaps 02/12/23 12:55:58.091
    Feb 12 12:55:58.100: INFO: Waiting up to 5m0s for pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b" in namespace "configmap-2425" to be "Succeeded or Failed"
    Feb 12 12:55:58.110: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.544284ms
    Feb 12 12:56:00.113: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012057123s
    Feb 12 12:56:02.123: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021712938s
    STEP: Saw pod success 02/12/23 12:56:02.123
    Feb 12 12:56:02.123: INFO: Pod "pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b" satisfied condition "Succeeded or Failed"
    Feb 12 12:56:02.134: INFO: Trying to get logs from node kube-3 pod pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b container env-test: <nil>
    STEP: delete the pod 02/12/23 12:56:02.163
    Feb 12 12:56:02.180: INFO: Waiting for pod pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b to disappear
    Feb 12 12:56:02.183: INFO: Pod pod-configmaps-31c853ef-2883-4023-bdf5-4a7405c3925b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Feb 12 12:56:02.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2425" for this suite. 02/12/23 12:56:02.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:02.192
Feb 12 12:56:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:56:02.193
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:02.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:02.211
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 02/12/23 12:56:02.213
Feb 12 12:56:02.222: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7" in namespace "projected-7746" to be "Succeeded or Failed"
Feb 12 12:56:02.226: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492018ms
Feb 12 12:56:04.234: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011378529s
Feb 12 12:56:06.719: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.496808042s
Feb 12 12:56:08.232: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009288851s
Feb 12 12:56:10.251: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.028956322s
STEP: Saw pod success 02/12/23 12:56:10.252
Feb 12 12:56:10.252: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7" satisfied condition "Succeeded or Failed"
Feb 12 12:56:10.261: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7 container client-container: <nil>
STEP: delete the pod 02/12/23 12:56:10.277
Feb 12 12:56:10.321: INFO: Waiting for pod downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7 to disappear
Feb 12 12:56:10.325: INFO: Pod downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Feb 12 12:56:10.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7746" for this suite. 02/12/23 12:56:10.331
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":355,"skipped":6587,"failed":0}
------------------------------
 [SLOW TEST] [8.176 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:02.192
    Feb 12 12:56:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:56:02.193
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:02.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:02.211
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 02/12/23 12:56:02.213
    Feb 12 12:56:02.222: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7" in namespace "projected-7746" to be "Succeeded or Failed"
    Feb 12 12:56:02.226: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.492018ms
    Feb 12 12:56:04.234: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011378529s
    Feb 12 12:56:06.719: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.496808042s
    Feb 12 12:56:08.232: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009288851s
    Feb 12 12:56:10.251: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.028956322s
    STEP: Saw pod success 02/12/23 12:56:10.252
    Feb 12 12:56:10.252: INFO: Pod "downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7" satisfied condition "Succeeded or Failed"
    Feb 12 12:56:10.261: INFO: Trying to get logs from node kube-3 pod downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7 container client-container: <nil>
    STEP: delete the pod 02/12/23 12:56:10.277
    Feb 12 12:56:10.321: INFO: Waiting for pod downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7 to disappear
    Feb 12 12:56:10.325: INFO: Pod downwardapi-volume-f61dd204-700e-4dbf-9f6e-036089f2d0b7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Feb 12 12:56:10.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7746" for this suite. 02/12/23 12:56:10.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:10.37
Feb 12 12:56:10.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename dns 02/12/23 12:56:10.371
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:10.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:10.437
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/12/23 12:56:10.439
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/12/23 12:56:10.439
STEP: creating a pod to probe DNS 02/12/23 12:56:10.439
STEP: submitting the pod to kubernetes 02/12/23 12:56:10.439
Feb 12 12:56:10.478: INFO: Waiting up to 15m0s for pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60" in namespace "dns-4296" to be "running"
Feb 12 12:56:10.500: INFO: Pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60": Phase="Pending", Reason="", readiness=false. Elapsed: 21.484034ms
Feb 12 12:56:12.504: INFO: Pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60": Phase="Running", Reason="", readiness=true. Elapsed: 2.025964586s
Feb 12 12:56:12.504: INFO: Pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60" satisfied condition "running"
STEP: retrieving the pod 02/12/23 12:56:12.504
STEP: looking for the results for each expected name from probers 02/12/23 12:56:12.509
Feb 12 12:56:12.525: INFO: DNS probes using dns-4296/dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60 succeeded

STEP: deleting the pod 02/12/23 12:56:12.526
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Feb 12 12:56:12.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4296" for this suite. 02/12/23 12:56:12.547
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":356,"skipped":6594,"failed":0}
------------------------------
 [2.184 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:10.37
    Feb 12 12:56:10.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename dns 02/12/23 12:56:10.371
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:10.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:10.437
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/12/23 12:56:10.439
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/12/23 12:56:10.439
    STEP: creating a pod to probe DNS 02/12/23 12:56:10.439
    STEP: submitting the pod to kubernetes 02/12/23 12:56:10.439
    Feb 12 12:56:10.478: INFO: Waiting up to 15m0s for pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60" in namespace "dns-4296" to be "running"
    Feb 12 12:56:10.500: INFO: Pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60": Phase="Pending", Reason="", readiness=false. Elapsed: 21.484034ms
    Feb 12 12:56:12.504: INFO: Pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60": Phase="Running", Reason="", readiness=true. Elapsed: 2.025964586s
    Feb 12 12:56:12.504: INFO: Pod "dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60" satisfied condition "running"
    STEP: retrieving the pod 02/12/23 12:56:12.504
    STEP: looking for the results for each expected name from probers 02/12/23 12:56:12.509
    Feb 12 12:56:12.525: INFO: DNS probes using dns-4296/dns-test-dd5db9a9-10bb-4c88-90a5-3cd066806e60 succeeded

    STEP: deleting the pod 02/12/23 12:56:12.526
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Feb 12 12:56:12.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4296" for this suite. 02/12/23 12:56:12.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:12.561
Feb 12 12:56:12.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename resourcequota 02/12/23 12:56:12.561
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:12.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:12.587
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 02/12/23 12:56:12.59
STEP: Creating a ResourceQuota 02/12/23 12:56:17.593
STEP: Ensuring resource quota status is calculated 02/12/23 12:56:17.602
STEP: Creating a Service 02/12/23 12:56:19.61
STEP: Creating a NodePort Service 02/12/23 12:56:19.63
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/12/23 12:56:19.663
STEP: Ensuring resource quota status captures service creation 02/12/23 12:56:19.696
STEP: Deleting Services 02/12/23 12:56:21.708
STEP: Ensuring resource quota status released usage 02/12/23 12:56:21.783
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Feb 12 12:56:23.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5144" for this suite. 02/12/23 12:56:23.809
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":357,"skipped":6632,"failed":0}
------------------------------
 [SLOW TEST] [11.268 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:12.561
    Feb 12 12:56:12.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename resourcequota 02/12/23 12:56:12.561
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:12.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:12.587
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 02/12/23 12:56:12.59
    STEP: Creating a ResourceQuota 02/12/23 12:56:17.593
    STEP: Ensuring resource quota status is calculated 02/12/23 12:56:17.602
    STEP: Creating a Service 02/12/23 12:56:19.61
    STEP: Creating a NodePort Service 02/12/23 12:56:19.63
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/12/23 12:56:19.663
    STEP: Ensuring resource quota status captures service creation 02/12/23 12:56:19.696
    STEP: Deleting Services 02/12/23 12:56:21.708
    STEP: Ensuring resource quota status released usage 02/12/23 12:56:21.783
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Feb 12 12:56:23.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5144" for this suite. 02/12/23 12:56:23.809
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:23.831
Feb 12 12:56:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename deployment 02/12/23 12:56:23.833
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:23.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:23.868
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 12 12:56:23.871: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 12 12:56:23.885: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 12 12:56:28.896: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/12/23 12:56:28.896
Feb 12 12:56:28.896: INFO: Creating deployment "test-rolling-update-deployment"
Feb 12 12:56:28.908: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 12 12:56:28.920: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 12 12:56:30.939: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 12 12:56:30.945: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 12 12:56:30.963: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9244  3aa4ec22-eab2-4d4f-a764-20d8e7558425 42024 1 2023-02-12 12:56:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-12 12:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bb3f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-12 12:56:28 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-12 12:56:29 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 12 12:56:30.968: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9244  3b794e21-463c-454b-9787-abf305f8e8e9 42014 1 2023-02-12 12:56:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3aa4ec22-eab2-4d4f-a764-20d8e7558425 0xc00766e497 0xc00766e498}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa4ec22-eab2-4d4f-a764-20d8e7558425\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00766e548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:56:30.968: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 12 12:56:30.968: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9244  d446edf5-81a0-4e63-b35c-6901a8cc900e 42023 2 2023-02-12 12:56:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3aa4ec22-eab2-4d4f-a764-20d8e7558425 0xc00766e367 0xc00766e368}] [] [{e2e.test Update apps/v1 2023-02-12 12:56:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa4ec22-eab2-4d4f-a764-20d8e7558425\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00766e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 12 12:56:30.975: INFO: Pod "test-rolling-update-deployment-78f575d8ff-rb2p2" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-rb2p2 test-rolling-update-deployment-78f575d8ff- deployment-9244  bde86209-7543-4a51-a1ee-c5a9b6f4d15b 42013 0 2023-02-12 12:56:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:24698b9c0952070d82c455307bad1321af4456b630488134541db7fa17fe5be8 cni.projectcalico.org/podIP:10.233.99.80/32 cni.projectcalico.org/podIPs:10.233.99.80/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 3b794e21-463c-454b-9787-abf305f8e8e9 0xc00342e997 0xc00342e998}] [] [{kube-controller-manager Update v1 2023-02-12 12:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b794e21-463c-454b-9787-abf305f8e8e9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kllns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kllns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.80,StartTime:2023-02-12 12:56:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:56:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://4218bc2902c4e40dea67b26f0f33fd7d7addb742f5b6f8e0e356cc5ca92b4279,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Feb 12 12:56:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9244" for this suite. 02/12/23 12:56:30.981
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":358,"skipped":6632,"failed":0}
------------------------------
 [SLOW TEST] [7.158 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:23.831
    Feb 12 12:56:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename deployment 02/12/23 12:56:23.833
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:23.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:23.868
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 12 12:56:23.871: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 12 12:56:23.885: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 12 12:56:28.896: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/12/23 12:56:28.896
    Feb 12 12:56:28.896: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 12 12:56:28.908: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 12 12:56:28.920: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb 12 12:56:30.939: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 12 12:56:30.945: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 12 12:56:30.963: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9244  3aa4ec22-eab2-4d4f-a764-20d8e7558425 42024 1 2023-02-12 12:56:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-12 12:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bb3f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-12 12:56:28 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-02-12 12:56:29 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 12 12:56:30.968: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-9244  3b794e21-463c-454b-9787-abf305f8e8e9 42014 1 2023-02-12 12:56:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3aa4ec22-eab2-4d4f-a764-20d8e7558425 0xc00766e497 0xc00766e498}] [] [{kube-controller-manager Update apps/v1 2023-02-12 12:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa4ec22-eab2-4d4f-a764-20d8e7558425\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00766e548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:56:30.968: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 12 12:56:30.968: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9244  d446edf5-81a0-4e63-b35c-6901a8cc900e 42023 2 2023-02-12 12:56:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3aa4ec22-eab2-4d4f-a764-20d8e7558425 0xc00766e367 0xc00766e368}] [] [{e2e.test Update apps/v1 2023-02-12 12:56:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3aa4ec22-eab2-4d4f-a764-20d8e7558425\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00766e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 12 12:56:30.975: INFO: Pod "test-rolling-update-deployment-78f575d8ff-rb2p2" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-rb2p2 test-rolling-update-deployment-78f575d8ff- deployment-9244  bde86209-7543-4a51-a1ee-c5a9b6f4d15b 42013 0 2023-02-12 12:56:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:24698b9c0952070d82c455307bad1321af4456b630488134541db7fa17fe5be8 cni.projectcalico.org/podIP:10.233.99.80/32 cni.projectcalico.org/podIPs:10.233.99.80/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 3b794e21-463c-454b-9787-abf305f8e8e9 0xc00342e997 0xc00342e998}] [] [{kube-controller-manager Update v1 2023-02-12 12:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b794e21-463c-454b-9787-abf305f8e8e9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-02-12 12:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.99.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kllns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kllns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kube-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-12 12:56:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.2.20.103,PodIP:10.233.99.80,StartTime:2023-02-12 12:56:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-12 12:56:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://4218bc2902c4e40dea67b26f0f33fd7d7addb742f5b6f8e0e356cc5ca92b4279,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.99.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Feb 12 12:56:30.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9244" for this suite. 02/12/23 12:56:30.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:30.994
Feb 12 12:56:30.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename kubectl 02/12/23 12:56:30.998
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:31.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:31.018
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 02/12/23 12:56:31.019
Feb 12 12:56:31.020: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8049 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/12/23 12:56:31.058
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Feb 12 12:56:31.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8049" for this suite. 02/12/23 12:56:31.069
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":359,"skipped":6640,"failed":0}
------------------------------
 [0.082 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:30.994
    Feb 12 12:56:30.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename kubectl 02/12/23 12:56:30.998
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:31.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:31.018
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 02/12/23 12:56:31.019
    Feb 12 12:56:31.020: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3448062762 --namespace=kubectl-8049 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/12/23 12:56:31.058
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Feb 12 12:56:31.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8049" for this suite. 02/12/23 12:56:31.069
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:31.076
Feb 12 12:56:31.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename webhook 02/12/23 12:56:31.077
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:31.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:31.095
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 02/12/23 12:56:31.111
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:56:31.53
STEP: Deploying the webhook pod 02/12/23 12:56:31.539
STEP: Wait for the deployment to be ready 02/12/23 12:56:31.552
Feb 12 12:56:31.561: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/12/23 12:56:33.602
STEP: Verifying the service has paired with the endpoint 02/12/23 12:56:33.631
Feb 12 12:56:34.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/12/23 12:56:34.848
STEP: create a namespace for the webhook 02/12/23 12:56:35.15
STEP: create a configmap should be unconditionally rejected by the webhook 02/12/23 12:56:36.115
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Feb 12 12:56:36.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1463" for this suite. 02/12/23 12:56:36.645
STEP: Destroying namespace "webhook-1463-markers" for this suite. 02/12/23 12:56:36.662
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":360,"skipped":6641,"failed":0}
------------------------------
 [SLOW TEST] [6.001 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:31.076
    Feb 12 12:56:31.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename webhook 02/12/23 12:56:31.077
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:31.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:31.095
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 02/12/23 12:56:31.111
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/12/23 12:56:31.53
    STEP: Deploying the webhook pod 02/12/23 12:56:31.539
    STEP: Wait for the deployment to be ready 02/12/23 12:56:31.552
    Feb 12 12:56:31.561: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/12/23 12:56:33.602
    STEP: Verifying the service has paired with the endpoint 02/12/23 12:56:33.631
    Feb 12 12:56:34.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/12/23 12:56:34.848
    STEP: create a namespace for the webhook 02/12/23 12:56:35.15
    STEP: create a configmap should be unconditionally rejected by the webhook 02/12/23 12:56:36.115
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Feb 12 12:56:36.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1463" for this suite. 02/12/23 12:56:36.645
    STEP: Destroying namespace "webhook-1463-markers" for this suite. 02/12/23 12:56:36.662
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:56:37.078
Feb 12 12:56:37.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename sched-preemption 02/12/23 12:56:37.079
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:37.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:37.277
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Feb 12 12:56:37.634: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 12 12:57:37.701: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 02/12/23 12:57:37.705
Feb 12 12:57:37.731: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 12 12:57:37.742: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 12 12:57:37.771: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 12 12:57:37.780: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 12 12:57:37.817: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 12 12:57:37.838: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/12/23 12:57:37.838
Feb 12 12:57:37.839: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9445" to be "running"
Feb 12 12:57:37.847: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.380776ms
Feb 12 12:57:39.851: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012085395s
Feb 12 12:57:39.851: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 12 12:57:39.851: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
Feb 12 12:57:39.854: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.800739ms
Feb 12 12:57:39.854: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 12:57:39.854: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
Feb 12 12:57:39.856: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631661ms
Feb 12 12:57:41.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015056411s
Feb 12 12:57:41.869: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 12:57:41.869: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
Feb 12 12:57:41.882: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.847693ms
Feb 12 12:57:41.882: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 12:57:41.882: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
Feb 12 12:57:41.898: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.456691ms
Feb 12 12:57:41.899: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 12 12:57:41.899: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
Feb 12 12:57:41.910: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.062946ms
Feb 12 12:57:41.910: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/12/23 12:57:41.91
Feb 12 12:57:41.933: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 12 12:57:41.937: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078251ms
Feb 12 12:57:44.899: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965617707s
Feb 12 12:57:45.949: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015556149s
Feb 12 12:57:47.942: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008674724s
Feb 12 12:57:49.944: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.011115803s
Feb 12 12:57:49.944: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Feb 12 12:57:50.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9445" for this suite. 02/12/23 12:57:50.032
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":361,"skipped":6672,"failed":0}
------------------------------
 [SLOW TEST] [73.020 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:56:37.078
    Feb 12 12:56:37.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename sched-preemption 02/12/23 12:56:37.079
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:56:37.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:56:37.277
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Feb 12 12:56:37.634: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 12 12:57:37.701: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 02/12/23 12:57:37.705
    Feb 12 12:57:37.731: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 12 12:57:37.742: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 12 12:57:37.771: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 12 12:57:37.780: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 12 12:57:37.817: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 12 12:57:37.838: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/12/23 12:57:37.838
    Feb 12 12:57:37.839: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9445" to be "running"
    Feb 12 12:57:37.847: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.380776ms
    Feb 12 12:57:39.851: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012085395s
    Feb 12 12:57:39.851: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 12 12:57:39.851: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
    Feb 12 12:57:39.854: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.800739ms
    Feb 12 12:57:39.854: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 12:57:39.854: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
    Feb 12 12:57:39.856: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631661ms
    Feb 12 12:57:41.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015056411s
    Feb 12 12:57:41.869: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 12:57:41.869: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
    Feb 12 12:57:41.882: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.847693ms
    Feb 12 12:57:41.882: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 12:57:41.882: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
    Feb 12 12:57:41.898: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 16.456691ms
    Feb 12 12:57:41.899: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 12 12:57:41.899: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9445" to be "running"
    Feb 12 12:57:41.910: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.062946ms
    Feb 12 12:57:41.910: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/12/23 12:57:41.91
    Feb 12 12:57:41.933: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 12 12:57:41.937: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078251ms
    Feb 12 12:57:44.899: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.965617707s
    Feb 12 12:57:45.949: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015556149s
    Feb 12 12:57:47.942: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008674724s
    Feb 12 12:57:49.944: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.011115803s
    Feb 12 12:57:49.944: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Feb 12 12:57:50.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9445" for this suite. 02/12/23 12:57:50.032
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 02/12/23 12:57:50.099
Feb 12 12:57:50.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
STEP: Building a namespace api object, basename projected 02/12/23 12:57:50.1
STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:57:50.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:57:50.128
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-7ccf639e-dde5-49cd-8f58-0819938e2115 02/12/23 12:57:50.13
STEP: Creating secret with name secret-projected-all-test-volume-23aa349c-efc3-4c24-8a23-897b1d360a52 02/12/23 12:57:50.135
STEP: Creating a pod to test Check all projections for projected volume plugin 02/12/23 12:57:50.141
Feb 12 12:57:50.149: INFO: Waiting up to 5m0s for pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887" in namespace "projected-7690" to be "Succeeded or Failed"
Feb 12 12:57:50.152: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.792947ms
Feb 12 12:57:52.168: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018899446s
Feb 12 12:57:54.166: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016846634s
STEP: Saw pod success 02/12/23 12:57:54.166
Feb 12 12:57:54.166: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887" satisfied condition "Succeeded or Failed"
Feb 12 12:57:54.180: INFO: Trying to get logs from node kube-3 pod projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887 container projected-all-volume-test: <nil>
STEP: delete the pod 02/12/23 12:57:54.228
Feb 12 12:57:54.249: INFO: Waiting for pod projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887 to disappear
Feb 12 12:57:54.253: INFO: Pod projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Feb 12 12:57:54.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7690" for this suite. 02/12/23 12:57:54.257
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":362,"skipped":6692,"failed":0}
------------------------------
 [4.164 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 02/12/23 12:57:50.099
    Feb 12 12:57:50.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3448062762
    STEP: Building a namespace api object, basename projected 02/12/23 12:57:50.1
    STEP: Waiting for a default service account to be provisioned in namespace 02/12/23 12:57:50.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/12/23 12:57:50.128
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-7ccf639e-dde5-49cd-8f58-0819938e2115 02/12/23 12:57:50.13
    STEP: Creating secret with name secret-projected-all-test-volume-23aa349c-efc3-4c24-8a23-897b1d360a52 02/12/23 12:57:50.135
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/12/23 12:57:50.141
    Feb 12 12:57:50.149: INFO: Waiting up to 5m0s for pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887" in namespace "projected-7690" to be "Succeeded or Failed"
    Feb 12 12:57:50.152: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.792947ms
    Feb 12 12:57:52.168: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018899446s
    Feb 12 12:57:54.166: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016846634s
    STEP: Saw pod success 02/12/23 12:57:54.166
    Feb 12 12:57:54.166: INFO: Pod "projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887" satisfied condition "Succeeded or Failed"
    Feb 12 12:57:54.180: INFO: Trying to get logs from node kube-3 pod projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887 container projected-all-volume-test: <nil>
    STEP: delete the pod 02/12/23 12:57:54.228
    Feb 12 12:57:54.249: INFO: Waiting for pod projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887 to disappear
    Feb 12 12:57:54.253: INFO: Pod projected-volume-bf0f7cfa-3588-4c13-b848-fafca8ca3887 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Feb 12 12:57:54.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7690" for this suite. 02/12/23 12:57:54.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Feb 12 12:57:54.265: INFO: Running AfterSuite actions on all nodes
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Feb 12 12:57:54.265: INFO: Running AfterSuite actions on node 1
Feb 12 12:57:54.265: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 12 12:57:54.265: INFO: Running AfterSuite actions on all nodes
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Feb 12 12:57:54.265: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Feb 12 12:57:54.265: INFO: Running AfterSuite actions on node 1
    Feb 12 12:57:54.265: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.049 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6610.982 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h50m11.208071109s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

